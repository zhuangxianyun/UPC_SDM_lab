paperId,paperUrl,conferenceJournalId,conferenceJournalTitle,paperTitle,paperAbstract,paperType,proceedingsVolumeIds
pap0,133bcd7488a3c07cb0f493a87564c30e5433768c,jou0,Nature Biotechnology,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",,Letter,vol0
pap1,fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,con77,International Conference on Artificial Neural Networks,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",Erratum,pro77
pap2,c082ccfcfe1afc696e371374146ba9380b84061e,con27,International Conference on Contemporary Computing,The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications.",Erratum,pro27
pap3,ed6473fd5294a0639d661e02092768f364d80f39,con20,ACM Conference on Economics and Computation,What is Data Science?,"The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM https://cacm.acm.org/blogs/blog-cacm Koby Mike and Orit Hazzan consider why multiple definitions are needed to pin down data science.",Erratum,pro20
pap4,8a4fc5f00cd4aca61e148e46a2125c3a406719f1,con0,International Conference on Machine Learning,DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.",Conference paper,pro0
pap5,f38a7b19bcb2aa9246543e3f224a5f509906d43c,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,"Data Science: 8th International Conference of Pioneering Computer Scientists, Engineers and Educators, ICPCSEE 2022, Chengdu, China, August 19–22, 2022, Proceedings, Part II",,Erratum,pro85
pap6,fb29359d794265c0931d756858a70c9265b5693d,con107,Chinese Conference on Biometric Recognition,The R Language: An Engine for Bioinformatics and Data Science,"The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.",Erratum,pro107
pap7,8bb6a6802027c7f2489accb0559e6f02984535c9,jou1,Journal of Organizational and End User Computing,"Smart Health Intelligent Healthcare Systems in the Metaverse, Artificial Intelligence, and Data Science Era","In recent decades, healthcare organizations around the world have increasingly appreciated the value of information technologies for a variety of applications. Three of the new technological advancements that are impacting smart health are metaverse, artificial intelligence (AI), and data science. The metaverse is the intersection of three major technologies — AI, augmented reality (AR), and virtual reality (VR). Metaverse provides new possibilities and potential that are still emerging. The increased work efficiency enabled by artificial intelligence and data science in hospitals not only improves patient care but also cuts costs and workload for healthcare providers.The availability of big data enables data scientists to use the data for descriptive, predictive, and prescriptive analytics. This article reviews multiple case studies and the literature on AI and data science applications in hospital administration. The article also presents unresolved research questions and challenges in the applications of the metaverse, AI, and data science in the smart health context.",Article,vol1
pap8,70fe060c12b3f100148d1a2be1e8f4254022543e,con92,Human Language Technology - The Baltic Perspectiv,The case for data science in experimental chemistry: examples and recommendations,,Erratum,pro92
pap9,88ca84ce36ddc1bf7b6593b7f73fe2663e2365ad,con26,Decision Support Systems,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",Erratum,pro26
pap10,108acf9a358512a40191d857e2456aeaaac3303b,jou2,Genome Research,Diversifying the genomic data science research community,"Over the past 20 years, the explosion of genomic data collection and the cloud computing revolution have made computational and data science research accessible to anyone with a web browser and an internet connection. However, students at institutions with limited resources have received relatively little exposure to curricula or professional development opportunities that lead to careers in genomic data science. To broaden participation in genomics research, the scientific community needs to support these programs in local education and research at underserved institutions (UIs). These include community colleges, historically Black colleges and universities, Hispanic-serving institutions, and tribal colleges and universities that support ethnically, racially, and socioeconomically underrepresented students in the United States. We have formed the Genomic Data Science Community Network to support students, faculty, and their networks to identify opportunities and broaden access to genomic data science. These opportunities include expanding access to infrastructure and data, providing UI faculty development opportunities, strengthening collaborations among faculty, recognizing UI teaching and research excellence, fostering student awareness, developing modular and open-source resources, expanding course-based undergraduate research experiences (CUREs), building curriculum, supporting student professional development and research, and removing financial barriers through funding programs and collaborator support.",Letter,vol2
pap11,4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,jou3,IEEE Transactions on Knowledge and Data Engineering,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",Conference paper,vol3
pap12,7f29044de1a0e5a6d3ec1d33fb6ad482f3d10dd4,jou4,SN Computer Science,"Data Science and Analytics: An Overview from Data-Driven Smart Computing, Decision-Making and Applications Perspective",,Letter,vol4
pap13,370d248f97b75c4040e5828a658bbe4c3b80bf1e,jou5,Genome Biology,Eleven grand challenges in single-cell data science,,Letter,vol5
pap14,72d3ddf1f7210d7e70144bbc09f770ec411fe909,con9,Big Data,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",Erratum,pro9
pap15,ede0a8039a561905f40777ec2ae66c2010e3f2bc,jou6,Journal of Big Data,Cybersecurity data science: an overview from machine learning perspective,,Conference paper,vol6
pap16,b79ca6fd3df135a9bcf778844be625b764fbcfb3,jou7,Nature Methods,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,,Article,vol7
pap17,9bcf291c6245a3c2ee101babf4c1f0bbfa166f92,jou6,Journal of Big Data,Data science approach to stock prices forecasting in Indonesia during Covid-19 using Long Short-Term Memory (LSTM),,Conference paper,vol6
pap18,7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,con1,International Conference on Human Factors in Computing Systems,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",Article,pro1
pap19,44321686d59d889af1760357940f04fbb6629597,jou8,Irish Journal of Medical Science,"The role of data science in healthcare advancements: applications, benefits, and future prospects",,Article,vol8
pap20,8bba999de25bfb288b3f7f88e1d907aab02638b6,jou9,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",Article,vol9
pap21,2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,con4,Conference on Innovative Data Systems Research,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",Erratum,pro4
pap22,f9d403c58db99e2214f43e5b1740694b9c79002f,con2,International Conference on Software Engineering,"The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large","Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large, Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",Letter,pro2
pap23,0405bfdc3f0ecb8e9d31ae68911731e61a65c01d,jou10,Journal of Surgical Oncology,Surgical data science and artificial intelligence for surgical education,"Surgical data science (SDS) aims to improve the quality of interventional healthcare and its value through the capture, organization, analysis, and modeling of procedural data. As data capture has increased and artificial intelligence (AI) has advanced, SDS can help to unlock augmented and automated coaching, feedback, assessment, and decision support in surgery. We review major concepts in SDS and AI as applied to surgical education and surgical oncology.",Letter,vol10
pap24,3df3bacc84593e9efb86d2c4d3ce30463048159f,con78,Neural Information Processing Systems,Data Science,,Erratum,pro78
pap25,f9b0b10713044c146caa84704b66804aa1e82d5e,jou11,Communications of the ACM,Automating data science,"Given the complexity of data science projects and related demand for human expertise, automation has the potential to transform the data science process.",Article,vol11
pap26,c13147ef0b86d5ec833c272840f8f3bdacf96e7f,jou12,International Journal of Data Science and Analysis,Data science: a game changer for science and innovation,,Conference paper,vol12
pap27,459c91a1593808c3d6edf50eba621b048d08cda2,con18,International Conference on Exploring Services Science,Data science,,Erratum,pro18
pap28,2d420c7f1675d41b01e694739a25b8b189f2c95f,con77,International Conference on Artificial Neural Networks,Data Science in the Food Industry.,"Food safety is one of the main challenges of the agri-food industry that is expected to be addressed in the current environment of tremendous technological progress, where consumers' lifestyles and preferences are in a constant state of flux. Food chain transparency and trust are drivers for food integrity control and for improvements in efficiency and economic growth. Similarly, the circular economy has great potential to reduce wastage and improve the efficiency of operations in multi-stakeholder ecosystems. Throughout the food chain cycle, all food commodities are exposed to multiple hazards, resulting in a high likelihood of contamination. Such biological or chemical hazards may be naturally present at any stage of food production, whether accidentally introduced or fraudulently imposed, risking consumers' health and their faith in the food industry. Nowadays, a massive amount of data is generated, not only from the next generation of food safety monitoring systems and along the entire food chain (primary production included) but also from the Internet of things, media, and other devices. These data should be used for the benefit of society, and the scientific field of data science should be a vital player in helping to make this possible.",Erratum,pro77
pap29,ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,con34,International Conference on Agile Software Development,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",Erratum,pro34
pap30,0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,jou13,IEEE Transactions on Artificial Intelligence,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",Article,vol13
pap31,1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,con96,Interspeech,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",Erratum,pro96
pap32,f42c69dbd792155fee6f4d2c525971f8d43f138b,con41,Asia-Pacific Software Engineering Conference,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",Erratum,pro41
pap33,306627dfa6a7f595676e7d0ac74f6162fdcb37f1,jou14,Journal on spesial topics in mobile networks and applications,"Mobile Data Science and Intelligent Apps: Concepts, AI-Based Modeling and Research Directions",,Article,vol14
pap34,5b9ea2abf1c5a04b3024367409284edceb741ef2,con32,International Conference on Software Technology: Methods and Tools,A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",Erratum,pro32
pap35,7dcd9585d08eb40f043ae2ffccb86897a6a031e6,con67,IEEE International Software Metrics Symposium,Supervised and Unsupervised Learning for Data Science,,Erratum,pro67
pap36,12f62537251cf8eb76fa11c59df68d2211008898,jou15,International Journal of Digital Earth,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",Conference paper,vol15
pap37,a4b6f802b3f416fb1af6d723e0549c5e6d34faae,con76,IEEE International Conference on Tools with Artificial Intelligence,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",Erratum,pro76
pap38,648ba966b63975c6859e1948ae3ddc30053884e4,jou16,Big Data & Society,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",Conference paper,vol16
pap39,deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,jou17,IEEE Transactions on Signal Processing,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",Article,vol17
pap40,0669286d8d4ca8ec2fdf16b7813157c21eb690be,jou18,Scientific Data,Heidelberg colorectal data set for surgical data science in the sensor operating room,,Conference paper,vol18
pap41,8ece479b5dfed4727d2d9b9763f777bb9a94096e,con44,International Conference Knowledge Engineering and Knowledge Management,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",Erratum,pro44
pap42,82620503cacf8ff6f8f3490e7bdf7508f1ab2021,jou19,Journal of Geographical Systems,Opening practice: supporting reproducibility and critical spatial data science,,Letter,vol19
pap43,398b154013db9d8025bf60f910bc156dedd9b40e,con1,International Conference on Human Factors in Computing Systems,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",Letter,pro1
pap44,a11e157cb828b800426223f0a3d79e8fb122c8cc,con83,Networks,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",Erratum,pro83
pap45,e57f360d4ffd1d3aa1dfbcc92d35b506f46f3afd,jou12,International Journal of Data Science and Analysis,Data science and AI in FinTech: an overview,,Article,vol12
pap46,3b16bcb226bb1c87a6e63e0658be30067ed03f57,con108,International Conference on Information Integration and Web-based Applications & Services,A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,,Erratum,pro108
pap47,68bee44cc58b1853c7ddcb41aa3c6d29f363637a,con3,Knowledge Discovery and Data Mining,Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.",Article,pro3
pap48,840b60da93c2776230d3e6123d708e1c7e66ebc0,jou20,Journal of Statistics and Data Science Education,Teaching Creative and Practical Data Science at Scale,"Abstract–Nolan and Temple Lang’s Computing in the Statistics Curricula (2010) advocated for a shift in statistical education to broadly include computing. In the time since, individuals with training in both computing and statistics have become increasingly employable in the burgeoning data science field. In response, universities have developed new courses and programs to meet the growing demand for data science education. To address this demand, we created Data Science in Practice, a large-enrollment undergraduate course. Here, we present our goals for teaching this course, including: (1) conceptualizing data science as creative problem solving, with a focus on project-based learning, (2) prioritizing practical application, teaching and using standardized tools and best practices, and (3) scaling education through coursework that enables hands-on and classroom learning in a large-enrollment course. Throughout this course we also emphasize social context and data ethics to best prepare students for the interdisciplinary and impactful nature of their work. We highlight creative problem solving and strategies for teaching automation-resilient skills, while providing students the opportunity to create a unique data science project that demonstrates their technical and creative capacities.",Letter,vol20
pap49,41cf91ee13a1d15983ede066ddf6b67cc94a41f4,con6,Annual Conference on Genetic and Evolutionary Computation,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.Keywords: applied statistics, data science, data science curriculum, data wrangling, machine learning, software engineering",Erratum,pro6
pap50,d88d39dd9c910105e7503aa43698c806d42d5198,con86,The Web Conference,Statistical Foundations of Data Science,,Erratum,pro86
pap51,c016852106ac787678105fd9dd22e57ba620517c,jou21,Patterns,Human Data Science,,Letter,vol21
pap52,579b64d2179a58a8bc586c30850ea238d3c14164,jou3,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",Article,vol3
pap53,d9a984e15b1a86a66ecbac9e66d458dae4cb616c,con97,ACM SIGMOD Conference,What Is Data Science,,Erratum,pro97
pap54,27b9d1182e913decc7ef6a3509245fa6b6fd509d,jou22,Proceedings of the National Academy of Sciences of the United States of America,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",Letter,vol22
pap55,62a9d4f1763c5071cb2476c100614ba9741f036b,con69,Formal Concept Analysis,Data science applications to string theory,,Erratum,pro69
pap56,e2055b85dab66c922ccf25a28046e8e559074824,jou23,Computer/law journal,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",Letter,vol23
pap57,b017bf6879e57077b4b4e180a02747b89878d7a1,jou20,Journal of Statistics and Data Science Education,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",Article,vol20
pap58,4b5505a54799d796ae94115409b01ee33a7e2b20,jou24,Journal of Epidemiology and Community Health,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",Letter,vol24
pap59,28cc044d5ba938472bc53d87240583982ad21663,con4,Conference on Innovative Data Systems Research,Data Management for Data Science - Towards Embedded Analytics,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",Conference paper,pro4
pap60,8e234be4cdc34ea8deba609c31858198ad941797,con17,International Conference on Statistical and Scientific Database Management,Passing the Data Baton: A Retrospective Analysis on Data Science Work and Workers,"Data science is a rapidly growing discipline and organizations increasingly depend on data science work. Yet the ambiguity around data science, what it is, and who data scientists are can make it difficult for visualization researchers to identify impactful research trajectories. We have conducted a retrospective analysis of data science work and workers as described within the data visualization, human computer interaction, and data science literature. From this analysis we synthesis a comprehensive model that describes data science work and breakdown to data scientists into nine distinct roles. We summarise and reflect on the role that visualization has throughout data science work and the varied needs of data scientists themselves for tooling support. Our findings are intended to arm visualization researchers with a more concrete framing of data science with the hope that it will help them surface innovative opportunities for impacting data science work.",Erratum,pro17
pap61,38f0c0f2567e074c775017e0e8dd1a43b1f6fcdd,jou20,Journal of Statistics and Data Science Education,"Data Science in 2020: Computing, Curricula, and Challenges for the Next 10 Years","Abstract In the past 10 years, new data science courses and programs have proliferated at the collegiate level. As faculty and administrators enter the race to provide data science training and attract new students, the road map for teaching data science remains elusive. In 2019, 69 college and university faculty teaching data science courses and developing data science curricula were surveyed to learn about their curricula, computing tools, and challenges they face in their classrooms. Faculty reported teaching a variety of computing skills in introductory data science (albeit fewer computing topics than statistics topics), and that one of the biggest challenges they face is teaching computing to a diverse audience with varying preparation. The ever-evolving nature of data science is a major hurdle for faculty teaching data science courses, and a call for more data science teaching resources was echoed in many responses.",Article,vol20
pap62,3746152e023e79b7d03cf12a560e473de2945d67,con97,ACM SIGMOD Conference,Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",Erratum,pro97
pap63,89f41c87c8849ce37e609c1010087291a4679a37,jou25,Philosophical Transactions of the Royal Society of London. Biological Sciences,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",Letter,vol25
pap64,4271faaa82eb722d079222211c30ab642bc734be,jou11,Communications of the ACM,The data science life cycle,A cycle that traces ways to define the landscape of data science.,Letter,vol11
pap65,9f2b2111cd65cc33c0c440f4f8e548b58d8dd851,jou26,Engineering,The State of the Art of Data Science and Engineering in Structural Health Monitoring,,Conference paper,vol26
pap66,f56425ec56586dcfd2694ab83643e9e76f314e91,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",Erratum,pro73
pap67,edca4813d7da7507ff5a8a89e6c0481f8967ac84,con26,Decision Support Systems,Data Science Challenges & Enhanced Data Science Methodology,,Erratum,pro26
pap68,1ec4d0e29455e47245edaa17368257df3efb6562,con1,International Conference on Human Factors in Computing Systems,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",Letter,pro1
pap69,2ad13329d44c74041626a60898ccf921b0bdacd3,con4,Conference on Innovative Data Systems Research,SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle,"Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.",Article,pro4
pap70,678da221aa156807bc2c191ed5f4bcbb0b25d421,jou27,Ethics and Information Technology,Data science ethical considerations: a systematic literature review and proposed project framework,,Conference paper,vol27
pap71,1ac524c713423bc50822b34e0aa1bbfab42d2b00,jou28,Small Business Economics,Data science for entrepreneurship research: studying demand dynamics for entrepreneurial skills in the Netherlands,,Conference paper,vol28
pap72,5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,jou29,Frontiers in Environmental Science,Data Science of the Natural Environment: A Research Roadmap,"Data science is the science of extracting meaning from potentially complex data. This is a fast moving field, drawing principles and techniques from a number of different disciplinary areas including computer science, statistics and complexity science. Data science is having a profound impact on a number of areas including commerce, health and smart cities. This paper argues that data science can have an equal if not greater impact in the area of earth and environmental sciences, offering a rich tapestry of new techniques to support both a deeper understanding of the natural environment in all its complexities, as well as the development of well-founded mitigation and adaptation strategies in the face of climate change. The paper argues that data science for the natural environment brings about new challenges for data science, particularly around complexity, spatial and temporal reasoning, and managing uncertainty. The paper also describes a case study in environmental data science which offers up insights into the promise of the area. The paper concludes with a research roadmap highlighting ten top challenges of environmental data science and also an invitation to become part of an international community working collaboratively on these problems.",Article,vol29
pap73,0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,jou30,Metabolomics,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,,Letter,vol30
pap74,2081ed6854290a479f796f2432c7951ff24232fe,con50,International Workshop on Green and Sustainable Software,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",Erratum,pro50
pap75,0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,jou31,"Annual review of political science (Palo Alto, Calif. Print)",The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",Conference paper,vol31
pap76,e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,jou32,Journal of Library and Information Sciences,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",Article,vol32
pap77,b00f836c62d0ea7678d0f20aeec3397138633060,jou33,Journal of Medical Internet Research,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",Conference paper,vol33
pap78,2ab2796390ac12df283e218907ed0ffef232dbc7,jou34,The Journal of the Learning Sciences,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",Conference paper,vol34
pap79,e799d31e1c2d80a971c1f956d62b98c0a9f27031,jou35,British Journal of Educational Technology,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",Article,vol35
pap80,b134d892f4e76081f5fa36b0b7c2e7118be53907,jou5,Genome Biology,Genomics and data science: an application within an umbrella,,Conference paper,vol5
pap81,eaa3bbe9e3c52781fd84149d8ee6e2670c90e5ec,con32,International Conference on Software Technology: Methods and Tools,Bayesian Optimization and Data Science,,Erratum,pro32
pap82,863a35bdd1ae803491801e283c2ae79fe973cf68,jou36,Journal of Biosciences,Microbiome data science,,Letter,vol36
pap83,702cd9a7a128706b8a6ec88e7424e06c326021e5,con62,Australian Software Engineering Conference,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",Erratum,pro62
pap84,747359803e9a734fa4f1338a83121a942f3da60e,jou37,Geographical Analysis,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",Conference paper,vol37
pap85,9d76e69f54bdf739fcd61d0fd25f92c7dd3923c2,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Data Science for Wind Energy,,Erratum,pro54
pap86,590ead4aeddbf8fea8414998b2dc3b74576a71cb,jou38,CHANCE : New Directions for Statistics and Computing,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",Letter,vol38
pap87,f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd,jou39,BMC Medicine,From hype to reality: data science enabling personalized medicine,,Conference paper,vol39
pap88,8d446e7af03d7c7f9fe5828b2d9939e23a3ed7b0,jou37,Geographical Analysis,A Data Science Framework for Movement,"Author(s): Dodge, S | Abstract: © 2019 The Ohio State University Movement is the driving force behind the form and function of many ecological and human systems. Identification and analysis of movement patterns that may relate to the behavior of individuals and their interactions is a fundamental first step in understanding these systems. With advances in IoT and the ubiquity of smart connected sensors to collect movement and contextual data, we now have access to a wealth of geo-enriched high-resolution tracking data. These data promise new forms of knowledge and insight into movement of humans, animals, and goods, and hence can increase our understanding of complex spatiotemporal processes such as disease outbreak, urban mobility, migration, and human-species interaction. To take advantage of the evolution in our data, we need a revolution in how we visualize, model, and analyze movement as a multidimensional process that involves space, time, and context. This paper introduces a data science paradigm with the aim of advancing research on movement.",Letter,vol37
pap89,bb44d1472bb281c699ef556f6eb6ccc66889f2d3,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Data Science and Machine Learning,"The purpose of Data Science and Machine Learning: Mathematical and Statistical Methods is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.",Erratum,pro73
pap90,bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,con50,International Workshop on Green and Sustainable Software,Knowledge-based Biomedical Data Science 2019,"Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.",Erratum,pro50
pap91,daec8baf1740a09725b375729d95caebc42f61c8,con5,Technical Symposium on Computer Science Education,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",Conference paper,pro5
pap92,b85ac20631159ca3e370afa9c1f81a4618242b4f,jou40,American Statistician,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",Letter,vol40
pap93,46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,con86,The Web Conference,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",Erratum,pro86
pap94,4f0218eb9ed62d5acc03f02bfa24b388a66067e8,jou41,TOP - An Official Journal of the Spanish Society of Statistics and Operations Research,Distance geometry and data science,,Conference paper,vol41
pap95,36708c11c2fde2efb50e75d81f174b2c205082c8,jou16,Big Data & Society,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",Letter,vol16
pap96,4aeda303fa0b9beae3f6d65e052dace9d4540116,jou42,Journal of Library Administration,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",Conference paper,vol42
pap97,08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,jou43,Social Science Research Network,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",Letter,vol43
pap98,fb566f2001e44a65433fb7cc2eb7bcf6513a7db8,con56,International Conference on Software Engineering and Knowledge Engineering,The 9 Pitfalls of Data Science,"Scientific rigor and critical thinking skills are indispensable in this age of big data because machine learning and artificial intelligence are often led astray by meaningless patterns. The 9 Pitfalls of Data Science is loaded with entertaining real-world examples of both successful and misguided approaches to interpreting data, both grand successes and epic failures. Anyone can learn to distinguish between good data science and nonsense. We are confident that readers will learn how to avoid being duped by data, and make better, more informed decisions. Whether they want to be effective creators, interpreters, or users of data, they need to know the nine pitfalls of data science.",Erratum,pro56
pap99,a9e447d4d6b91f75ac4d8e336c609cfcbcbcfc0a,con45,International Conference on Global Software Engineering,Data Science,"The Bachelor of Science in Data Science studies the collection, manipulation, storage, retrieval, and computational analysis of data in its various forms, including numeric, textual, image, and video data from small to large volumes. The program combines computer science, information science, mathematics, statistics, and probability theory into an integrated curriculum that prepares students for careers or graduate studies in big data analysis, data science, and data analytics. The coursework covers exploratory data analysis, data manipulation in a variety of programming languages, large-scale data storage, predictive analytics, machine learning, data mining, and information visualization and presentation. Data science has emerged as a discipline due to the confluence of two major events:",Erratum,pro45
pap100,e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,jou3,IEEE Transactions on Knowledge and Data Engineering,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",Letter,vol3
pap101,ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,jou44,Journal of Social Computing,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",Letter,vol44
pap102,bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,jou11,Communications of the ACM,Realizing the potential of data science,"Data science promises new insights, helping transform information into knowledge that can drive science and industry.",Article,vol11
pap103,6bec0106bebc93fc30ec47af9779d7e327639034,con53,Workshop on Web 2.0 for Software Engineering,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",Erratum,pro53
pap104,140a6476f7b8dde9e7bbcd199d248fc629721faa,con41,Asia-Pacific Software Engineering Conference,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",Erratum,pro41
pap105,3335c340c20609b4e6de481c9eaf67ecd6c960dc,con6,Annual Conference on Genetic and Evolutionary Computation,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",Conference paper,pro6
pap106,c0b1eedfa2031a69fbdf02a4abc8a741faf6a912,con108,International Conference on Information Integration and Web-based Applications & Services,"Introduction to Data Science - A Python Approach to Concepts, Techniques and Applications",,Erratum,pro108
pap107,0a4b3c33e830d8cde364443a52e673c2c07dcfe8,con7,International Symposium on Intelligent Data Analysis,Open Data Science,,Article,pro7
pap108,305600f3cba8a63bad1bedeab34a299bf748754b,jou45,Proceedings of the VLDB Endowment,Northstar: An Interactive Data Science System,"In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the ""guts."" Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the ""protection"" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.",Letter,vol45
pap109,577564ac25a12b37972d77a35b589f6b2270a45f,jou46,Chest,Big Data and Data Science in Critical Care.,,Letter,vol46
pap110,f968cdd4637e7b26ca6c057a2f7f593b8cea2d18,con71,Annual Conference on Innovation and Technology in Computer Science Education,Fundamentals of Clinical Data Science,,Erratum,pro71
pap111,6bf9d589f80823735084956f056728ae1a7bcfa8,jou47,BioScience,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",Article,vol47
pap112,a2d7efb8b174702111e713765cbf741dff2bf9b8,con106,International Conference on Mobile Data Management,Searching for Hidden Perovskite Materials for Photovoltaic Systems by Combining Data Science and First Principle Calculations,"Undiscovered perovskite materials for applications in capturing solar lights are explored through the implementation of data science. In particular, 15000 perovskite materials data is analyzed where visualization of the data reveals hidden trends and clustering of data. Random forest classification within machine learning is used in order to predict the band gap of perovskite materials where 18 physical descriptors are revealed to determine the band gap. With trained random forest, 9328 perovskite materials with potential for applications in solar cell materials are predicted. The selected Li and Na based perovskite materials within predicted 9328 perovskite materials are evaluated with first principle calculations where 11 undiscovered Li(Na) based perovskite materials fall into the ideal band gap and formation energy ranges for solar cell applications. Thus, the implementation of data science accelerates the discovery of hidden perovskite materials and the approach can be applied to the materials scienc...",Erratum,pro106
pap113,ff6586ab32e9ed45d20a486ec7c5be02da5d3f1f,jou12,International Journal of Data Science and Analysis,Data Science: the impact of statistics,,Conference paper,vol12
pap114,2a85f034ae7a6119ae6b718c8f73a58dc1fbd7b4,con29,ACM-SIAM Symposium on Discrete Algorithms,Curriculum Guidelines for Undergraduate Programs in Data Science,"The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in Data Science. The group consisted of 25 undergraduate faculty from a variety of institutions in the U.S., primarily from the disciplines of mathematics, statistics and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in Data Science.",Erratum,pro29
pap115,2146edb37621d80f53c1261c8a53c94d3dda84c8,con8,Frontiers in Education Conference,Smart Blockchain Badges for Data Science Education,"Blockchain technology has the potential to revolutionise education in a number of ways. In this paper, we explore the applications of Smart Blockchain Badges on data science education. In particular, we investigate how Smart Blockchain Badges can support learners that want to advance their careers in data science, by offering them personalised recommendations based on their learning achievements. This work aims at enhancing data science accreditation by introducing a robust system based on the Blockchain technology. Learners will benefit from a sophisticated, open and transparent accreditation system, as well as from receiving job recommendations that match their skills and can potentially progress their careers. As a result, this work contributes towards closing the data science skills gap by linking data science education to the industry.",Conference paper,pro8
pap116,e4c66275e46a66586365c851f0974a3c88baf3d7,con40,Conference on Software Engineering Education and Training,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",Erratum,pro40
pap117,0ec2d4c804dd2b4446e1808dc85b4fe4a27b1766,jou48,Nature Biomedical Engineering,Surgical data science for next-generation interventions,,Conference paper,vol48
pap118,79be83a308a9a75ef4e64f63a938b201531c0bbf,jou49,ACM Computing Surveys,Data Science,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",Letter,vol49
pap119,c0225f99c9b1619c3be74b63241faffe02d275d7,jou22,Proceedings of the National Academy of Sciences of the United States of America,Science and data science,"Data science has attracted a lot of attention, promising to turn vast amounts of data into useful predictions and insights. In this article, we ask why scientists should care about data science. To answer, we discuss data science from three perspectives: statistical, computational, and human. Although each of the three is a critical component of data science, we argue that the effective combination of all three components is the essence of what data science is about.",Article,vol22
pap120,89535aa63bc5dac6f3beb60b813abb77aa4309d1,con9,Big Data,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",Article,pro9
pap121,a1dbdc2ce338d694a720163f591e4eb5c4070140,con68,Experimental Software Engineering Network,Deep Learning in Biomedical Data Science,"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. The breadth, complexity, and rapidly expanding size of biomedical data have stimulated the development of novel deep learning methods, and application of these methods to biomedical data have led to scientific discoveries and practical solutions. This overview provides technical and historical pointers to the field, and surveys current applications of deep learning to biomedical data organized around five subareas, roughly of increasing spatial scale: chemoinformatics, proteomics, genomics and transcriptomics, biomedical imaging, and health care. The black box problem of deep learning methods is also briefly discussed.",Erratum,pro68
pap122,5a44f70130875b212452ad777ab02a4eb5cd35d9,jou50,International Journal of Population Data Science,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",Article,vol50
pap123,b154d9ce0a551be90557d7a24a49b1988add2a81,con50,International Workshop on Green and Sustainable Software,"Three principles of data science: predictability, computability, and stability (PCS)","In this talk, I'd like to discuss the intertwining importance and connections of three principles of data science in the title and the PCS workflow that is built on the three principles. The principles will be demonstrated in the context of two collaborative projects in neuroscience and genomics for interpretable data results and testable hypothesis generation.",Erratum,pro50
pap124,fde0b586e3bc9e5139a14493044bce9ff61706d4,con5,Technical Symposium on Computer Science Education,Inverse statistical problems: from the inverse Ising problem to data science,"Inverse problems in statistical physics are motivated by the challenges of ‘big data’ in different fields, in particular high-throughput experiments in biology. In inverse problems, the usual procedure of statistical physics needs to be reversed: Instead of calculating observables on the basis of model parameters, we seek to infer parameters of a model based on observations. In this review, we focus on the inverse Ising problem and closely related problems, namely how to infer the coupling strengths between spins given observed spin correlations, magnetizations, or other data. We review applications of the inverse Ising problem, including the reconstruction of neural connections, protein structure determination, and the inference of gene regulatory networks. For the inverse Ising problem in equilibrium, a number of controlled and uncontrolled approximate solutions have been developed in the statistical mechanics community. A particularly strong method, pseudolikelihood, stems from statistics. We also review the inverse Ising problem in the non-equilibrium case, where the model parameters must be reconstructed based on non-equilibrium statistics.",Erratum,pro5
pap125,3c51a892ce5a8fc78d57ea290c6e5144ee9db579,con5,Technical Symposium on Computer Science Education,Key Concepts for a Data Science Ethics Curriculum,"Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.",Article,pro5
pap126,afe79672aa99b7f606cbff234ec2454cf2295554,jou51,Ethnicity & Disease,Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century.,"Addressing minority health and health disparities has been a missing piece of the puzzle in Big Data science. This article focuses on three priority opportunities that Big Data science may offer to the reduction of health and health care disparities. One opportunity is to incorporate standardized information on demographic and social determinants in electronic health records in order to target ways to improve quality of care for the most disadvantaged populations over time. A second opportunity is to enhance public health surveillance by linking geographical variables and social determinants of health for geographically defined populations to clinical data and health outcomes. Third and most importantly, Big Data science may lead to a better understanding of the etiology of health disparities and understanding of minority health in order to guide intervention development. However, the promise of Big Data needs to be considered in light of significant challenges that threaten to widen health disparities. Care must be taken to incorporate diverse populations to realize the potential benefits. Specific recommendations include investing in data collection on small sample populations, building a diverse workforce pipeline for data science, actively seeking to reduce digital divides, developing novel ways to assure digital data privacy for small populations, and promoting widespread data sharing to benefit under-resourced minority-serving institutions and minority researchers. With deliberate efforts, Big Data presents a dramatic opportunity for reducing health disparities but without active engagement, it risks further widening them.",Letter,vol51
pap127,cdd5d0a3e2ba0e1f4b5bcd3115e5f5b6536e24f9,con2,International Conference on Software Engineering,The Data Science Design Manual,,Erratum,pro2
pap128,f4e66bd035e195f539f1b65a5aaec0e873cdee29,jou52,Computer Applications in Engineering Education,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,Conference paper,vol52
pap129,b0fbdffb9733e7857afbb21ccbcd9cd74803ca1d,jou53,Data Science,"Data Science and symbolic AI: Synergies, challenges and opportunities","Symbolic approaches to Artificial Intelligence (AI) represent things within a domain of knowledge through physical symbols, combine symbols into symbol expressions, and manipulate symbols and symbol expressions through inference processes. While a large part of Data Science relies on statistics and applies statistical approaches to AI, there is an increasing potential for successfully applying symbolic approaches as well. Symbolic representations and symbolic inference are close to human cognitive representations and therefore comprehensible and interpretable; they are widely used to represent data and metadata, and their specific semantic content must be taken into account for analysis of such information; and human communication largely relies on symbols, making symbolic representations a crucial part in the analysis of natural language. Here we discuss the role symbolic representations and inference can play in Data Science, highlight the research challenges from the perspective of the data scientist, and argue that symbolic methods should become a crucial component of the data scientists’ toolbox.",Letter,vol53
pap130,c9ed1ad1a3a08bf5ebebe8105805dd102546b8f3,con96,Interspeech,Process-Structure Linkages Using a Data Science Approach: Application to Simulated Additive Manufacturing Data,,Erratum,pro96
pap131,b34b9758b36c92c023c3c10f3a39aeb8f5c83927,con10,Americas Conference on Information Systems,Exploring Project Management Methodologies Used Within Data Science Teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams’ outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework.",Letter,pro10
pap132,d2f83aa22def149095f1dd89b4cf36d09a748a87,con88,European Conference on Computer Vision,Data science is science's second chance to get causal inference right: A classification of data science tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term""data science""provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",Erratum,pro88
pap133,88761dffd173cd0e75e88c02d68f866f8cc43c14,jou53,Data Science,Knowledge-based biomedical Data Science,"Computational manipulation of knowledge is an important, and often under-appreciated, aspect of biomedical Data Science. The first Data Science initiative from the US National Institutes of Health was entitled “Big Data to Knowledge (BD2K).” The main emphasis of the more than $200M allocated to that program has been on “Big Data;” the “Knowledge” component has largely been the implicit assumption that the work will lead to new biomedical knowledge. However, there is long-standing and highly productive work in computational knowledge representation and reasoning, and computational processing of knowledge has a role in the world of Data Science. Knowledge-based biomedical Data Science involves the design and implementation of computer systems that act as if they knew about biomedicine. There are many ways in which a computational approach might act as if it knew something: for example, it might be able to answer a natural language question about a biomedical topic, or pass an exam; it might be able to use existing biomedical knowledge to rank or evaluate hypotheses; it might explain or interpret data in light of prior knowledge, either in a Bayesian or other sort of framework. These are all examples of automated reasoning that act on computational representations of knowledge. After a brief survey of existing approaches to knowledge-based data science, this position paper argues that such research is ripe for expansion, and expanded application.",Article,vol53
pap134,5de20ffb7852ae0665c382084c8a56918f23dc0b,con11,European Conference on Modelling and Simulation,Drafting a Data Science Curriculum for Secondary Schools,"Data science as the art of generating information and knowledge from data is increasingly becoming an important part of most operational processes. But up to now, data science is hardly an issue in German computer science education at secondary schools. For this reason, we are developing a data science curriculum for German secondary schools, which first guidelines and ideas we present in this paper. The curriculum is designed as interdisciplinary approach between maths and computer science education, with also a strong focus on societal aspects. After a brief discussion of important concepts and challenges in data science, a first draft of the curriculum and an outline of a data science course for upper secondary schools accompanying the development are presented.",Conference paper,pro11
pap135,dd1f93c3faae464d50d2e97c2bf4ac8d43681cb1,con62,Australian Software Engineering Conference,Twinning data science with information science in schools of library and information science,"As an emerging discipline, data science represents a vital new current of school of library and information science (LIS) education. However, it remains unclear how it relates to information science within LIS schools. The purpose of this paper is to clarify this issue.,Mission statement and nature of both data science and information science are analyzed by reviewing existing work in the two disciplines and drawing DIKW hierarchy. It looks at the ways in which information science theories bring new insights and shed new light on fundamentals of data science.,Data science and information science are twin disciplines by nature. The mission, task and nature of data science are consistent with those of information science. They greatly overlap and share similar concerns. Furthermore, they can complement each other. LIS school should integrate both sciences and develop organizational ambidexterity. Information science can make unique contributions to data science research, including conception of data, data quality control, data librarianship and theory dualism. Document theory, as a promising direction of unified information science, should be introduced to data science to solve the disciplinary divide.,The results of this paper may contribute to the integration of data science and information science within LIS schools and iSchools. It has particular value for LIS school development and reform in the age of big data.",Erratum,pro62
pap136,798e5e09c20b0270701b194a3198427fec6a4fcd,jou12,International Journal of Data Science and Analysis,What makes Data Science different? A discussion involving Statistics2.0 and Computational Sciences,,Letter,vol12
pap137,e36022198f21f46d066007ee5cf901ea55080e21,con59,Annual Workshop of the Psychology of Programming Interest Group,"Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications","This accessible and classroom-tested textbook/reference presents an introduction to the fundamentals of the emerging and interdisciplinary field of data science. The coverage spans key concepts adopted from statistics and machine learning, useful techniques for graph analysis and parallel programming, and the practical application of data science for such tasks as building recommender systems or performing sentiment analysis. Topics and features: provides numerous practical case studies using real-world data throughout the book; supports understanding through hands-on experience of solving data science problems using Python; describes techniques and tools for statistical analysis, machine learning, graph analysis, and parallel programming; reviews a range of applications of data science, including recommender systems and sentiment analysis of text data; provides supplementary code resources and data at an associated website.",Erratum,pro59
pap138,dcecbf916b9e2c61042f0dc992bdfd8ac1c99b8d,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Materials Knowledge Systems in Python—a Data Science Framework for Accelerated Development of Hierarchical Materials,,Erratum,pro21
pap139,e420259fd53d15c2b6cb8906027d5a100ca356d7,con4,Conference on Innovative Data Systems Research,Data science for building energy management: A review,,Erratum,pro4
pap140,1901a26945ecb5445a9d58b4c32a0dc6dbd12f1a,jou54,"Science, Technology and Human Values","STS, Meet Data Science, Once Again","Science and technology studies (STS) and the emerging field of data science share surprising elective affinities. At the growing intersections of these fields, there will be many opportunities and not a few thorny difficulties for STS scholars. First, I discuss how both fields frame the rollout of data science as a simultaneously social and technical endeavor, even if in distinct ways and for diverging purposes. Second, I discuss the logic of domains in contemporary computer, information, and data science circles. While STS is often agnostic about the borders between the sciences or with industry and state—occasionally taking those boundaries as an object of study—data science takes those boundaries as its target to overcome. These two elective affinities present analytic and practical challenges for STS but also opportunities for engagement. Overall, in addition to these typifications, I urge STS scholars to strategically position themselves to investigate and contribute to the breadth of transformations that seek to touch virtually every science and newly bind spheres of academy, industry, and state.",Article,vol54
pap141,04d7b3457dc78b2d2282e6af2c787308f75c9b26,con12,The Compass,Care and the Practice of Data Science for Social Good,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",Conference paper,pro12
pap142,843793928e308b5414d2883ac869e813ec16f65d,con57,International Workshop on Agent-Oriented Software Engineering,Progressive Data Science: Potential and Challenges,"Data science requires time-consuming iterative manual activities. In particular, activities such as data selection, preprocessing, transformation, and mining, highly depend on iterative trial-and-error processes that could be sped up significantly by providing quick feedback on the impact of changes. The idea of progressive data science is to compute the results of changes in a progressive manner, returning a first approximation of results quickly and allow iterative refinements until converging to a final result. Enabling the user to interact with the intermediate results allows an early detection of erroneous or suboptimal choices, the guided definition of modifications to the pipeline and their quick assessment. In this paper, we discuss the progressiveness challenges arising in different steps of the data science pipeline. We describe how changes in each step of the pipeline impact the subsequent steps and outline why progressive data science will help to make the process more effective. Computing progressive approximations of outcomes resulting from changes creates numerous research challenges, especially if the changes are made in the early steps of the pipeline. We discuss these challenges and outline first steps towards progressiveness, which, we argue, will ultimately help to significantly speed-up the overall data science process.",Erratum,pro57
pap143,b4332aaabec46e386fff31d066f278fc27cfa1cb,jou55,Nature Human Behaviour,How data science can advance mental health research,,Conference paper,vol55
pap144,a0ef3467c09acc3106b915258b7b8db7bb663b77,con96,Interspeech,Data Science Methodology for Cybersecurity Projects,"Cyber-security solutions are traditionally static and signature-based. The traditional solutions along with the use of analytic models, machine learning and big data could be improved by automatically trigger mitigation or provide relevant awareness to control or limit consequences of threats. This kind of intelligent solutions is covered in the context of Data Science for Cyber-security. Data Science provides a significant role in cyber-security by utilising the power of data (and big data), high-performance computing and data mining (and machine learning) to protect users against cyber-crimes. For this purpose, a successful data science project requires an effective methodology to cover all issues and provide adequate resources. In this paper, we are introducing popular data science methodologies and will compare them in accordance with cyber-security challenges. A comparison discussion has also delivered to explain methodologies strengths and weaknesses in case of cyber-security projects.",Erratum,pro96
pap145,1d045f4f347409f0635c9d15d538dbfabb6b38fa,jou56,Environmental Modelling & Software,Environmental Data Science,,Letter,vol56
pap146,9b54c9a7d2060f800961c2f9195fcf5408288f17,con30,PS,Data Science for Undergraduates,,Erratum,pro30
pap147,818c9fd2df1229f962af3c50ef493e8633433fb9,con4,Conference on Innovative Data Systems Research,Data Science Thinking,,Erratum,pro4
pap148,e78be911203960b3b2a417465d726734367f8e30,con60,European Conference on Software Process Improvement,Counter‐mapping data science,"Counter-mapping is a combination of critical ideas and practices for social change that offers a productive and promising approach for grassroots data science initiatives. Current information technologies collect, store, and analyze data with new degrees of size, speed, heterogeneity, and detail. While much work utilizing data science technologies is dedicated to generating profit or to national security, some data science projects explicitly attempt to facilitate new social relations, though with inconsistent results and consequences. This paper reviews counter-mapping's particular combination of theory and practice as a potential point of reference for such initiatives. Counter-mapping takes the tools of institutional map-making at government agencies and corporations and applies them in situated, bottom-up ways. Moreover, counter-mapping's multiple theoretical approaches and polyglot practices offer a variety of inspirations and avenues for future work in identifying and realizing alternative, ideally better, possibilities. This paper defines counter-mapping; outlines its multiple theorizations; briefly describes three relevant case studies, The Detroit Geographical Expedition and Institute, Mapping Police Violence, and the Counter-Cartographies Collective; and concludes with a few hard-learned considerations from counter-mapping that are directly pertinent for data-oriented projects focused on change.",Erratum,pro60
pap149,ea07f64ad84542e04acc41db6b171007f344efd7,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Milo: A visual programming environment for Data Science Education,"Most courses on Data Science offered at universities or online require students to have familiarity with at least one programming language. In this paper, we present, “Milo”, a web-based visual programming environment for Data Science Education, designed as a pedagogical tool that can be used by students without prior-programming experience. To that end, Milo uses graphical blocks as abstractions of language specific implementations of Data Science and Machine Learning(ML) concepts along with creation of interactive visualizations. Using block definitions created by a user, Milo generates equivalent source code in JavaScript to run entirely in the browser. Based on a preliminary user study with a focus group of undergraduate computer science students, Milo succeeds as an effective tool for novice learners in the field of Data Science.",Article,pro13
pap150,010f65dd2fa979892a8229db825954871652fb8f,con75,Intelligent Systems in Molecular Biology,Defining Data Science by a Data-Driven Quantification of the Community,"Data science is a new academic field that has received much attention in recent years. One reason for this is that our increasingly digitalized society generates more and more data in all areas of our lives and science and we are desperately seeking for solutions to deal with this problem. In this paper, we investigate the academic roots of data science. We are using data of scientists and their citations from Google Scholar, who have an interest in data science, to perform a quantitative analysis of the data science community. Furthermore, for decomposing the data science community into its major defining factors corresponding to the most important research fields, we introduce a statistical regression model that is fully automatic and robust with respect to a subsampling of the data. This statistical model allows us to define the ‘importance’ of a field as its predictive abilities. Overall, our method provides an objective answer to the question ‘What is data science?’.",Erratum,pro75
pap151,00b1fa3c7170563567fb22a9bb6ff4c7b2e8853e,con14,Hawaii International Conference on System Sciences,Comparing Data Science Project Management Methodologies via a Controlled Experiment,"Data Science is an emerging field with a significant research focus on improving the techniques available to analyze data. However, there has been much less focus on how people should work together on a data science project. In this paper, we report on the results of an experiment comparing four different methodologies to manage and coordinate a data science project. We first introduce a model to compare different project management methodologies and then report on the results of our experiment. The results from our experiment demonstrate that there are significant differences based on the methodology used, with an Agile Kanban methodology being the most effective and surprisingly, an Agile Scrum methodology being the least effective.",Conference paper,pro14
pap152,6e8d94181832771bc5dca8d288c52b6ad5914029,jou57,Philosophy & Technology,Data Science as Machinic Neoplatonism,,Conference paper,vol57
pap153,972edbd8bd19486a37c0a9f34508634fc8733529,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Our path to better science in less time using open data science tools,,Erratum,pro21
pap154,8d89159249e0faf5deae508cc8533010898bbda5,con9,Big Data,Data Science in Action,,Erratum,pro9
pap155,82feed9f0f8d077046b9b8be36e664483a66e33b,con30,PS,Teaching Stats for Data Science,"ABSTRACT “Data science” is a useful catchword for methods and concepts original to the field of statistics, but typically being applied to large, multivariate, observational records. Such datasets call for techniques not often part of an introduction to statistics: modeling, consideration of covariates, sophisticated visualization, and causal reasoning. This article re-imagines introductory statistics as an introduction to data science and proposes a sequence of 10 blocks that together compose a suitable course for extracting information from contemporary data. Recent extensions to the mosaic packages for R together with tools from the “tidyverse” provide a concise and readable notation for wrangling, visualization, model-building, and model interpretation: the fundamental computational tasks of data science.",Erratum,pro30
pap156,a1dc9a3df54ac24712fc47ac5f0b116f1043b95e,con89,Conference on Uncertainty in Artificial Intelligence,R – Data Science,,Erratum,pro89
pap157,bfd6caddec8a98d531ee9f1f7ebf5833797cd5e3,con5,Technical Symposium on Computer Science Education,Introducing Data Science to School Kids,"Data-driven decision making is fast becoming a necessary skill in jobs across the board. The industry today uses analytics and machine learning to get useful insights from a wealth of digital information in order to make decisions. With data science becoming an important skill needed in varying degrees of complexity by the workforce of the near future, we felt the need to expose school-goers to its power through a hands-on exercise. We organized a half-day long data science tutorial for kids in grades 5 through 9 (10-15 years old). Our aim was to expose them to the full cycle of a typical supervised learning approach - data collection, data entry, data visualization, feature engineering, model building, model testing and data permissions. We discuss herein the design choices made while developing the dataset, the method and the pedagogy for the tutorial. These choices aimed to maximize student engagement while ensuring minimal pre-requisite knowledge. This was a challenging task given that we limited the pre-requisites for the kids to the knowledge of counting, addition, percentages, comparisons and a basic exposure to operating computers. By designing an exercise with the stated principles, we were able to provide to kids an exciting, hands-on introduction to data science, as confirmed by their experiences. To the best of the authors' knowledge, the tutorial was the first of its kind. Considering the positive reception of such a tutorial, we hope that educators across the world are encouraged to introduce data science in their respective curricula for high-schoolers and are able to use the principles laid out in this work to build full-fledged courses.",Article,pro5
pap158,61b3ce156347a7f107df75924a45f81f12a0ef14,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Surgical data science: the new knowledge domain,"Abstract Healthcare in general, and surgery/interventional care in particular, is evolving through rapid advances in technology and increasing complexity of care, with the goal of maximizing the quality and value of care. Whereas innovations in diagnostic and therapeutic technologies have driven past improvements in the quality of surgical care, future transformation in care will be enabled by data. Conventional methodologies, such as registry studies, are limited in their scope for discovery and research, extent and complexity of data, breadth of analytical techniques, and translation or integration of research findings into patient care. We foresee the emergence of surgical/interventional data science (SDS) as a key element to addressing these limitations and creating a sustainable path toward evidence-based improvement of interventional healthcare pathways. SDS will create tools to measure, model, and quantify the pathways or processes within the context of patient health states or outcomes and use information gained to inform healthcare decisions, guidelines, best practices, policy, and training, thereby improving the safety and quality of healthcare and its value. Data are pervasive throughout the surgical care pathway; thus, SDS can impact various aspects of care, including prevention, diagnosis, intervention, or postoperative recovery. The existing literature already provides preliminary results, suggesting how a data science approach to surgical decision-making could more accurately predict severe complications using complex data from preoperative, intraoperative, and postoperative contexts, how it could support intraoperative decision-making using both existing knowledge and continuous data streams throughout the surgical care pathway, and how it could enable effective collaboration between human care providers and intelligent technologies. In addition, SDS is poised to play a central role in surgical education, for example, through objective assessments, automated virtual coaching, and robot-assisted active learning of surgical skill. However, the potential for transforming surgical care and training through SDS may only be realized through a cultural shift that not only institutionalizes technology to seamlessly capture data but also assimilates individuals with expertise in data science into clinical research teams. Furthermore, collaboration with industry partners from the inception of the discovery process promotes optimal design of data products as well as their efficient translation and commercialization. As surgery continues to evolve through advances in technology that enhance delivery of care, SDS represents a new knowledge domain to engineer surgical care of the future.",Erratum,pro82
pap159,796d70a6eb0428ae19f1187ae1c81185d4ae6701,con90,Computer Vision and Pattern Recognition,Automating Biomedical Data Science Through Tree-Based Pipeline Optimization,,Erratum,pro90
pap160,da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,con83,Networks,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",Erratum,pro83
pap161,843149b649b888fdb3649b8d4852263b62356799,con15,Pacific Symposium on Biocomputing,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.",Letter,pro15
pap162,f447afeccbdb9ed5df15c44011aec9c018d4b2c4,jou58,Journal of Data and Information Science,Big Data and Data Science: Opportunities and Challenges of iSchools,"Abstract Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools’ opportunities and suggestions in data science education. We argue that iSchools should empower their students with “information computing” disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula.",Letter,vol58
pap163,b2114228411d367cfa6ca091008291f250a2c490,jou59,Nature,Deep learning and process understanding for data-driven Earth system science,,Article,vol59
pap164,589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,jou22,Proceedings of the National Academy of Sciences of the United States of America,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",Conference paper,vol22
pap165,19bb52bec8b5ced3175f4c3ef1b8fb7027cc5ff1,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Applications of Python to evaluate environmental data science problems,"There is a significant convergence of interests in the research community efforts to advance the development and application of software resources (capable of handling the relevant mathematical algorithms to provide scalable information) for solving data science problems. Anaconda is one of the many open source platforms that facilitate the use of open source programming languages (R, Python) for large‐scale data processing, predictive analytics, and scientific computing. The environmental research community may choose to adapt the use of either of the R or the Python programming languages for analyzing the data science problems on the Anaconda platform. This study demonstrated the applications of using Scikit‐learn (a Python machine learning library package) on Anaconda platform for analyzing the in‐bus carbon dioxide concentrations by (i) importing the data into Spyder (Python 3.6) in Anaconda, (ii) performing an exploratory data analysis, (iii) performing dimensionality reduction through RandomForestRegressor feature selection, (iv) developing statistical regression models, and (v) generating regression decision tree models with DecisionTreeRegressor feature. The readers may adopt the methods (inclusive of the Python coding) discussed in this article to successfully address their own data science problems. © 2017 American Institute of Chemical Engineers Environ Prog, 36: 1580–1586, 2017",Erratum,pro13
pap166,d83a99bfb6f81565a186e0eb86858864568c1327,jou11,Communications of the ACM,Data science,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",Article,vol11
pap167,021865bb9fcc59814d2ce84d086554e5e0259779,jou58,Journal of Data and Information Science,"Big Metadata, Smart Metadata, and Metadata Capital: Toward Greater Synergy Between Data Science and Metadata","Abstract Purpose The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. Design/methodology/approach This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. Findings The “utilitarian nature” and “historical and traditional views” of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. Research limitations There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. Practical implications The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Originality/value Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science.",Letter,vol58
pap168,97a3726b3f9395c8919c6271540d87d1c44e10ac,con16,International Conference on Data Science and Advanced Analytics,Deep feature synthesis: Towards automating data science endeavors,"In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94% of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6% of the teams and achieved 95.7% of the top submissions score.",Article,pro16
pap169,96f5a9360ccfd1c5c4210dc62948baac234c372d,con69,Formal Concept Analysis,Predicting data science sociotechnical execution challenges by categorizing data science projects,"The challenge in executing a data science project is more than just identifying the best algorithm and tool set to use. Additional sociotechnical challenges include items such as how to define the project goals and how to ensure the project is effectively managed. This paper reports on a set of case studies where researchers were embedded within data science teams and where the researcher observations and analysis was focused on the attributes that can help describe data science projects and the challenges faced by the teams executing these projects, as opposed to the algorithms and technologies that were used to perform the analytics. Based on our case studies, we identified 14 characteristics that can help describe a data science project. We then used these characteristics to create a model that defines two key dimensions of the project. Finally, by clustering the projects within these two dimensions, we identified four types of data science projects, and based on the type of project, we identified some of the sociotechnical challenges that project teams should expect to encounter when executing data science projects.",Erratum,pro69
pap170,94c52a7516ef8955f76c3ee1319ff4fd8bf071fd,con94,Vision,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",Erratum,pro94
pap171,3a8da09a87f06273c19fb61573b299388f8d1673,jou60,Japanese Journal of Statistics and Data Science,Data science vs. statistics: two cultures?,,Conference paper,vol60
pap172,427a613d349d305726e1c4c7935b33c79de5850a,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Python Data Science Handbook: Essential Tools for Working with Data,"For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools. Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python. With this handbook, youll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms",Erratum,pro82
pap173,9141efc0d91ab0bda9b264ff6d1df5f20fd1dbb0,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Transdisciplinary Foundations of Geospatial Data Science,"Recent developments in data mining and machine learning approaches have brought lots of excitement in providing solutions for challenging tasks (e.g., computer vision). However, many approaches have limited interpretability, so their success and failure modes are difficult to understand and their scientific robustness is difficult to evaluate. Thus, there is an urgent need for better understanding of the scientific reasoning behind data mining and machine learning approaches. This requires taking a transdisciplinary view of data science and recognizing its foundations in mathematics, statistics, and computer science. Focusing on the geospatial domain, we apply this crucial transdisciplinary perspective to five common geospatial techniques (hotspot detection, colocation detection, prediction, outlier detection and teleconnection detection). We also describe challenges and opportunities for future advancement.",Erratum,pro98
pap174,12e17fa5dd5715c563aadf705427da84817f100f,con20,ACM Conference on Economics and Computation,Data science: Data science tutorials,,Erratum,pro20
pap175,5b42e8ab6542fbbc11d84b07b34443a6853f96f1,jou61,Business & Information Systems Engineering,Responsible Data Science,,Article,vol61
pap176,4a6d46962d3f58d278cfb46d3ddebbb30bf275f5,jou62,IEEE Computer Graphics and Applications,Geographic Data Science,"Data science methods and approaches address all stages of transition from data to knowledge and action. Visualization of this data is essential for human understanding of the subject under study, analytical reasoning about it, and generating new knowledge. Geographic data science deals with data that incorporates spatial and, often, temporal elements. The articles selected for this special issue represent a mix of theoretical approaches and novel applications of geographic data science.",Letter,vol62
pap177,af1fed4f5226292afffc0b736ddfa777acb8eb86,con68,Experimental Software Engineering Network,An Introduction to Data Science,"An Introduction to Data Scienceby Jeffrey S. Saltz and Jeffrey M. Stanton is an easy-to-read, gentle introduction for people with a wide range of backgrounds into the world of data science. Needing no prior coding experience or a deep understanding of statistics, this book uses the R programming language and RStudio platform to make data science welcoming and accessible for all learners. After introducing the basics of data science, the book builds on each previous concept to explain R programming from the ground up. Readers will learn essential skills in data science through demonstrations of how to use data to construct models, predict outcomes, and visualize data.",Erratum,pro68
pap178,f6705e68c71bc0b51ddb8d1e4f986c894ba8f34f,con27,International Conference on Contemporary Computing,"Data Science, Predictive Analytics, and Big Data in Supply Chain Management: Current State and Future Potential","While data science, predictive analytics, and big data have been frequently used buzzwords, rigorous academic investigations into these areas are just emerging. In this forward thinking article, we discuss the results of a recent large-scale survey on these topics among supply chain management (SCM) professionals, complemented with our experiences in developing, implementing, and administering one of the first master's degree programs in predictive analytics. As such, we effectively provide an assessment of the current state of the field via a large-scale survey, and offer insight into its future potential via the discussion of how a research university is training next-generation data scientists. Specifically, we report on the current use of predictive analytics in SCM and the underlying motivations, as well as perceived benefits and barriers. In addition, we highlight skills desired for successful data scientists, and provide illustrations of how predictive analytics can be implemented in the curriculum. Relying on one of the largest data sets of predictive analytics users in SCM collected to date and our experiences with one of the first master's degree programs in predictive analytics, it is our intent to provide a timely assessment of the field, illustrate its future potential, and motivate additional research and pedagogical advancements in this domain.",Erratum,pro27
pap179,0a9b30386408595ff0b3155d4de4a56dad80a97b,con77,International Conference on Artificial Neural Networks,The ambiguity of data science team roles and the need for a data science workforce framework,"This paper first reviews the benefits of well-defined roles and then discusses the current lack of standardized roles within the data science community, perhaps due to the newness of the field. Specifically, the paper reports on five case studies exploring five different attempts to define a standard set of roles. These case studies explore the usage of roles from an industry perspective as well as from national standard big data committee efforts. The paper then leverages the results of these case studies to explore the use of data science roles within online job postings. While some roles appeared frequently, such as data scientist and data engineer, no role was consistently used across all five case studies. Hence, the paper concludes by noting the need to create a data science workforce framework that could be used by students, employers, and academic institutions. This framework would enable organizations to staff their data science teams more accurately with the desired skillsets.",Erratum,pro77
pap180,cd247b7830fb58c6f019a79ae9679251176e8342,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Game Theory for Data Science: Eliciting Truthful Information,,Erratum,pro47
pap181,9a552de12d0b5a1561cf741f3170a0864f2b15d2,con78,Neural Information Processing Systems,Ushering in a New Frontier in Geospace Through Data Science,"Our understanding and specification of solar‐terrestrial interactions benefit from taking advantage of comprehensive data‐intensive approaches. These data‐driven methods are taking on new importance in light of the shifting data landscape of the geospace system, which extends from the near Earth space environment, through the magnetosphere and interplanetary space, to the Sun. The space physics community faces both an exciting opportunity and an important imperative to create a new frontier built at the intersection of traditional approaches and state‐of‐the‐art data‐driven sciences and technologies. This brief commentary addresses the current paradigm of geospace science and the emerging need for data science innovation, discusses the meaning of data science in the context of geospace, and highlights community efforts to respond to the changing landscape.",Erratum,pro78
pap182,dd340315c44a9c68391d8d2f600a0adc76b70c09,con17,International Conference on Statistical and Scientific Database Management,Fides: Towards a Platform for Responsible Data Science,"Issues of responsible data analysis and use are coming to the forefront of the discourse in data science research and practice, with most significant efforts to date on the part of the data mining, machine learning, and security and privacy communities. In these fields, the research has been focused on analyzing the fairness, accountability and transparency (FAT) properties of specific algorithms and their outputs. Although these issues are most apparent in the social sciences where fairness is interpreted in terms of the distribution of resources across protected groups, management of bias in source data affects a variety of fields. Consider climate change studies that require representative data from geographically diverse regions, or supply chain analyses that require data that represents the diversity of products and customers. Any domain that involves sparse or sampled data has exposure to potential bias. In this vision paper, we argue that FAT properties must be considered as database system issues, further upstream in the data science lifecycle: bias in source data goes unnoticed, and bias may be introduced during pre-processing (fairness), spurious correlations lead to reproducibility problems (accountability), and assumptions made during pre-processing have invisible but significant effects on decisions (transparency). As machine learning methods continue to be applied broadly by non-experts, the potential for misuse increases. We see a need for a data sharing and collaborative analytics platform with features to encourage (and in some cases, enforce) best practices at all stages of the data science lifecycle. We describe features of such a platform, which we term Fides, in the context of urban analytics, outlining a systems research agenda in responsible data science.",Article,pro17
pap183,3af056b2aed8724dcddea074eb68aff6dd11c926,jou63,PLoS Biology,Building the biomedical data science workforce,"This article describes efforts at the National Institutes of Health (NIH) from 2013 to 2016 to train a national workforce in biomedical data science. We provide an analysis of the Big Data to Knowledge (BD2K) training program strengths and weaknesses with an eye toward future directions aimed at any funder and potential funding recipient worldwide. The focus is on extramurally funded programs that have a national or international impact rather than the training of NIH staff, which was addressed by the NIH’s internal Data Science Workforce Development Center. From its inception, the major goal of BD2K was to narrow the gap between needed and existing biomedical data science skills. As biomedical research increasingly relies on computational, mathematical, and statistical thinking, supporting the training and education of the workforce of tomorrow requires new emphases on analytical skills. From 2013 to 2016, BD2K jump-started training in this area for all levels, from graduate students to senior researchers.",Conference paper,vol63
pap184,38fadf7c21c32b183fa3dcf32da1044e8441b813,con105,British Machine Vision Conference,The Data Science Handbook,"microbial community dynamics, Support Vector Machines, a robust prediction method with applications in bioinformatics, Bayesian Model Selection for Data with High Dimension, High dimensional statistical inference: theoretical development to data analytics, Big data challenges in genomics, Analysis of microarray gene expression data using information theory and stochastic algorithm, Hybrid Models, Markov Chain Monte Carlo Methods: Theory and Practice, and more. Provides the authority and expertise of leading contributors from an international board of authors Presents the latest release in the Handbook of Statistics series Updated release includes the latest information on",Erratum,pro105
pap185,ce0b7ee60920f9b37f88cab785cb8b4dc337e89f,con99,North American Chapter of the Association for Computational Linguistics,Educational data science in massive open online courses,"The current massive open online course (MOOC) euphoria is revolutionizing online education. Despite its expediency, there is considerable skepticism over various concerns. In order to resolve some of these problems, educational data science (EDS) has been used with success. MOOCs provide a wealth of information about the way in which a large number of learners interact with educational platforms and engage with the courses offered. This extensive amount of data provided by MOOCs concerning students' usage information is a gold mine for EDS. This paper aims to provide the reader with a complete and comprehensive review of the existing literature that helps us understand the application of EDS in MOOCs. The main works in this area are described and grouped by task or issue to be solved, along with the techniques used. WIREs Data Mining Knowl Discov 2017, 7:e1187. doi: 10.1002/widm.1187",Erratum,pro99
pap186,224eb3407b50533668b6c1caa55a720688b8b532,jou64,International Journal of Information Management,"A review and future direction of agile, business intelligence, analytics and data science",,Article,vol64
pap187,31485e1213dd886fa2b668eefcd9b13533d8a9fe,con20,ACM Conference on Economics and Computation,Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",Erratum,pro20
pap188,aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,con91,Symposium on the Theory of Computing,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",Erratum,pro91
pap189,e1a1ad4025e2c7a82882c7389d937cbdfd10b799,jou65,Data Science Journal,Towards Data Science,"Currently, a huge amount of data is being rapidly generated in cyberspace. Datanature (all data in cyberspace) is forming due to a data explosion. Exploring the patterns and rules in datanature is necessary but difficult. A new discipline called Data Science is coming. It provides a type of novel research method (a data-intensive method) for natural and social sciences and goes beyond computer science in researching data. This paper presents the challenges presented by data and discusses what differentiates data science from the established sciences, data technologies, and big data. Our goal is to encourage data related researchers to transfer their focus towards this new science.",Conference paper,vol65
pap190,47134cbdfa7c44a55de5697a35b6652d0fcfee30,con100,International Conference on Automatic Face and Gesture Recognition,Data Science in Libraries,"EDITOR'S SUMMARY 
 
The new field of data science involves advanced knowledge in statistics and computer science, combined with copious amounts of data. A report from the Big Data and Research Initiative under the Obama Administration, The Federal Big Data Research and Development Strategic Plan, calls attention to the roles that librarians will play in the future of data science. However, there are skills and management gaps librarians face that inhibit their ability to move forward in data science. A number of educational programs are now offered to remedy this problem, such as the Data and Visualization Institute for Librarians from North Carolina State University, the volunteer-led Library Carpentry program, and most recently, the Data Sciences in Libraries Project, funded by the IMLS. This project aims to get librarians and library managers together to discuss the world of data science and create a roadmap for strategic planning.",Erratum,pro100
pap191,0ec1992151e28c5678832c0923e56aeb58caad53,con18,International Conference on Exploring Services Science,From Data Science to Value Creation,,Conference paper,pro18
pap192,5a56bbd762e9dd70dd20afe8740a6d09ec85ffed,con6,Annual Conference on Genetic and Evolutionary Computation,Data science from scratch,"This is a first-principles-based, practical introduction to the fundamentals of data science aimed at the mathematically-comfortable reader with some programming skills. The book covers: The important parts of Python to know The important parts of Math / Probability / Statistics to know The basics of data science How commonly-used data science techniques work (learning by implementing them) What is Map-Reduce and how to do it in Python Other applications such as NLP, Network Analysis, and more",Erratum,pro6
pap193,3b963487cbf944d51f33c2a0b41eb2aed7c68b89,con66,International Conference on Software Reuse,Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems,"The distributed and global nature of data science creates challenges for evaluating the quality, import and potential impact of the data and knowledge claims being produced. This has significant consequences for the management and oversight of responsibilities and accountabilities in data science. In particular, it makes it difficult to determine who is responsible for what output, and how such responsibilities relate to each other; what ‘participation’ means and which accountabilities it involves, with regard to data ownership, donation and sharing as well as data analysis, re-use and authorship; and whether the trust placed on automated tools for data mining and interpretation is warranted (especially as data processing strategies and tools are often developed separately from the situations of data use where ethical concerns typically emerge). To address these challenges, this paper advocates a participative, reflexive management of data practices. Regulatory structures should encourage data scientists to examine the historical lineages and ethical implications of their work at regular intervals. They should also foster awareness of the multitude of skills and perspectives involved in data science, highlighting how each perspective is partial and in need of confrontation with others. This approach has the potential to improve not only the ethical oversight for data science initiatives, but also the quality and reliability of research outputs. This article is part of the themed issue ‘The ethical impact of data science’.",Erratum,pro66
pap194,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,jou66,Information systems research,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",Conference paper,vol66
pap195,ae118a88ada51dfdb2296cbaa948eb4a467942b6,con59,Annual Workshop of the Psychology of Programming Interest Group,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",Erratum,pro59
pap196,b26c93eba9e1d99a5c99b07d2476714b386c4d54,con30,PS,Agile big data analytics: AnalyticsOps for data science,"Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process — completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics — to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics.",Erratum,pro30
pap197,62806c60226d54ba1a4455bb1d7d2f034ef7c29a,con18,International Conference on Exploring Services Science,"Introducing Data Science: Big Data, Machine Learning, and more, using Python tools","Summary Introducing Data Science teaches you how to accomplish the fundamental tasks that occupy data scientists. Using the Python language and common Python libraries, you'll experience firsthand the challenges of dealing with data at scale and gain a solid foundation in data science. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Many companies need developers with data science skills to work on projects ranging from social media marketing to machine learning. Discovering what you need to learn to begin a career as a data scientist can seem bewildering. This book is designed to help you get started. About the BookIntroducing Data Science Introducing Data Science explains vital data science concepts and teaches you how to accomplish the fundamental tasks that occupy data scientists. Youll explore data visualization, graph databases, the use of NoSQL, and the data science process. Youll use the Python language and common Python libraries as you experience firsthand the challenges of dealing with data at scale. Discover how Python allows you to gain insights from data sets so big that they need to be stored on multiple machines, or from data moving so quickly that no single machine can handle it. This book gives you hands-on experience with the most popular Python data science libraries, Scikit-learn and Stats Models. After reading this book, youll have the solid foundation you need to start a career in data science. Whats Inside Handling large data Introduction to machine learning Using Python to work with data Writing data science algorithms About the ReaderThis book assumes you're comfortable reading code in Python or a similar language, such as C, Ruby, or JavaScript. No prior experience with data science is required. About the Authors Davy Cielen, Arno D. B. Meysman, and Mohamed Ali are the founders and managing partners of Optimately and Maiton, where they focus on developing data science projects and solutions in various sectors.",Erratum,pro18
pap198,52ff64f7f26b28447af255fedeb2216a70b48d66,con3,Knowledge Discovery and Data Mining,Large Scale Distributed Data Science using Apache Spark,"Apache Spark is an open-source cluster computing framework for big data processing. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices.",Article,pro3
pap199,7c614fe86cc11c0430dd12b44e018e16e5dcf742,jou40,American Statistician,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",Letter,vol40
pap200,4c05d4410c0023e14f2bb0cbcf7613468855430b,con90,Computer Vision and Pattern Recognition,A Data Science Course for Undergraduates: Thinking With Data,"Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]",Erratum,pro90
pap201,0442b04b4e8741900b65de0721f0c3e152e044ef,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",Erratum,pro85
pap202,259d81ced1837bb74f3eeeb30ca3217d535e0c31,con59,Annual Workshop of the Psychology of Programming Interest Group,Introduction to HPC with MPI for Data Science,,Erratum,pro59
pap203,c740a6816155fd123081d2f78926a0d3819926e7,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,LibGuides: *Data Science: Data Science Resources,"Data science resources, from finding ebooks and blogs, to finding raw datasets and analysis. Learn about data science resources, analysis, communities and data management. Also learn about hte datasets openly available and dataset purchase program.",Erratum,pro82
pap204,37095b714dad5895d946b1f8435a3a38dee1be8b,con9,Big Data,"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications",,Erratum,pro9
pap205,c694c6a685a067393204f36e21c8917a49f02a9b,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Big data at work : the data science revolution and organizational psychology,"Foreword by Richard Klimoski 1. Building Understanding of the Data Science Revolution and IO Psychology Eden B. King, Scott Tonidandel, Jose M. Cortina, & Alexis A. Fink Part I: Big Issues for Big Data Methods 2. Big Data Platform Jacqueline Ryan 3. Statistical Methods for Big Data: A Scenic Tour Frederick L. Oswald & Dan J. Putka 4. Twitter Analysis: Methods for Data Management and a Word Count Dictionary to Measure City-Level Job Satisfaction Ivan Hernandez, Daniel A. Newman, & Gahyun Jeon 5. Data Visualization Evan F. Sinar 6. Sensing Big Data: Multimodal Information Interfaces for Exploration of Large Data Sets Jeffrey Stanton Part II: Big Ideas for Big Data in Organization 7. Implications of the Big Data Movement for the Advancement I-O Science and Practice Dan J. Putka & Frederick L. Oswald 8. Big Data in Talent Selection and Assessment A. James Illingworth, Michael Lippstreu, & Anne-Sophie Deprez-Sims 9. Big Data in Turnover/Retention John P. Hausknecht & Huisi (Jessica) Li 10. Using Big Data to Advance the Science of Team Effectiveness Steve W. J. Kozlowski, Georgia T. Chao, Chu-Hsiang (Daisy) Chang, & Rosemarie Fernandez 11. Using Big Data to Create Diversity and Inclusion in Organizations Whitney Botsford Morgan, Eric Dunleavy, & Peter D. DeVries 12. How Big Data Matters Richard A. Guzzo",Erratum,pro98
pap206,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,con9,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",Conference paper,pro9
pap207,9c1b9598f82f9ed7d75ef1a9e627496759aa2387,con99,North American Chapter of the Association for Computational Linguistics,"Data Science, Predictive Analytics, and Big Data: A Revolution that Will Transform Supply Chain Design and Management","We illuminate the myriad of opportunities for research where supply chain management intersects with data science, predictive analytics, and big data, collectively referred to as DPB. We show that these terms are not only becoming popular but are also relevant to supply chain research and education. Data science requires both domain knowledge and a broad set of quantitative skills, but there is a dearth of literature on the topic and many questions. We call for research on skills that are needed by SCM data scientists and discuss how such skills and domain knowledge affect the effectiveness of a SCM data scientist. Such knowledge is crucial to developing future supply chain leaders. We propose definitions of data science and predictive analytics as applied to supply chain management. We examine possible applications of DPB in practice and provide examples of research questions from these applications, as well as examples of research questions employing DPB that stem from management theories. Finally, we propose specific steps interested researchers can take to respond to our call for research on the intersection of supply chain management and DPB.",Erratum,pro99
pap208,c04aaf36c8587e40747212e316d9bf44186ef64a,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Developing a Research Agenda for Human-Centered Data Science,"The study and analysis of large and complex data sets offer a wealth of insights in a variety of applications. Computational approaches provide researchers access to broad assemblages of data, but the insights extracted may lack the rich detail that qualitative approaches have brought to the understanding of sociotechnical phenomena. How do we preserve the richness associated with traditional qualitative methods while utilizing the power of large data sets? How do we uncover social nuances or consider ethics and values in data use? These and other questions are explored by human-centered data science, an emerging field at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science. This workshop, the first of its kind at CSCW, seeks to bring together researchers interested in human-centered approaches to data science to collaborate, define a research agenda, and form a community.",Erratum,pro82
pap209,b9888bb70d6f246c7ffb53dcb9498bfafe113d8f,con31,International Conference on Evaluation & Assessment in Software Engineering,Thinking by classes in data science: the symbolic data analysis paradigm,"Data Science, considered as a science by itself, is in general terms, the extraction of knowledge from data. Symbolic data analysis (SDA) gives a new way of thinking in Data Science by extending the standard input to a set of classes of individual entities. Hence, classes of a given population are considered to be units of a higher level population to be studied. Such classes often represent the real units of interest. In order to take variability between the members of each class into account, classes are described by intervals, distributions, set of categories or numbers sometimes weighted and the like. In that way, we obtain new kinds of data, called ‘symbolic’ as they cannot be reduced to numbers without losing much information. The first step in SDA is to build the symbolic data table where the rows are classes and the variables can take symbolic values. The second step is to study and extract new knowledge from these new kinds of data by at least an extension of Computer Statistics and Data Mining to symbolic data. SDA is a new paradigm which opens up a vast domain of research and applications by giving complementary results to classical methods applied to standard data. SDA also gives answers to big data and complex data challenges as big data can be reduced and summarized by classes and as complex data with multiple unstructured data tables and unpaired variables can be transformed into a structured data table with paired symbolic‐valued variables. WIREs Comput Stat 2016, 8:172–205. doi: 10.1002/wics.1384",Erratum,pro31
pap210,12f3b97d76e2e07c3bf2914606d26bbfbbe85bd1,con63,International Colloquium on Theoretical Aspects of Computing,Role of materials data science and informatics in accelerated materials innovation,"The goal of the Materials Genome Initiative is to substantially reduce the time and cost of materials design and deployment. Achieving this goal requires taking advantage of the recent advances in data and information sciences. This critical need has impelled the emergence of a new discipline, called materials data science and informatics. This emerging new discipline not only has to address the core scientific/technological challenges related to datafication of materials science and engineering, but also, a number of equally important challenges around data-driven transformation of the current culture, practices, and workflows employed for materials innovation. A comprehensive effort that addresses both of these aspects in a synergistic manner is likely to succeed in realizing the vision of scaled-up materials innovation. Key toolsets needed for the successful adoption of materials data science and informatics in materials innovation are identified and discussed in this article. Prototypical examples of emerging novel toolsets and their functionality are described along with select case studies.",Erratum,pro63
pap211,071036abe55e7247d7e6ec28a4afc8ef2670f479,con12,The Compass,A Comparison of Open Source Tools for Data Science,"The next decade of competitive advantage revolves around the ability to make predictions and discover patterns in data. Data science is at the center of this revolution. Data science has been termed the sexiest job of the 21st century. Data science combines data mining, machine learning, and statistical methodologies to extract knowledge and leverage predictions from data. Given the need for data science in organizations, many small or medium organizations are not adequately funded to acquire expensive data science tools. Open source tools may provide the solution to this issue. While studies comparing open source tools for data mining or business intelligence exist, an update on the current state of the art is necessary. This work explores and compares common open source data science tools. Implications include an overview of the state of the art and knowledge for practitioners and academics to select an open source data science tool that suits the requirements of specific data science projects.",Erratum,pro12
pap212,6b705d7ef453d42d87a9099b31344adad2367f40,con7,International Symposium on Intelligent Data Analysis,EDISON Data Science Framework: A Foundation for Building Data Science Profession for Research and Industry,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and should be built with a strong link to emerging Big Data and data driven technologies, and consequently needs re-thinking and re-design of both traditional educational models and existing courses. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects by design the whole lifecycle of data handling in modern, data driven research and the digital economy. This paper presents the EDISON Data Science Framework (EDSF) that is intended to create a foundation for the Data Science profession definition. The EDSF includes the following core components: Data Science Competence Framework (CF-DS), Data Science Body of Knowledge (DS-BoK), Data Science Model Curriculum (MC-DS), and Data Science Professional profiles (DSP profiles). The MC-DS is built based on CF-DS and DS-BoK, where Learning Outcomes are defined based on CF-DS competences and Learning Units are mapped to Knowledge Units in DS-BoK. In its own turn, Learning Units are defined based on the ACM Classification of Computer Science (CCS2012) and reflect typical courses naming used by universities in their current programmes. The paper provides example how the proposed EDSF can be used for designing effective Data Science curricula and reports the experience of implementing EDSF by the Champion Universities that cooperate with the EDISON project.",Erratum,pro7
pap213,dc86a7295d737fc2f195b67a273e90b549bd6272,jou61,Business & Information Systems Engineering,Business Analytics and Data Science: Once Again?,,Article,vol61
pap214,ca9f74a1a7b69214c670202bb4f66eb16194f836,con5,Technical Symposium on Computer Science Education,Datathons: An Experience Report of Data Hackathons for Data Science Education,Large amounts of data are becoming increasingly available through open data repositories as well as companies and governments collecting data to improve decision making and efficiencies. Consequently there is a need to increase the data literacy of computer science students. Data science is a relatively new area within computer science and the curriculum is rapidly evolving along with the tools required to perform analytics which students need to learn how to effectively use. To address the needs of students learning key data science and analytics skills we propose augmenting existing data science curriculums with hackathon events that focus on data also known as datathons. In this paper we present our experience at hosting and running four datathons that involved students and members from the community coming together to solve challenging problems with data from not-for-profit social good organizations and publicly open data. Our reported experience from our datathons will help inform other academics and community groups who also wish to host datathons to help facilitate their students and members to learn key data science and analytics skills.,Conference paper,pro5
pap215,8ffe80d758a78810c7d5a33a088cd4529b8a6a4b,jou67,Journal of Decision Systems,Data science: supporting decision-making,"Abstract Data science is a new academic trans-discipline that builds on 60 years of research about supporting decision-making in organisations. It is an important and potentially significant concept and practice. Contemplating the need for data scientists encourages academics and managers to examine issues of decision-maker rationality, data and data analysis needs, analytical tools, job skills and academic preparation. This article explores data science and the data professionals who will use new data streams and analytics to support decision-making. It also examines the dimensions that are changing in the data stream and the skills needed by data scientists to analyse the new data streams. Organisations need data scientists, but academics need to understand the new data science jobs to prepare more people to support decision-making.",Article,vol67
pap216,3e209c705350761fe676ac330503e8662279fbf2,jou68,IEEE Transactions on Services Computing,Processes Meet Big Data: Connecting Data Science with Process Science,"As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",Letter,vol68
pap217,659890e52fe234cde0e02a2305e213d3e8cb14b2,con22,Grid Computing Environments,Data science and cyberinfrastructure: critical enablers for accelerated development of hierarchical materials,"The slow pace of new/improved materials development and deployment has been identified as the main bottleneck in the innovation cycles of most emerging technologies. Much of the continuing discussion in the materials development community is therefore focused on the creation of novel materials innovation ecosystems designed to dramatically accelerate materials development efforts, while lowering the overall cost involved. In this paper, it is argued that the recent advances in data science can be leveraged suitably to address this challenge by effectively mediating between the seemingly disparate, inherently uncertain, multiscale and multimodal measurements and computations involved in the current materials’ development efforts. Proper utilisation of modern data science in the materials’ development efforts can lead to a new generation of data-driven decision support tools for guiding effort investment (for both measurements and computations) at various stages of the materials development. It should also be recognised that the success of such ecosystems is predicated on the creation and utilisation of integration platforms for promoting intimate, synchronous collaborations between cross-disciplinary and distributed team members (i.e. cyberinfrastructure). Indeed, data sciences and cyberinfrastructure form the two main pillars of the emerging new discipline broadly referred to as materials informatics (MI). This paper provides a summary of current capabilities in this emerging new field as they relate to the accelerated development of advanced hierarchical materials (the internal structure plays a dominant role in controlling overall properties/performance in these materials) and identifies specific directions of research that offer the most promising avenues.",Erratum,pro22
pap218,2ce0b954b5180fdc0834c3e4f0d14b5a0e668d53,con9,Big Data,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.",Letter,pro9
pap219,f152a4008f114ac19076ee6b98d431268f4aea9e,con5,Technical Symposium on Computer Science Education,A Practical and Sustainable Model for Learning and Teaching Data Science,"This paper details our experiences with design and implementation of data science curriculum at University at Buffalo (UB). We discuss (i) briefly the history of project, (ii) a certificate program that we created, (iii) a data-intensive computing course that forms the core of the curriculum and (iv) some of the challenges we faced and how we addressed them. Major goal of the project was to improve the preparedness of our workforce for the emerging data-intensive computing area. We measured this through assessment of student learning on various concepts and topics related to data-intensive computing. We also discuss the best practices in building a data science program. We highlight the importance of external funding support and multi-disciplinary collaborations in the success of the project. The pedagogical resources created for the project are freely available to help educators and other learners navigate the path to learning data science. We expect this paper about our experience will provide a road map for educators who desire to introduce data science in their curriculum.",Article,pro5
pap220,8ea48934b6f6a0717efb4e5355be3b008fc5b1bd,con83,Networks,Coding the biodigital child: the biopolitics and pedagogic strategies of educational data science,"Abstract Educational data science is an emerging transdisciplinary field formed from an amalgamation of data science and elements of biological, psychological and neuroscientific knowledge about learning, or learning science. This article conceptualises educational data science as a biopolitical strategy focused on the evaluation and management of the corporeal, emotional and embrained lives of children. Such strategies are enacted through the development of new kinds of digitally-mediated ‘biopedagogies’ of body optimisation, ‘psychopedagogies’ of emotional maximisation, and ‘neuropedagogies’ of brain empowerment. The data practices, scientific knowledges, digital devices and pedagogies that constitute educational data science produce new systems of knowledge about the child that are consequential to their formation as ‘biodigital’ subjects, whose assumed qualities and capacities are defined through expert practices of biosensing, emotion analytics, and neurocomputation, combined with associated scientific knowledges. The article develops the concept of transcoding to account for the processes involved in the formation of the biodigital child.",Erratum,pro83
pap221,e12b5363078a6d435bfba80e9d5cbab6b2cac897,con99,North American Chapter of the Association for Computational Linguistics,Data Science and Digital Art History,"I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photo­graphy collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture.",Erratum,pro99
pap222,b0150dd118ebedbc3ece68726e065f9afaaf3b18,con86,The Web Conference,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",Erratum,pro86
pap223,49522df4fab1ebbeb831fc265196c2c129bf6087,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Survey on data science with population-based algorithms,,Erratum,pro21
pap224,2081508c05ebe4fc0b7b2a1fd6a356a0e933186b,con19,International Conference on Conceptual Structures,Teaching Data Science,,Letter,pro19
pap225,3eb5f2d152ead21ce528f9781c66197010eea3c8,con17,International Conference on Statistical and Scientific Database Management,A Case for Data Commons: Toward Data Science as a Service,"Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. An architecture for data commons is described, as well as some lessons learned from operating several large-scale data commons.",Erratum,pro17
pap226,469fb7c7178c8370a93fb27dadc9c5c839a9b8ec,jou58,Journal of Data and Information Science,Information Science Roles in the Emerging Field of Data Science,"There has long been discussion about the distinctions of library science, information science, and informatics, and how these areas differ and overlap with computer science. Today the term data science is emerging that generates excitement and questions about how it relates to and differs from these other areas of study. For our purposes here, I consider information science to be the general term that subsumes library science and informatics and focuses on distinctions and similarities among these disciplines that each informs data science. At the most general levels, information science deals with the genesis, flow, use, and preservation of information; computer science deals with algorithms and techniques for computational processes. Data science as a concept emerges from the applications of existing studies of measurement, representation, interpretation, and management to problems in Citation: Gary Marchionini (2016). Information Science Roles in the Emerging Field of Data Science. Received: Mar. 10, 2016 Accepted: Mar. 22, 2016",Conference paper,vol58
pap227,061c3291d817076dbb3e5a41c51f99800a390e94,con78,Neural Information Processing Systems,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",Erratum,pro78
pap228,fa15d626d8905d08953abe646a75a31417ad61fa,con30,PS,"Data science on the ground: Hype, criticism, and everyday work","Modern organizations often employ data scientists to improve business processes using diverse sets of data. Researchers and practitioners have both touted the benefits and warned of the drawbacks associated with data science and big data approaches, but few studies investigate how data science is carried out “on the ground.” In this paper, we first review the hype and criticisms surrounding data science and big data approaches. We then present the findings of semistructured interviews with 18 data analysts from various industries and organizational roles. Using qualitative coding techniques, we evaluated these interviews in light of the hype and criticisms surrounding data science in the popular discourse. We found that although the data analysts we interviewed were sensitive to both the allure and the potential pitfalls of data science, their motivations and evaluations of their work were more nuanced. We conclude by reflecting on the relationship between data analysts' work and the discourses around data science and big data, suggesting how future research can better account for the everyday practices of this profession.",Erratum,pro30
pap229,e926ef463fa96b3d06a321fcbcccdab9ff5f3da0,jou69,Statistics and computing,Statistics and computing: the genesis of data science,,Article,vol69
pap230,de84e808462b8240c75987364a6d518eff7d8813,con9,Big Data,Statistics: a data science for the 21st century,"The rise of data science could be seen as a potental threat to the long‐term status of the statistics discipline. I first argue that, although there is a threat, there is also a much greater opportunity to re‐emphasize the universal relevance of statistical method to the interpretation of data, and I give a short historical outline of the increasingly important links between statistics and information technology. The core of the paper is a summary of several recent research projects, through which I hope to demonstrate that statistics makes an essential, but incomplete, contribution to the emerging field of ‘electronic health’ research. Finally, I offer personal thoughts on how statistics might best be organized in a research‐led university, on what we should teach our students and on some issues broadly related to data science where the Royal Statistical Society can take a lead.",Erratum,pro9
pap231,8f6a4609531ca9ff35915c32dae5cd146fc57c40,con59,Annual Workshop of the Psychology of Programming Interest Group,HEALTH BANK - A Workbench for Data Science Applications in Healthcare,"The enormous amounts of data that are generated in the healthcare process and stored in electronic health record (EHR) systems are an underutilized resource that, with the use of data science applica- tions, can be exploited to improve healthcare. To foster the development and use of data science applications in healthcare, there is a fundamen- tal need for access to EHR data, which is typically not readily available to researchers and developers. A relatively rare exception is the large EHR database, the Stockholm EPR Corpus, comprising data from more than two million patients, that has been been made available to a lim- ited group of researchers at Stockholm University. Here, we describe a number of data science applications that have been developed using this database, demonstrating the potential reuse of EHR data to support healthcare and public health activities, as well as facilitate medical re- search. However, in order to realize the full potential of this resource, it needs to be made available to a larger community of researchers, as well as to industry actors. To that end, we envision the provision of an in- frastructure around this database called HEALTH BANK – the Swedish Health Record Research Bank. It will function both as a workbench for the development of data science applications and as a data explo- ration tool, allowing epidemiologists, pharmacologists and other medical researchers to generate and evaluate hypotheses. Aggregated data will be fed into a pipeline for open e-access, while non-aggregated data will be provided to researchers within an ethical permission framework. We believe that HEALTH BANK has the potential to promote a growing industry around the development of data science applications that will ultimately increase the efficiency and effectiveness of healthcare.",Erratum,pro59
pap232,b885916e9af51010ca7ebafbc9270f0e5e207b38,jou70,Nursing Administration Quarterly,Nursing Knowledge: Big Data Science—Implications for Nurse Leaders,"The integration of Big Data from electronic health records and other information systems within and across health care enterprises provides an opportunity to develop actionable predictive models that can increase the confidence of nursing leaders' decisions to improve patient outcomes and safety and control costs. As health care shifts to the community, mobile health applications add to the Big Data available. There is an evolving national action plan that includes nursing data in Big Data science, spearheaded by the University of Minnesota School of Nursing. For the past 3 years, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated through the “Nursing Knowledge: Big Data Science” conferences to create and act on recommendations for inclusion of nursing data, integrated with patient-generated, interprofessional, and contextual data. It is critical for nursing leaders to understand the value of Big Data science and the ways to standardize data and workflow processes to take advantage of newer cutting edge analytics to support analytic methods to control costs and improve patient quality and safety.",Conference paper,vol70
pap233,e78d7fa72a5dbe5f3bc93f6e200826004f23530b,jou71,IEEE Intelligent Systems,Data Science: Nature and Pitfalls,Data science is creating exciting trends as well as significant controversy. A critical matter for the healthy development of data science in its early stages is to deeply understand the nature of data and data science and discuss the various pitfalls. These important issues motivate the discussions in this article.,Conference paper,vol71
pap234,d4dd5eb7a6081c0857ddae0caca1fdcf288553e1,jou72,Journal of Biomechanics,Gait biomechanics in the era of data science.,,Letter,vol72
pap235,d343c9823bcacf31ea4aca105d0366f3f18a75e5,jou71,IEEE Intelligent Systems,The Role of Data Science in Web Science,"Web science relies on an interdisciplinary approach that seeks to go beyond what any one subject can say about the World Wide Web. By incorporating numerous disciplinary perspectives and relying heavily on domain knowledge and expertise, data science has emerged as an important new area that integrates statistics with computational knowledge, data collection, cleaning and processing, analysis methods, and visualization to produce actionable insights from big data. As a discipline to use within Web science research, data science offers significant opportunities for uncovering trends in large Web-based datasets. A Web science observatory exemplifies this relationship by offering an online platform of tools for carrying out Web science research, allowing users to carry out data science techniques to produce insights into Web science issues such as community development, online behavior, and information propagation. The authors outline the similarities and differences of these two growing subject areas to demonstrate the important relationship developing between them.",Article,vol71
pap236,bff0d1d3a3251cb7bcbeb424ae0580c3085649f7,jou73,International Journal of System Dynamics Applications,Integrating Systems Modelling and Data Science: The Joint Future of Simulation and 'Big Data' Science,"Although System Dynamics modelling is sometimes referred to as data-poor modelling, it often is -or could be-applied in a data-rich manner. However, more can be done in the era of 'big data'. Big data refers here to situations with much more available data than was until recently manageable. The field of data science makes bigger data manageable. This paper provides a perspective on the future of System Dynamics with a prominent place for bigger data and data science. It discusses different approaches for dealing with bigger data. It reviews methods, techniques and tools for dealing with bigger data in System Dynamics, and sheds light on the modelling phases for which data science is most useful. Finally, it provides several examples of current applications in which big data, data science, and System Dynamics modelling and simulation are being merged.",Conference paper,vol73
pap237,def43235dba7eb98659fb8879fa9d27695029df2,jou74,IEEE Geoscience and Remote Sensing Magazine,Recent Activities in Earth Data Science [Technical Committees],"Recent trends on big Earth-observing (EO) data lead to some questions that the Earth science community needs to address. Are we experiencing a paradigm shift in Earth science research now? How can we better utilize the explosion of technology maturation to create new forms of EO data processing? Can we summarize the existing methodologies and technologies scaling to big EO data as a new field named earth data science? Big data technologies are being widely practiced in Earth sciences and remote sensing communities to support EO data access, processing, and knowledge discovery. The data-intensive scientific discovery, named the fourth paradigm, leads to data science in the big data era [1]. According to the definition by the U.S. National Institute of Standards and Technology, the data science paradigm is the ""extraction of actionable knowledge directly from data through a process of discovery, hypothesis, and hypothesis testing"" [2]. Earth data science is the art and science of applying the data science paradigm to EO data.",Letter,vol74
pap238,b70cfcc6bbb764728f8aa55aa173cc692eb77bdf,jou75,Frontiers in Genetics,The Process of Analyzing Data is the Emergent Feature of Data Science,"In recent years the term “data science” gained considerable attention worldwide. In a A Very Short History Of Data Science by Press (2013), the first appearance of the term is ascribed to Peter Naur in 1974 (Concise Survey of Computer Methods). Regardless who used the term first and in what context it has been used, we think that data science is a good term to indicate that data are the focus of scientific research. This is in analogy to computer science, where the first department of computer science in the USA had been established in 1962 at Purdue University, at a time when the first electronic computers became available and it was still not clear enough what computers can do, one created therefore a new field where the computer was the focus of the study. In this paper, we want to address a couple of questions in order to demystify the meaning and the goals of data science in general.",Conference paper,vol75
pap239,5ea0821f37481dafab363a47bf9b904e986f5a20,jou12,International Journal of Data Science and Analysis,Data science and analytics: a new era,,Conference paper,vol12
pap240,23a57b1e2beb4235d2020ed57f484c947e3d0816,con9,Big Data,The Quantified Self: Fundamental Disruption in Big Data Science and Biological Discovery,"A key contemporary trend emerging in big data science is the quantified self (QS)-individuals engaged in the self-tracking of any kind of biological, physical, behavioral, or environmental information as n=1 individuals or in groups. There are opportunities for big data scientists to develop new models to support QS data collection, integration, and analysis, and also to lead in defining open-access database resources and privacy standards for how personal data is used. Next-generation QS applications could include tools for rendering QS data meaningful in behavior change, establishing baselines and variability in objective metrics, applying new kinds of pattern recognition techniques, and aggregating multiple self-tracking data streams from wearable electronics, biosensors, mobile phones, genomic data, and cloud-based services. The long-term vision of QS activity is that of a systemic monitoring approach where an individual's continuous personal information climate provides real-time performance optimization suggestions. There are some potential limitations related to QS activity-barriers to widespread adoption and a critique regarding scientific soundness-but these may be overcome. One interesting aspect of QS activity is that it is fundamentally a quantitative and qualitative phenomenon since it includes both the collection of objective metrics data and the subjective experience of the impact of these data. Some of this dynamic is being explored as the quantified self is becoming the qualified self in two new ways: by applying QS methods to the tracking of qualitative phenomena such as mood, and by understanding that QS data collection is just the first step in creating qualitative feedback loops for behavior change. In the long-term future, the quantified self may become additionally transformed into the extended exoself as data quantification and self-tracking enable the development of new sense capabilities that are not possible with ordinary senses. The individual body becomes a more knowable, calculable, and administrable object through QS activity, and individuals have an increasingly intimate relationship with data as it mediates the experience of reality.",Letter,pro9
pap241,682b105746238d3c39bd4f6cd0baa375dc0c2534,con5,Technical Symposium on Computer Science Education,Perspectives on Data Science for Software Engineering,,Erratum,pro5
pap242,520515cfffcd2f439469398d7c959f8baa9ccc8b,con14,Hawaii International Conference on System Sciences,Philosophy of Big Data: Expanding the Human-Data Relation with Big Data Science Services,"Big data is growing as an area of information technology, service, and science, and so too is the need for its intellectual understanding and interpretation from a theoretical, philosophical, and societal perspective. The Philosophy of Big Data is the branch of philosophy concerned with the foundations, methods, and implications of big data, the definitions, meaning, conceptualization, knowledge possibilities, truth standards, and practices in situations involving very-large data sets that are big in volume, velocity, variety, veracity, and variability. The Philosophy of Big Data is evolving into a discipline at two levels, one internal to the field as a generalized articulation of the concepts, theory, and systems that comprise the overall conduct of big data science. The other is external to the field, as a consideration of the impact of big data science more broadly on individuals, society, and the world. Methods, tools, and concepts are evaluated at both the level of industry practice theory and social impact. Three aspects are considered: what might constitute a Philosophy of Big Data, how the disciplines of the Philosophy of Information and the Philosophy of Big Data are developing, and an example of the Philosophy of Big Data in application in the data-intensive science field of Synthetic Biology. Overall a Philosophy of Big Data might helpful in conceptualizing and realizing big data science as a service practice, and also in transitioning to data-rich futures with human and data entities more productively co-existing in mutual growth and collaboration.",Erratum,pro14
pap243,57039a11d9fb423595a4e16129f7cc7f3ff2cac7,con102,Annual Haifa Experimental Systems Conference,Data science ethics in government,"Data science can offer huge opportunities for government. With the ability to process larger and more complex datasets than ever before, it can provide better insights for policymakers and make services more tailored and efficient. As with all new technologies, there is a risk that we do not take up its opportunities and miss out on its enormous potential. We want people to feel confident to innovate with data. So, over the past 18 months, the Government Data Science Partnership has taken an open, evidence-based and user-centred approach to creating an ethical framework. It is a practical document that brings all the legal guidance together in one place, and is written in the context of new data science capabilities. As part of its development, we ran a public dialogue on data science ethics, including deliberative workshops, an experimental conjoint survey and an online engagement tool. The research supported the principles set out in the framework as well as provided useful insight into how we need to communicate about data science. It found that people had a low awareness of the term ‘data science’, but that showing data science examples can increase broad support for government exploring innovative uses of data. But people's support is highly context driven. People consider acceptability on a case-by-case basis, first thinking about the overall policy goals and likely intended outcome, and then weighing up privacy and unintended consequences. The ethical framework is a crucial start, but it does not solve all the challenges it highlights, particularly as technology is creating new challenges and opportunities every day. Continued research is needed into data minimization and anonymization, robust data models, algorithmic accountability, and transparency and data security. It also has revealed the need to set out a renewed deal between the citizen and state on data, to maintain and solidify trust in how we use people's data for social good. This article is part of the themed issue ‘The ethical impact of data science’.",Erratum,pro102
pap244,04831fedd16110da4cbd0798d16e21fbbc34ad06,jou76,International Journal of Intelligent Computing and Cybernetics,A survey of open source data science tools,"Purpose – Data science is the study of the generalizable extraction of knowledge from data. It includes a variety of components and develops on methods and concepts from many domains, containing mathematics, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization and data warehousing aiming to extract value from data. The purpose of this paper is to provide an overview of open source (OS) data science tools, proposing a classification scheme that can be used to study OS data science software. Design/methodology/approach – The proposed classification scheme is based on general characteristics, project activity, operational characteristics and data mining characteristics. The authors then use the proposed scheme to examine 70 identified Open Source Software. From this the authors provide insight about the current status of OS data science tools and reveal the state-of-the-art tools. Findings – The features of 70 OS t...",Letter,vol76
pap245,57c82a005ae353f4683938b15a52e1b0561f6e43,con12,The Compass,"R for Data Science: Import, Tidy, Transform, Visualize, and Model Data","Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. Youll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what youve learned along the way. Youll learn how to: Wrangletransform your datasets into a form convenient for analysisProgramlearn powerful R tools for solving data problems with greater clarity and easeExploreexamine your data, generate hypotheses, and quickly test themModelprovide a low-dimensional summary that captures true ""signals"" in your datasetCommunicatelearn R Markdown for integrating prose, code, and results",Erratum,pro12
pap246,dd153ebd44a07dde2259c22d43bb9cd18db44d2a,con9,Big Data,Modelling and Simulation in Materials Science and Engineering Visualization and analysis of atomistic simulation data with OVITO – the Open Visualization Tool,"The Open Visualization Tool (OVITO) is a new 3D visualization software designed for post-processing atomistic data obtained from molecular dynamics or Monte Carlo simulations. Unique analysis, editing and animations functions are integrated into its easy-to-use graphical user interface. The software is written in object-oriented C++, controllable via Python scripts and easily extendable through a plug-in interface. It is distributed as open-source software and can be downloaded from the website http://ovito.sourceforge.net/. (Some figures in this article are in colour only in the electronic version)",Erratum,pro9
pap247,1a95f1e8ff32488f228a25764af64531cb758ff0,con104,Biometrics and Identity Management,Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters,,Erratum,pro104
pap248,6f989651c4f592613e92c9e37a8c4ac205998cfe,con89,Conference on Uncertainty in Artificial Intelligence,Data Science in Statistics Curricula: Preparing Students to “Think with Data”,"A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]",Erratum,pro89
pap249,9ba08d45d60130c7e5880f63a980b185a86e177c,con9,Big Data,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.",Letter,pro9
pap250,92efba7c622f54b8cd7b0d70d7cc09063e17b4f3,con5,Technical Symposium on Computer Science Education,An undergraduate degree in data science: curriculum and a decade of implementation experience,"We describe Data Science, a four-year undergraduate program in predictive analytics, machine learning, and data mining implemented at the College of Charleston, Charleston, South Carolina, USA. We present a ten-year status report detailing the program's origins, successes, and challenges. Our experience demonstrates that education and training for big data concepts are possible and practical at the undergraduate level. The development of this program parallels the growing demand for finding utility in data sets and streaming data. The curriculum is a seventy-seven credit-hour program that has been successfully implemented in a liberal arts and sciences institution by the faculties of computer science and mathematics.",Letter,pro5
pap251,c7d7d579d94b7fc67c75b68361e01ba8f59b1d40,jou77,Future generations computer systems,Intelligent services for Big Data science,,Article,vol77
pap252,0040c830969302a8c88c0c083aee5051e405bfe5,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"Big Data, Big Problems: Emerging Issues in the Ethics of Data Science and Journalism","As big data techniques become widespread in journalism, both as the subject of reporting and as newsgathering tools, the ethics of data science must inform and be informed by media ethics. This article explores emerging problems in ethical research using big data techniques. It does so using the duty-based framework advanced by W.D. Ross, who has significantly influenced both research science and media ethics. A successful framework must provide stability and flexibility. Without stability, ethical precommitments will vanish as technology rapidly shifts costs. Without flexibility, traditional approaches will rapidly become obsolete in the face of technological change. The article concludes that Ross's duty-based approach both provides stability in the face of rapid technological change and flexibility to innovate to achieve the original purpose of basic ethical principles.",Erratum,pro98
pap253,516a53c59a53b0a471cd8a277b229925e0582114,con4,Conference on Innovative Data Systems Research,DataHub: Collaborative Data Science & Dataset Version Management at Scale,"Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.",Article,pro4
pap254,f707f6d7c3f874cb1a8aa961a50e50706731cd2d,con20,ACM Conference on Economics and Computation,Mechanism design for data science,"The promise of data science is that if data from a system can be recorded and understood then this understanding can potentially be utilized to improve the system. Behavioral and economic data, however, is different from scientific data in that it is subjective to the system. Behavior changes when the system changes, and to predict behavior for any given system change or to optimize over system changes, the behavioral model that generates the data must be inferred from the data. The ease with which this inference can be performed generally also depends on the system. Trivially, a system that ignores behavior does not admit any inference of a behavior generating model that can be used to predict behavior in a system that is responsive to behavior. To realize the promise of data science in economic systems, a theory for the design of such systems must also incorporate the desired inference properties. Consider as an example the revenue-maximizing auctioneer. If the auctioneer has knowledge of the distribution of bidder values then she can run the first-price auction with a reserve price that is tuned to the distribution. Under some mild distributional assumptions, with the appropriate reserve price the first-price auction is revenue optimal [Myerson 1981]. Notice that the historical bid data for the first-price auction with a reserve price will in most cases not have bids for bidders whose values are below the reserve. Therefore, there is no data analysis that the auctioneer can perform that will enable properties of the distribution of bidder values below the reserve price to be inferred. It could be, nonetheless, that over time the population of potential bidders evolves and the optimal reserve price lowers. This change could go completely unnoticed in the auctioneer's data. The two main tools for optimizing revenue in an auction are reserve prices (as above) and ironing. Both of these tools cause pooling behavior (i.e., bidders with distinct values take the same action) and economic inference cannot thereafter differentiate these pooled bidders. In order to maintain the distributional knowledge necessary to be able to run a good auction in the long term, the auctioneer must sacrifice the short-term revenue by running a non-revenue-optimal auction.",Letter,pro20
pap255,b9111489ec08b50bc573982ede11f5bc2d7a4e88,con111,International Conference on Image Analysis and Processing,Sjplot - Data Visualization For Statistics In Social Science.,"New functions


 tab_model() as replacement for sjt.lm() , sjt.glm() , sjt.lmer() and sjt.glmer() . Furthermore, tab_model() is designed to work with the same model-objects as plot_model() .
 New colour scales for ggplot-objects: scale_fill_sjplot() and scale_color_sjplot() . These provide predifined colour palettes from this package.
 show_sjplot_pals() to show all predefined colour palettes provided by this package.
 sjplot_pal() to return colour values of a specific palette.


Deprecated

Following functions are now deprecated:


 sjp.lm() , sjp.glm() , sjp.lmer() , sjp.glmer() and sjp.int() . Please use plot_model() instead.
 sjt.frq() . Please use sjmisc::frq(out = ""v"") instead.


Removed / Defunct

Following functions are now defunct:


 sjt.grpmean() , sjt.mwu() and sjt.df() . The replacements are sjstats::grpmean() , sjstats::mwu() and tab_df() resp. tab_dfs() .


Changes to functions


 plot_model() and plot_models() get a prefix.labels -argument, to prefix automatically retrieved term labels with either the related variable name or label.
 plot_model() gets a show.zeroinf -argument to show or hide the zero-inflation-part of models in the plot.
 plot_model() gets a jitter -argument to add some random variation to data points for those plot types that accept show.data = TRUE .
 plot_model() gets a legend.title -argument to define the legend title for plots that display a legend.
 plot_model() now passes more arguments in ... down to ggeffects::plot() for marginal effects plots.
 plot_model() now plots the zero-inflated part of the model for brmsfit -objects.
 plot_model() now plots multivariate response models, i.e. models with multiple outcomes.
 Diagnostic plots in plot_model() ( type = ""diag"" ) can now also be used with brmsfit -objects.
 Axis limits of diagnostic plots in plot_model() ( type = ""diag"" ) for Stan-models ( brmsfit or stanreg resp. stanfit ) can now be set with the axis.lim -argument.
 The grid.breaks -argument for plot_model() and plot_models() now also takes a vector of values to directly define the grid breaks for the plot.
 Better default calculation for grid breaks in plot_model() and plot_models() when the grid.breaks -argument is of length one.
 The terms -argument for plot_model() now also allows the specification of a range of numeric values in square brackets for marginal effects plots, e.g. terms = ""age [30:50]"" or terms = ""age [pretty]"" .
 For coefficient-plots, the terms - and rm.terms -arguments for plot_model() now also allows specification of factor levels for categorical terms. Coefficients for the indicted factor levels are kept resp. removed (see ?plot_model for details).
 plot_model() now supports clmm -objects (package ordinal).
 plot_model(type = ""diag"") now also shows random-effects QQ-plots for glmmTMB -models, and also plots random-effects QQ-plots for all random effects (if model has more than one random effect term).


Bug fixes


 plot_model(type = ""re"") now supports standard errors and confidence intervals for glmmTMB -objects.
 Fixed typo for glmmTMB -tidier, which may have returned wrong data for zero-inflation part of model.
 Multiple random intercepts for multilevel models fitted with brms area now shown in each own facet per intercept.
 Remove unnecessary warning in sjp.likert() for uneven category count when neutral category is specified.
 plot_model(type = ""int"") could not automatically select mdrt.values properly for non-integer variables.
 sjp.grpfrq() now correctly uses the complete space in facets when facet.grid = TRUE .
 sjp.grpfrq(type = ""boxplot"") did not correctly label the x-axis when one category had no elements in a vector.
 Problems with German umlauts when printing HTML tables were fixed.",Erratum,pro111
pap256,9d653160d048eecf1a8138407994bfc69952324b,con108,International Conference on Information Integration and Web-based Applications & Services,Practical Data Science with R,"Summary Practical Data Science with R lives up to its name. It explains basic principles without the theoretical mumbo-jumbo and jumps right to the real use cases you'll face as you collect, curate, and analyze the data crucial to the success of your business. You'll apply the R programming language and statistical analysis techniques to carefully explained examples based in marketing, business intelligence, and decision support. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Book Business analysts and developers are increasingly collecting, curating, analyzing, and reporting on crucial business data. The R language and its associated tools provide a straightforward way to tackle day-to-day data science tasks without a lot of academic theory or advanced mathematics. Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed. What's Inside Data science for the business professional Statistical analysis using the R language Project lifecycle, from planning to delivery Numerous instantly familiar use cases Keys to effective data presentations About the Authors Nina Zumel and John Mount are cofounders of a San Francisco-based data science consulting firm. Both hold PhDs from Carnegie Mellon and blog on statistics, probability, and computer science at win-vector.com.",Erratum,pro108
pap257,0e341b2a181f71dea088dbba800e70262f91a79e,con7,International Symposium on Intelligent Data Analysis,"Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition",Physical Data. The Eye. Colorimetry. Photometry. Visual Equivalence and Visual Matching. Uniform Color Scales. Visual Thresholds. Theories and Models of Color Vision. Appendix. References. Author and Subject Indexes.,Erratum,pro7
pap258,5a92ccd20e551c191ff19bdd8e75bf1b64faa54b,jou78,College and Research Libraries,Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions,"As long as empirical research has existed, researchers have been doing “data management” in one form or another. However, funding agency mandates for doing formal data management are relatively recent, and academic libraries’ involvement has been concentrated mainly in the last few years. The National Science Foundation implemented a new mandate in January 2011, requiring researchers to include a data management plan with their proposals for funding. This has prompted many academic libraries to work more actively than before in data management, and science librarians in particular are uniquely poised to step into new roles to meet researchers’ data management needs. This study, a survey of science librarians at institutions affiliated with the Association of Research Libraries, investigates science librarians’ awareness of and involvement in institutional repositories, data repositories, and data management support services at their institutions. The study also explores the roles and responsibilities, both new and traditional, that science librarians have assumed related to data management, and the skills that science librarians believe are necessary to meet the demands of data management work. The results reveal themes of both uncertainty and optimism—uncertainty about the roles of librarians, libraries, and other campus entities; uncertainty about the skills that will be required; but also optimism about applying “traditional” librarian skills to this emerging field of academic librarianship.",Conference paper,vol78
pap259,0a7dd279ee312c9ef9c6fe04cd6f4f5e974abae3,con12,The Compass,A Data Science Solution for Mining Interesting Patterns from Uncertain Big Data,"Nowadays, high volumes of valuable uncertain data can be easily collected or generated at high velocity in many real-life applications. Mining these uncertain Big data is computationally intensive due to the presence of existential probability values associated with items in every transaction in the uncertain data. Each existential probability value expresses the likelihood of that item to be present in a particular transaction in the Big data. In some situations, users may be interested in mining all frequent patterns from these uncertain Big data, in other situations, users may be interested in only a tiny portion of these mined patterns. To reduce the computation and to focus the mining for the latter situations, we propose a tree-based algorithm that (i) allows users to express the patterns to be mined according to their intention via the use of constraints and (ii) uses MapReduce to mine uncertain Big data for only those frequent patterns that satisfy user-specified constraints. Experimental results show the effectiveness of our algorithm in mining interesting patterns from uncertain Big data.",Erratum,pro12
pap260,92b68d5a59262971d0f4a563c6abe1be6f2dab56,con61,International Conference on Predictive Models in Software Engineering,Data Science,,Erratum,pro61
pap261,5bbb90ae23803b8bb115d5d7f60c8defc5376e2a,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication","Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.
 In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.",Article,pro21
pap262,3d7fcd1399573fb5cb455de6f85f149e0ab53828,con9,Big Data,The Science of Data Science,,Conference paper,pro9
pap263,1163c2996dfd0a46639b094e34ad783e969a0692,con106,International Conference on Mobile Data Management,Data science and prediction,Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.,Erratum,pro106
pap264,3858f600d0187c28f381b034a70226213e82a54e,jou79,Nature Reviews Methods Primers,Network analysis of multivariate data in psychological science,,Conference paper,vol79
pap265,f3eda875e14bf933759f3b777131a4a9973537b4,con56,International Conference on Software Engineering and Knowledge Engineering,"Data-driven science and engineering: machine learning, dynamical systems, and control",,Erratum,pro56
pap266,3402835f33e3e1342eb86b4d13907e3c9121c82b,con9,Big Data,Data science for business,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You'll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your company's data science projects. You'll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making. Understand how data science fits in your organization - and how you can use it for competitive advantage Treat data as a business asset that requires careful investment if you're to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",Erratum,pro9
pap267,c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,con76,IEEE International Conference on Tools with Artificial Intelligence,Doing Data Science: Straight Talk from the Frontline,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field thats so clouded in hype? This insightful book, based on Columbia Universitys Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If youre familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include:Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy ONeil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",Erratum,pro76
pap268,0636653b82e152ba99b1d921b0aa2798aa845d1e,jou80,Quantitative Science Studies,"Scopus as a curated, high-quality bibliometric data source for academic research in quantitative science studies","Abstract Scopus is among the largest curated abstract and citation databases, with a wide global and regional coverage of scientific journals, conference proceedings, and books, while ensuring only the highest quality data are indexed through rigorous content selection and re-evaluation by an independent Content Selection and Advisory Board. Additionally, extensive quality assurance processes continuously monitor and improve all data elements in Scopus. Besides enriched metadata records of scientific articles, Scopus offers comprehensive author and institution profiles, obtained from advanced profiling algorithms and manual curation, ensuring high precision and recall. The trustworthiness of Scopus has led to its use as bibliometric data source for large-scale analyses in research assessments, research landscape studies, science policy evaluations, and university rankings. Scopus data have been offered for free for selected studies by the academic research community, such as through application programming interfaces, which have led to many publications employing Scopus data to investigate topics such as researcher mobility, network visualizations, and spatial bibliometrics. In June 2019, the International Center for the Study of Research was launched, with an advisory board consisting of bibliometricians, aiming to work with the scientometric research community and offering a virtual laboratory where researchers will be able to utilize Scopus data.",Conference paper,vol80
pap269,cbf6a6d8fff87b74f36c5e4ede09f55e7a71506c,con76,IEEE International Conference on Tools with Artificial Intelligence,Numerical data and functional relationships in science and technology,,Erratum,pro76
pap270,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,jou81,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,,Letter,vol81
pap271,e9931ea8ae9b8db38b519ef9ae32ec41a06d8445,con74,IEEE International Conference on Information Reuse and Integration,Doing Data Science,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field that's so clouded in hype? This insightful book, based on Columbia University's Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If you're familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include: Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy O'Neil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",Erratum,pro74
pap272,379e9576dea9690cf88d9132287edbefb7626232,con26,Decision Support Systems,Data Smart: Using Data Science to Transform Information into Insight,"Data Science gets thrown around in the press like it's magic. Major retailers are predicting everything from when their customers are pregnant to when they want a new pair of Chuck Taylors. It's a brave new world where seemingly meaningless data can be transformed into valuable insight to drive smart business decisions.But how does one exactly do data science? Do you have to hire one of these priests of the dark arts, the ""data scientist,"" to extract this gold from your data? Nope.Data science is little more than using straight-forward steps to process raw data into actionable insight. And inData Smart, author and data scientist John Foreman will show you how that's done within the familiar environment of a spreadsheet.",Erratum,pro26
pap273,6a324214a73610d8819e004e7ebd7dd23107d1f8,jou59,Nature,Computing: A vision for data science,,Letter,vol59
pap274,010a8ed71c6a80c2c02c7f55e1718151f91ff35a,jou80,Quantitative Science Studies,Web of Science as a data source for research on scientific and scholarly activity,"Abstract Web of Science (WoS) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by WoS from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information (ISI) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.",Letter,vol80
pap275,e084f4021f30c483564dcccc29d1230ab213ce70,jou80,Quantitative Science Studies,"Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic","We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.",Article,vol80
pap276,30c9e3fcb1ead2a827f91ff5cd203aa0d8058bff,con95,IEEE International Conference on Computer Vision,Data-Driven Science and Engineering,,Erratum,pro95
pap277,425744cb05e854071d06af0da2b8ef2d677f33d5,jou82,EOS,Harnessing the GPS Data Explosion for Interdisciplinary Science,"More GPS stations, faster data delivery, and better data processing provide an abundance of information for all kinds of Earth scientists.",Article,vol82
pap278,6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,jou22,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",Article,vol22
pap279,b1691f718aedc8e6b31105c50773761c753676f4,jou83,Women in Sport & Physical Activity Journal,“Invisible Sportswomen”: The Sex Data Gap in Sport and Exercise Science Research,"This study aimed to conduct an updated exploration of the ratio of male and female participants in sport and exercise science research. Publications involving humans were examined from The European Journal of Sports Science, Medicine & Science in Sport & Exercise, The Journal of Sport Science & Medicine, The Journal of Physiology, The American Journal of Sports Medicine, and The British Journal of Sports Medicine, 2014–2020. The total number of participants, the number of male and female participants, the title, and the topic, were recorded for each publication. Data were expressed in frequencies and percentages. Chi-square analyses were used to assess the differences in frequencies in each of the journals. About 5,261 publications and 12,511,386 participants were included in the analyses. Sixty-three percentage of publications included both males and females, 31% included males only, and 6% included females only (p < .0001). When analyzing participants included in all journals, a total of 8,253,236 (66%) were male and 4,254,445 (34%) were female (p < .0001). Females remain significantly underrepresented within sport and exercise science research. Therefore, at present most conclusions made from sport and exercise science research might only be applicable to one sex. As such, researchers and practitioners should be aware of the ongoing sex data gap within the current literature, and future research should address this.",Article,vol83
pap280,1110da1c238a7b09258136e7a2e7d558fb16f272,jou84,Nuclear Data Sheets,TENDL: Complete Nuclear Data Library for Innovative Nuclear Science and Technology,,Letter,vol84
pap281,abc0a9eb3ae901ece2f532f504c336fbb6ba81ca,jou85,Advancement of science,"Data‐Driven Materials Science: Status, Challenges, and Perspectives","Data‐driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning—typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high‐throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data‐driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data‐driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",Conference paper,vol85
pap282,e60d9464935582cda41becd7c1455c09392a2a93,jou86,Psychological Science in the Public Interest,The Science of Visual Data Communication: What Works,"Effectively designed data visualizations allow viewers to use their powerful visual systems to understand patterns in data across science, education, health, and public policy. But ineffectively designed visualizations can cause confusion, misunderstanding, or even distrust—especially among viewers with low graphical literacy. We review research-backed guidelines for creating effective and intuitive visualizations oriented toward communicating data to students, coworkers, and the general public. We describe how the visual system can quickly extract broad statistics from a display, whereas poorly designed displays can lead to misperceptions and illusions. Extracting global statistics is fast, but comparing between subsets of values is slow. Effective graphics avoid taxing working memory, guide attention, and respect familiar conventions. Data visualizations can play a critical role in teaching and communication, provided that designers tailor those visualizations to their audience.",Letter,vol86
pap283,ee5825861645ec9b9d11a2882f3aa15ec9e6e4dd,con97,ACM SIGMOD Conference,"ENDF/B-VII.1 Nuclear Data for Science and Technology: Cross Sections, Covariances, Fission Product Yields and Decay Data",,Erratum,pro97
pap284,40de1c316a7f4de8e547a717c905013642378996,jou87,Frontiers in Climate,The Critical Importance of Citizen Science Data,"Citizen science is an important vehicle for democratizing science and promoting the goal of universal and equitable access to scientific data and information. Data generated by citizen science groups have become an increasingly important source for scientists, applied users and those pursuing the 2030 Agenda for Sustainable Development. Citizen science data are used extensively in studies of biodiversity and pollution; crowdsourced data are being used by UN operational agencies for humanitarian activities; and citizen scientists are providing data relevant to monitoring the sustainable development goals (SDGs). This article provides an International Science Council (ISC) perspective on citizen science data generating activities in support of the 2030 Agenda and on needed improvements to the citizen science community's data stewardship practices for the benefit of science and society by presenting results of research undertaken by an ISC-sponsored Task Group.",Conference paper,vol87
pap285,ee013b1477e8f81cb5c66a9a93a342281f740042,jou88,bioRxiv,Assessing data quality in citizen science (preprint),"Ecological and environmental citizen science projects have enormous potential to advance science, influence policy, and guide resource management by producing datasets that are otherwise infeasible to generate. This potential can only be realized, though, if the datasets are of high quality. While scientists are often skeptical of the ability of unpaid volunteers to produce accurate datasets, a growing body of publications clearly shows that diverse types of citizen science projects can produce data with accuracy equal to or surpassing that of professionals. Successful projects rely on a suite of methods to boost data accuracy and account for bias, including iterative project development, volunteer training and testing, expert validation, replication across volunteers, and statistical modeling of systematic error. Each citizen science dataset should therefore be judged individually, according to project design and application, rather than assumed to be substandard simply because volunteers generated it.",Article,vol88
pap286,e257edf34abd9a191fea1023a423abb497cca70f,con110,Very Large Data Bases Conference,The data science education dilemma,"The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter- disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.",Erratum,pro110
pap287,8e981ddb4877615f7d5f944a8d64789d1388ee87,jou89,Astrophysical Journal,LSST: From Science Drivers to Reference Design and Anticipated Data Products,"We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320–1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.",Conference paper,vol89
pap288,00a4bdc5158945a0b9463a29da4810838e474875,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",Erratum,pro98
pap289,7ec947261f5a3eabdaddb8e53d58a36b986c4e71,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,ENDF/B-VII.0: Next Generation Evaluated Nuclear Data Library for Nuclear Science and Technology,,Erratum,pro13
pap290,4672c6cf14ce2e49cc39f9361b9e73d9442361c5,con25,IEEE International Parallel and Distributed Processing Symposium,Data Quality in Citizen Science,,Erratum,pro25
pap291,7281fb2c44f4d73c86ffefd1fde7e4f8a1f5e75c,jou90,Science Education,Engagement in science through citizen science: Moving beyond data collection,"""To date, most studies of citizen science engagement focus on quantifiable measures related to the contribution of data or other output measures. Few studies have attempted to qualitatively characterize citizen science engagement across multiple projects and from the perspective of the participants. Building on pertinent literature and sociocultural learning theories, this study operationalizes engagement in citizen science through an analysis of interviews of 72 participants from six different environmentally based projects. We document engagement in citizen science through an examination of cognitive, affective, social, behavioral, and motivational dimensions. We assert that engagement in citizen science is enhanced by acknowledging these multiple dimensions and creating opportunities for volunteers to find personal relevance in their work with scientists. A Dimensions of Engagement framework is presented that can facilitate the innovation of new questions and methodologies for studying engagement in citizen science and other forms of informal science education.""",Conference paper,vol90
pap292,81e1dbe1e8152d0ddbf89e861468d799dbebe367,jou91,Biological Conservation,Social media data for conservation science: A methodological overview,,Conference paper,vol91
pap293,40b2324cde863db7670178f0151fae400a9a2b93,jou92,American Political Science Review,Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation,"We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that “multiple imputation” is a superior approach to the problem of missing data scattered through one’s explanatory and dependent variables than the methods currently used in applied data analysis. The discrepancy occurs because the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and have demanded considerable expertise. We adapt an algorithm and use it to implement a general-purpose, multiple imputation model for missing data. This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature. We also quantify the risks of current missing data practices, illustrate how to use the new procedure, and evaluate this alternative through simulated data as well as actual empirical examples. Finally, we offer easy-to-use software that implements all methods discussed.",Letter,vol92
pap294,ca5e7580993170b1fe621bc16383ad2dfa6803b5,jou93,Comprehensive Reviews in Food Science and Food Safety,Utilization of text mining as a big data analysis tool for food science and nutrition.,"Big data analysis has found applications in many industries due to its ability to turn huge amounts of data into insights for informed business and operational decisions. Advanced data mining techniques have been applied in many sectors of supply chains in the food industry. However, the previous work has mainly focused on the analysis of instrument-generated data such as those from hyperspectral imaging, spectroscopy, and biometric receptors. The importance of digital text data in the food and nutrition has only recently gained attention due to advancements in big data analytics. The purpose of this review is to provide an overview of the data sources, computational methods, and applications of text data in the food industry. Text mining techniques such as word-level analysis (e.g., frequency analysis), word association analysis (e.g., network analysis), and advanced techniques (e.g., text classification, text clustering, topic modeling, information retrieval, and sentiment analysis) will be discussed. Applications of text data analysis will be illustrated with respect to food safety and food fraud surveillance, dietary pattern characterization, consumer-opinion mining, new-product development, food knowledge discovery, food supply-chain management, and online food services. The goal is to provide insights for intelligent decision-making to improve food production, food safety, and human nutrition.",Conference paper,vol93
pap295,adc22a722f2ad1d972de507779041e340f20a6a2,jou65,Data Science Journal,Kadi4Mat: A Research Data Infrastructure for Materials Science,"The concepts and current developments of a research data infrastructure for materials science are presented, extending and combining the features of an electronic lab notebook and a repository. The objective of this infrastructure is to incorporate the possibility of structured data storage and data exchange with documented and reproducible data analysis and visualization, which finally leads to the publication of the data. This way, researchers can be supported throughout the entire research process. The software is being developed as a web-based and desktop-based system, offering both a graphical user interface and a programmatic interface. The focus of the development is on the integration of technologies and systems based on both established as well as new concepts. Due to the heterogeneous nature of materials science data, the current features are kept mostly generic, and the structuring of the data is largely left to the users. As a result, an extension of the research data infrastructure to other disciplines is possible in the future. The source code of the project is publicly available under a permissive Apache 2.0 license.",Conference paper,vol65
pap296,4449bd1f5bb8f86fec9ca7ded29ac8bf322c0114,jou94,Ecology Letters,"AVONET: morphological, ecological and geographical data for all birds.","Functional traits offer a rich quantitative framework for developing and testing theories in evolutionary biology, ecology and ecosystem science. However, the potential of functional traits to drive theoretical advances and refine models of global change can only be fully realised when species-level information is complete. Here we present the AVONET dataset containing comprehensive functional trait data for all birds, including six ecological variables, 11 continuous morphological traits, and information on range size and location. Raw morphological measurements are presented from 90,020 individuals of 11,009 extant bird species sampled from 181 countries. These data are also summarised as species averages in three taxonomic formats, allowing integration with a global phylogeny, geographical range maps, IUCN Red List data and the eBird citizen science database. The AVONET dataset provides the most detailed picture of continuous trait variation for any major radiation of organisms, offering a global template for testing hypotheses and exploring the evolutionary origins, structure and functioning of biodiversity.",Letter,vol94
pap297,fff51615943e08d05080682009c9c656321ef0b2,jou95,MRS bulletin,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",Conference paper,vol95
pap298,3f809897b51d824846cf5a56f2a7b4292f7bc4a4,jou59,Nature,Materials science: Share corrosion data,,Letter,vol59
pap299,0e00f0dbfc381661826f8ddbafe73e33bcfe040f,jou47,BioScience,Using Semistructured Surveys to Improve Citizen Science Data for Monitoring Biodiversity,"Abstract Biodiversity is being lost at an unprecedented rate, and monitoring is crucial for understanding the causal drivers and assessing solutions. Most biodiversity monitoring data are collected by volunteers through citizen science projects, and often crucial information is lacking to account for the inevitable biases that observers introduce during data collection. We contend that citizen science projects intended to support biodiversity monitoring must gather information about the observation process as well as species occurrence. We illustrate this using eBird, a global citizen science project that collects information on bird occurrences as well as vital contextual information on the observation process while maintaining broad participation. Our fundamental argument is that regardless of what species are being monitored, when citizen science projects collect a small set of basic information about how participants make their observations, the scientific value of the data collected will be dramatically improved.",Article,vol47
pap300,e27acaf97f5b2eae4257bb5d8278fbe0e6405c39,con22,Grid Computing Environments,Creating the CIPRES Science Gateway for inference of large phylogenetic trees,"Understanding the evolutionary history of living organisms is a central problem in biology. Until recently the ability to infer evolutionary relationships was limited by the amount of DNA sequence data available, but new DNA sequencing technologies have largely removed this limitation. As a result, DNA sequence data are readily available or obtainable for a wide spectrum of organisms, thus creating an unprecedented opportunity to explore evolutionary relationships broadly and deeply across the Tree of Life. Unfortunately, the algorithms used to infer evolutionary relationships are NP-hard, so the dramatic increase in available DNA sequence data has created a commensurate increase in the need for access to powerful computational resources. Local laptop or desktop machines are no longer viable for analysis of the larger data sets available today, and progress in the field relies upon access to large, scalable high-performance computing resources. This paper describes development of the CIPRES Science Gateway, a web portal designed to provide researchers with transparent access to the fastest available community codes for inference of phylogenetic relationships, and implementation of these codes on scalable computational resources. Meeting the needs of the community has included developing infrastructure to provide access, working with the community to improve existing community codes, developing infrastructure to insure the portal is scalable to the entire systematics community, and adopting strategies that make the project sustainable by the community. The CIPRES Science Gateway has allowed more than 1800 unique users to run jobs that required 2.5 million Service Units since its release in December 2009. (A Service Unit is a CPU-hour at unit priority).",Letter,pro22
pap301,8a9f26a4cee210e51c96f4016737605e31d490ee,jou96,MRS Communications,A data ecosystem to support machine learning in materials science,"Facilitating the application of machine learning (ML) to materials science problems requires enhancing the data ecosystem to enable discovery and collection of data from many sources, automated dissemination of new data across the ecosystem, and the connecting of data with materials-specific ML models. Here, we present two projects, the Materials Data Facility (MDF) and the Data and Learning Hub for Science (DLHub), that address these needs. We use examples to show how MDF and DLHub capabilities can be leveraged to link data with ML models and how users can access those capabilities through web and programmatic interfaces.",Conference paper,vol96
pap302,3fe3924a5315fbb5b5cd0edf98533b8c61a3bbdf,jou97,ICES Journal of Marine Science,Machine intelligence and the data-driven future of marine science,"
 Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.",Conference paper,vol97
pap303,652c77a90d84df639622efdc9cd7475e96a248c9,jou98,Comptes rendus. Mecanique,Data-driven modeling and learning in science and engineering,,Article,vol98
pap304,22737046fbbe822deaaffddddb8f16be076d3f95,con14,Hawaii International Conference on System Sciences,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",Erratum,pro14
pap305,4a6e74d4bf4fd0106891e5518692a77c7aa8811d,jou99,Regular Issue,Outlier Detection in High Dimensional Data,"Artificial intelligence (AI) is the science that allows
computers to replicate human intelligence in areas such as
decision-making, text processing, visual perception. Artificial
Intelligence is the broader field that contains several subfields
such as machine learning, robotics, and computer vision.
Machine Learning is a branch of Artificial Intelligence that
allows a machine to learn and improve at a task over time. Deep
Learning is a subset of machine learning that makes use of deep
artificial neural networks for training. The paper proposed on
outlier detection for multivariate high dimensional data for
Autoencoder unsupervised model.",Conference paper,vol99
pap306,6a697a4b3bdbbfb7681d9f9a518fc0be73744037,jou100,Physical Review Letters,Big data of materials science: critical role of the descriptor.,"Statistical learning of materials properties or functions so far starts with a largely silent, nonchallenged step: the choice of the set of descriptive parameters (termed descriptor). However, when the scientific connection between the descriptor and the actuating mechanisms is unclear, the causality of the learned descriptor-property relation is uncertain. Thus, a trustful prediction of new promising materials, identification of anomalies, and scientific advancement are doubtful. We analyze this issue and define requirements for a suitable descriptor. For a classic example, the energy difference of zinc blende or wurtzite and rocksalt semiconductors, we demonstrate how a meaningful descriptor can be found systematically.",Conference paper,vol100
pap307,4e2f43dab69d690dc86422949e410ebf37f522d4,con81,International Conference on Learning Representations,Bayesian data analysis.,"Bayesian methods have garnered huge interest in cognitive science as an approach to models of cognition and perception. On the other hand, Bayesian methods for data analysis have not yet made much headway in cognitive science against the institutionalized inertia of 20th century null hypothesis significance testing (NHST). Ironically, specific Bayesian models of cognition and perception may not long endure the ravages of empirical verification, but generic Bayesian methods for data analysis will eventually dominate. It is time that Bayesian data analysis became the norm for empirical methods in cognitive science. This article reviews a fatal flaw of NHST and introduces the reader to some benefits of Bayesian data analysis. The article presents illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power. Copyright © 2010 John Wiley & Sons, Ltd. For further resources related to this article, please visit the WIREs website.",Erratum,pro81
pap308,f4c01d8780c86abdcfdd52c60843a2499fd5c1b6,jou101,Perspectives on Psychological Science,Using Smartphones to Collect Behavioral Data in Psychological Science,"Smartphones now offer the promise of collecting behavioral data unobtrusively, in situ, as it unfolds in the course of daily life. Data can be collected from the onboard sensors and other phone logs embedded in today’s off-the-shelf smartphone devices. These data permit fine-grained, continuous collection of people’s social interactions (e.g., speaking rates in conversation, size of social groups, calls, and text messages), daily activities (e.g., physical activity and sleep), and mobility patterns (e.g., frequency and duration of time spent at various locations). In this article, we have drawn on the lessons from the first wave of smartphone-sensing research to highlight areas of opportunity for psychological research, present practical considerations for designing smartphone studies, and discuss the ongoing methodological and ethical challenges associated with research in this domain. It is our hope that these practical guidelines will facilitate the use of smartphones as a behavioral observation tool in psychological science.",Article,vol101
pap309,0db731c99879bb74c3850c53923d1df2c510f8c3,jou102,IEEE Transactions on Geoscience and Remote Sensing,"AIRS/AMSU/HSB on the Aqua mission: design, science objectives, data products, and processing systems","The Atmospheric Infrared Sounder (AIRS), the Advanced Microwave Sounding Unit (AMSU), and the Humidity Sounder for Brazil (HSB) form an integrated cross-track scanning temperature and humidity sounding system on the Aqua satellite of the Earth Observing System (EOS). AIRS is an infrared spectrometer/radiometer that covers the 3.7-15.4-/spl mu/m spectral range with 2378 spectral channels. AMSU is a 15-channel microwave radiometer operating between 23 and 89 GHz. HSB is a four-channel microwave radiometer that makes measurements between 150 and 190 GHz. In addition to supporting the National Aeronautics and Space Administration's interest in process study and climate research, AIRS is the first hyperspectral infrared radiometer designed to support the operational requirements for medium-range weather forecasting of the National Ocean and Atmospheric Administration's National Centers for Environmental Prediction (NCEP) and other numerical weather forecasting centers. AIRS, together with the AMSU and HSB microwave radiometers, will achieve global retrieval accuracy of better than 1 K in the lower troposphere under clear and partly cloudy conditions. This paper presents an overview of the science objectives, AIRS/AMSU/HSB data products, retrieval algorithms, and the ground-data processing concepts. The EOS Aqua was launched on May 4, 2002 from Vandenberg AFB, CA, into a 705-km-high, sun-synchronous orbit. Based on the excellent radiometric and spectral performance demonstrated by AIRS during prelaunch testing, which has by now been verified during on-orbit testing, we expect the assimilation of AIRS data into the numerical weather forecast to result in significant forecast range and reliability improvements.",Letter,vol102
pap310,4436ca7e9f91b7ad9ad6a09dbe12f48d9f6c3e7f,jou103,Science,Data-driven predictions in the science of science,"The desire to predict discoveries—to have some idea, in advance, of what will be discovered, by whom, when, and where—pervades nearly all aspects of modern science, from individual scientists to publishers, from funding agencies to hiring committees. In this Essay, we survey the emerging and interdisciplinary field of the “science of science” and what it teaches us about the predictability of scientific discovery. We then discuss future opportunities for improving predictions derived from the science of science and its potential impact, positive and negative, on the scientific community.",Letter,vol103
pap311,2ff6d7e05b1f74e0b17dbf97a59ac0d75ef65efc,jou104,Data Intelligence,FAIR Data and Services in Biodiversity Science and Geoscience,"We examine the intersection of the FAIR principles (Findable, Accessible, Interoperable and Reusable), the challenges and opportunities presented by the aggregation of widely distributed and heterogeneous data about biological and geological specimens, and the use of the Digital Object Architecture (DOA) data model and components as an approach to solving those challenges that offers adherence to the FAIR principles as an integral characteristic. This approach will be prototyped in the Distributed System of Scientific Collections (DiSSCo) project, the pan-European Research Infrastructure which aims to unify over 110 natural science collections across 21 countries. We take each of the FAIR principles, discuss them as requirements in the creation of a seamless virtual collection of bio/geo specimen data, and map those requirements to Digital Object components and facilities such as persistent identification, extended data typing, and the use of an additional level of abstraction to normalize existing heterogeneous data structures. The FAIR principles inform and motivate the work and the DO Architecture provides the technical vision to create the seamless virtual collection vitally needed to address scientific questions of societal importance.",Conference paper,vol104
pap312,d1db1c83c64f556fd4005cc12bddf7963f82a77f,jou18,Scientific Data,Open science resources for the discovery and analysis of Tara Oceans data,,Conference paper,vol18
pap313,bd8a307efcffbf57d2e5c3c23577de44d883d865,con23,International Conference on Open and Big Data,MedRec: Using Blockchain for Medical Data Access and Permission Management,"Years of heavy regulation and bureaucratic inefficiency have slowed innovation for electronic medical records (EMRs). We now face a critical need for such innovation, as personalization and data science prompt patients to engage in the details of their healthcare and restore agency over their medical data. In this paper, we propose MedRec: a novel, decentralized record management system to handle EMRs, using blockchain technology. Our system gives patients a comprehensive, immutable log and easy access to their medical information across providers and treatment sites. Leveraging unique blockchain properties, MedRec manages authentication, confidentiality, accountability and data sharing- crucial considerations when handling sensitive information. A modular design integrates with providers' existing, local data storage solutions, facilitating interoperability and making our system convenient and adaptable. We incentivize medical stakeholders (researchers, public health authorities, etc.) to participate in the network as blockchain “miners”. This provides them with access to aggregate, anonymized data as mining rewards, in return for sustaining and securing the network via Proof of Work. MedRec thus enables the emergence of data economics, supplying big data to empower researchers while engaging patients and providers in the choice to release metadata. The purpose of this short paper is to expose, prior to field tests, a working prototype through which we analyze and discuss our approach.",Conference paper,pro23
pap314,2660fbc3b666145a87f05de10066fc2a3e7467dd,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",The Science Of Real Time Data Capture Self Reports In Health Research,"The National Cancer Institute (NCI) has designated the topic of real-time data capture as an important and innovative research area. As such, the NCI sponsored a national meeting of distinguished research scientists to discuss the state of the science in this emerging and burgeoning field. This book reflects the findings of the conference and discusses the state of the science of real-time data capture and its application to health and cancer research. It provides a conceptual framework for minute-by-minute data captureecological momentary assessments (EMA)and discusses health-related topics where these assessements have been applied. In addition, future directions in real-time data capture assessment, interventions, methodology, and technology are discussed.",Erratum,pro73
pap315,02a9428b5b28d85ea330033fb990dc10cd15cc4e,jou105,Methods in Ecology and Evolution,Occupancy models for citizen‐science data,"Large‐scale citizen‐science projects, such as atlases of species distribution, are an important source of data for macroecological research, for understanding the effects of climate change and other drivers on biodiversity, and for more applied conservation tasks, such as early‐warning systems for biodiversity loss. However, citizen‐science data are challenging to analyse because the observation process has to be taken into account. Typically, the observation process leads to heterogeneous and non‐random sampling, false absences, false detections, and spatial correlations in the data. Increasingly, occupancy models are being used to analyse atlas data. We advocate a dual approach to strengthen inference from citizen science data for the questions the programme is intended to address: (a) the survey design should be chosen with a particular set of questions and associated analysis strategy in mind and (b) the statistical methods should be tailored not only to those questions but also to the specific characteristics of the data. We review the consequences of particular survey design choices that typically need to be made in atlas‐style citizen‐science projects. These include spatial resolution of the sampling units, allocation of effort in space, and collection of information about the observation process. On the analysis side, we review extensions of the basic occupancy models that are frequently necessary with atlas data, including methods for dealing with heterogeneity, non‐independent detections, false detections, and violation of the closure assumption. New technologies, such as cell‐phone apps and fixed remote detection devices, are revolutionizing citizen‐science projects. There is an opportunity to maximize the usefulness of the resulting datasets if the protocols are rooted in robust statistical designs and data analysis issues are being considered. Our review provides guidelines for designing new projects and an overview of the current methods that can be used to analyse data from such projects.",Letter,vol105
pap316,3e02906da7498b5fdbc6f0eea4b6bb9f2d86dd00,con111,International Conference on Image Analysis and Processing,ON PATIENT FLOW IN HOSPITALS: A DATA-BASED QUEUEING-SCIENCE PERSPECTIVE,"Hospitals are complex systems with essential societal benefits and huge mounting costs. These costs are exacerbated by inefficiencies in hospital processes, which are often manifested by congestion and long delays in patient care. Thus, a queueing-network view of patient flow in hospitals is natural for studying and improving its performance. The goal of our research is to explore patient flow data through the lens of a queueing scientist. The means is exploratory data analysis (EDA) in a large Israeli hospital, which reveals important features that are not readily explainable by existing models. Questions raised by our EDA include: Can a simple (parsimonious) queueing model usefully capture the complex operational reality of the Emergency Department (ED)? What time scales and operational regimes are relevant for modeling patient length of stay in the Internal Wards (IWs)? How do protocols of patient transfer between the ED and the IWs influence patient delay, workload division and fairness? EDA also unde...",Erratum,pro111
pap317,9445423239efb633f5c15791a7abe352199ce678,con90,Computer Vision and Pattern Recognition,General Data Protection Regulation,"Presentacio sobre l'Oficina de Proteccio de Dades Personals de la UAB i la politica Open Science. Va formar part de la conferencia ""Les politiques d'Open Data / Open Acces: Implicacions a la recerca"" orientada a investigadors i gestors de projectes europeus que va tenir lloc el 20 de setembre de 2018 a la Universitat Autonoma de Barcelona",Erratum,pro90
pap318,51995dc568874ea34911833355234b1f696dacfc,jou58,Journal of Data and Information Science,Science Mapping: A Systematic Review of the Literature,"Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain’s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.",Conference paper,vol58
pap319,87f7c170aecf8f3465b26a11b9a384fef934337b,con84,Workshop on Interdisciplinary Software Engineering Research,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",Erratum,pro84
pap320,2809d4876e34b8c64fc1783fe6a0a278770505b0,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,A survey of data provenance in e-science,"Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.",Erratum,pro47
pap321,439ede62248e5f6202982afead02b33d3feffae7,jou106,Nucleic Acids Research,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",Conference paper,vol106
pap322,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,jou107,Social Science Research,The role of administrative data in the big data revolution in social science research.,,Conference paper,vol107
pap323,16f4135a229c79e60fa25259100c8cdcedfab8cc,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Patent citation data in social science research: Overview and best practices,"The last 2 decades have witnessed a dramatic increase in the use of patent citation data in social science research. Facilitated by digitization of the patent data and increasing computing power, a community of practice has grown up that has developed methods for using these data to: measure attributes of innovations such as impact and originality; to trace flows of knowledge across individuals, institutions and regions; and to map innovation networks. The objective of this article is threefold. First, it takes stock of these main uses. Second, it discusses 4 pitfalls associated with patent citation data, related to office, time and technology, examiner, and strategic effects. Third, it highlights gaps in our understanding and offers directions for future research.",Erratum,pro21
pap324,d9a668e849fe780081b9d133d84c72039283f30b,jou108,Earth and Space Science,From Open Data to Open Science,"The open science movement continues to gain momentum, attention, and discussion. However, there are a number of different interpretations, viewpoints, and perspectives as to what the term “open science” means. In this study, we define open science as a collaborative culture enabled by technology that empowers the open sharing of data, information, and knowledge within the scientific community and the wider public to accelerate scientific research and understanding. As science has become increasingly data driven, data programs now play a critical role in enabling and accelerating open science.",Letter,vol108
pap325,233e702fa7ccfd55061680e3af9bd2f7efe5e08f,jou59,Nature,Science of Science,"The whys and wherefores of SciSci The science of science (SciSci) is based on a transdisciplinary approach that uses large data sets to study the mechanisms underlying the doing of science—from the choice of a research problem to career trajectories and progress within a field. In a Review, Fortunato et al. explain that the underlying rationale is that with a deeper understanding of the precursors of impactful science, it will be possible to develop systems and policies that improve each scientist's ability to succeed and enhance the prospects of science as a whole. Science, this issue p. eaao0185 BACKGROUND The increasing availability of digital data on scholarly inputs and outputs—from research funding, productivity, and collaboration to paper citations and scientist mobility—offers unprecedented opportunities to explore the structure and evolution of science. The science of science (SciSci) offers a quantitative understanding of the interactions among scientific agents across diverse geographic and temporal scales: It provides insights into the conditions underlying creativity and the genesis of scientific discovery, with the ultimate goal of developing tools and policies that have the potential to accelerate science. In the past decade, SciSci has benefited from an influx of natural, computational, and social scientists who together have developed big data–based capabilities for empirical analysis and generative modeling that capture the unfolding of science, its institutions, and its workforce. The value proposition of SciSci is that with a deeper understanding of the factors that drive successful science, we can more effectively address environmental, societal, and technological problems. ADVANCES Science can be described as a complex, self-organizing, and evolving network of scholars, projects, papers, and ideas. This representation has unveiled patterns characterizing the emergence of new scientific fields through the study of collaboration networks and the path of impactful discoveries through the study of citation networks. Microscopic models have traced the dynamics of citation accumulation, allowing us to predict the future impact of individual papers. SciSci has revealed choices and trade-offs that scientists face as they advance both their own careers and the scientific horizon. For example, measurements indicate that scholars are risk-averse, preferring to study topics related to their current expertise, which constrains the potential of future discoveries. Those willing to break this pattern engage in riskier careers but become more likely to make major breakthroughs. Overall, the highest-impact science is grounded in conventional combinations of prior work but features unusual combinations. Last, as the locus of research is shifting into teams, SciSci is increasingly focused on the impact of team research, finding that small teams tend to disrupt science and technology with new ideas drawing on older and less prevalent ones. In contrast, large teams tend to develop recent, popular ideas, obtaining high, but often short-lived, impact. OUTLOOK SciSci offers a deep quantitative understanding of the relational structure between scientists, institutions, and ideas because it facilitates the identification of fundamental mechanisms responsible for scientific discovery. These interdisciplinary data-driven efforts complement contributions from related fields such as scientometrics and the economics and sociology of science. Although SciSci seeks long-standing universal laws and mechanisms that apply across various fields of science, a fundamental challenge going forward is accounting for undeniable differences in culture, habits, and preferences between different fields and countries. This variation makes some cross-domain insights difficult to appreciate and associated science policies difficult to implement. The differences among the questions, data, and skills specific to each discipline suggest that further insights can be gained from domain-specific SciSci studies, which model and identify opportunities adapted to the needs of individual research fields. The complexity of science. Science can be seen as an expanding and evolving network of ideas, scholars, and papers. SciSci searches for universal and domain-specific laws underlying the structure and dynamics of science. ILLUSTRATION: NICOLE SAMAY Identifying fundamental drivers of science and developing predictive models to capture its evolution are instrumental for the design of policies that can improve the scientific enterprise—for example, through enhanced career paths for scientists, better performance evaluation for organizations hosting research, discovery of novel effective funding vehicles, and even identification of promising regions along the scientific frontier. The science of science uses large-scale data on the production of science to search for universal and domain-specific patterns. Here, we review recent developments in this transdisciplinary field.",Letter,vol59
pap326,33aeb033401ec748633bdd5b806db4f58288ee69,con67,IEEE International Software Metrics Symposium,The Accuracy of Citizen Science Data: A Quantitative Review,"Author(s): Aceves-Bueno, Erendira; Adeleye, Adeyemi S; Feraud, Marina; Huang, Yuxiong; Tao, Mengya; Yang, Yi; Anderson, Sarah E",Erratum,pro67
pap327,accb5390f75ef6daa91cf5441333a7ebcc42a41f,jou79,Nature Reviews Methods Primers,Citizen science in environmental and ecological sciences,,Conference paper,vol79
pap328,362f50f59a280d7cc526fb626fdf44ad382cee57,jou109,Scientometrics,The journal coverage of Web of Science and Scopus: a comparative analysis,,Article,vol109
pap329,1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0,jou6,Journal of Big Data,Deep learning applications and challenges in big data analytics,,Article,vol6
pap330,08a2ef1648fa5ea539ebe1718da577dc79124a21,jou29,Frontiers in Environmental Science,Prospects and challenges for social media data in conservation science,"Social media data have been extensively used in numerous fields of science, but examples of their use in conservation science are still very limited. In this paper, we propose a framework on how social media data could be useful for conservation science and practice. We present the commonly used social media platforms and discuss how their content could be providing new data and information for conservation science. Based on this, we discuss how future work in conservation science and practice would benefit from social media data.",Letter,vol29
pap331,06a81f63fc4ccfcf02934647a7c17454b91853b0,con35,IEEE Working Conference on Mining Software Repositories,Machine Learning - The Art and Science of Algorithms that Make Sense of Data,"As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.",Erratum,pro35
pap332,30d6f200f8b4bae78dbb4f69f1730bcad131d523,con40,Conference on Software Engineering Education and Training,The Materials Data Facility: Data Services to Advance Materials Science Research,,Erratum,pro40
pap333,ad3d83248eae66580d4deada76e72e3be9a9b44c,con24,International Conference on Data Technologies and Applications,Named data networking,"Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006. The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",Erratum,pro24
pap334,391a5f286f814d852dddcab1b2b68e5c1af6c79e,jou3,IEEE Transactions on Knowledge and Data Engineering,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",Letter,vol3
pap335,54d9fc3ed4937ee546ed45aee7bef16b4ae3775d,con64,British Computer Society Conference on Human-Computer Interaction,Statistics for citizen science: extracting signals of change from noisy ecological data,"Policy‐makers increasingly demand robust measures of biodiversity change over short time periods. Long‐term monitoring schemes provide high‐quality data, often on an annual basis, but are taxonomically and geographically restricted. By contrast, opportunistic biological records are relatively unstructured but vast in quantity. Recently, these data have been applied to increasingly elaborate science and policy questions, using a range of methods. At present, we lack a firm understanding of which methods, if any, are capable of delivering unbiased trend estimates on policy‐relevant time‐scales. We identified a set of candidate methods that employ data filtering criteria and/or correction factors to deal with variation in recorder activity. We designed a computer simulation to compare the statistical properties of these methods under a suite of realistic data collection scenarios. We measured the Type I error rates of each method–scenario combination, as well as the power to detect genuine trends. We found that simple methods produce biased trend estimates, and/or had low power. Most methods are robust to variation in sampling effort, but biases in spatial coverage, sampling effort per visit, and detectability, as well as turnover in community composition, all induced some methods to fail. No method was wholly unaffected by all forms of variation in recorder activity, although some performed well enough to be useful. We warn against the use of simple methods. Sophisticated methods that model the data collection process offer the greatest potential to estimate timely trends, notably Frescalo and occupancy–detection models. The potential of these methods and the value of opportunistic data would be further enhanced by assessing the validity of model assumptions and by capturing small amounts of information about sampling intensity at the point of data collection.",Erratum,pro64
pap336,d2a595c5efb4b26245c4353d5d85cbe6c7ecac0f,jou103,Science,Machine learning for data-driven discovery in solid Earth geoscience,"Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth’s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth’s behavior and by the inaccessibility of nearly all of Earth’s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.",Letter,vol103
pap337,d4825585c5b036bb789ad183635cc2d4d89ff394,con69,Formal Concept Analysis,Opening the archive: How free data has enabled the science and monitoring promise of Landsat,,Erratum,pro69
pap338,16cecb0173adc68762b6e70daecb25089a5a6b6a,jou0,Nature Biotechnology,ProteomeXchange provides globally co-ordinated proteomics data submission and dissemination,,Article,vol0
pap339,852c3c29c319ba1ae2c6efac3471a3f5c5b4a232,jou110,Public Understanding of Science,Comparing science communication theory with practice: An assessment and critique using Australian data,"Scholars have variously described different models of science communication over the past 20 years. However, there has been little assessment of theorised models against science communication practice. This article compares 515 science engagement activities recorded in a 2012 Australian audit against the theorised characteristics of the three dominant models of deficit, dialogue and participation. Most engagement activities had objectives that reflected a mix of deficit and dialogue activities. Despite increases in scientific controversies like climate change, there appears to be a paucity of participatory activities in Australia. Those that do exist are mostly about people being involved with science through activities like citizen science. These participatory activities appear to coexist with and perhaps even depend on deficit activities. Science communication scholars could develop their models by examining the full range of objectives for engagement found in practice and by recognising that any engagement will likely include a mix of approaches.",Letter,vol110
pap340,a461233e56079fc5af6e48d75f38be8c9ff87c1e,jou111,Environmental Science and Technology,Machine Learning: New Ideas and Tools in Environmental Science and Engineering.,"The rapid increase in both the quantity and complexity of data that are being generated daily in the field of environmental science and engineering (ESE) demands accompanied advancement in data analytics. Advanced data analysis approaches, such as machine learning (ML), have become indispensable tools for revealing hidden patterns or deducing correlations for which conventional analytical methods face limitations or challenges. However, ML concepts and practices have not been widely utilized by researchers in ESE. This feature explores the potential of ML to revolutionize data analysis and modeling in the ESE field, and covers the essential knowledge needed for such applications. First, we use five examples to illustrate how ML addresses complex ESE problems. We then summarize four major types of applications of ML in ESE: making predictions; extracting feature importance; detecting anomalies; and discovering new materials or chemicals. Next, we introduce the essential knowledge required and current shortcomings in ML applications in ESE, with a focus on three important but often overlooked components when applying ML: correct model development, proper model interpretation, and sound applicability analysis. Finally, we discuss challenges and future opportunities in the application of ML tools in ESE to highlight the potential of ML in this field.",Conference paper,vol111
pap341,c36991759325bedd19f69264f72d1cbf59a6158c,con26,Decision Support Systems,Data Mining: Concepts and Techniques,"The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data",Erratum,pro26
pap342,daaf02de10f338d98ed6f58c13987b63b275825a,jou112,Astronomy & Astrophysics,Gaia Early Data Release 3,"Context. Since July 2014, the Gaia mission has been engaged in a high-spatial-resolution, time-resolved, precise, accurate astrometric, and photometric survey of the entire sky.
Aims. We present the Gaia Science Alerts project, which has been in operation since 1 June 2016. We describe the system which has been developed to enable the discovery and publication of transient photometric events as seen by Gaia.
Methods. We outline the data handling, timings, and performances, and we describe the transient detection algorithms and filtering procedures needed to manage the high false alarm rate. We identify two classes of events: (1) sources which are new to Gaia and (2) Gaia sources which have undergone a significant brightening or fading. Validation of the Gaia transit astrometry and photometry was performed, followed by testing of the source environment to minimise contamination from Solar System objects, bright stars, and fainter near-neighbours.
Results. We show that the Gaia Science Alerts project suffers from very low contamination, that is there are very few false-positives. We find that the external completeness for supernovae, CE = 0.46, is dominated by the Gaia scanning law and the requirement of detections from both fields-of-view. Where we have two or more scans the internal completeness is CI = 0.79 at 3 arcsec or larger from the centres of galaxies, but it drops closer in, especially within 1 arcsec.
Conclusions. The per-transit photometry for Gaia transients is precise to 1% at G = 13, and 3% at G = 19. The per-transit astrometry is accurate to 55 mas when compared to Gaia DR2. The Gaia Science Alerts project is one of the most homogeneous and productive transient surveys in operation, and it is the only survey which covers the whole sky at high spatial resolution (subarcsecond), including the Galactic plane and bulge.",Letter,vol112
pap343,969f983d000ac68ca77548b5bba2e8d1b89086c4,con55,Workshop on Learning from Authoritative Security Experiment Results,Materials science with large-scale data and informatics: Unlocking new opportunities,"Universal access to abundant scientific data, and the software to analyze the data at scale, could fundamentally transform the field of materials science. Today, the materials community faces serious challenges to bringing about this data-accelerated research paradigm, including diversity of research areas within materials, lack of data standards, and missing incentives for sharing, among others. Nonetheless, the landscape is rapidly changing in ways that should benefit the entire materials research enterprise. We provide an overview of the current state of the materials data and informatics landscape, highlighting a few selected efforts that make more data freely available and useful to materials researchers.",Erratum,pro55
pap344,2f35b305b6c56046b631b0cdb6d2f08e4ee577a7,jou113,Behavior Research Methods,TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences,,Article,vol113
pap345,b48a917258f4e7e2b78a41289d005513db1de8c9,con24,International Conference on Data Technologies and Applications,Earth Observation Open Science: Enhancing Reproducible Science Using Data Cubes,"Earth Observation Data Cubes (EODC) have emerged as a promising solution to efficiently and effectively handle Big Earth Observation (EO) Data generated by satellites and made freely and openly available from different data repositories. The aim of this Special Issue, “Earth Observation Data Cube”, in Data, is to present the latest advances in EODC development and implementation, including innovative approaches for the exploitation of satellite EO data using multi-dimensional (e.g., spatial, temporal, spectral) approaches. This Special Issue contains 14 articles covering a wide range of topics such as Synthetic Aperture Radar (SAR), Analysis Ready Data (ARD), interoperability, thematic applications (e.g., land cover, snow cover mapping), capacity development, semantics, processing techniques, as well as national implementations and best practices. These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science, reducing the gap between users’ expectations for decision-ready products and current Big Data analytical capabilities, and ultimately unlocking the information power of EO data by transforming them into actionable knowledge.",Article,pro24
pap346,3aa1b70fdc97ae96091c5fb39cd911015ac5253e,con25,IEEE International Parallel and Distributed Processing Symposium,Novel methods improve prediction of species' distributions from occurrence data,"Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.",Erratum,pro25
pap347,88bcdfd021d935a28f245e178792207881b14794,jou114,Cambridge International Law Journal,Learning from Imbalanced Data Sets,,Conference paper,vol114
pap348,9b18fbe281496ad72bdd18e0a5883d235ebdfd87,jou115,Clinical and Translational Science,"Biolink Model: A universal schema for knowledge graphs in clinical, biomedical, and translational science","Within clinical, biomedical, and translational science, an increasing number of projects are adopting graphs for knowledge representation. Graph‐based data models elucidate the interconnectedness among core biomedical concepts, enable data structures to be easily updated, and support intuitive queries, visualizations, and inference algorithms. However, knowledge discovery across these “knowledge graphs” (KGs) has remained difficult. Data set heterogeneity and complexity; the proliferation of ad hoc data formats; poor compliance with guidelines on findability, accessibility, interoperability, and reusability; and, in particular, the lack of a universally accepted, open‐access model for standardization across biomedical KGs has left the task of reconciling data sources to downstream consumers. Biolink Model is an open‐source data model that can be used to formalize the relationships between data structures in translational science. It incorporates object‐oriented classification and graph‐oriented features. The core of the model is a set of hierarchical, interconnected classes (or categories) and relationships between them (or predicates) representing biomedical entities such as gene, disease, chemical, anatomic structure, and phenotype. The model provides class and edge attributes and associations that guide how entities should relate to one another. Here, we highlight the need for a standardized data model for KGs, describe Biolink Model, and compare it with other models. We demonstrate the utility of Biolink Model in various initiatives, including the Biomedical Data Translator Consortium and the Monarch Initiative, and show how it has supported easier integration and interoperability of biomedical KGs, bringing together knowledge from multiple sources and helping to realize the goals of translational science.",Letter,vol115
pap349,7bd598f6a7c6eb4265fe5a9ca64504d1d639684a,con68,Experimental Software Engineering Network,Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title “data mining in education”. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data‐Driven Education, Data‐Driven Decision‐Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.",Erratum,pro68
pap350,e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,con57,International Workshop on Agent-Oriented Software Engineering,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",Erratum,pro57
pap351,0870c1ea2b7d5a515c7b5b954f1433b379fe1e02,jou116,Earth-Science Reviews,Principles and methods of scaling geospatial Earth science data,,Letter,vol116
pap352,efa5558bddd68abe4adc81adbbef6f739e648392,jou63,PLoS Biology,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",Article,vol63
pap353,b7d5dda24d0c540929cd58b0226eac8a85e9769b,jou117,Review of Economics and Statistics,Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data,"Many panel data sets encountered in macroeconomics, international economics, regional science, and finance are characterized by cross-sectional or spatial dependence. Standard techniques that fail to account for this dependence will result in inconsistently estimated standard errors. In this paper we present conditions under which a simple extension of common nonparametric covariance matrix estimation techniques yields standard error estimates that are robust to very general forms of spatial and temporal dependence as the time dimension becomes large. We illustrate the relevance of this approach using Monte Carlo simulations and a number of empirical examples.",Article,vol117
pap354,5703617b9d9d40e90b6c8ffa21a52734d9822d60,con105,British Machine Vision Conference,Defining Computational Thinking for Mathematics and Science Classrooms,,Erratum,pro105
pap355,da619a6c524f5ab800b44c8728db3cef3d3b25d9,jou16,Big Data & Society,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",Conference paper,vol16
pap356,7fc70d4cc5118fdbc8e8807979eae8b61948ff91,con59,Annual Workshop of the Psychology of Programming Interest Group,"The elements of statistical learning: data mining, inference and prediction",,Erratum,pro59
pap357,88dde718acafeaedbe9768883d274a81fd8313d7,con25,IEEE International Parallel and Distributed Processing Symposium,DLHub: Model and Data Serving for Science,"While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the ""learning systems"" needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its self-service model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications.",Article,pro25
pap358,6802bbeea45ea9c44b8e9f69ee1d775f5af0717f,jou118,International Journal of Exercise Science,Ethical Issues Relating to Scientific Discovery in Exercise Science.,"This work aims to present concepts related to ethical issues in conducting and reporting scientific research in a clear and straightforward manner. Considerations around research design including authorship, sound research practices, non-discrimination in subject recruitment, objectivity, respect for intellectual property, and financial interests are detailed. Further, concepts relating to the conducting of research including the competency of the researcher, conflicts of interest, accurately representing data, and ethical practices in human and animal research are presented. Attention pertaining to the dissemination of research including plagiarism, duplicate submission, redundant publication, and figure manipulation is offered. Other considerations including responsible mentoring, respect for colleagues, and social responsibility are set forth. The International Journal of Exercise Science will now require a statement in all subsequent published manuscripts that the authors have complied with each of the ethics statements contained in this work.",Letter,vol118
pap359,7579330e89bffd736fee19d25359ab3ae65bf5f7,con74,IEEE International Conference on Information Reuse and Integration,rioja: Analysis of Quaternary Science Data,,Erratum,pro74
pap360,c2c29dfb1d0071f77d0a2a003daa8a3bbeccc96a,con69,Formal Concept Analysis,Computational Social Science,,Erratum,pro69
pap361,f2b66923db74a16169d040a51ada555d5b6f8851,con94,Vision,Data Mining and Analysis: Fundamental Concepts and Algorithms,"The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more",Erratum,pro94
pap362,fb3140c9766a5bc92400ac8ce9d48a4272bba69e,jou119,Artificial Life,A New Kind of Science,"nationwide data set of losses from 1975 to 1998 was compiled to assess the trends. Temporal patterns of deaths and injuries, monetary damages, and—in some cases—the number of events are systematically examined by year in chapter 5, and the authors undertake a systematic spatial assessment of the statewide totals in chapter 6. Explanations for some of the patterns are offered, particularly for the most significant disasters and for the states with most events or the greatest losses. Further refinement and evaluation of patterns of economic losses and death are undertaken by normalizing losses by population, land area, and gross domestic product (GDP). The authors advance the discussion from simple descriptions of loss patterns to explanations of the patterns of disaster-loss burden, and some surprises emerge from the arithmetic. For instance, North Dakota, Iowa, andMississippi not only suffered the greatest monetary losses per capita during the period, but also suffered the greatest losses of property and crops compared to their state GDP!For afinal analysis, the authors created an overall hazard score (averaged proportion of the states’ contributions to the national totals of events, deaths, and damages) and used it to rank the states. Using this ranking, states were assigned to categories of ‘‘proneness,’’ from highest (Florida, Texas, andCalifornia) to lowest (Rhode Island, Delaware, Alaska and other small or lightly populated states). The conclusion we are to draw is that the amount of loss a state has experienced indicates its disaster proneness. Finally, ‘‘Charting a Course for theNext Two Decades’’ by Cutter describes what is needed to produce the models and data appropriate for mitigation and planning assessments. In order for an effective assessment of events and losses to occur, progress is required in several areas: development of vulnerability science, the creation of a national hazard events and losses database, and the establishment of a national loss inventory and events clearinghouse. To do so, Cutter argues, we need to rethink thewaywe monitor, assess, andmanage our vulnerabilities. She briefly describes the shifts needed in data gathering and provision, sustainability and distributive justice, strategic planning, research funding, and societal awareness of issues that influence the prospects for disaster. While American Hazardscapes is intended to provide a broadunderstanding of the geography of loss due to hazards in the United States, it suffers from its openly acknowledged limitations. Though criticizing the quality of currently available data, the authors use those data to indicate the prospects for future disasters. The elimination of extreme events is no longer believed tobe the key loss reduction. Instead,we must identify and avoid places too dynamic for permanent occupation and adjust to the inevitable events in ways that limit prospects for loss. Mitigation must address the vulnerabilities that cause greater exposure and profound upset of our social systems and create more complex catastrophes. The data employed in this assessment describe (however imperfectly) the losses suffered over two and a half decades. The largest disasters overwhelm the patterns of loss in their analysis. The authors imply, based on proneness rankings, that those who lost the most are the most prone to loss. But in reality, losses are byproducts of the interplay of two dynamic geographies: the pattern of extreme events and the pattern of human use of the landscape. The former is often poorly understood, may not behave consistently, andmay operate on greater than twenty-five-year cycles. The latter may change so rapidly that it surpasses our capacity to measure it and map it, and postdisaster land use and human perception may be radically changed. These geographies were outside the scope of this book, however, and given new homeland security efforts and reorganization of the Federal Emergency Management Agency, the past is an even poorer indicator of the future.",Letter,vol119
pap363,eaf5a5e0b32a055e288d5edcc5cd39f9f4d335ad,jou120,Nature Communications,The misuse of colour in science communication,,Article,vol120
pap364,24931dc3ddedfc2db5405af236e3ca84944d66d7,con38,International Symposium on Empirical Software Engineering and Measurement,Big Data and Social Science: A Practical Guide to Methods and Tools,"Both Traditional Students and Working Professionals Acquire the Skills to Analyze Social Problems. Big Data and Social Science: A Practical Guide to Methods and Tools shows how to apply data science to real-world problems in both research and the practice. The book provides practical guidance on combining methods and tools from computer science, statistics, and social science. This concrete approach is illustrated throughout using an important national problem, the quantitative study of innovation. The text draws on the expertise of prominent leaders in statistics, the social sciences, data science, and computer science to teach students how to use modern social science research principles as well as the best analytical and computational tools. It uses a real-world challenge to introduce how these tools are used to identify and capture appropriate data, apply data science models and tools to that data, and recognize and respond to data errors and limitations. For more information, including sample chapters and news, please visit the author's website.",Erratum,pro38
pap365,a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,jou63,PLoS Biology,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",Article,vol63
pap366,b970f9c088beee99666a40374dd5ccb06eeda112,con26,Decision Support Systems,Understanding the paradigm shift to computational social science in the presence of big data,,Article,pro26
pap367,cebed64039064dfe950587b919ddc01dee7d871f,con84,Workshop on Interdisciplinary Software Engineering Research,From Little Science to Big Science,"In Little Science, Big Science (1963), Derek J. de Solla Price undertook a sociology of science that dealt with the growth and changing shape of scientific publishing and the social organization of science. The focus of Price’s work was on the long-term, gradual shift from “little science,” with the solo scientist, small laboratory, and minimal research funds, to “big science,” with collaborative research teams, large-scale research hardware, extensive funding, and corporate-political suitors of scientists. We extend Price’s focus on scientific publications by moving beyond his analysis of practices in physics and chemistry to examine a social science; namely, sociology. Specifically, we analyze 3,000 articles in four long-standing sociology journals over the fifty-year period from 1960-2010 to determine the gender of authors, the prestige of authors’ departments, length of articles, number of references, sources of data for studies, and patterns of funding for research. We find that sociology is not immune from the shift from “little science” to “big science.”",Erratum,pro84
pap368,cf83811d697dc3419a52c9853807afb410eb3943,jou121,American Journal of Political Science,Tree-Based Models for Political Science Data,"Political scientists often find themselves analyzing data sets with a large number of observations, a large number of variables, or both. Yet, traditional statistical techniques fail to take full advantage of the opportunities inherent in “big data,” as they are too rigid to recover nonlinearities and do not facilitate the easy exploration of interactions in high-dimensional data sets. In this article, we introduce a family of tree-based nonparametric techniques that may, in some circumstances, be more appropriate than traditional methods for confronting these data challenges. In particular, tree models are very effective for detecting nonlinearities and interactions, even in data sets with many (potentially irrelevant) covariates. We introduce the basic logic of tree-based models, provide an overview of the most prominent methods in the literature, and conduct three analyses that illustrate how the methods can be implemented while highlighting both their advantages and limitations. Replication Materials: The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network at: https://doi.org/10.7910/DVN/8ZJBLI. Social science scholars often work with data sets containing a large number of observations, many potential covariates, or (increasingly) both. Indeed, political scientists now regularly analyze data with levels of complexity unimaginable just two decades ago. Widely used surveys, for instance, interview tens of thousands of respondents about hundreds of topics. Scholars of institutions can quickly assemble data sets with thousands of observations using resources like the Comparative Agendas Project. Moreover, new measurement methods, such as text analysis, have combined with data sources, such as Twitter, to generate databases of almost unmanageable sizes. It is clear that political science, like all areas of the social sciences, will increasingly have access to a deluge of data so vast that it will dwarf everything that has come before. What statistical methods are needed in this datasaturated world? Surely, there is no one correct answer. Yet, just as surely, traditional statistical models are not always equipped to take full advantage of new data sources. Traditional models—largely variants of linear regressions—are ideal for evaluating theories that imply specific functional forms relating outcomes to predictors. In particular, they excel in their ability to leverage assumptions about the data-generating process, or DGP (additivity, linearity in the parameters, homoskedasticity, Jacob M. Montgomery is Associate Professor, Department of Political Science, Washington University in St. Louis, Campus Box 1063, One Brookings Drive, St. Louis, MO 63130 (jacob.montgomery@wustl.edu). Santiago Olivella is Assistant Professor, Department of Political Science, University of North Carolina at Chapel Hill, Hamilton Hall 361, CB 3265, Chapel Hill, NC 27599 (olivella@unc.edu). etc.) to make valid inferences despite inherent data limitations. Although appropriate when testing theories that conform with these assumptions, standard models are often insufficiently flexible to capture nuances in the data—such as complex nonlinear functional forms and deep interactions—when no clear a priori expectations exist. In this article, we introduce a family of tree-based nonparametric techniques from the machine learning literature. We argue that, under specific circumstances, regression and classification tree models are an appropriate standard choice for analyzing high-dimensional data sets. In particular, past research has shown tree-based methods to be very useful for making accurate predictions when the underlying DGP includes nonlinearities, discontinuities, and interactions among many covariates. Further, tree models require few assumptions. Rather than imposing a presumed structure on the DGP, tree-based methods allow the data to “speak for themselves.” Thus, our goal in this article is to introduce political scientists to this promising family of methods, which are well suited for today’s data analysis demands. In the next sections, we discuss the promise and perils of high-dimensional, “large”-N data sets and introduce the basic logic of tree models. We then provide an overview of the most prominent methods in the literature. American Journal of Political Science, Vol. 62, No. 3, July 2018, Pp. 729–744 C ©2018, Midwest Political Science Association DOI: 10.1111/ajps.12361",Article,vol121
pap369,0b510ee69a507407008661aacb2345f73c70f8cb,con57,International Workshop on Agent-Oriented Software Engineering,Strategies Employed by Citizen Science Programs to Increase the Credibility of Their Data,"The success of citizen science in producing important and unique data is attracting interest from scientists and resource managers. Nonetheless, questions remain about the credibility of citizen science data. Citizen science programs desire to meet the same standards of credibility as academic science, but they usually work within a different context, for example, training and managing significant numbers of volunteers with limited resources. We surveyed the credibility-building strategies of 30 citizen science programs that monitor environmental aspects of the California coast. We identified a total of twelve strategies: Three that are applied during training and planning; four that are applied during data collection; and five that are applied during data analysis and program evaluation. Variation in the application of these strategies by program is related to factors such as the number of participants, the focus on group or individual work, and the time commitment required of volunteers. The structure of each program and available resources require program designers to navigate tradeoffs in the choices of their credibility strategies. Our results illustrate those tradeoffs and provide a framework for the necessary discussions between citizen science programs and potential users of their data—including scientists and decision makers—about shared expectations for credibility and practical approaches for meeting those expectations. This article has been corrected here: http://dx.doi.org/10.5334/cstp.91",Erratum,pro57
pap370,b598b8dd79654dc865b02c2af0a0bdb565d24049,jou122,Ambio,Taking a ‘Big Data’ approach to data quality in a citizen science project,,Letter,vol122
pap371,500b73ecdf8ff5590718edb03367e3836a368485,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Secondary Data Analysis: A Method of which the Time Has Come,"Technological advances have led to vast amounts of data that has been collected, compiled, and archived, and that is now easily accessible for research. As a result, utilizing existing data for research is becoming more prevalent, and therefore secondary data analysis. While secondary analysis is flexible and can be utilized in several ways, it is also an empirical exercise and a systematic method with procedural and evaluative steps, just as in collecting and evaluating primary data. This paper asserts that secondary data analysis is a viable method to utilize in the process of inquiry when a systematic procedure is followed and presents an illustrative research application utilizing secondary data analysis in library and information science research.",Erratum,pro98
pap372,ac8db14cbc7ad0119d0130e88f98ccb3ec61780f,con0,International Conference on Machine Learning,"Big Data, Digital Media, and Computational Social Science",forecasts and misrepresent,Erratum,pro0
pap373,46d71d947231f86e1f9d4581e61212385debbe14,con44,International Conference Knowledge Engineering and Knowledge Management,OpenML: networked science in machine learning,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",Erratum,pro44
pap374,cf9ecfbbd0095687c4cfbbbfa0546914e651b109,jou123,Medicine & Science in Sports & Exercise,"Calibration of the Computer Science and Applications, Inc. accelerometer.","PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",Article,vol123
pap375,b8f75b848b6cef0f2b5a1a11b794332ca9bccb45,jou124,Environmental Reviews,A review of machine learning applications in wildfire science and management,"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then, the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date and then review the use of ML in wildfire science as broadly categorized into six problem domains, including (i) fuels characterization, fire detection, and mapping; (ii) fire weather and climate change; (iii) fire occurrence, susceptibility, and risk; (iv) fire behavior prediction; (v) fire effects; and (vi) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, to the end of 2019, we identified 300 relevant publications in which the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods — including deep learning and agent-based learning — in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods such as deep learning requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high-quality, and freely available wildfire data for use by practitioners of ML methods.",Conference paper,vol124
pap376,938a6209fe95dd4e5f801a14b6b650dc7b2f6108,jou125,EMBO Reports,Could Big Data be the end of theory in science?,"Afew years ago, Chris Anderson, former editor in chief of Wired magazine, published a provocative and thought‐provoking article: “The end of theory: the data deluge makes the scientific method obsolete” (http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory/). As the title indicates, Anderson asserted that in the era of petabyte information and supercomputing, the traditional, hypothesis‐driven scientific method would become obsolete. No more theories or hypotheses, no more discussions whether the experimental results refute or support the original hypotheses. In this new era, what counts are sophisticated algorithms and statistical tools to sift through a massive amount of data to find information that could be turned into knowledge.

> … [an] imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button…

Anderson's essay started an intense discussion about the relative merits of data‐driven research versus hypothesis‐driven research that has much relevance for many areas of research, including bioinformatics, systems biology, epidemiology and ecology. Yet, his imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button deserves some inquiry from an epistemological point of view. Is data‐driven research a genuine mode of knowledge production, or is it above all a tool to identify potentially useful information? Given the amount of scientific data available, is it now possible to dismiss the role of theoretical assumptions and hypotheses? Should this new mode of gathering information supersede the old way of doing research?

The scientific method encompasses an ongoing process of formulate a hypothesis‐test with an experiment–analyze the results‐reformulate the hypothesis. Such a way of proceeding has been in use for centuries and is basically accepted in our Western society as the most reliable way to produce robust knowledge.

However, Anderson is not the …",Article,vol125
pap377,993c9eb9bba80e2d8993e8c99acca1825cd0302f,jou103,Science,Next Steps for Citizen Science,"Strategic investments and coordination are needed for citizen science to reach its full potential. Around the globe, thousands of research projects are engaging millions of individuals—many of whom are not trained as scientists—in collecting, categorizing, transcribing, or analyzing scientific data. These projects, known as citizen science, cover a breadth of topics from microbiomes to native bees to water quality to galaxies. Most projects obtain or manage scientific information at scales or resolutions unattainable by individual researchers or research teams, whether enrolling thousands of individuals collecting data across several continents, enlisting small armies of participants in categorizing vast quantities of online data, or organizing small groups of volunteers to tackle local problems.",Conference paper,vol103
pap378,48fc9c42522184c652742255fdf31f7b9ed7ebae,jou126,Journal of Evidence-Based Medicine,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",Conference paper,vol126
pap379,fe5bb5d8d6b7ac251d87bc16e75ea5889cc92425,jou127,Political Science Research and Methods,Explaining Fixed Effects: Random Effects Modeling of Time-Series Cross-Sectional and Panel Data*,"This article challenges Fixed Effects (FE) modeling as the ‘default’ for time-series-cross-sectional and panel data. Understanding different within and between effects is crucial when choosing modeling strategies. The downside of Random Effects (RE) modeling—correlated lower-level covariates and higher-level residuals—is omitted-variable bias, solvable with Mundlak's (1978a) formulation. Consequently, RE can provide everything that FE promises and more, as confirmed by Monte-Carlo simulations, which additionally show problems with Plümper and Troeger's FE Vector Decomposition method when data are unbalanced. As well as incorporating time-invariant variables, RE models are readily extendable, with random coefficients, cross-level interactions and complex variance functions. We argue not simply for technical solutions to endogeneity, but for the substantive importance of context/heterogeneity, modeled using RE. The implications extend beyond political science to all multilevel datasets. However, omitted variables could still bias estimated higher-level variable effects; as with any model, care is required in interpretation.",Conference paper,vol127
pap380,8e600778160ff986b5460bc2584066148e55e5d4,jou103,Science,Protein structure determination using metagenome sequence data,"Filling in the protein fold picture Fewer than a third of the 14,849 known protein families have at least one member with an experimentally determined structure. This leaves more than 5000 protein families with no structural information. Protein modeling using residue-residue contacts inferred from evolutionary data has been successful in modeling unknown structures, but it requires large numbers of aligned sequences. Ovchinnikov et al. augmented such sequence alignments with metagenome sequence data (see the Perspective by Söding). They determined the number of sequences required to allow modeling, developed criteria for model quality, and, where possible, improved modeling by matching predicted contacts to known structures. Their method predicted quality structural models for 614 protein families, of which about 140 represent newly discovered protein folds. Science, this issue p. 294; see also p. 248 Combining metagenome data with protein structure prediction generates models for 614 families with unknown structures. Despite decades of work by structural biologists, there are still ~5200 protein families with unknown structure outside the range of comparative modeling. We show that Rosetta structure prediction guided by residue-residue contacts inferred from evolutionary information can accurately model proteins that belong to large families and that metagenome sequence data more than triple the number of protein families with sufficient sequences for accurate modeling. We then integrate metagenome data, contact-based structure matching, and Rosetta structure calculations to generate models for 614 protein families with currently unknown structures; 206 are membrane proteins and 137 have folds not represented in the Protein Data Bank. This approach provides the representative models for large protein families originally envisioned as the goal of the Protein Structure Initiative at a fraction of the cost.",Letter,vol103
pap381,2daffab3ebd3fc034f8f78d6a546606c33a5d398,jou109,Scientometrics,"Google Scholar, Scopus and the Web of Science: a longitudinal and cross-disciplinary comparison",,Article,vol109
pap382,5b5332e79aefa3b913d42a434b8ddb09b31b5b2e,con0,International Conference on Machine Learning,Voronoi diagrams—a survey of a fundamental geometric data structure,"Computational geometry is concerned with the design and analysis of algorithms for geometrical problems. In addition, other more practically oriented, areas of computer science— such as computer graphics, computer-aided design, robotics, pattern recognition, and operations research—give rise to problems that inherently are geometrical. This is one reason computational geometry has attracted enormous research interest in the past decade and is a well-established area today. (For standard sources, we refer to the survey article by Lee and Preparata [19841 and to the textbooks by Preparata and Shames [1985] and Edelsbrunner [1987bl.) Readers familiar with the literature of computational geometry will have noticed, especially in the last few years, an increasing interest in a geometrical construct called the Voronoi diagram. This trend can also be observed in combinatorial geometry and in a considerable number of articles in natural science journals that address the Voronoi diagram under different names specific to the respective area. Given some number of points in the plane, their Voronoi diagram divides the plane according to the nearest-neighbor",Erratum,pro0
pap383,28aecd08b2488c5300abf399feeb83a1f9c19890,con33,International Conference on Automated Software Engineering,Open Government Data,"Story Slides Slide 1 W3C eGovernment Community: Data Science Slide 2 Agenda Slide 3 The Changing Landscape of Federal Information Technology Slide 4 Cloud: SOA, Semantic, & Data Science: September 10-11th Slide 5 Opportunities for Data Science Slide 6 Discussion 1 Slide 7 Discussion 2 Slide 8 Discussion 3 Slide 9 Discussion 4 Slide 10 Discussion 5 Spotfire Dashboard Research Notes Joshua Tauberer’s Blog Open Government Data 1 Big Data Meets Open Government Figure 1 The New Federal Register 2.0 Figure 2 This animated visualization of live wind speeds and directions Figure 3 Data is like refrigerator poetry Figure 4 http://www.GovTrack.us Figure 5 John Oliver parodies Schoolhouse Rock’s “I’m Just a Bill” References 1 2 3 4 5 6 7",Erratum,pro33
pap384,b41fd82432999628e34d07e64ccda783273c15c0,jou22,Proceedings of the National Academy of Sciences of the United States of America,Data integration enables global biodiversity synthesis,"Significance As anthropogenic impacts to Earth systems accelerate, biodiversity knowledge integration is urgently required to support responses to underpin a sustainable future. Consolidating information from disparate sources (e.g., community science programs, museums) and data types (e.g., environmental, biological) can connect the biological sciences across taxonomic, disciplinary, geographical, and socioeconomic boundaries. In an analysis of the research uses of the world’s largest cross-taxon biodiversity data network, we report the emerging roles of open-access data aggregation in the development of increasingly diverse, global research. These results indicate a new biodiversity science landscape centered on big data integration, informing ongoing initiatives and the strategic prioritization of biodiversity data aggregation across diverse knowledge domains, including environmental sciences and policy, evolutionary biology, conservation, and human health. The accessibility of global biodiversity information has surged in the past two decades, notably through widespread funding initiatives for museum specimen digitization and emergence of large-scale public participation in community science. Effective use of these data requires the integration of disconnected datasets, but the scientific impacts of consolidated biodiversity data networks have not yet been quantified. To determine whether data integration enables novel research, we carried out a quantitative text analysis and bibliographic synthesis of >4,000 studies published from 2003 to 2019 that use data mediated by the world’s largest biodiversity data network, the Global Biodiversity Information Facility (GBIF). Data available through GBIF increased 12-fold since 2007, a trend matched by global data use with roughly two publications using GBIF-mediated data per day in 2019. Data-use patterns were diverse by authorship, geographic extent, taxonomic group, and dataset type. Despite facilitating global authorship, legacies of colonial science remain. Studies involving species distribution modeling were most prevalent (31% of literature surveyed) but recently shifted in focus from theory to application. Topic prevalence was stable across the 17-y period for some research areas (e.g., macroecology), yet other topics proportionately declined (e.g., taxonomy) or increased (e.g., species interactions, disease). Although centered on biological subfields, GBIF-enabled research extends surprisingly across all major scientific disciplines. Biodiversity data mobilization through global data aggregation has enabled basic and applied research use at temporal, spatial, and taxonomic scales otherwise not possible, launching biodiversity sciences into a new era.",Conference paper,vol22
pap385,c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,con67,IEEE International Software Metrics Symposium,Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar,"The Institute for Scientific Information's (ISI, now Thomson Scientific, Philadelphia, PA) citation databases have been used for decades as a starting point and often as the only tools for locating citations andsor conducting citation analyses. The ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science (LIS) faculty members as a case study, the authors examine the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. The WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours. © 2007 Wiley Periodicals, Inc.",Erratum,pro67
pap386,b9921fb4d1448058642897797e77bdaf8f444404,jou128,Political Analysis,Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,"Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.",Article,vol128
pap387,bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,con32,International Conference on Software Technology: Methods and Tools,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",Erratum,pro32
pap388,88a55ee54aae117f06441459d1ad2330ce18d7e0,jou129,Conservation Biology,Emerging problems of data quality in citizen science,,Article,vol129
pap389,bf96377353bf9daa8dc0e98eee17335f54cbcc60,jou65,Data Science Journal,Data science as an academic discipline,"I recall being a proud young academic about 1970; I had just received a research grant to build and study a scientific database, and I had joined CODATA. I was looking forward to the future in this new exciting discipline when the head of my department, an internationally known professor, advised me that data was “a low level activity” not suitable for an academic. I recall my dismay. What can we do to ensure that this does not happen again and that data science is universally recognized as a worthwhile academic activity? Incidentally, I did not take that advice, or I would not be writing this essay, but moved into computer science. I will use my experience to draw comparisons between the problems computer science had to become academically recognized and those faced by data science.",Letter,vol65
pap390,116927fbe4c9732fd1e392035a100c33b14e9d59,jou15,International Journal of Digital Earth,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",Letter,vol15
pap391,1842a5fb9739149dadba962c94dd7243a5f62242,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,What is Data Science ? Fundamental Concepts and a Heuristic Example,,Erratum,pro13
pap392,cff7b1b98da6de583bf2d5ffd496c2e6d70a794c,con78,Neural Information Processing Systems,From DFT to machine learning: recent approaches to materials science–a review,"Recent advances in experimental and computational methods are increasing the quantity and complexity of generated data. This massive amount of raw data needs to be stored and interpreted in order to advance the materials science field. Identifying correlations and patterns from large amounts of complex data is being performed by machine learning algorithms for decades. Recently, the materials science community started to invest in these methodologies to extract knowledge and insights from the accumulated data. This review follows a logical sequence starting from density functional theory as the representative instance of electronic structure methods, to the subsequent high-throughput approach, used to generate large amounts of data. Ultimately, data-driven strategies which include data mining, screening, and machine learning techniques, employ the data generated. We show how these approaches to modern computational materials science are being used to uncover complexities and design novel materials with enhanced properties. Finally, we point to the present research problems, challenges, and potential future perspectives of this new exciting field.",Erratum,pro78
pap393,06ba782753bad19254db5d28ad4155556f286ee0,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Data Management and Analysis Methods,"This chapter is about methods for managing and analyzing qualitative data. By qualitative data the authors mean text: newspapers, movies, sitcoms, e-mail traffic, folktales, life histories. They also mean narratives--narratives about getting divorced, about being sick, about surviving hand-to-hand combat, about selling sex, about trying to quit smoking. In fact, most of the archaeologically recoverable information about human thought and human behavior is text, the good stuff of social science.",Erratum,pro52
pap394,c3665722a7cc81caca8c90ac3c5b0572f7bba055,jou110,Public Understanding of Science,Can citizen science enhance public understanding of science?,"Over the past 20 years, thousands of citizen science projects engaging millions of participants in collecting and/or processing data have sprung up around the world. Here we review documented outcomes from four categories of citizen science projects which are defined by the nature of the activities in which their participants engage – Data Collection, Data Processing, Curriculum-based, and Community Science. We find strong evidence that scientific outcomes of citizen science are well documented, particularly for Data Collection and Data Processing projects. We find limited but growing evidence that citizen science projects achieve participant gains in knowledge about science knowledge and process, increase public awareness of the diversity of scientific research, and provide deeper meaning to participants’ hobbies. We also find some evidence that citizen science can contribute positively to social well-being by influencing the questions that are being addressed and by giving people a voice in local environmental decision making. While not all citizen science projects are intended to achieve a greater degree of public understanding of science, social change, or improved science -society relationships, those projects that do require effort and resources in four main categories: (1) project design, (2) outcomes measurement, (3) engagement of new audiences, and (4) new directions for research.",Article,vol110
pap395,ff7a79011e4ddba98474efe776432ac2b4431473,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Citizen science and the United Nations Sustainable Development Goals,,Erratum,pro52
pap396,ff1068a7e2acaa41fae2a8e1b180264434f06ce8,jou103,Science,Liberating field science samples and data,"Promote reproducibility by moving beyond “available upon request” Transparency and reproducibility enhance the integrity of research results for scientific and public uses and empower novel research applications. Access to data, samples, methods, and reagents used to conduct research and analysis, as well as to the code used to analyze and process data and samples, is a fundamental requirement for transparency and reproducibility. The field sciences (e.g., geology, ecology, and archaeology), where each study is temporally (and often spatially) unique, provide exemplars for the importance of preserving data and samples for further analysis. Yet field sciences, if they even address such access, commonly do so by simply noting “data and samples available upon request.” They lag behind some laboratory sciences in making data and samples available to the broader research community. It is time for this to change. We discuss cultural, financial, and technical barriers to change and ways in which funders, publishers, scientific societies, and others are responding.",Article,vol103
pap397,ecb81c5d18e38b29316da77f69c8a36d5b98f196,jou7,Nature Methods,scmap: projection of single-cell RNA-seq data across data sets,,Letter,vol7
pap398,952241d28abed7d221fc059845043a6463a522bc,con38,International Symposium on Empirical Software Engineering and Measurement,Qualitative Descriptive Methods in Health Science Research,"Objective: The purpose of this methodology paper is to describe an approach to qualitative design known as qualitative descriptive that is well suited to junior health sciences researchers because it can be used with a variety of theoretical approaches, sampling techniques, and data collection strategies. Background: It is often difficult for junior qualitative researchers to pull together the tools and resources they need to embark on a high-quality qualitative research study and to manage the volumes of data they collect during qualitative studies. This paper seeks to pull together much needed resources and provide an overview of methods. Methods: A step-by-step guide to planning a qualitative descriptive study and analyzing the data is provided, utilizing exemplars from the authors’ research. Results: This paper presents steps to conducting a qualitative descriptive study under the following headings: describing the qualitative descriptive approach, designing a qualitative descriptive study, steps to data analysis, and ensuring rigor of findings. Conclusions: The qualitative descriptive approach results in a summary in everyday, factual language that facilitates understanding of a selected phenomenon across disciplines of health science researchers.",Erratum,pro38
pap399,915cd8e2b39eb02723553913d592b2237d4d9960,jou130,Statistical analysis and data mining,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",Letter,vol130
pap400,952241d28abed7d221fc059845043a6463a522bc,con55,Workshop on Learning from Authoritative Security Experiment Results,Qualitative Descriptive Methods in Health Science Research,"Objective: The purpose of this methodology paper is to describe an approach to qualitative design known as qualitative descriptive that is well suited to junior health sciences researchers because it can be used with a variety of theoretical approaches, sampling techniques, and data collection strategies. Background: It is often difficult for junior qualitative researchers to pull together the tools and resources they need to embark on a high-quality qualitative research study and to manage the volumes of data they collect during qualitative studies. This paper seeks to pull together much needed resources and provide an overview of methods. Methods: A step-by-step guide to planning a qualitative descriptive study and analyzing the data is provided, utilizing exemplars from the authors’ research. Results: This paper presents steps to conducting a qualitative descriptive study under the following headings: describing the qualitative descriptive approach, designing a qualitative descriptive study, steps to data analysis, and ensuring rigor of findings. Conclusions: The qualitative descriptive approach results in a summary in everyday, factual language that facilitates understanding of a selected phenomenon across disciplines of health science researchers.",Erratum,pro55
pap401,915cd8e2b39eb02723553913d592b2237d4d9960,jou130,Statistical analysis and data mining,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",Article,vol130
pap402,d62126bfe0e1b299c9383bb30ee099c77aee5222,con9,Big Data,"Interpreting Qualitative Data: Methods for Analysing Talk, Text and Interaction","This a much expanded and updated version of David Silvermans best-selling introductory textbook for the beginning qualitative researcher. 
 
Features of the New Edition: 
• Takes account of the flood of qualitative work since the 1990s 
• All chapters have been substantially rewritten with the aim of greater clarity 
• A new chapter on Visual Images and a considerably expanded treatment of discourse analysis are provided 
• The number of student exercises has been considerably increased and are now present at the end of every chapter 
• An even greater degree of student accessibility: Key Points and Recommended Readings appear at the end of each chapter and technical terms are highlighted and appear in a Glossary 
• A more inter-disciplinary social science text which takes account of the growing interest in qualitative research outside sociology and anthropology from psychology to geography, information systems, health promotion, management and many other disciplines 
• Expanded coverage 50% longer than the First Edition 
This book has a more recent edition (2006)",Erratum,pro9
pap403,835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,jou131,Royal Society Open Science,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",Letter,vol131
pap404,43d75d3a22db904d052d4c435e2d1f22be3887e0,jou3,IEEE Transactions on Knowledge and Data Engineering,Outlier Detection for Temporal Data: A Survey,"In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatio-temporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.",Letter,vol3
pap405,e281464d9a558cc1d25084687efb75683e65d4f0,con38,International Symposium on Empirical Software Engineering and Measurement,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",Erratum,pro38
pap406,7ac8f533a18f584387dd412a0a27feb9af1c5c93,jou49,ACM Computing Surveys,A Systematic Review on Imbalanced Data Challenges in Machine Learning,"In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.",Article,vol49
pap407,2e096b5fe420e09e3a7ea3b1e8f1501495d8b07e,jou18,Scientific Data,Operationalizing the CARE and FAIR Principles for Indigenous data futures,,Conference paper,vol18
pap408,0efa865dd45bcee8194bfe709b0f81789f6d5341,jou18,Scientific Data,Data sharing practices and data availability upon request differ across scientific disciplines,,Letter,vol18
pap409,023eb29b711014b1a3d2895e19a0fc2aed7a6ab4,con20,ACM Conference on Economics and Computation,Data Science and Classification,,Erratum,pro20
pap410,e048f6fdb0a728638af5d8684a32b3dc2ee83259,jou132,Big Data Research,Big Data and Science: Myths and Reality,,Letter,vol132
pap411,f03a847c6325d7d5973efd687d2ca86a9c06dd76,con110,Very Large Data Bases Conference,Advances in data science and classification,,Erratum,pro110
pap412,de5acd80c5fd8db442a4a5e5ffbc3f3f51161237,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"Data Science, Classification and Related Methods",,Erratum,pro98
pap413,197b30ab1460fe200dba90dc3392ad49a92c2ca4,con44,International Conference Knowledge Engineering and Knowledge Management,Between Data Science and Applied Data Analysis,,Erratum,pro44
pap414,4d12b00963aa6e0d9b9b84a62f0543de608fccb5,jou133,PLoS ONE,"If We Share Data, Will Anyone Use Them? Data Sharing and Reuse in the Long Tail of Science and Technology","Research on practices to share and reuse data will inform the design of infrastructure to support data collection, management, and discovery in the long tail of science and technology. These are research domains in which data tend to be local in character, minimally structured, and minimally documented. We report on a ten-year study of the Center for Embedded Network Sensing (CENS), a National Science Foundation Science and Technology Center. We found that CENS researchers are willing to share their data, but few are asked to do so, and in only a few domain areas do their funders or journals require them to deposit data. Few repositories exist to accept data in CENS research areas.. Data sharing tends to occur only through interpersonal exchanges. CENS researchers obtain data from repositories, and occasionally from registries and individuals, to provide context, calibration, or other forms of background for their studies. Neither CENS researchers nor those who request access to CENS data appear to use external data for primary research questions or for replication of studies. CENS researchers are willing to share data if they receive credit and retain first rights to publish their results. Practices of releasing, sharing, and reusing of data in CENS reaffirm the gift culture of scholarship, in which goods are bartered between trusted colleagues rather than treated as commodities.",Conference paper,vol133
pap415,27245e65a27bde90b5b0bb25d157bb75a0ad8b5a,jou134,EURASIP Journal on Advances in Signal Processing,A survey of machine learning for big data processing,,Conference paper,vol134
pap416,ecef432e7f6c9f431d5b34706a8de1fdebec46f9,jou135,Frontiers in Medicine,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",Conference paper,vol135
pap417,d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,con18,International Conference on Exploring Services Science,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",Erratum,pro18
pap418,c1e49d830e67269d4d2053a5f124ea773c79b740,jou103,Science,Computational social science: Obstacles and opportunities,"Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.",Article,vol103
pap419,846883b7761cb5fe4468d42bf9d328b5d1030175,jou136,Publications of the Astronomical Society of the Pacific,"The Zwicky Transient Facility: Data Processing, Products, and Archive","The Zwicky Transient Facility (ZTF) is a new robotic time-domain survey currently in progress using the Palomar 48-inch Schmidt Telescope. ZTF uses a 47 square degree field with a 600 megapixel camera to scan the entire northern visible sky at rates of ∼3760 square degrees/hour to median depths of g ∼ 20.8 and r ∼ 20.6 mag (AB, 5σ in 30 sec). We describe the Science Data System that is housed at IPAC, Caltech. This comprises the data-processing pipelines, alert production system, data archive, and user interfaces for accessing and analyzing the products. The real-time pipeline employs a novel image-differencing algorithm, optimized for the detection of point-source transient events. These events are vetted for reliability using a machine-learned classifier and combined with contextual information to generate data-rich alert packets. The packets become available for distribution typically within 13 minutes (95th percentile) of observation. Detected events are also linked to generate candidate moving-object tracks using a novel algorithm. Objects that move fast enough to streak in the individual exposures are also extracted and vetted. We present some preliminary results of the calibration performance delivered by the real-time pipeline. The reconstructed astrometric accuracy per science image with respect to Gaia DR1 is typically 45 to 85 milliarcsec. This is the RMS per-axis on the sky for sources extracted with photometric S/N ≥ 10 and hence corresponds to the typical astrometric uncertainty down to this limit. The derived photometric precision (repeatability) at bright unsaturated fluxes varies between 8 and 25 millimag. The high end of these ranges corresponds to an airmass approaching ∼2—the limit of the public survey. Photometric calibration accuracy with respect to Pan-STARRS1 is generally better than 2%. The products support a broad range of scientific applications: fast and young supernovae; rare flux transients; variable stars; eclipsing binaries; variability from active galactic nuclei; counterparts to gravitational wave sources; a more complete census of Type Ia supernovae; and solar-system objects.",Conference paper,vol136
pap420,e7318c770fc084a39436f4e043b18ecea2c2d6cd,jou137,Bulletin of The American Meteorological Society - (BAMS),THE GLOBAL PRECIPITATION MEASUREMENT (GPM) MISSION FOR SCIENCE AND SOCIETY.,The GPM mission collects essential rain and snow data for scientific studies and societal benefit.,Article,vol137
pap421,3859aef8d52ef1bad6351ec25c4fe4009b184689,con110,Very Large Data Bases Conference,Characterization of the LIGO detectors during their sixth science run,"In 2009-2010, the Laser Interferometer Gravitational-wave Observa- tory (LIGO) operated together with international partners Virgo and GEO600 as a network to search for gravitational waves of astrophysical origin. The sensitiv- ity of these detectors was limited by a combination of noise sources inherent to the instrumental design and its environment, often localized in time or frequency, that couple into the gravitational-wave readout. Here we review the performance of the LIGO instruments during this epoch, the work done to characterize the de- tectors and their data, and the effect that transient and continuous noise artefacts have on the sensitivity of LIGO to a variety of astrophysical sources.",Erratum,pro110
pap422,09ee0ba924ffd21fc7e14ad3147284133cf2f576,con14,Hawaii International Conference on System Sciences,"Color Science, Concepts and Methods. Quantitative Data and Formulas","G. Wyszecki and W. S. Stiles London: John Wiley. 1967. Pp. xiv + 628. Price £11. This remarkable and unusual book is by two outstanding authorities on the science of colour: Dr. Stiles, for many years a senior member of the Light Division at the National Physical Laboratory, and Dr. Wyszecki, currently in charge of the Radiation Optics Section of the Canadian National Research Council. The authors' aim has been to provide a comprehensive source book of data required by the practical and theoretical worker in the field of colour and they have achieved this aim so successfully that their book is likely to become the standard work on the subject and to remain so for a good many years.",Erratum,pro14
pap423,04e5f980428e1ec35429356b3e43ea611fc0e975,jou138,Sociological Methods & Research,Using Twitter for Demographic and Social Science Research: Tools for Data Collection and Processing,"Despite recent and growing interest in using Twitter to examine human behavior and attitudes, there is still significant room for growth regarding the ability to leverage Twitter data for social science research. In particular, gleaning demographic information about Twitter users—a key component of much social science research—remains a challenge. This article develops an accurate and reliable data processing approach for social science researchers interested in using Twitter data to examine behaviors and attitudes, as well as the demographic characteristics of the populations expressing or engaging in them. Using information gathered from Twitter users who state an intention to not vote in the 2012 presidential election, we describe and evaluate a method for processing data to retrieve demographic information reported by users that is not encoded as text (e.g., details of images) and evaluate the reliability of these techniques. We end by assessing the challenges of this data collection strategy and discussing how large-scale social media data may benefit demographic researchers.",Letter,vol138
pap424,d33d879ea94fd36363dc7f015896ac6c0236acac,con8,Frontiers in Education Conference,Data Preprocessing in Data Mining,,Erratum,pro8
pap425,86b05bc7e953e683fa839ad01d6100a8f99558df,con48,ACM Symposium on Applied Computing,Concrete mathematics - a foundation for computer science,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",Erratum,pro48
pap426,4b4b63405efd22a96cc45b22c08124d62a475d6f,jou6,Journal of Big Data,Big healthcare data: preserving security and privacy,,Article,vol6
pap427,db019eec15d8080086bbc7dc8f5832e431202e0e,con81,International Conference on Learning Representations,Jupyter: Thinking and Storytelling With Code and Data,"Project Jupyter is an open-source project for interactive computing widely used in data science, machine learning, and scientific computing. We argue that even though Jupyter helps users perform complex, technical work, Jupyter itself solves problems that are fundamentally human in nature. Namely, Jupyter helps humans to think and tell stories with code and data. We illustrate this by describing three dimensions of Jupyter: 1) interactive computing; 2) computational narratives; and 3) the idea that Jupyter is more than software. We illustrate the impact of these dimensions on a community of practice in earth and climate science.",Erratum,pro81
pap428,f4156a05a47fdeda30638e10954d3674cc056ab6,con34,International Conference on Agile Software Development,Discovering Knowledge in Data: An Introduction to Data Mining,"This book is the first volume of a three-volume series on data mining, which introduces the reader to this rapidly growing field. Data mining, which has gained noticeable popularity in the past decade, is essentially an interdisciplinary field bringing together techniques from machine learning, pattern recognition, statistics, databases, and visualization (Cabena et al., 1998) to address the issue of exploring large and complicated databases to identify “interesting” relationships, e.g., high order interactions, or very non-linear relationships that ordinarily would not be detected by standard statistical analyses (Borok, 1997; Szolvits, 1995). This area has been approached by computer scientists and statisticians from slightly different perspectives. The author of the book is a statistician, but has tried to include a computer science theme throughout the book, in which I think he has been successful. As he mentions in the preface, the book is intended to be used either by analysts, managers, and decision makers in industry or as a textbook for an introductory course in data mining for graduate or advanced undergraduate students (in computer science or statistics). Chapter 1 is a short introductory chapter, in which in addition to a brief description of the Cross-Industry Standard Process for Data Mining (CRISP-DM), several real-world case studies are covered to motivate the topics of subsequent chapters. These case studies are also used to describe the first phase of the CRISPDM process, namely business understanding. Chapters 2 and 3 examine the next two phases of the CRISP-DM process, i.e., data understanding and data preparation. Chapter 2 is on data preprocessing, which is divided into the two major tasks of data cleaning and data transformation. In data cleaning, general methods for handling missing data, identifying misclassified records in the data, and also a graphical method for detecting outliers are described. In the data transformation section, min-max normalization and z-score standardization methods are discussed. A numerical method for detecting outliers based on z-score standardization is also covered. Exploratory data analysis is the topic of Chapter 3, which focuses on data understanding. The chapter begins with making a contrast between hypothesis testing and exploratory data analysis, and is followed by the basic ideas of dealing with correlated variables in the data set. Most of this chapter is dedicated to exploring the variables in a real data set, in which by using several diagrams a number of intuitive approaches for obtaining a high level understanding of the data are proposed.",Erratum,pro34
pap429,8cd71d704f9d3eeb5eb697e412ba54b680f00636,jou139,JMIR Medical Informatics,Big Data and Clinicians: A Review on the State of the Science,"Background In the past few decades, medically related data collection saw a huge increase, referred to as big data. These huge datasets bring challenges in storage, processing, and analysis. In clinical medicine, big data is expected to play an important role in identifying causality of patient symptoms, in predicting hazards of disease incidence or reoccurrence, and in improving primary-care quality. Objective The objective of this review was to provide an overview of the features of clinical big data, describe a few commonly employed computational algorithms, statistical methods, and software toolkits for data manipulation and analysis, and discuss the challenges and limitations in this realm. Methods We conducted a literature review to identify studies on big data in medicine, especially clinical medicine. We used different combinations of keywords to search PubMed, Science Direct, Web of Knowledge, and Google Scholar for literature of interest from the past 10 years. Results This paper reviewed studies that analyzed clinical big data and discussed issues related to storage and analysis of this type of data. Conclusions Big data is becoming a common feature of biological and clinical studies. Researchers who use clinical big data face multiple challenges, and the data itself has limitations. It is imperative that methodologies for data analysis keep pace with our ability to collect and store data.",Letter,vol139
pap430,8807a8327e27298fd601fc65e6a9ccfae1cca195,jou133,PLoS ONE,What Is Citizen Science? – A Scientometric Meta-Analysis,"Context The concept of citizen science (CS) is currently referred to by many actors inside and outside science and research. Several descriptions of this purportedly new approach of science are often heard in connection with large datasets and the possibilities of mobilizing crowds outside science to assists with observations and classifications. However, other accounts refer to CS as a way of democratizing science, aiding concerned communities in creating data to influence policy and as a way of promoting political decision processes involving environment and health. Objective In this study we analyse two datasets (N = 1935, N = 633) retrieved from the Web of Science (WoS) with the aim of giving a scientometric description of what the concept of CS entails. We account for its development over time, and what strands of research that has adopted CS and give an assessment of what scientific output has been achieved in CS-related projects. To attain this, scientometric methods have been combined with qualitative approaches to render more precise search terms. Results Results indicate that there are three main focal points of CS. The largest is composed of research on biology, conservation and ecology, and utilizes CS mainly as a methodology of collecting and classifying data. A second strand of research has emerged through geographic information research, where citizens participate in the collection of geographic data. Thirdly, there is a line of research relating to the social sciences and epidemiology, which studies and facilitates public participation in relation to environmental issues and health. In terms of scientific output, the largest body of articles are to be found in biology and conservation research. In absolute numbers, the amount of publications generated by CS is low (N = 1935), but over the past decade a new and very productive line of CS based on digital platforms has emerged for the collection and classification of data.",Article,vol133
pap431,0578dfb2a28b77abde19b32de777e0365df3020e,jou140,Applied Physics Reviews,Data-driven materials research enabled by natural language processing and information extraction,"Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains—enabled by techniques adapted from the field of natural language processing—therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.",Article,vol140
pap432,db8335198bd47c8865d0b3408b97e547abfd9ba2,con23,International Conference on Open and Big Data,The Fourth Paradigm: Data-Intensive Scientific Discovery,"This presentation will set out the eScience agenda by explaining the current scientific data deluge and the case for a “Fourth Paradigm” for scientific exploration. Examples of data intensive science will be used to illustrate the explosion of data and the associated new challenges for data capture, curation, analysis, and sharing. The role of cloud computing, collaboration services, and research repositories will be discussed.",Erratum,pro23
pap433,c278f3e91bf11c72be6808972f00810f15d877a4,jou141,Sustainability Science,Mapping citizen science contributions to the UN sustainable development goals,,Letter,vol141
pap434,fd40e458a67f9a3854834fd42b66b0d6ed43ab8d,con30,PS,Educational Data Mining and Learning Analytics,,Erratum,pro30
pap435,a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,con5,Technical Symposium on Computer Science Education,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",Erratum,pro5
pap436,0131258a516da6f9d86795fc6ed4968206dba005,jou58,Journal of Data and Information Science,A Criteria-based Assessment of the Coverage of Scopus and Web of Science,"Abstract Purpose The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. Design/methodology/approach The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Findings Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Research limitation Comparing with data from only one country is a limitation of the study, but the criteria used to define a country’s scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. Originality/value The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources.",Article,vol58
pap437,851a4c4e9d9bf8f023bc4cd29e023e4c43957b7d,con43,IEEE International Conference on Software Maintenance and Evolution,The Art and Science of Data-Driven Journalism,"Journalists have been using data in their stories for as long as the profession has existed. A revolution in computing in the 20th century created opportunities for data integration into investigations, as journalists began to bring technology into their work. In the 21st century, a revolution in connectivity is leading the media toward new horizons. The Internet, cloud computing, agile development, mobile devices, and open source software have transformed the practice of journalism, leading to the emergence of a new term: data journalism. Although journalists have been using data in their stories for as long as they have been engaged in reporting, data journalism is more than traditional journalism with more data. Decades after early pioneers successfully applied computer-assisted reporting and social science to investigative journalism, journalists are creating news apps and interactive features that help people understand data, explore it, and act upon the insights derived from it. New business models are emerging in which data is a raw material for profit, impact, and insight, co-created with an audience that was formerly reduced to passive consumption. Journalists around the world are grappling with the excitement and the challenge of telling compelling stories by harnessing the vast quantity of data that our increasingly networked lives, devices, businesses, and governments produce every day. While the potential of data journalism is immense, the pitfalls and challenges to its adoption throughout the media are similarly significant, from digital literacy to competition for scarce resources in newsrooms. Global threats to press freedom, digital security, and limited access to data create difficult working conditions for journalists in many countries. A combination of peer-to-peer learning, mentorship, online training, open data initiatives, and new programs at journalism schools rising to the challenge, however, offer reasons to be optimistic about more journalists learning to treat data as a source.",Erratum,pro43
pap438,377f1e43c5a48f12b0592b09a142322e74729409,jou142,Annals of Data Science,Genetic Algorithms in the Fields of Artificial Intelligence and Data Sciences,,Letter,vol142
pap439,8958efba7a02e3653f27c0e759882b2f3352e896,jou18,Scientific Data,"Materials Cloud, a platform for open computational science",,Article,vol18
pap440,fbd9ddc0a3862512ce7a0ba2bb9cb159da0a9d2f,jou143,"Marketing science (Providence, R.I.)",Editorial - Marketing Science and Big Data,"This article was downloaded by: [128.97.27.20] On: 25 May 2016, At: 09:44 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA Marketing Science Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Editorial—Marketing Science and Big Data Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser To cite this article: Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser (2016) Editorial—Marketing Science and Big Data. Marketing Science 35(3):341-342. http://dx.doi.org/10.1287/mksc.2016.0996 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",Article,vol143
pap441,64ad643e8084486ca7d3312ed491a814d3fe440c,con16,International Conference on Data Science and Advanced Analytics,The Synthetic Data Vault,"The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.",Conference paper,pro16
pap442,2e888654c68524163fbf7a54396488249e73a702,con106,International Conference on Mobile Data Management,Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy,"Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science.",Erratum,pro106
pap443,760d38a08bff329ff67719935c18fa1631e3ded8,con32,International Conference on Software Technology: Methods and Tools,The View from Above: Applications of Satellite Data in Economics,"The past decade or so has seen a dramatic change in the way that economists can learn by watching our planet from above. A revolution has taken place in remote sensing and allied fields such as computer science, engineering, and geography. Petabytes of satellite imagery have become publicly accessible at increasing resolution, many algorithms for extracting meaningful social science information from these images are now routine, and modern cloud-based processing power allows these algorithms to be run at global scale. This paper seeks to introduce economists to the science of remotely sensed data, and to give a flavor of how this new source of data has been used by economists so far and what might be done in the future.",Erratum,pro32
pap444,53834f0ee8df731cf0e629cd594dce0afaaa3d97,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer",Erratum,pro98
pap445,d43e2d9b90c0f509c9f569b9d4bd431ebd711f4f,jou144,Psychology Science,Sharing Data and Materials in Psychological Science,,Conference paper,vol144
pap446,83b2bc2583862fa662cdfeb6cc7950bb2972347d,jou145,Immunologic research,ImmPort: disseminating data to the public for the future of immunology,,Conference paper,vol145
pap447,25e0d93ca47d86510d6a0f9cda9ae3594f3d05b2,con55,Workshop on Learning from Authoritative Security Experiment Results,"Color Science: Concepts and Methods, Quantitative Data and Formulas","Eventually, you will agreed discover a further experience and achievement by spending more cash. still when? attain you acknowledge that you require to acquire those every needs in imitation of having significantly cash? Why don't you try to acquire something basic in the beginning? That's something that will lead you to comprehend even more in the region of the globe, experience, some places, later than history, amusement, and a lot more?",Erratum,pro55
pap448,0bc97adfb3c77f27397d19395af2fdff9f04aaa0,con50,International Workshop on Green and Sustainable Software,The TESS science processing operations center,"The Transiting Exoplanet Survey Satellite (TESS) will conduct a search for Earth's closest cousins starting in early 2018 and is expected to discover ∼1,000 small planets with Rp < 4 R⊕ and measure the masses of at least 50 of these small worlds. The Science Processing Operations Center (SPOC) is being developed at NASA Ames Research Center based on the Kepler science pipeline and will generate calibrated pixels and light curves on the NASA Advanced Supercomputing Division's Pleiades supercomputer. The SPOC will also search for periodic transit events and generate validation products for the transit-like features in the light curves. All TESS SPOC data products will be archived to the Mikulski Archive for Space Telescopes (MAST).",Erratum,pro50
pap449,d90f276316589f503690d541392989031f9d046b,con66,International Conference on Software Reuse,Online Citizen Science: A Systematic Review of Effects on Learning and Scientific Literacy,"Participation in online citizen science is increasingly popular, yet studies that examine the impact on participants’ learning are limited. The aims of this paper are to identify the learning impact on volunteers who participate in online citizen science projects and to explore the methods used to study the impact. The ten empirical studies, examined in this systematic review, report learning impacts on citizens’ attitudes towards science, on their understanding of the nature of science, on topic-specific knowledge, on science knowledge, and on generic knowledge. These impacts were measured using self-reports, content analysis of contributed data and of forum posts, accuracy checks of contributed data, science and project-specific quizzes, and instruments for measuring scientific attitudes and beliefs. The findings highlight that certain technological affordances in online citizen science projects can cultivate citizens’ knowledge and skills, and they point to unexplored areas, including the lack of experimental and long-term studies, and studies in formal education settings.",Erratum,pro66
pap450,929607741b2a12656ff8d3360ca96fe76a6557a4,con8,Frontiers in Education Conference,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",Erratum,pro8
pap451,1a46465ab69ec13d3c84d66166e979989afa596d,jou103,Science,Comment on “Estimating the reproducibility of psychological science”,"A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.",Article,vol103
pap452,e1ababf08c9ec103db854a2c1b4db611142cfdb7,con93,International Conference on Computational Logic,Linear Mixed Models for Longitudinal Data,,Erratum,pro93
pap453,720400bf69c1af50795d7ec1b58e95c682d217aa,jou88,bioRxiv,Best Practices in Data Analysis and Sharing in Neuroimaging using MRI,"Neuroimaging enables rich noninvasive measurements of human brain activity, but translating such data into neuroscientific insights and clinical applications requires complex analyses and collaboration among a diverse array of researchers. The open science movement is reshaping scientific culture and addressing the challenges of transparency and reproducibility of research. To advance open science in neuroimaging the Organization for Human Brain Mapping created the Committee on Best Practice in Data Analysis and Sharing (COBIDAS), charged with creating a report that collects best practice recommendations from experts and the entire brain imaging community. The purpose of this work is to elaborate the principles of open and reproducible research for neuroimaging using Magnetic Resonance Imaging (MRI), and then distill these principles to specific research practices. Many elements of a study are so varied that practice cannot be prescribed, but for these areas we detail the information that must be reported to fully understand and potentially replicate a study. For other elements of a study, like statistical modelling where specific poor practices can be identified, and the emerging areas of data sharing and reproducibility, we detail both good practice and reporting standards. For each of seven areas of a study we provide tabular listing of over 100 items to help plan, execute, report and share research in the most transparent fashion. Whether for individual scientists, or for editors and reviewers, we hope these guidelines serve as a benchmark, to raise the standards of practice and reporting in neuroimaging using MRI.",Conference paper,vol88
pap454,edacaedb1b2312023c4b0cf1d42bbdbed2793c65,con90,Computer Vision and Pattern Recognition,The Electric and Magnetic Field Instrument Suite and Integrated Science (EMFISIS) on RBSP,,Erratum,pro90
pap455,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,jou16,Big Data & Society,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",Article,vol16
pap456,c8bc2d5edb9307b5c420adc4eee3cf641a781b14,con33,International Conference on Automated Software Engineering,Online analysis enhances use of NASA Earth science data,"Giovanni, the Goddard Earth Sciences Data and Information Services Center (GES DISC) Interactive Online Visualization and Analysis Infrastructure, has provided researchers with advanced capabilities to perform data exploration and analysis with observational data from NASA Earth observation satellites. In the past 5–10 years, examining geophysical events and processes with remote-sensing data required a multistep process of data discovery, data acquisition, data management, and ultimately data analysis. Giovanni accelerates this process by enabling basic visualization and analysis directly on the World Wide Web. In the last two years, Giovanni has added new data acquisition functions and expanded analysis options to increase its usefulness to the Earth science research community.",Erratum,pro33
pap457,43789305e5d2212da05f9c16b148e84aae5614b2,con92,Human Language Technology - The Baltic Perspectiv,Citizen Science and Volunteered Geographic Information: Overview and Typology of Participation,,Erratum,pro92
pap458,917943472ec4a00443d78bb696ed4d8f8d8c7f0a,con101,International Conference on Biometrics,Understanding the Science Experiences of Successful Women of Color: Science Identity as an Analytic Lens.,"In this study, we develop a model of science identity to make sense of the science experiences of 15 successful women of color over the course of their undergraduate and graduate studies in science and into science-related careers. In our view, science identity accounts both for how women make meaning of science experiences and how society structures possible meanings. Primary data included ethnographic interviews during students' undergraduate careers, follow-up interviews 6 years later, and ongoing member-checking. Our results highlight the importance of recognition by others for women in the three science identity trajectories: research scientist; altruistic scientist; and disrupted scientist. The women with research scientist identities were passionate about science and recognized themselves and were recognized by science faculty as science people. The women with altruistic scientist identities regarded science as a vehicle for altruism and created innovative meanings of ''science,'' ''recognition by others,'' and ''woman of color in science.'' The women with disrupted scientist identities sought, but did not often receive, recognition by meaningful scientific others. Although they were ultimately successful, their trajectories were more difficult because, in part, their bids for recognition were disrupted by the interaction with gendered, ethnic, and racial factors. This study clarifies theoretical conceptions of science identity, promotes a rethinking of recruitment and retention efforts, and illuminates various ways women of color experience, make meaning of, and negotiate the culture of science. 2007 Wiley Periodicals, Inc. J Res Sci Teach 44: 1187-1218, 2007.",Erratum,pro101
pap459,40f19bdaa4e869ab9784880fec5e9e229a2a61ab,jou146,Astrophysical Journal Supplement Series,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",Article,vol146
pap460,f567f5a4a57509c2288f510d6703212ce8499527,jou108,Earth and Space Science,The Ames Stereo Pipeline: NASA's Open Source Software for Deriving and Processing Terrain Data,"The NASA Ames Stereo Pipeline is a suite of free and open source automated geodesy and stereogrammetry tools designed for processing stereo images captured from satellites (around Earth and other planets), robotic rovers, aerial cameras, and historical images, with and without accurate camera pose information. It produces cartographic products, including digital terrain models, ortho‐projected images, 3‐D models, and bundle‐adjusted networks of cameras. Ames Stereo Pipeline's data products are suitable for science analysis, mission planning, and public outreach.",Conference paper,vol108
pap461,6d962e9f04c653f732da82073a3446f75a371055,con93,International Conference on Computational Logic,The KDD process for extracting useful knowledge from volumes of data,"AS WE MARCH INTO THE AGE of digital information, the problem of data overload looms ominously ahead. Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data. A new generation of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data. These techniques and tools are the subject of the emerging field of knowledge discovery in databases (KDD) and data mining. Large databases of digital information are ubiquitous. Data from the neighborhood store’s checkout register, your bank’s credit card authorization device, records in your doctor’s office, patterns in your telephone calls, and many more applications generate streams of digital records archived in huge databases, sometimes in so-called data warehouses. Current hardware and database technology allow efficient and inexpensive reliable data storage and access. However, whether the context is business, medicine, science, or government, the datasets themselves (in raw form) are of little direct value. What is of value is the knowledge that can be inferred from the data and put to use. For example, the marketing database of a consumer U s a m a F a y y a d ,",Erratum,pro93
pap462,34ad09cda075101dc4ce3c04006ff804aca3ebf8,con27,International Conference on Contemporary Computing,"Big data: Issues, challenges, tools and Good practices","Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.",Conference paper,pro27
pap463,62e0c6cf57bc345026d56fd654e80beaf9315c92,con86,The Web Conference,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",Erratum,pro86
pap464,9a7dfcd3c35ebfbce9e359a1a97d6892b83a37ec,con43,IEEE International Conference on Software Maintenance and Evolution,Citizen Science as an Ecological Research Tool: Challenges and Benefits,"Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global “experiments” altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across spa...",Erratum,pro43
pap465,a971f856fcf4a4a7589dbf711dd2544f51c5e9b2,con28,International Conference Geographic Information Science,Linked Data - A Paradigm Shift for Geographic Information Science,,Letter,pro28
pap466,b7118fca8e7cd69d76090a5c145e89f303249eb8,con105,British Machine Vision Conference,The current state of citizen science as a tool for ecological research and public engagement,"Approaches to citizen science – an indispensable means of combining ecological research with environmental education and natural history observation – range from community-based monitoring to the use of the internet to “crowd-source” various scientific tasks, from data collection to discovery. With new tools and mechanisms for engaging learners, citizen science pushes the envelope of what ecologists can achieve, both in expanding the potential for spatial ecology research and in supplementing existing, but localized, research programs. The primary impacts of citizen science are seen in biological studies of global climate change, including analyses of phenology, landscape ecology, and macro-ecology, as well as in sub-disciplines focused on species (rare and invasive), disease, populations, communities, and ecosystems. Citizen science and the resulting ecological data can be viewed as a public good that is generated through increasingly collaborative tools and resources, while supporting public participation in science and Earth stewardship.",Erratum,pro105
pap467,b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,con90,Computer Vision and Pattern Recognition,Handbook of theoretical computer science - Part A: Algorithms and complexity; Part B: Formal models and semantics,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",Erratum,pro90
pap468,90478017154dd6e4dbcb71895c64c9ddddebfb8c,jou147,Scientific Reports,Taxonomic bias in biodiversity data and societal preferences,,Article,vol147
pap469,05859c8d47b16ce84c817c16d29ad6ec9d1d3a33,con50,International Workshop on Green and Sustainable Software,The Science DMZ: A network design pattern for data-intensive science,"The ever-increasing scale of scientific data has become a significant challenge for researchers that rely on networks to interact with remote computing systems and transfer results to collaborators worldwide. Despite the availability of high-capacity connections, scientists struggle with inadequate cyberinfrastructure that cripples data transfer performance, and impedes scientific progress. The Science DMZ paradigm comprises a proven set of network design patterns that collectively address these problems for scientists. We explain the Science DMZ model, including network architecture, system configuration, cybersecurity, and performance tools, that creates an optimized network environment for science. We describe use cases from universities, supercomputing centers and research laboratories, highlighting the effectiveness of the Science DMZ model in diverse operational settings. In all, the Science DMZ model is a solid platform that supports any science workflow, and flexibly accommodates emerging network technologies. As a result, the Science DMZ vastly improves collaboration, accelerating scientific discovery.",Erratum,pro50
pap470,97156d041b6cae2095dd29d76e24e0017a7ec799,con31,International Conference on Evaluation & Assessment in Software Engineering,Functional Data Analysis,,Erratum,pro31
pap471,85cd1c3c6346d8fe3b245cc41e2757631301bc27,jou110,Public Understanding of Science,The lure of rationality: Why does the deficit model persist in science communication?,"Science communication has been historically predicated on the knowledge deficit model. Yet, empirical research has shown that public communication of science is more complex than what the knowledge deficit model suggests. In this essay, we pose four lines of reasoning and present empirical data for why we believe the deficit model still persists in public communication of science. First, we posit that scientists’ training results in the belief that public audiences can and do process information in a rational manner. Second, the persistence of this model may be a product of current institutional structures. Many graduate education programs in science, technology, engineering, and math (STEM) fields generally lack formal training in public communication. We offer empirical evidence that demonstrates that scientists who have less positive attitudes toward the social sciences are more likely to adhere to the knowledge deficit model of science communication. Third, we present empirical evidence of how scientists conceptualize “the public” and link this to attitudes toward the deficit model. We find that perceiving a knowledge deficit in the public is closely tied to scientists’ perceptions of the individuals who comprise the public. Finally, we argue that the knowledge deficit model is perpetuated because it can easily influence public policy for science issues. We propose some ways to uproot the deficit model and move toward more effective science communication efforts, which include training scientists in communication methods grounded in social science research and using approaches that engage community members around scientific issues.",Letter,vol110
pap472,41692ed07f393c1c3e335db99c7e3c5a0d265a78,jou103,Science,Citation indexes for science; a new dimension in documentation through association of ideas.,"‘The uncritical citation of disputed data by a writer, whether it be deliberate or not, is a serious matter. Of course, knowingly propagandizing unsubstantiated claims is particularly abhorrent, but just as many naive students may be swayed by unfounded assertions presented by a writer who is unaware of the criticisms. Buried in scholarly journals, critical notes are increasingly likely to be overlooked with the passage of time, while the studies to which they pertain, having been reported more widely, are apt to be rediscovered.’ 1",Letter,vol103
pap473,a3324c0dcb1efaf5d88003b3fe22a3351b4c16da,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,"""Big Data"" : big gaps of knowledge in the field of internet science","Research on so-called ‘Big Data’ has received a considerable momentum and is expected to grow in the future. One very interesting stream of research on Big Data analyzes online networks. Many online networks are known to have some typical macro-characteristics, such as ‘small world’ properties. Much less is known about underlying micro-processes leading to these properties. The models used by Big Data researchers usually are inspired by mathematical ease of exposition. We propose to follow in addition a different strategy that leads to knowledge about micro-processes that match with actual online behavior. This knowledge can then be used for the selection of mathematically-tractable models of online network formation and evolution. Insight from social and behavioral research is needed for pursuing this strategy of knowledge generation about micro-processes. Accordingly, our proposal points to a unique role that social scientists could play in Big Data research.",Erratum,pro82
pap474,18a940ff6dce8bc140658da52d686291ca965979,con1,International Conference on Human Factors in Computing Systems,The Analysis of Social Science Data with Missing Values,"Methods for handling missing data in social science data sets are reviewed. Limitations of common practical approaches, including complete-case analysis, available-case analysis and imputation, are illustrated on a simple missing-data problem with one complete and one incomplete variable. Two more principled approaches, namely maximum likelihood under a model for the data and missing-data mechanism and multiple imputation, are applied to the bivariate problem. General properties of these methods are outlined, and applications to more complex missing-data problems are discussed. The EM algorithm, a convenient method for computing maximum likelihood estimates in missing-data problems, is described and applied to two common models, the multivariate normal model for continuous data and the multinomial model for discrete data. Multiple imputation under explicit or implicit models is recommended as a method that retains the advantages of imputation and overcomes its limitations.",Erratum,pro1
pap475,5ae073986408c9931bf6887fafb85e253866f7cc,con95,IEEE International Conference on Computer Vision,Fuzzy-Set Social Science,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",Erratum,pro95
pap476,7a1b9cc42e6fc611970b451fbef795e72cbea46d,jou148,Trends in Ecology & Evolution,Ecoinformatics: supporting ecology as a data-intensive science.,,Article,vol148
pap477,a6e594b11bd8195e96a1826f591fcec9a20fdcf3,con37,International Symposium on Search Based Software Engineering,"Frascati manual 2015 : guidelines for collecting and reporting data in research and experimental development: the measurement of scientific, technological and innovation activities.","The Frascati Manual is firmly based on experience gained from collecting R&D 
statistics in both OECD and non-member countries. It is a result of the collective work 
of national experts in NESTI, the OECD Working Party of National Experts on Science 
and Technology Indicators. This group, with support from the OECD Secretariat, has 
worked over now more than 50 years as an effective community of practitioners to 
implement measurement approaches for the concepts of science, technology and 
innovation. This effort has resulted in a series of methodological manuals known as the 
“Frascati Family”, which in addition to this manual includes guidance documents on 
the measurement of innovation (the Oslo Manual), human resources devoted to science 
and technology, patents, and technological balance of payments, but most importantly, 
it has provided the basis for the main statistics and indicators on science and technology 
that are currently used.",Erratum,pro37
pap478,954f2a7b1c6f28c4a845ccda5761eb09da032a64,jou103,Science,Data sharing,"The Science family of journals is committed to sharing data relevant to public health emergencies, and therefore we are signatories to, and wholeheartedly endorse, the following statement by funders and journals.*",Article,vol103
pap479,06d2a3fde80c5644f14f743b29a57f6b02e850d9,jou63,PLoS Biology,The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences,"The iPlant Collaborative provides life science research communities access to comprehensive, scalable, and cohesive computational infrastructure for data management; identity management; collaboration tools; and cloud, high-performance, high-throughput computing. iPlant provides training, learning material, and best practice resources to help all researchers make the best use of their data, expand their computational skill set, and effectively manage their data and computation when working as distributed teams. iPlant’s platform permits researchers to easily deposit and share their data and deploy new computational tools and analysis workflows, allowing the broader community to easily use and reuse those data and computational analyses.",Conference paper,vol63
pap480,69732dcf45024f28e5c43de68d1208f6e737eada,con110,Very Large Data Bases Conference,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",Erratum,pro110
pap481,f6ce14f91b4641942947882062682125369847f7,jou43,Social Science Research Network,The V–Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data,"This material is based upon work supported by the National Science Foundation (SES-1423944, PI: Daniel Pemstein), Riksbankens Jubileumsfond (Grant M13-0559:1, PI: Staffan I. Lindberg), the Swedish Research Council (2013.0166, PI: Staffan I. Lindberg and Jan Teorell), the Knut and Alice Wallenberg Foundation (PI: Staffan I. Lindberg), and the University of Gothenburg (E 2013/43); as well as internal grants from the Vice-Chancellor’s office, the Dean of the College of Social Sciences, and the Department of Political Science at University of Gothenburg. Marquardt acknowledges research support from the Russian Academic Excellence Project ‘5-100.’ We performed simulations and other computational tasks using resources provided by the Notre Dame Center for Research Computing (CRC) through the High Performance Computing section and the Swedish National Infrastructure for Computing (SNIC) at the National Supercomputer Centre in Sweden (SNIC 2016/1-382, SNIC 2017/1-406 and 2017/1-68). We specifically acknowledge the assistance of In-Saeng Suh at CRC and Johan Raber and Peter Mu nger at SNIC in facilitating our use of their respective systems.",Article,vol43
pap482,edf27bb5272ea6fe244deb3bbc8da0429bfe3ac5,jou103,Science,The reusable holdout: Preserving validity in adaptive data analysis,"Testing hypotheses privately Large data sets offer a vast scope for testing already-formulated ideas and exploring new ones. Unfortunately, researchers who attempt to do both on the same data set run the risk of making false discoveries, even when testing and exploration are carried out on distinct subsets of data. Based on ideas drawn from differential privacy, Dwork et al. now provide a theoretical solution. Ideas are tested against aggregate information, whereas individual data set components remain confidential. Preserving that privacy also preserves statistical inference validity. Science, this issue p. 636 A statistical approach allows large data sets to be reanalyzed to test new hypotheses. Misapplication of statistical data analysis is a common cause of spurious discoveries in scientific research. Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. In common practice, however, data analysis is an intrinsically adaptive process, with new analyses generated on the basis of data exploration, as well as the results of previous analyses on the same data. We demonstrate a new approach for addressing the challenges of adaptivity based on insights from privacy-preserving data analysis. As an application, we show how to safely reuse a holdout data set many times to validate the results of adaptively chosen analyses.",Article,vol103
pap483,9386590554c429e80402c082e9d6a2398bcc36b3,con29,ACM-SIAM Symposium on Discrete Algorithms,Data streams: algorithms and applications,"Data stream algorithms as an active research agenda emerged only over the past few years, even though the concept of making few passes over the data for performing computations has been around since the early days of Automata Theory. The data stream agenda now pervades many branches of Computer Science including databases, networking, knowledge discovery and data mining, and hardware systems. Industry is in synch too, with Data Stream Management Systems (DSMSs) and special hardware to deal with data speeds. Even beyond Computer Science, data stream concerns are emerging in physics, atmospheric science and statistics. Data Streams: Algorithms and Applications focuses on the algorithmic foundations of data streaming. In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Data Streams: Algorithms and Applications surveys the emerging area of algorithms for processing data streams and associated applications. An extensive bibliography with over 200 entries points the reader to further resources for exploration.",Article,pro29
pap484,f7d7f1eb559d8e2f410289fca37bb6cec7a3a907,jou16,Big Data & Society,Data politics,"The commentary raises political questions about the ways in which data has been constituted as an object vested with certain powers, influence, and rationalities. We place the emergence and transformation of professional practices such as ‘data science’, ‘data journalism’, ‘data brokerage’, ‘data mining’, ‘data storage’, and ‘data analysis’ as part of the reconfiguration of a series of fields of power and knowledge in the public and private accumulation of data. Data politics asks questions about the ways in which data has become such an object of power and explores how to critically intervene in its deployment as an object of knowledge. It is concerned with the conditions of possibility of data that involve things (infrastructures of servers, devices, and cables), language (code, programming, and algorithms), and people (scientists, entrepreneurs, engineers, information technologists, designers) that together create new worlds. We define ‘data politics’ as both the articulation of political questions about these worlds and the ways in which they provoke subjects to govern themselves and others by making rights claims. We contend that without understanding these conditions of possibility – of worlds, subjects and rights – it would be difficult to intervene in or shape data politics if by that it is meant the transformation of data subjects into data citizens.",Conference paper,vol16
pap485,29196eb8c80a6fd6a159373f14ff323f081a8b7a,jou103,Science,Physical and Virtual Laboratories in Science and Engineering Education,"The world needs young people who are skillful in and enthusiastic about science and who view science as their future career field. Ensuring that we will have such young people requires initiatives that engage students in interesting and motivating science experiences. Today, students can investigate scientific phenomena using the tools, data collection techniques, models, and theories of science in physical laboratories that support interactions with the material world or in virtual laboratories that take advantage of simulations. Here, we review a selection of the literature to contrast the value of physical and virtual investigations and to offer recommendations for combining the two to strengthen science learning.",Conference paper,vol103
pap486,c50dca78e97e335d362d6b991ae0e1448914e9a3,con29,ACM-SIAM Symposium on Discrete Algorithms,Reducing the Dimensionality of Data with Neural,"http://www.sciencemag.org/cgi/content/full/313/5786/504 version of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/cgi/content/full/313/5786/504/DC1 can be found at: Supporting Online Material found at: can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/cgi/content/full/313/5786/504#related-content http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles , 6 of which can be accessed for free: cites 8 articles This article 15 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles 4 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining",Erratum,pro29
pap487,5952a9f10ef65983042794369d376e23d2682d7e,con30,PS,Openness in Political Science: Data Access and Research Transparency,"In 2012, the American Political Science Association (APSA) Council adopted new policies guiding data access and research transparency in political science. The policies appear as a revision to APSA's Guide to Professional Ethics in Political Science. The revisions were the product of an extended and broad consultation with a variety of APSA committees and the association's membership.",Article,pro30
pap488,a418d8fd1cc0abb34cf131d81723bc5da8817c93,con108,International Conference on Information Integration and Web-based Applications & Services,Politicization of Science in the Public Sphere,"This study explores time trends in public trust in science in the United States from 1974 to 2010. More precisely, I test Mooney’s (2005) claim that conservatives in the United States have become increasingly distrustful of science. Using data from the 1974 to 2010 General Social Survey, I examine group differences in trust in science and group-specific change in these attitudes over time. Results show that group differences in trust in science are largely stable over the period, except for respondents identifying as conservative. Conservatives began the period with the highest trust in science, relative to liberals and moderates, and ended the period with the lowest. The patterns for science are also unique when compared to public trust in other secular institutions. Results show enduring differences in trust in science by social class, ethnicity, gender, church attendance, and region. I explore the implications of these findings, specifically, the potential for political divisions to emerge over the cultural authority of science and the social role of experts in the formation of public policy.",Erratum,pro108
pap489,1715fdc4df6774d95ed63f3feb58fa93a84dbed7,jou148,Trends in Ecology & Evolution,Data-intensive science applied to broad-scale citizen science.,,Conference paper,vol148
pap490,8801ce73bea0c97f2d35f5e3bd4f4fdb49698461,con38,International Symposium on Empirical Software Engineering and Measurement,Lessons from lady beetles: accuracy of monitoring data from US and UK citizen-science programs,"Citizen scientists have the potential to play a crucial role in the study of rapidly changing lady beetle (Coccinellidae) populations. We used data derived from three coccinellid-focused citizen-science programs to examine the costs and benefits of data collection from direct citizen-science (data used without verification) and verified citizen-science (observations verified by trained experts) programs. Data collated through direct citizen science overestimated species richness and diversity values in comparison to verified data, thereby influencing interpretation. The use of citizen scientists to collect data also influenced research costs; our analysis shows that verified citizen science was more cost effective than traditional science (in terms of data gathered per dollar). The ability to collect a greater number of samples through direct citizen science may compensate for reduced accuracy, depending on the type of data collected and the type(s) and extent of errors committed by volunteers.",Erratum,pro38
pap491,3954e2d220d9a7b7a46f9561cafb6251524d8ee5,con30,PS,Mars Reconnaissance Orbiter's High Resolution Imaging Science Experiment (HiRISE),"[1] The HiRISE camera features a 0.5 m diameter primary mirror, 12 m effective focal length, and a focal plane system that can acquire images containing up to 28 Gb (gigabits) of data in as little as 6 seconds. HiRISE will provide detailed images (0.25 to 1.3 m/pixel) covering ∼1% of the Martian surface during the 2-year Primary Science Phase (PSP) beginning November 2006. Most images will include color data covering 20% of the potential field of view. A top priority is to acquire ∼1000 stereo pairs and apply precision geometric corrections to enable topographic measurements to better than 25 cm vertical precision. We expect to return more than 12 Tb of HiRISE data during the 2-year PSP, and use pixel binning, conversion from 14 to 8 bit values, and a lossless compression system to increase coverage. HiRISE images are acquired via 14 CCD detectors, each with 2 output channels, and with multiple choices for pixel binning and number of Time Delay and Integration lines. HiRISE will support Mars exploration by locating and characterizing past, present, and future landing sites, unsuccessful landing sites, and past and potentially future rover traverses. We will investigate cratering, volcanism, tectonism, hydrology, sedimentary processes, stratigraphy, aeolian processes, mass wasting, landscape evolution, seasonal processes, climate change, spectrophotometry, glacial and periglacial processes, polar geology, and regolith properties. An Internet Web site (HiWeb) will enable anyone in the world to suggest HiRISE targets on Mars and to easily locate, view, and download HiRISE data products.",Erratum,pro30
pap492,8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,jou149,SIAM Journal on Mathematics of Data Science,Why Are Big Data Matrices Approximately Low Rank?,"Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how...",Letter,vol149
pap493,04638c67b715b9d85ae5a44afd3730b83330fb66,jou103,Science,Economics in the age of big data,"Background Economic science has evolved over several decades toward greater emphasis on empirical work. The data revolution of the past decade is likely to have a further and profound effect on economic research. Increasingly, economists make use of newly available large-scale administrative data or private sector data that often are obtained through collaborations with private firms, giving rise to new opportunities and challenges. The rising use of non–publicly available data in economic research. Here we show the percentage of papers published in the American Economic Review (AER) that obtained an exemption from the AER’s data availability policy, as a share of all papers published by the AER that relied on any form of data (excluding simulations and laboratory experiments). Notes and comments, as well as AER Papers and Proceedings issues, are not included in the analysis. We obtained a record of exemptions directly from the AER administrative staff and coded each exemption manually to reflect public sector versus private data. Our check of nonexempt papers suggests that the AER records may possibly understate the percentage of papers that actually obtained exemptions. The asterisk indicates that data run from when the AER started collecting these data (December 2005 issue) to the September 2014 issue. To make full use of the data, we define year 2006 to cover October 2005 through September 2006, year 2007 to cover October 2006 through September 2007, and so on. Advances These new data are affecting economic research along several dimensions. Many fields have shifted from a reliance on relatively small-sample government surveys to administrative data with universal or near-universal population coverage. This shift is transformative, as it allows researchers to rigorously examine variation in wages, health, productivity, education, and other measures across different subpopulations; construct consistent long-run statistical indices; generate new quasi-experimental research designs; and track diverse outcomes from natural and controlled experiments. Perhaps even more notable is the expansion of private sector data on economic activity. These data, sometimes available from public sources but other times obtained through data-sharing agreements with private firms, can help to create more granular and real-time measurement of aggregate economic statistics. The data also offer researchers a look inside the “black box” of firms and markets by providing meaningful statistics on economic behavior such as search and information gathering, communication, decision-making, and microlevel transactions. Collaborations with data-oriented firms also create new opportunities to conduct and evaluate randomized experiments. Economic theory plays an important role in the analysis of large data sets with complex structure. It can be difficult to organize and study this type of data (or even to decide which variables to construct) without a simplifying conceptual framework, which is where economic models become useful. Better data also allow for sharper tests of existing models and tests of theories that had previously been difficult to assess. Outlook The advent of big data is already allowing for better measurement of economic effects and outcomes and is enabling novel research designs across a range of topics. Over time, these data are likely to affect the types of questions economists pose, by allowing for more focus on population variation and the analysis of a broader range of economic activities and interactions. We also expect economists to increasingly adopt the large-data statistical methods that have been developed in neighboring fields and that often may complement traditional econometric techniques. These data opportunities also raise some important challenges. Perhaps the primary one is developing methods for researchers to access and explore data in ways that respect privacy and confidentiality concerns. This is a major issue in working with both government administrative data and private sector firms. Other challenges include developing the appropriate data management and programming capabilities, as well as designing creative and scalable approaches to summarize, describe, and analyze large-scale and relatively unstructured data sets. These challenges notwithstanding, the next few decades are likely to be a very exciting time for economic research. The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.",Conference paper,vol103
pap494,687e00a5fec7d747d18866f60b7a21973e80b04f,con2,International Conference on Software Engineering,The ethics of smart cities and urban science,"Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. This article is part of the themed issue ‘The ethical impact of data science’.",Erratum,pro2
pap495,c32b03c3b5bbc97b0ec30663da1ff555f30acd95,jou150,Prevention Science,Principled Missing Data Treatments,,Conference paper,vol150
pap496,59b2796c176636a3222d7b129c6209fa6e979aa7,jou16,Big Data & Society,Data infrastructure literacy,"A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",Article,vol16
pap497,832139bd87f51f0a173b5bd9255944748bc31a96,con26,Decision Support Systems,Global multi-resolution terrain elevation data 2010 (GMTED2010),"For more information on the USGS—the Federal source for science about the Earth, its natural and living resources, natural hazards, and the environment, visit http://www.usgs.gov or call 1–888–ASK–USGS. For an overview of USGS information products, including maps, imagery, and publications, Any use of trade, product, or firm names is for descriptive purposes only and does not imply endorsement by the U.S. Government. Although this report is in the public domain, permission must be secured from the individual copyright owners to reproduce any copyrighted materials contained within this report. 10. Diagram showing the GMTED2010 layer extents (minimum and maximum latitude and longitude) are a result of the coordinate system inherited from the 1-arc-second SRTM",Erratum,pro26
pap498,951eab2b27c673e0ff1a20800f576d4792f60d5f,jou103,Science,Crisis informatics—New data for extraordinary times,"Focus on behaviors, not on fetishizing social media tools Crisis informatics is a multidisciplinary field combining computing and social science knowledge of disasters; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. We study and develop computational support for collection and sociobehavioral analysis of online participation (i.e., tweets and Facebook posts) to address challenges in disaster warning, response, and recovery. Because such data are rarely tidy, we offer lessons—learned the hard way, as we have made every mistake described below—with respect to the opportunities and limitations of social media research on crisis events.",Conference paper,vol103
pap499,0d7a9a5233b1460941b51a50e032b3c5d3a711cc,con107,Chinese Conference on Biometric Recognition,The Interview: Data Collection in Descriptive Phenomenological Human Scientific Research*,"Abstract In this article, interviewing from a descriptive, phenomenological, human scientific perspective is examined. Methodological issues are raised in relation to evaluative criteria as well as reflective matters that concern the phenomenological researcher. The data collection issues covered are 1) the selection of participants, 2) the number of participants in a study, 3) the interviewer and the questions, and 4) data collection procedures. Certain conclusions were drawn indicating that phenomenological research methods cannot be evaluated on the basis of an empiricist theory of science, but must be critiqued from within a phenomenological theory of science. Some reflective matters, experienced by the phenomenological researcher, are also elaborated upon.",Erratum,pro107
pap500,55bdaa9d27ed595e2ccf34b3a7847020cc9c946c,con2,International Conference on Software Engineering,Performing systematic literature reviews in software engineering,"Context: Making best use of the growing number of empirical studies in Software Engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. Adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in Software Engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.Objectives: To provide an introduction to the role, form and processes involved in performing Systematic Literature Reviews. After the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.Method: We will use a blend of information presentation (including some experiences of the problems that can arise in the Software Engineering domain), and also of interactive working, using review material prepared in advance.",Conference paper,pro2
pap501,72910077a29caf411dbb03148997c72b47e65ab0,jou151,IEEE Transactions on Software Engineering,Software Engineering Economics,"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.",Article,vol151
pap502,27e57cc2f22c1921d2a1c3954d5062e3fe391553,jou152,Empirical Software Engineering,Guidelines for conducting and reporting case study research in software engineering,,Article,vol152
pap503,81dbfc1bc890368979399874e47e0529ddceaece,con22,Grid Computing Environments,Software Engineering: A Practitioner's Approach,,Erratum,pro22
pap504,f70b2f20be241f445a61f33c4b8e76e554760340,con24,International Conference on Data Technologies and Applications,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",Erratum,pro24
pap505,0961e2650b3a62a1d198a046bef5f0700ab8c08f,con31,International Conference on Evaluation & Assessment in Software Engineering,Guidelines for snowballing in systematic literature studies and a replication in software engineering,"Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.
 Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.
 Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.
 Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.
 Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.",Letter,pro31
pap506,1a6fc05a37ae1b2fc7c6193cb6dc8282767c2cb6,jou153,ACM Transactions on Software Engineering and Methodology,Software Engineering for AI-Based Systems: A Survey,"AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.",Letter,vol153
pap507,335ddd3cbbea7b20012c781a260ca61c91a2ed77,jou154,IEEE spectrum,Software engineering,"The need for automation of software development is discussed in the context of next-generation computing. The lag in the use of available tools is pointed out. Rapid prototyping and the reuse of existing program components are two solutions discussed here. Finally, the elements needed for an effective integrated, automated programming environment are considered.",Letter,vol154
pap508,775a9c722262c7b656876a5fef20f4577afd8981,con2,International Conference on Software Engineering,Multilingual training for Software Engineering,"Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this data-augmenting approach is broadly compatible with different tasks, languages, and machine-learning models.",Article,pro2
pap509,0ae62f98b822bea0a27c575d8d7480643c1d31a0,con32,International Conference on Software Technology: Methods and Tools,Object-Oriented Software Engineering - a Use Case Driven Approach,,Conference paper,pro32
pap510,2bd576ce574df33c834b6032962cd5ae0be5299f,jou155,Information and Software Technology,Guidelines for conducting systematic mapping studies in software engineering: An update,,Letter,vol155
pap511,1c11551dc313de80afe3c39f68f0cdb51aaf319a,jou156,Journal of Systems and Software,Exploring the intersection between software industry and Software Engineering education - A systematic mapping of Software Engineering Trends,,Conference paper,vol156
pap512,a963d05b9d4acd347ad528e7d098eb53d8f555a2,jou155,Information and Software Technology,Systematic literature reviews in software engineering - A systematic literature review,,Letter,vol155
pap513,e28bdc373de80d7ec0e64631a89e64fbdcdae230,con31,International Conference on Evaluation & Assessment in Software Engineering,Systematic Mapping Studies in Software Engineering,"BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. 
 
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. 
 
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. 
 
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. 
 
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).",Conference paper,pro31
pap514,849a6be5adf1b9a2b6e59ba0290bca06692c0efd,jou152,Empirical Software Engineering,Sampling in software engineering research: a critical review and guidelines,,Conference paper,vol152
pap515,947b29eb3cde8ccb8df9342bb2384ec480ea3964,con1,International Conference on Human Factors in Computing Systems,Experimentation in software engineering: an introduction,,Erratum,pro1
pap516,d0d3f83a5e03ce71fc41852bcda22129d9bdedc0,con15,Pacific Symposium on Biocomputing,Quantum Software Engineering: Landscapes and Horizons,"Quantum software plays a critical role in exploiting the full potential of quantum computing systems. As a result, it is drawing increasing attention recently. This paper defines the term ""quantum software engineering"" and introduces a quantum software life cycle. Based on these, the paper provides a comprehensive survey of the current state of the art in the field and presents the challenges and opportunities that we face. The survey summarizes the technology available in the various phases of the quantum software life cycle, including quantum software requirements analysis, design, implementation, test, and maintenance. It also covers the crucial issue of quantum software reuse.",Erratum,pro15
pap517,4644afca80b98dc39ee8ad4cf1ce1e940214c366,jou152,Empirical Software Engineering,A practical guide on conducting eye tracking studies in software engineering,,Letter,vol152
pap518,0a5d358e643f46f5a5d20892417260800cccc345,con33,International Conference on Automated Software Engineering,Experimentation in Software Engineering,,Erratum,pro33
pap519,73bb47d973d2c8aca4f697c83a99af0e9edba68f,con33,International Conference on Automated Software Engineering,Explainable AI for Software Engineering,"The success of software engineering projects largely depends on complex decision-making. For example, which tasks should a developer do first, who should perform this task, is the software of high quality, is a software system reliable and resilient enough to deploy, etc. However, erroneous decision-making for these complex questions is costly in terms of money and reputation. Thus, Artificial Intelligence/Machine Learning (AI/ML) techniques have been widely used in software engineering for developing software analytics tools and techniques to improve decision-making, developer productivity, and software quality. However, the predictions of such AI/ML models for software engineering are still not practical (i.e., coarse-grained), not explainable, and not actionable. These concerns often hinder the adoption of AI/ML models in software engineering practices. In addition, many recent studies still focus on improving the accuracy, while a few of them focus on improving explainability. Are we moving in the right direction? How can we better improve the SE community (both research and education)?In this tutorial, we first provide a concise yet essential introduction to the most important aspects of Explainable AI and a hands-on tutorial of Explainable AI tools and techniques. Then, we introduce the fundamental knowledge of defect prediction (an example application of AI for Software Engineering). Finally, we demonstrate three successful case studies on how Explainable AI techniques can be used to address the aforementioned challenges by making the predictions of software defect prediction models more practical, explainable, and actionable. The materials are available at https://xai4se.github.io.",Letter,pro33
pap520,21c6beb2a6df81f424e3d1283fbb9cc3157a3115,con34,International Conference on Agile Software Development,A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation,,Letter,pro34
pap521,ed0759b7001f8be53bb4282750e98198b359307d,con33,International Conference on Automated Software Engineering,No Silver Bullet: Essence and Accidents of Software Engineering,"But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one orderof-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed.",Erratum,pro33
pap522,f463018624b6f4b8dd576732b6cce36e31bac978,con60,European Conference on Software Process Improvement,Software Engineering of Self-adaptive Systems,,Erratum,pro60
pap523,30b71975e26dd2709f58372419b712d97536402f,jou156,Journal of Systems and Software,Continuous software engineering: A roadmap and agenda,,Article,vol156
pap524,4052eacd5a0af8a6d4254521244a93d540bf88c1,con14,Hawaii International Conference on System Sciences,Guidelines for including the grey literature and conducting multivocal literature reviews in software engineering,"Context: A Multivocal Literature Review (MLR) is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and -practice in a given area. Objective: There are several guidelines to conduct SLR studies in SE. However, given the facts that several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE. Method: To develop the MLR guidelines, we benefit from three inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. All derived guidelines are discussed in the context of three examples MLRs as running examples (two from SE and one MLR from the medical sciences). Results: The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations. Conclusion: Having been developed on the basis of three types of solid experience and evidence, the provided MLR guidelines support researchers to effectively and efficiently conduct new MLRs in any area of SE.",Erratum,pro14
pap525,acaa6508e741a4fa218f53703046a7ae9592647f,con44,International Conference Knowledge Engineering and Knowledge Management,Open Science in Software Engineering,,Erratum,pro44
pap526,967f4eb786aa143b7eb09f00d9ba8ddfe44e039f,con2,International Conference on Software Engineering,Sentiment Analysis for Software Engineering: How Far Can We Go?,"Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context. We describe our experience in building a software library recommender exploiting crowdsourced opinions mined from Stack Overflow (e.g., what is the sentiment of developers about the usability of a library). To reach our goal, we retrained—on a set of 40k manually labeled sentences/words extracted from Stack Overflow—a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of these tools on a variety of SE datasets. Our results should warn the research community about the strong limitations of current sentiment analysis tools.",Article,pro2
pap527,51b502b9ce774a615474ed8629e74d0dfaa33ee3,jou153,ACM Transactions on Software Engineering and Methodology,The ABC of Software Engineering Research,"A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.",Letter,vol153
pap528,a75965753906c24a6a5715a695757a0de8447f26,jou151,IEEE Transactions on Software Engineering,A Survey of App Store Analysis for Software Engineering,"App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.",Letter,vol151
pap529,fb2bb5777f1b1bd745070c006265edf8feb5f29f,con110,Very Large Data Bases Conference,Smart contracts vulnerabilities: a call for blockchain software engineering?,"Smart Contracts have gained tremendous popularity in the past few years, to the point that billions of US Dollars are currently exchanged every day through such technology. However, since the release of the Frontier network of Ethereum in 2015, there have been many cases in which the execution of Smart Contracts managing Ether coins has led to problems or conflicts. Compared to traditional Software Engineering, a discipline of Smart Contract and Blockchain programming, with standardized best practices that can help solve the mentioned problems and conflicts, is not yet sufficiently developed. Furthermore, Smart Contracts rely on a non-standard software life-cycle, according to which, for instance, delivered applications can hardly be updated or bugs resolved by releasing a new version of the software. In this paper we advocate the need for a discipline of Blockchain Software Engineering, addressing the issues posed by smart contract programming and other applications running on blockchains.We analyse a case of study where a bug discovered in a Smart Contract library, and perhaps ""unsafe"" programming, allowed an attack on Parity, a wallet application, causing the freezing of about 500K Ethers (about 150M USD, in November 2017). In this study we analyze the source code of Parity and the library, and discuss how recognised best practices could mitigate, if adopted and adapted, such detrimental software misbehavior. We also reflect on the specificity of Smart Contract software development, which makes some of the existing approaches insufficient, and call for the definition of a specific Blockchain Software Engineering.",Erratum,pro110
pap530,6f58d8b98c652897842afdd023c535f9724b4eb2,con35,IEEE Working Conference on Mining Software Repositories,A Benchmark Study on Sentiment Analysis for Software Engineering Research,"A recent research trend has emerged to identify developers' emotions, by applying sentiment analysis to the content of communication traces left in collaborative development environments. Trying to overcome the limitations posed by using off-the-shelf sentiment analysis tools, researchers recently started to develop their own tools for the software engineering domain. In this paper, we report a benchmark study to assess the performance and reliability of three sentiment analysis tools specifically customized for software engineering. Furthermore, we offer a reflection on the open challenges, as they emerge from a qualitative analysis of misclassified texts.",Letter,pro35
pap531,6e1cafd50333b3812bf002a51bcb1f720e35b7ed,con35,IEEE Working Conference on Mining Software Repositories,Word Embeddings for the Software Engineering Domain,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pretrained models. State of the art pretrained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pretrained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model.",Letter,pro35
pap532,9ada0c69d4d8cb6fefb8f2dd3370d32df3b627c5,jou152,Empirical Software Engineering,Software engineering in start-up companies: An analysis of 88 experience reports,,Article,vol152
pap533,c94673d81f2b8fb21327f579d30163804480980c,con36,Central and Eastern European Software Engineering Conference in Russia,An Agile Software Engineering Method to Design Blockchain Applications,"Cryptocurrencies and their foundation technology, the Blockchain, are reshaping finance and economics, allowing a decentralized approach enabling trusted applications with no trusted counterpart. More recently, the Blockchain and the programs running on it, called Smart Contracts, are also finding more and more applications in all fields requiring trust and sound certifications. Some people have come to the point of saying that the ""Blockchain revolution"" can be compared to that of the Internet and the Web in their early days. As a result, all the software development revolving around the Blockchain technology is growing at a staggering rate. The feeling of many software engineers about such huge interest in Blockchain technologies is that of unruled and hurried software development, a sort of competition on a first-come-first-served basis which does not assure neither software quality, nor that the basic concepts of software engineering are taken into account.
 This paper tries to cope with this issue, proposing a software development process to gather the requirement, analyze, design, develop, test and deploy Blockchain applications. The process is based on several Agile practices, such as User Stories and iterative and incremental development based on them. However, it makes also use of more formal notations, such as some UML diagrams describing the design of the system, with additions to represent specific concepts found in Blockchain development. The method is described in good detail, and an example is given to show how it works.",Article,pro36
pap534,abc2fb10082454dcd38c86c55763062d230de344,con33,International Conference on Automated Software Engineering,Automated Software Engineering,,Letter,pro33
pap535,f9a9f3f016fa3123c3059cca66314d26f2357155,con103,IEEE International Conference on Multimedia and Expo,"Model-Driven Software Engineering in Practice, Second Edition",,Erratum,pro103
pap536,bcc7041e0fb7717a7d67a9c00e08b7fb81384cbf,jou152,Empirical Software Engineering,Robust Statistical Methods for Empirical Software Engineering,,Conference paper,vol152
pap537,2251b878a1ab6adfbf86fe4c53f947e9a7308541,jou157,Requirements Engineering,On user rationale in software engineering,,Letter,vol157
pap538,ee8c968b55eac1a834f4977486437801d4d716d3,con37,International Symposium on Search Based Software Engineering,Deploying Search Based Software Engineering with Sapienz at Facebook,,Letter,pro37
pap539,bca7c0902f600fa77b1e16d0e093e23f7d75f649,jou152,Empirical Software Engineering,Empirical software engineering experts on the use of students and professionals in experiments,,Letter,vol152
pap540,86ce7a3939c60c497c1fa2800f035968afde4290,con10,Americas Conference on Information Systems,"Software Engineering for Computational Science: Past, Present, Future","Despite the increasing importance of in silico experiments to the scientific discovery process, state-of-the-art software engineering practices are rarely adopted in computational science. To understand the underlying causes for this situation and to identify ways to improve it, the authors conducted a literature survey on software engineering practices in computational science. They identified 13 recurring key characteristics of scientific software development that are the result of the nature of scientific challenges, the limitations of computers, and the cultural environment of scientific software development. Their findings allow them to point out shortcomings of existing approaches for bridging the gap between software engineering and computational science and to provide an outlook on promising research directions that could contribute to improving the current situation.",Erratum,pro10
pap541,c74ad18fe4b9d705affdc2174d17283ecd9db3c9,con38,International Symposium on Empirical Software Engineering and Measurement,Measuring human values in software engineering,"Background: Human values, such as prestige, social justice, and financial success, influence software production decision-making processes. While their subjectivity makes some values difficult to measure, their impact on software motivates our research. Aim: To contribute to the scientific understanding and the empirical investigation of human values in Software Engineering (SE). Approach: Drawing from social psychology, we consider values as mental representations to be investigated on three levels: at a system (L1), personal (L2), and instantiation level (L3). Method: We design and develop a selection of tools for the investigation of values at each level, and focus on the design, development, and use of the Values Q-Sort. Results: From our study with 12 software practitioners, it is possible to extract three values `prototypes' indicative of an emergent typology of values considerations in SE. Conclusions: The Values Q-Sort generates quantitative values prototypes indicating values relations (L1) as well as rich personal narratives (L2) that reflect specific software practices (L3). It thus offers a systematic, empirical approach to capturing values in SE.",Letter,pro38
pap542,11ebce51f6b0a901948953a024abf1cdb9111f3f,con74,IEEE International Conference on Information Reuse and Integration,Design Science Methodology for Information Systems and Software Engineering,,Erratum,pro74
pap543,2b19768afa6fbf5abb19790cd1ee991574129933,jou156,Journal of Systems and Software,A survey of the use of crowdsourcing in software engineering,,Conference paper,vol156
pap544,b8d38c29fb33440b41caa9a1a3baa0d8815d3042,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Software Engineering Challenges of Deep Learning,"Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.",Article,pro39
pap545,261043d80a2a66044e1ee0eaee46c7331687afa4,con35,IEEE Working Conference on Mining Software Repositories,Leveraging Automated Sentiment Analysis in Software Engineering,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed in developing SentiStrength-SE, a tool for improved sentiment analysis especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both quantitative and qualitative evaluations of our tool. SentiStrength-SE achieves 73.85% precision and 85% recall, which are significantly higher than a state-of-the-art sentiment analysis tool we compare with.",Article,pro35
pap546,37a0fcb9ce00b16b47c7030a50075d149a614762,jou158,Lecture Notes in Computer Science,Fundamentals of Software Engineering,,Article,vol158
pap547,63b8ebfe57af400c8bbadc5c111cb5fe71f331bd,jou152,Empirical Software Engineering,On negative results when using sentiment analysis tools for software engineering research,,Letter,vol152
pap548,9a9643601989088ace41382b3c1cc61e1b4d5633,con84,Workshop on Interdisciplinary Software Engineering Research,Blockchain-Oriented Software Engineering: Challenges and New Directions,"In this work, we acknowledge the need for software engineers to devise specialized tools and techniques for blockchain-oriented software development. Ensuring effective testing activities, enhancing collaboration in large teams, and facilitating the development of smart contracts all appear as key factors in the future of blockchain-oriented software development.",Erratum,pro84
pap549,326a2f3705dc7e761e1753aadf95259d7e0e0596,con31,International Conference on Evaluation & Assessment in Software Engineering,Construct Validity in Software Engineering Research and Software Metrics,"Construct validity is essentially the degree to which our scales, metrics and instruments actually measure the properties they are supposed to measure. Although construct validity is widely considered an important quality criterion for most empirical research, many software engineering studies simply assume that proposed measures are valid and make no attempt to assess construct validity. Researchers may ignore construct validity because evaluating it is intrinsically difficult, or due to lack of specific guidance for addressing it. In any case, some research inevitably produces erroneous conclusions, because due to invalid measures. This article therefore attempts to address these problems by explaining the theoretical basis of construct validity, presenting a framework for understanding it, and developing specific guidelines for assessing it. The paper draws on a detailed example involving 15 software metrics, which ostensibly measure the size, coupling and cohesion of Java classes.",Article,pro31
pap550,4b0156d3e002e9a468de26ee8bf4fb773d2eed08,jou159,Clinical engineering,Software engineering,,Conference paper,vol159
pap551,24359b0a34715df2179e084285c16a10f997a145,con40,Conference on Software Engineering Education and Training,Software Engineering Education: Converging with the Startup Industry,"Startups are agents of change that bring in innovations and find solutions to problems at various scales. An all-rounded engineering team is a key driver for the ability to execute the entrepreneurial ambition, from building a minimum viable product to later stages of product vision. Software engineering education provides students with the knowledge to transition to mature companies with defined structure in place successfully. However, the fluidity, risk, time-sensitivity, and uncertainty of startups demand a dynamic and agile set of skills to rapidly identify, conceptualize and deliver features as per market needs. This requires the adoption of latest development trends in software processes, engineering and DevOps practices with automation to iterate fast with low governance and the ability to take on multiple roles. This paper presents a study of the dynamics and engineering at startups and compares it with the current curriculum of software engineering.",Conference paper,pro40
pap552,819f36c0ddae12132d60ebc5cc0a19f7db8668b1,jou151,IEEE Transactions on Software Engineering,Cognitive Biases in Software Engineering: A Systematic Mapping Study,"One source of software project challenges and failures is the systematic errors introduced by human cognitive biases. Although extensively explored in cognitive psychology, investigations concerning cognitive biases have only recently gained popularity in software engineering research. This paper therefore systematically maps, aggregates and synthesizes the literature on cognitive biases in software engineering to generate a comprehensive body of knowledge, understand state-of-the-art research and provide guidelines for future research and practise. Focusing on bias antecedents, effects and mitigation techniques, we identified 65 articles (published between 1990 and 2016), which investigate 37 cognitive biases. Despite strong and increasing interest, the results reveal a scarcity of research on mitigation techniques and poor theoretical foundations in understanding and interpreting cognitive biases. Although bias-related research has generated many new insights in the software engineering community, specific bias mitigation techniques are still needed for software professionals to overcome the deleterious effects of cognitive biases on their work.",Article,vol151
pap553,c176cf31862a7c5b324556e8dc3fdbef2a108391,jou160,IEEE Software,Model-Based Software Engineering to Tame the IoT Jungle,"The Internet of Things (IoT) is a challenging combination of distribution and heterogeneity. A number of software engineering solutions address those challenges in isolation, but few solutions tackle them in combination, which poses a set of concrete challenges. The ThingML (Internet of Things Modeling Language) approach attempts to address those challenges. This model-driven, generative approach, which was inspired by UML, integrates concepts targeted at the IoT. Over the past six years, it has been continuously evolved and applied to cases in different domains, including a commercial e-health solution.",Article,vol160
pap554,9579ed0d182ba134ab3ed14ba0defbb324147399,jou160,IEEE Software,Key Abstractions for IoT-Oriented Software Engineering,"Despite the progress in Internet of Things (IoT) research, a general software engineering approach for systematic development of IoT systems and applications is still missing. A synthesis of the state of the art in the area can help frame the key abstractions related to such development. Such a framework could be the basis for guidelines for IoT-oriented software engineering.",Letter,vol160
pap555,8fe2f59ff3733f9ee50ffa295beda502f4e268e2,con2,International Conference on Software Engineering,Grounded Theory in Software Engineering Research: A Critical Review and Guidelines,"Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.",Letter,pro2
pap556,794c598e037ffac8b8dec326ba29a5dd9044ece6,con70,International Conference on Graph Transformation,Modeling in Event-B - System and Software Engineering,"A practical text suitable for an introductory or advanced course in formal methods, this book presents a mathematical approach to modelling and designing systems using an extension of the B formal method: Event-B. Based on the idea of refinement, the author's systematic approach allows the user to construct models gradually and to facilitate a systematic reasoning method by means of proofs. Readers will learn how to build models of programs and, more generally, discrete systems, but this is all done with practice in mind. The numerous examples provided arise from various sources of computer system developments, including sequential programs, concurrent programs and electronic circuits. The book also contains a large number of exercises and projects ranging in difficulty. Each of the examples included in the book has been proved using the Rodin Platform tool set, which is available free for download at www.event-b.org.",Erratum,pro70
pap557,266aa9741c6559af0c6dcee2e1947ced0385b4bd,con90,Computer Vision and Pattern Recognition,Evidence-Based Software Engineering and Systematic Reviews,"In the decade since the idea of adapting the evidence-based paradigm for software engineering was first proposed, it has become a major tool of empirical software engineering. Evidence-Based Software Engineering and Systematic Reviews provides a clear introduction to the use of an evidence-based model for software engineering research and practice. The book explains the roles of primary studies (experiments, surveys, case studies) as elements of an over-arching evidence model, rather than as disjointed elements in the empirical spectrum. Supplying readers with a clear understanding of empirical software engineering best practices, it provides up-to-date guidance on how to conduct secondary studies in software engineeringreplacing the existing 2004 and 2007 technical reports. The book is divided into three parts. The first part discusses the nature of evidence and the evidence-based practices centered on a systematic review, both in general and as applying to software engineering. The second part examines the different elements that provide inputs to a systematic review (usually considered as forming a secondary study), especially the main forms of primary empirical study currently used in software engineering. The final part provides practical guidance on how to conduct systematic reviews (the guidelines), drawing together accumulated experiences to guide researchers and students in planning and conducting their own studies. The book includes an extensive glossary and an appendix that provides a catalogue of reviews that may be useful for practice and teaching.",Erratum,pro90
pap558,7da816d0f1d2a2b33d6512a1e694c04cbe4d4963,con100,International Conference on Automatic Face and Gesture Recognition,Experimentation in Software Engineering,,Erratum,pro100
pap559,3041a9265afb2ebdb4915aa9572668bb7f32b0ef,con2,International Conference on Software Engineering,From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering,"The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.",Article,pro2
pap560,2d1e79e057a9111ea6863378ffeca526a4e41c5f,con51,Brazilian Symposium on Software Engineering,On Non-Functional Requirements in Software Engineering,,Erratum,pro51
pap561,cdc5bf80451d1f447cf82e5c37fec8089b1e6878,jou156,Journal of Systems and Software,A framework for gamification in software engineering,,Letter,vol156
pap562,1c32125cc9fb052b881a6dec812b62ed998915d7,jou156,Journal of Systems and Software,Lessons from applying the systematic literature review process within the software engineering domain,,Letter,vol156
pap563,ece51631d79e2c017471f31767d7c3b62dd45769,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",Guide to the Software Engineering Body of Knowledge (SWEBOK(R)): Version 3.0,"In the Guide to the Software Engineering Body of Knowledge (SWEBOK Guide), the IEEE Computer Society establishes a baseline for the body of knowledge for the field of software engineering, and the work supports the Societys responsibility to promote the advancement of both theory and practice in this field. It should be noted that the Guide does not purport to define the body of knowledge but rather to serve as a compendium and guide to the knowledge that has been developing and evolving over the past four decades. Now in Version 3.0, the Guides 15 knowledge areas summarize generally accepted topics and list references for detailed information. The editors for Version 3.0 of the SWEBOK Guide are Pierre Bourque (cole de technologie suprieure (TS), Universit du Qubec) and Richard E. (Dick) Fairley (Software and Systems Engineering Associates (S2EA)).",Erratum,pro87
pap564,7e991438547d69c1001b7664cd60a68d4dbc4023,con41,Asia-Pacific Software Engineering Conference,A Map of Threats to Validity of Systematic Literature Reviews in Software Engineering,"Context: The assessment of Threats to Validity (TTVs) is critical to secure the quality of empirical studies in Software Engineering (SE). In the recent decade, Systematic Literature Review (SLR) was becoming an increasingly important empirical research method in SE. One of the mechanisms of insuring the level of scientific value in the findings of an SLR is to rigorously assess its validity. Hence, it is necessary to realize the status quo and issues of TTVs of SLRs in SE. Objective: This study aims to investigate thestate-of-the-practice of TTVs of the SLRs published in SE, and further support SE researchers to improve the assessment and strategies against TTVs in order to increase the quality of SLRs in SE. Method: We conducted a tertiary study by reviewing the SLRs in SE that report the assessment of TTVs. Results: We identified 316 SLRs published from 2004 to the first half of 2015, in which TTVs are discussed. The issues associated to TTVs were also summarized and categorized. Conclusion: The common TTVs related to SLR research, such as internal validity and reliability, were thoroughly discussed in most SLRs. The threats to construct validity and external validity drew less attention. Moreover, there are few strategies and tactics being reported to cope with the various TTVs.",Conference paper,pro41
pap565,d1cc35e2a547ba79f1b07fdd81ee0da264c0d6b6,con2,International Conference on Software Engineering,Belief & Evidence in Empirical Software Engineering,"Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.",Conference paper,pro2
pap566,1fe3f8c49567b71ef5537f4ff8686bbfec9b2cef,jou160,IEEE Software,Software-Engineering the Internet of Things,New wiring transformed ENIAC into a versatile stored-program computer. Rewiring Internet of Things infrastructures into a general-purpose computing fabric can similarly change how modern computation interfaces with our environment.,Conference paper,vol160
pap567,0d56d0dfc3f3e841625a7d0be56a955b32b3cac3,jou152,Empirical Software Engineering,On the pragmatic design of literature studies in software engineering: an experience-based guideline,,Letter,vol152
pap568,0e50bf23cb16ba66e738d88f0fffab75c338e02a,jou160,IEEE Software,"Crowdsourcing in Software Engineering: Models, Motivations, and Challenges","Almost surreptitiously, crowdsourcing has entered software engineering practice. In-house development, contracting, and outsourcing still dominate, but many development projects use crowdsourcing-for example, to squash bugs, test software, or gather alternative UI designs. Although the overall impact has been mundane so far, crowdsourcing could lead to fundamental, disruptive changes in how software is developed. Various crowdsourcing models have been applied to software development. Such changes offer exciting opportunities, but several challenges must be met for crowdsourcing software development to reach its potential.",Letter,vol160
pap569,64cc4ef5def3919049bdd3a645af198922d626c2,con2,International Conference on Software Engineering,An Empirical Study of Practitioners' Perspectives on Green Software Engineering,"The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.",Conference paper,pro2
pap570,a717f569c788ae23a3c0d2d6a18b109590a119c8,con41,Asia-Pacific Software Engineering Conference,A discipline for software engineering,,Erratum,pro41
pap571,5602cbe797d20b17edfd08b3dc94622c842fbe80,jou151,IEEE Transactions on Software Engineering,Crossover Designs in Software Engineering Experiments: Benefits and Perils,"In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats.",Conference paper,vol151
pap572,a5a6eb69869d2a25a5915ebec8e0991ba78c4769,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Are Students Representatives of Professionals in Software Engineering Experiments?,"Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments.",Erratum,pro13
pap573,564614da76b5d9020c700b78e1fe154bd590c47d,jou151,IEEE Transactions on Software Engineering,The Role of Ethnographic Studies in Empirical Software Engineering,"Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.",Article,vol151
pap574,ec59569fdee17844ae071be1536a08f937f08c57,jou160,IEEE Software,"Speed, Data, and Ecosystems: The Future of Software Engineering","An evaluation of recent industrial and societal trends revealed three key factors driving software engineering's future: speed, data, and ecosystems. These factors' implications have led to guidelines for companies to evolve their software engineering practices. This article is part of a special issue on the Future of Software Engineering.",Article,vol160
pap575,b73694c24ec259da39e125c2c7d9496b2f222ba0,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Future Trends in Software Engineering Research for Mobile Apps,"There has been tremendous growth in the use of mobile devices over the last few years. This growth has fueled the development of millions of software applications for these mobile devices often called as 'apps'. Current estimates indicate that there are hundreds of thousands of mobile app developers. As a result, in recent years, there has been an increasing amount of software engineering research conducted on mobile apps to help such mobile app developers. In this paper, we discuss current and future research trends within the framework of the various stages in the software development life-cycle: requirements (including non-functional), design and development, testing, and maintenance. While there are several non-functional requirements, we focus on the topics of energy and security in our paper, since mobile apps are not necessarily built by large companies that can afford to get experts for solving these two topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, we first present the recent advances done in these stages and then we present the challenges present in current work, followed by the future opportunities and the risks present in pursuing such research.",Article,pro42
pap576,b681cd520e9b2c4e2c96dda31d09b51edcf28e32,con50,International Workshop on Green and Sustainable Software,Systems and Software Engineering,,Erratum,pro50
pap577,994b8dbfa12027ed24285a1e34c20cae8831173c,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Software-Specific Named Entity Recognition in Software Engineering Social Content,"Software engineering social content, such as Q&A discussions on Stack Overflow, has become a wealth of information on software engineering. This textual content is centered around software-specific entities, and their usage patterns, issues-solutions, and alternatives. However, existing approaches to analyzing software engineering texts treat software-specific entities in the same way as other content, and thus cannot support the recent advance of entity-centric applications, such as direct answers and knowledge graph. The first step towards enabling these entity-centric applications for software engineering is to recognize and classify software-specific entities, which is referred to as Named Entity Recognition (NER) in the literature. Existing NER methods are designed for recognizing person, location and organization in formal and social texts, which are not applicable to NER in software engineering. Existing information extraction methods for software engineering are limited to API identification and linking of a particular programming language. In this paper, we formulate the research problem of NER in software engineering. We identify the challenges in designing a software-specific NER system and propose a machine learning based approach applied on software engineering social content. Our NER system, called S-NER, is general for software engineering in that it can recognize a broad category of software entities for a wide range of popular programming languages, platform, and library. We conduct systematic experiments to evaluate our machine learning based S-NER against a well-designed, and to study the effectiveness of widely-adopted NER techniques and features in the face of the unique characteristics of software engineering social content.",Article,pro42
pap578,768b444c84340d2210bd2782ce3aa39b723bd0b9,jou161,Journal of Software Engineering Research and Development,Game development software engineering process life cycle: a systematic review,,Conference paper,vol161
pap579,7a28a601877a3722126c74149efc2a5f207b08e2,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Docker [Software engineering],"In episode 217 of Software Engineering Radio, host Charles Anderson talks with James Turnbull, a software developer and security specialist who's vice president of services at Docker. Lightweight Docker containers are rapidly becoming a tool for deploying microservice-based architectures.",Erratum,pro52
pap580,9445d843e8416a4cffb6da681250162ac38e1e2f,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Student Experiences Using GitHub in Software Engineering Courses: A Case Study,"GitHub has been embraced by the software development community as an important social platform for managing software projects and to support collaborative development. More recently, educators have begun to adopt it for hosting course content and student assignments. From our previous research, we found that educators leverage GitHub’s collaboration and transparency features to create, reuse and remix course materials, and to encourage student contributions and monitor student activity on assignments and projects. However, our previous research did not consider the student perspective. In this paper, we present a case study where GitHub is used as a learning platform for two software engineering courses. We gathered student perspectives on how the use of GitHub in their courses might benefit them and to identify the challenges they may face. The findings from our case study indicate that software engineering students do benefit from GitHub’s transparent and open workflow. However, students were concerned that since GitHub is not inherently an educational tool, it lacks key features important for education and poses learning and privacy concerns. Our findings provide recommendations for designers on how tools such as GitHub can be used to improve software engineering education, and also point to recommendations for instructors on how to use it more effectively in their courses.",Erratum,pro54
pap581,641bb6adbbef003b3b74a1bbd5e11dbe251d6a66,jou152,Empirical Software Engineering,Towards a decision-making structure for selecting a research design in empirical software engineering,,Article,vol152
pap582,2f063beeab119d3542570f5589f56fa85fe290a5,jou155,Information and Software Technology,Gamification in software engineering - A systematic mapping,,Conference paper,vol155
pap583,f6384b37b5c75e4e3c36bf9ea314efcc05ddf2d3,con109,International Society for Music Information Retrieval Conference,Acm Sigsoft Software Engineering Notes Vol 17 No 4 Foundations for the Study of Software Architecture,"The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architecture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements-that is, the constraints on the elements. The rationale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system :requirements. We discuss the components of the model in the context of both architectures and architectural styles and present an extended example to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, summarizing our contributions, and relating our approach to other current work.",Erratum,pro109
pap584,2e3a9a4dbf4862757fee5a074592929910e7034d,con38,International Symposium on Empirical Software Engineering and Measurement,Survey Guidelines in Software Engineering: An Annotated Review,"Background: Survey is a method of research aiming to gather data from a large population of interest. Despite being extensively used in software engineering, survey-based research faces several challenges, such as selecting a representative population sample and designing the data collection instruments. Objective: This article aims to summarize the existing guidelines, supporting instruments and recommendations on how to conduct and evaluate survey-based research. Methods: A systematic search using manual search and snowballing techniques were used to identify primary studies supporting survey research in software engineering. We used an annotated review to present the findings, describing the references of interest in the research topic. Results: The summary provides a description of 15 available articles addressing the survey methodology, based upon which we derived a set of recommendations on how to conduct survey research, and their impact in the community. Conclusion: Survey-based research in software engineering has its particular challenges, as illustrated by several articles in this review. The annotated review can contribute by raising awareness of such challenges and present the proper recommendations to overcome them.",Article,pro38
pap585,7aaa884f017f3f71a893b1755f75d801e17339ac,con97,ACM SIGMOD Conference,Software Engineering Component Based Software Engineering,"Component-based software engineering (CBSE) (also known as component-based development (CBD)) is a branch of software engineering that emphasizes the separation of concerns in respect of the wide-ranging functionality available throughout a given software system. It is a reuse-based approach to defining, implementing and composing loosely coupled independent components into systems. This practice aims to bring about an equally wide-ranging degree of benefits in both the short-term and the long-term for the software itself and for organizations that sponsor such software. This approach promises to alleviate the software crisis at great extents. The objective of this paper is to gain attention towards this new component based software development paradigm and to highlight the benefits of the approach for making it a successful software development approach to the concerned community",Erratum,pro97
pap586,64fd3ee86c29633d439d02bbdc044b132e56ec7c,con43,IEEE International Conference on Software Maintenance and Evolution,Choosing your weapons: On sentiment analysis tools for software engineering research,"Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack Overflow questions) and different sentiment analysis tools and observe that the disagreement between the tools can lead to contradictory conclusions.",Conference paper,pro43
pap587,72c32a1672fad31ef32fadc4e119a5c5569e4cd0,con41,Asia-Pacific Software Engineering Conference,Eye-Tracking Metrics in Software Engineering,"Eye-tracking studies are getting more prevalent in software engineering. Researchers often use different metrics when publishing their results in eye-tracking studies. Even when the same metrics are used, they are given different names, causing difficulties in comparing studies. To encourage replications and facilitate advancing the state of the art, it is important that the metrics used by researchers be clearly and consistently defined in the literature. There is therefore a need for a survey of eye-tracking metrics to support the (future) goal of standardizing eye-tracking metrics. This paper seeks to bring awareness to the use of different metrics along with practical suggestions on using them. It compares and contrasts various eye-tracking metrics used in software engineering. It also provides definitions for common metrics and discusses some metrics that the software engineering community might borrow from other fields.",Article,pro41
pap588,27c9101fa2f41ce15100f0f07802bb656eda50ad,jou152,Empirical Software Engineering,On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach,,Article,vol152
pap589,2f9a1286e7af4ab7706ad8cfcc8c8742a1964939,con53,Workshop on Web 2.0 for Software Engineering,Views on Internal and External Validity in Empirical Software Engineering,"Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.",Erratum,pro53
pap590,9018881fd8dfb4992cfd07664d94a5f6154aa6e7,con44,International Conference Knowledge Engineering and Knowledge Management,SEON: A Software Engineering Ontology Network,,Letter,pro44
pap591,b69a938050093e592e19a3b6321b3382d4f8bc7a,con45,International Conference on Global Software Engineering,Global Software Engineering: Evolution and Trends,"Professional software products and IT systems and services today are developed mostly by globally distributed teams, projects, and companies. Successfully orchestrating Global Software Engineering (GSE) has become the major success factor both for organizations and practitioners. Yet, more than a half of all distributed projects does not achieve the intended objectives and is canceled. This paper summarizes experiences from academia and industry in a way to facilitate knowledge and technology transfer. It is based on an evaluation of 10 years of research, and industry collaboration and experience reported at the IEEE International Conference on Software Engineering (ICGSE) series. The outcomes of our analysis show GSE as a field highly attached to industry and, thus, a considerable share of ICGSE papers address the transfer of Software Engineering concepts and solutions to the global stage. We found collaboration and teams, processes and organization, sourcing and supplier management, and success factors to be the topics gaining the most interest of researchers and practitioners. Beyond the analysis of the past conferences, we also look at current trends in GSE to motivate further research and industrial collaboration.",Letter,pro45
pap592,f420ebc31120a40bbac99273945f3a8cb7d03bf2,jou152,Empirical Software Engineering,Open innovation in software engineering: a systematic mapping study,,Article,vol152
pap593,898f657c30f37d8b674cc7383bcccad72334b933,con75,Intelligent Systems in Molecular Biology,Software Engineering,SE 3162 Professional Responsibility in Computer Science and Software Engineering (1 semester credit hour) Professional and ethical responsibilities of computer scientists and software engineers as influenced by growth in computer use and networks. Costs and benefits of computer technology. Risks and liabilities of safety-critical systems. Social implications of the Internet. Interaction between human values and technical decisions involving computing. Intellectual Property. Global impact of computing. Prerequisites or Corequisites: CS 3345 and CS 3354 and ECS 3361. (Same as CS 3162) (1-0) S,Erratum,pro75
pap594,9156f270cbe7d9627b7509f871f0bd075403f6db,con100,International Conference on Automatic Face and Gesture Recognition,Software Product Line Engineering Foundations Principles And Techniques,"Thank you for reading software product line engineering foundations principles and techniques. Maybe you have knowledge that, people have look numerous times for their favorite books like this software product line engineering foundations principles and techniques, but end up in malicious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some infectious bugs inside their desktop computer.",Erratum,pro100
pap595,969d1e52140dbf8f58a1b3f61a29b03255490c86,con90,Computer Vision and Pattern Recognition,Software Engineering Meets Control Theory,"The software engineering community has proposed numerous approaches for making software self-adaptive. These approaches take inspiration from machine learning and control theory, constructing software that monitors and modifies its own behavior to meet goals. Control theory, in particular, has received considerable attention as it represents a general methodology for creating adaptive systems. Control-theoretical software implementations, however, tend to be ad hoc. While such solutions often work in practice, it is difficult to understand and reason about the desired properties and behavior of the resulting adaptive software and its controller. This paper discusses a control design process for software systems which enables automatic analysis and synthesis of a controller that is guaranteed to have the desired properties and behavior. The paper documents the process and illustrates its use in an example that walks through all necessary steps for self-adaptive controller synthesis.",Erratum,pro90
pap596,57a5b99eceff9da205e244337c9f4678b5b23d25,con108,International Conference on Information Integration and Web-based Applications & Services,ISO / IEC 25010 : 2011 Systems and software engineering — Systems and software Quality Requirements and Evaluation ( SQuaRE ) — System and software quality models,,Erratum,pro108
pap597,b3de8bfff5b2a3ec070fbd021a4544af974542fc,jou162,"Software testing, verification & reliability",A Hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering,"Randomized algorithms are widely used to address many types of software engineering problems, especially in the area of software verification and validation with a strong emphasis on test automation. However, randomized algorithms are affected by chance and so require the use of appropriate statistical tests to be properly analysed in a sound manner. This paper features a systematic review regarding recent publications in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms in software engineering tend to not properly account for the random nature of these algorithms. Many of the novel techniques presented clearly appear promising, but the lack of soundness in their empirical evaluations casts unfortunate doubts on their actual usefulness. In software engineering, although there are guidelines on how to carry out empirical analyses involving human subjects, those guidelines are not directly and fully applicable to randomized algorithms. Furthermore, many of the textbooks on statistical analysis are written from the viewpoints of social and natural sciences, which present different challenges from randomized algorithms. To address the questionable overall quality of the empirical analyses reported in the systematic review, this paper provides guidelines on how to carry out and properly analyse randomized algorithms applied to solve software engineering tasks, with a particular focus on software testing, which is by far the most frequent application area of randomized algorithms within software engineering. Copyright © 2012 John Wiley & Sons, Ltd.",Conference paper,vol162
pap598,c24d6c1bed6af6b99e506f2ba83e9552481b5615,jou114,Cambridge International Law Journal,Green in Software Engineering,,Conference paper,vol114
pap599,fa2a0ecf236164d19c5da8dc45d72c2014bc1eff,jou156,Journal of Systems and Software,Twenty-eight years of component-based software engineering,,Conference paper,vol156
pap600,0e528eb8167c68930c2e1ab20ab5c14f98446927,jou151,IEEE Transactions on Software Engineering,Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering,"Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.",Article,vol151
pap601,389aa97e59372fbc65cab81fe18d9f86fbb70a82,jou156,Journal of Systems and Software,Behavioral software engineering: A definition and systematic literature review,,Article,vol156
pap602,402a9ff765bbf5a590a59e59790256c1be0ca330,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Guidelines for Conducting Surveys in Software Engineering,,Erratum,pro98
pap603,188e778ae2b5c1dee1dd76ce9c786f566f7e7079,con6,Annual Conference on Genetic and Evolutionary Computation,Software Product Line Engineering,,Erratum,pro6
pap604,7915ecb6554d5afa03eebcc0f3b8fa9bae63a503,jou155,Information and Software Technology,A systematic mapping study of search-based software engineering for software product lines,,Conference paper,vol155
pap605,dce99209120ebed7f5d68e3644fdcd160d4c366c,con93,International Conference on Computational Logic,Standard Glossary of Software Engineering Terminology,"IEEE Std 610.12-1990, IEEE Standard Glossary of Software Engineering Terminology, identifies terms currently in use in the field of Software Engineering. Standard definitions for those terms are established.",Erratum,pro93
pap606,5a4674e987c2d7c130c5303cbad3f4e4531f3259,con45,International Conference on Global Software Engineering,Case Study Research in Software Engineering - Guidelines and Examples,"Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.",Erratum,pro45
pap607,b9618c3e0428e365f7600476b89e3c2cbb3e54e5,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Introduction to Green in Software Engineering,,Erratum,pro42
pap608,0d70f756c410370d56f9e61f25f68ab606533bf2,jou88,bioRxiv,BEAST 2.5: An advanced software platform for Bayesian evolutionary analysis,"Elaboration of Bayesian phylogenetic inference methods has continued at pace in recent years with major new advances in nearly all aspects of the joint modelling of evolutionary data. It is increasingly appreciated that some evolutionary questions can only be adequately answered by combining evidence from multiple independent sources of data, including genome sequences, sampling dates, phenotypic data, radiocarbon dates, fossil occurrences, and biogeographic range information among others. Including all relevant data into a single joint model is very challenging both conceptually and computationally. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing such software frameworks is increasingly a major scientific activity in its own right, and comes with specific challenges, from practical software design, development and engineering challenges to statistical and conceptual modelling challenges. BEAST 2 is one such computational software platform, and was first announced over 4 years ago. Here we describe a series of major new developments in the BEAST 2 core platform and model hierarchy that have occurred since the first release of the software, culminating in the recent 2.5 release. Author summary Bayesian phylogenetic inference methods have undergone considerable development in recent years, and joint modelling of rich evolutionary data, including genomes, phenotypes and fossil occurrences is increasingly common. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing scientific software is increasingly crucial to advancement in many fields of biology. The challenges range from practical software development and engineering, distributed team coordination, conceptual development and statistical modelling, to validation and testing. BEAST 2 is one such computational software platform for phylogenetics, population genetics and phylodynamics, and was first announced over 4 years ago. Here we describe the full range of new tools and models available on the BEAST 2.5 platform, which expand joint evolutionary inference in many new directions, especially for joint inference over multiple data types, non-tree models and complex phylodynamics.",Article,vol88
pap609,bcd2c5379a34068040750a751e4fd2710d90c15c,jou151,IEEE Transactions on Software Engineering,The “Physics” of Notations: Toward a Scientific Basis for Constructing Visual Notations in Software Engineering,"Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.",Article,vol151
pap610,0d63d4bcf9fb27ee0930d71cd0108fe629671b7a,con0,International Conference on Machine Learning,Software Engineering: A Practitioners Approach,"Software engineering is the art of war. So if you don't know how to wage a war, then the weapons are useless. Software engineering has become very important because of the impact of large, expensive software systems and the role of software in safety-critical applications. This book supports a process to refound software engineering based on a solid theory, proven principles and best practices and fills a long-standing need in the software development communities to make the essential aspects of software development available in one comprehensive work. Written in an easy-to-understand tutorial format, SOFTWARE ENGINEERING: A Practitioners Approach provides professionals, researchers, and students at all levels with a clear coverage of: Analyzing, designing, programming and testing software projects. Set of objectives to which a prospective should be targeting to achieve. Two types of review questions-short answer type and descriptive type. List of key terms referring to abstract concepts, which may be used for better and crisp communication. Solution manual in electronic form available for qualified teachers on demand. Instructor's manual including power point slides, brief notes on teaching and list of projects with descriptions on demand. List of key references for the concepts in the chapter. Useful websites appended to each chapter for quick reference",Erratum,pro0
pap611,a5a717572be243e6ad1f874ba3567eb22c4026f9,con64,British Computer Society Conference on Human-Computer Interaction,Design patterns: elements of reuseable object-oriented software,"The book is an introduction to the idea of design patterns in software engineering, and a catalog of twenty-three common patterns. The nice thing is, most experienced OOP designers will find out they've known about patterns all along. It's just that they've never considered them as such, or tried to centralize the idea behind a given pattern so that it will be easily reusable.",Erratum,pro64
pap612,cf5e3bdb305241876cce27b93f930cd74c127798,jou163,IEEE Annals of the History of Computing,The mythical man-month: Essays on software engineering,"Like Bahbage, he lobbied for mathematical reform, stumped for the centrality of science in cultural advancement, argued that government support was crucial, and proved a stubborn and crotchety opponent when crossed. And, as Colin Burke reminds us in this fine and fresh new look at Bush, Bush envisioned machines relevant to the history of computing that never lived up to their promise. I doubt that Burke would agree with my description of Bush as a latter-day Babbage; nevertheless, this detailed study makes the comparison almost inevitable. Burke helps us appreciate how Bush's fascination with the mechanization of calculation and comparison caused his inventive work to swirl around problems relevant to the emergence of the modern computer. Moreover, Burke suggests that two of Bush's less familiar engines-one, the Rapid Selector, a bibliographic machine and a close cousin of the Memex of faddish fame; and the other, the Comparator, a cryptanalytic device-provide the stuff to fill in the holes in the history of the computer [p. ix). It is never very clear just what these holes are; this reader, at least, was not convinced that the careers of these two machines were anything but eddies along the shore of the main currents of computer evolution. They were decisive failures, as Burke admits, rooted in a stubborn commitment to intractdbk and ultimately unfashion-able if not outdated technologies. The strengths of this book indeed lie elsewhere. These exotic devices are of interest in themselves and deserve their biographer's attention. Burke details the labors of Bush and friends to use microfilm, electronics, and photoelectricity to mechanize the library-hereby resolving a putative information overload (it turns out that there wasn't one)-and help the U.S. Navy's cryptographers break enemy codes during World War 11. Burke is best, however, when discussing not machines themselves but when individuals and bureaucracies are at loggerheads. Ego, ambition, and organizational and technological vision were at stake. On the military side, and against much intcrnal resistance , Bush allies such as Stanford C. Hooper and Joseph Wenger dreamed of building the next generation of rapid analytic machines and, in doing so, dreamed of upgrading the scientific navy by forging alliances with "" college professors "" like Bush; on the civilian side, Bush and his "" boys "" worked to maneuver the navy into a project that promised much in the way of personal and institutional prestige, income for research, and opportunities for graduate …",Article,vol163
pap613,689dfa4a927f579f784159add17dc006c72abb1e,con76,IEEE International Conference on Tools with Artificial Intelligence,Agent-Oriented Software Engineering,,Erratum,pro76
pap614,a652692b51c786bfa3ceb43f3ae9f6acb796921f,con107,Chinese Conference on Biometric Recognition,The (R) Evolution of social media in software engineering,"Software developers rely on media to communicate, learn, collaborate, and coordinate with others. Recently, social media has dramatically changed the landscape of software engineering, challenging some old assumptions about how developers learn and work with one another. We see the rise of the social programmer who actively participates in online communities and openly contributes to the creation of a large body of crowdsourced socio-technical content. In this paper, we examine the past, present, and future roles of social media in software engineering. We provide a review of research that examines the use of different media channels in software engineering from 1968 to the present day. We also provide preliminary results from a large survey with developers that actively use social media to understand how they communicate and collaborate, and to gain insights into the challenges they face. We find that while this particular population values social media, traditional channels, such as face-to-face communication, are still considered crucial. We synthesize findings from our historical review and survey to propose a roadmap for future research on this topic. Finally, we discuss implications for research methods as we argue that social media is poised to bring about a paradigm shift in software engineering research.",Erratum,pro107
pap615,a639919277b2d0683d21b637bec2192fd475fa80,jou164,PeerJ,Happy software developers solve problems better: psychological measurements in empirical software engineering,"For more than thirty years, it has been claimed that a way to improve software developers’ productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states—emotions and moods—deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint.",Letter,vol164
pap616,9992f1d6978c1e9442bf519aa213f5ca4e2159f8,con46,Software Product Lines Conference,Search based software engineering for software product line engineering: a survey and directions for future work,"This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.",Letter,pro46
pap617,2aa4b8ab64847b5d040a4508a4029dea07e9cb41,jou152,Empirical Software Engineering,An empirically based terminology and taxonomy for global software engineering,,Conference paper,vol152
pap618,75fb25c1dae20546b13f233bcfbbdffebbb0c3f1,con2,International Conference on Software Engineering,Software engineering at the speed of light: how developers stay current using twitter,"The microblogging service Twitter has over 500 million users posting over 500 million tweets daily. Research has established that software developers use Twitter in their work, but this has not yet been examined in detail. Twitter is an important medium in some software engineering circles—understanding its use could lead to improved support, and learning more about the reasons for non-adoption could inform the design of improved tools. In a qualitative study, we surveyed 271 and interviewed 27 developers active on GitHub. We find that Twitter helps them keep up with the fast-paced development landscape. They use it to stay aware of industry changes, for learning, and for building relationships. We discover the challenges they experience and extract their coping strategies. Some developers do not want to or cannot embrace Twitter for their work—we show their reasons and alternative channels. We validate our findings in a follow-up survey with more than 1,200 respondents.",Letter,pro2
pap619,372fcbfccd9b02c69f0e42f1c94a64a044e1a11a,jou152,Empirical Software Engineering,Replication of empirical studies in software engineering research: a systematic mapping study,,Conference paper,vol152
pap620,717a8985cc3fcbd285efe50dbcb8fe5bb12921b1,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Challenges for Software Engineering in Automation,"This paper gives an 
introduction to the essential challenges of software engineering and 
requirements that software has to fulfill in the domain of automation. Besides, 
the functional characteristics, specific constraints and circumstances are 
considered for deriving requirements concerning usability, the technical 
process, the automation functions, used platform and the well-established 
models, which are described in detail. On the other hand, challenges result 
from the circumstances at different points in the single phases of the life 
cycle of the automated system. The requirements for life-cycle-management, 
tools and the changeability during runtime are described in detail.",Erratum,pro13
pap621,77b391249a96ddb83f3acd843a79924e1cb04f87,jou160,IEEE Software,Bringing the Human Factor to Software Engineering,"The human aspects involved in the software development process are vital to a successful completion of a software project. The author advocates for human factor topics to be part of mainstream software engineering education in order to elevate job satisfaction, improve performance, and increase productivity of software engineers. Emphasis should be on providing a practical overview of software engineering processes from a human perspective, offering alternative viewpoints within technically saturated curricula.",Article,vol160
pap622,41c00f70344c941921b64f64234512ceebdf474d,con55,Workshop on Learning from Authoritative Security Experiment Results,Empirical Software Engineering,"Empirical Software Engineering provides a forum for applied software engineering research with a strong empirical component, and a venue for publishing empirical results relevant to both researchers and practitioners. Empirical studies presented here usually involve the collection and analysis of data and experience that can be used to characterize, evaluate and reveal relationships between software development deliverables, practices, and technologies. Over time, it is expected that such empirical results will form a body of knowledge leading to widely accepted and well-formed theories.  The journal also offers industrial experience reports detailing the application of software technologies processes, methods, or tools and their effectiveness in industrial settings.  Empirical Software Engineering promotes the publication of industry-relevant research, to address the significant gap between research and practice.",Erratum,pro55
pap623,6723f2324a2f48d11d219ea47450ba6860d8856e,con31,International Conference on Evaluation & Assessment in Software Engineering,Systematic mapping study on software engineering for sustainability (SE4S),"Background/Context: The objective of achieving higher sustainability in our lifestyles by information and communication technology has lead to a plethora of research activities in related fields. Consequently, Software Engineering for Sustainability (SE4S) has developed as an active area of research. Objective/Aim: Though SE4S gained much attention over the past few years and has resulted in a number of contributions, there is only one rigorous survey of the field. We follow up on this systematic mapping study from 2012 with a more in-depth overview of the status of research, as most work has been conducted in the last 4 years. Method: The applied method is a systematic mapping study through which we investigate which contributions were made, which knowledge areas are most explored, and which research type facets have been used, to distill a common understanding of the state-of-the-art in SE4S. Results: We contribute an overview of current research topics and trends, and their distribution according to the research type facet and the application domains. Furthermore, we aggregate the topics into clusters and list proposed and used methods, frameworks, and tools. Conclusion: The research map shows that impact currently is limited to few knowledge areas and there is need for a future roadmap to fill the gaps.",Article,pro31
pap624,41116eae430b585242e2b1e4cd2daa7f74a0a950,jou155,Information and Software Technology,A systematic review of systematic review process research in software engineering,,Letter,vol155
pap625,07eb0080630f63d9d7688f28fc57892b2d7c6ca6,con69,Formal Concept Analysis,Model-Driven Software Engineering in Practice,"Model based software development differs from the conventional software development process and used in conjunction with a range of agile techniques. Despite improvements in third generation programming languages and runtime platforms, the levels of abstraction at which PLAs are developed today remains low-level relative to the concepts and concerns within the application domains themselves, such as manually tracking the library dependency or ensuring component composition syntactical and semantic correctness. [1] A promising means to address this problem involves developing PLAs using model-driven engineering (MDE) [2] which involves systematic use of models as key design and implementation artifacts throughout the software lifecycle. Thus to induce the works on architectures and models, researchers have been working to create structured models. The literature in this area hence proliferates; where the current edition [3] tries to address some of the significant aspects.",Erratum,pro69
pap626,606b072c6ca0f03cbaa3757f5897748b3921d466,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Agent-based software engineering,"The technology of intelligent agents and multi-agent systems is expected to alter radically the way in which complex, distributed, open systems are conceptualised and implemented. The paper considers the problem of building a multi-agent system as a software engineering enterprise. Three issues are focused on: how agents might be specified; how these specifications might be refined or otherwise transformed into efficient implementations: and how implemented agents and multi-agent systems might subsequently be verified, to show that they are correct with respect to their specifications. These issues are discussed with reference to a number of case studies. The paper concludes by setting out some issues and open problems for future research.",Erratum,pro73
pap627,09402e20ca49842e1d1d967d3c5a530e8167b9b1,con2,International Conference on Software Engineering,The dimensions of software engineering success,"Software engineering research and practice are hampered by the lack of a well-understood, top-level dependent variable. Recent initiatives on General Theory of Software Engineering suggest a multifaceted variable – Software Engineering Success. However, its exact dimensions are unknown. This paper investigates the dimensions (not causes) of software engineering success. An interdisciplinary sample of 191 design professionals (68 in the software industry) were interviewed concerning their perceptions of success. Non-software designers (e.g. architects) were included to increase the breadth of ideas and facilitate comparative analysis. Transcripts were subjected to supervised, semi-automated semantic content analysis, including a software developer vs. other professionals comparison. Findings suggest that participants view their work as time-constrained projects with explicit clients and other stakeholders. Success depends on stakeholder impacts – financial, social, physical and emotional – and is understood through feedback. Concern with meeting explicit requirements is peculiar to software engineering and design is not equated with aesthetics in many other fields. Software engineering success is a complex multifaceted variable, which cannot sufficiently be explained by traditional dimensions including user satisfaction, profitability or meeting requirements, budgets and schedules. A proto-theory of success is proposed, which models success as the net impact on a particular stakeholder at a particular time. Stakeholder impacts are driven by project efficiency, artifact quality and market performance. Success is not additive, e.g., ‘low’ success for clients does not average with ‘high’ success for developers to make ‘moderate’ success overall; rather, a project may be simultaneously successful and unsuccessful from different perspectives.",Letter,pro2
pap628,76157cdbd7ca2f0c74eae4aba4ded14985a3e2b6,jou155,Information and Software Technology,Understanding replication of experiments in software engineering: A classification,,Conference paper,vol155
pap629,dce27ed67f146a38fa79159df9891e73712f75ed,con65,IEEE International Conference on Software Engineering and Formal Methods,Continuous Software Engineering: An Introduction,,Erratum,pro65
pap630,845afdf05ac75fedb65532487aadd0538bc4c6da,con6,Annual Conference on Genetic and Evolutionary Computation,Qualitative Methods in Empirical Studies of Software Engineering,"While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behaviour. The paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout.",Erratum,pro6
pap631,eb2640199a8bf20b85b54757a99e07942b5909ee,jou151,IEEE Transactions on Software Engineering,A Systematic Literature Review on Fault Prediction Performance in Software Engineering,"Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.",Conference paper,vol151
pap632,c02b2769a0b4197f93477b5d222dfb04257f7ec6,jou155,Information and Software Technology,Software engineering beyond the project - Sustaining software ecosystems,,Article,vol155
pap633,9e4699c0f566e009a79dbe6482f0e169e8b085e1,jou158,Lecture Notes in Computer Science,Search Based Software Engineering,,Conference paper,vol158
pap634,92ca9f57ebb0cfdbfa07e5d5956e11509f902f0b,con2,International Conference on Software Engineering,A practical guide for using statistical tests to assess randomized algorithms in software engineering,"Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering",Letter,pro2
pap635,1112e2422dbb90df87da2df6afc719e90f5e0af6,con77,International Conference on Artificial Neural Networks,"Search-based software engineering: Trends, techniques and applications","In the past five years there has been a dramatic increase in work on Search-Based Software Engineering (SBSE), an approach to Software Engineering (SE) in which Search-Based Optimization (SBO) algorithms are used to address problems in SE. SBSE has been applied to problems throughout the SE lifecycle, from requirements and project planning to maintenance and reengineering. The approach is attractive because it offers a suite of adaptive automated and semiautomated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives.
 This article1 provides a review and classification of literature on SBSE. The work identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.",Erratum,pro77
pap636,5ed409e2bee9203af02f63b0a57aa139f9ea2cc3,con2,International Conference on Software Engineering,How to effectively use topic models for software engineering tasks? An approach based on Genetic Algorithms,"Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is able to identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.",Conference paper,pro2
pap637,acdc89274e34d2f905a055aad41e7f815aef52eb,con55,Workshop on Learning from Authoritative Security Experiment Results,Selecting Empirical Methods for Software Engineering Research,,Erratum,pro55
pap638,fa9f622a1182400067d911b0900733695ec1b358,jou152,Empirical Software Engineering,Parameter tuning or default values? An empirical investigation in search-based software engineering,,Letter,vol152
pap639,5c7e1b47e0864c8e9e075389d17c31352b0484ee,con15,Pacific Symposium on Biocomputing,Software Engineering for Self-Adaptive Systems: A Research Roadmap,,Erratum,pro15
pap640,7ab9367fbaff7f8a0dce861a6b90a7511029b369,jou165,IEEE Transactions on Industrial Informatics,Software Engineering in Industrial Automation: State-of-the-Art Review,"This paper presents one perspective on recent developments related to software engineering in the industrial automation sector that spans from manufacturing factory automation to process control systems and energy automation systems. The survey's methodology is based on the classic SWEBOK reference document that comprehensively defines the taxonomy of software engineering domain. This is mixed with classic automation artefacts, such as the set of the most influential international standards and dominating industrial practices. The survey focuses mainly on research publications which are believed to be representative of advanced industrial practices as well.",Letter,vol165
pap641,19c64da92a8c67c14ddd9cced0d1d0b9ff6b39d1,jou166,"Software, Practice & Experience",An open graph visualization system and its applications to software engineering,"We describe a package of practical tools and libraries for manipulating graphs and their drawings. Our design, which is aimed at facilitating the combination of the package components with other tools, includes stream and event interfaces for graph operations, high‐quality static and dynamic layout algorithms, and the ability to handle sizeable graphs. We conclude with a description of the applications of this package to a variety of software engineering tools. Copyright © 2000 John Wiley & Sons, Ltd.",Article,vol166
pap642,e708fe09fcbcebba443db826c6f0d6c839137cb0,con2,International Conference on Software Engineering,On the value of user preferences in search-based software engineering: A case study in software product lines,"Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.",Article,pro2
pap643,1e62a8afbe6018540c60d9dcce1ff6bd98f2e404,con2,International Conference on Software Engineering,Automatic query reformulations for text retrieval in software engineering,"There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place. We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from five open source systems in Java and C++ and it is used in the context of TR-based concept location in source code. Refoqus outperformed the baselines and its recommendations lead to query performance improvement or preservation in 84% of the cases (in average).",Article,pro2
pap644,0ed846e87ce0961d162e9115b4e9837537138e3a,con2,International Conference on Software Engineering,Analyze this! 145 questions for data scientists in software engineering,"In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.",Letter,pro2
pap645,433f2ad582b75d593d12f47d717d0fea0bd824de,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,"Worldviews, Research Methods, and their Relationship to Validity in Empirical Software Engineering Research","Background - Validity threats should be considered and consistently reported to judge the value of an empirical software engineering research study. The relevance of specific threats for a particular research study depends on the worldview or philosophical worldview of the researchers of the study. Problem/Gap - In software engineering, different categorizations exist, which leads to inconsistent reporting and consideration of threats. Contribution - In this paper, we relate different worldviews to software engineering research methods, identify generic categories for validity threats, and provide a categorization of validity threats with respect to their relevance for different world views. Thereafter, we provide a checklist aiding researchers in identifying relevant threats. Method - Different threat categorizations and threats have been identified in literature, and are reflected on in relation to software engineering research. Results - Software engineering is dominated by the pragmatist worldviews, and therefore use multiple methods in research. Maxwell's categorization of validity threats has been chosen as very suitable for reporting validity threats in software engineering research. Conclusion - We recommend to follow a checklist approach, and reporting first the philosophical worldview of the researcher when doing the research, the research methods and all threats relevant, including open, reduced, and mitigated threats.",Article,pro47
pap646,63b35f109c6962247cbbf9458d55082c653f1d9e,jou152,Empirical Software Engineering,A practical guide to controlled experiments of software engineering tools with human participants,,Letter,vol152
pap647,b9da07181b1b37a79199b46613d46b32167fcb0b,jou158,Lecture Notes in Computer Science,Software Engineering for Self-Adaptive Systems II,,Letter,vol158
pap648,0bfca74dd16aad83216742115231d400d60a9f0d,con63,International Colloquium on Theoretical Aspects of Computing,Diversity in software engineering research,"One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).",Erratum,pro63
pap649,09f16130185fac1a9766707cbfc4285c7a108f65,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",A Green Model for Sustainable Software Engineering,"Information Communication Technology (ICT) has a strong impact on sustainable development due its rising demands for energy and resources needed when building hardware and software products. Most of the efforts spent on Green ICT/IT have been dedicated to addressing the effects of hardware on the environment but little have been considering the effects of building software products as well. Efficient software will indirectly consume less energy by using up less hardware equipment to run. Our contributions in this paper are devoted to building a two level green software model that covers the sustainable life cycle of a software product and the software tools promoting green and environmentally sustainable software. In the first level we propose a new green software engineering process that is a hybrid process between sequential, iterative, and agile development processes to produce an environmentally sustainable one. Each stage of the software process is then further studied to produce a green and sustainable stage. We propose either green guidelines or green processes for each software stage in the engineering process. We add to the software life cycle the requirements stage and the testing stage. We also include in the first level a complete list of metrics to measure the greenness of each stage in terms of the first order effects of ICT on the environment for a green software engineering process. No effort has been placed before in designing a green software engineering process. The second level explains how software itself can be used as a tool to aid in green computing by monitoring resources in an energy efficient manner. Finally, we show and explain relationships that can be found between the two levels in our proposed model to make the software engineering process and product green and sustainable.",Erratum,pro73
pap650,009ae8dd5ef8034aba907fb8397f3261b62f193e,con32,International Conference on Software Technology: Methods and Tools,Object-Oriented Software Engineering,"A text on industrial system development using object- oriented techniques, rather than a book on object-oriented programming. Will be useful to systems developers and those seeking a deeper understanding of object orientation as it relates to the development process.",Conference paper,pro32
pap651,09b4abe4142cfe4aef372d75288f5cf94893fea5,con48,ACM Symposium on Applied Computing,Towards a definition of sustainability in and for software engineering,"Sustainability is not supported by traditional software engineering methods. This lack of support leads to inefficient efforts to address sustainability or complete omission of this important concept. Defining and developing adequate support requires a commonly accepted definition of what sustainability means in and for software engineering.
 We contribute a description of the aspects of sustainability in software engineering.",Letter,pro48
pap652,7fc5d33b8891b51d9eecd7bf3eed766a94ff5789,con20,ACM Conference on Economics and Computation,Software Engineering Processes for Self-Adaptive Systems,,Erratum,pro20
pap653,e8f19d716d3d5391d1bb6254b11fa88e7f011f62,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Organizational social structures for software engineering,"Software engineering evolved from a rigid process to a dynamic interplay of people (e.g., stakeholders or developers). Organizational and social literature call this interplay an Organizational Social Structure (OSS). Software practitioners still lack a systematic way to select, analyze, and support OSSs best fitting their problems (e.g., software development). We provide the state-of-the-art in OSSs, and discuss mechanisms to support OSS-related decisions in software engineering (e.g., choosing the OSS best fitting development scenarios). Our data supports two conclusions. First, software engineering focused on building software using project teams alone, yet these are one of thirteen OSS flavors from literature. Second, an emerging OSS should be further explored for software development: social networks. This article represents a first glimpse at OSS-aware software engineering, that is, to engineer software using OSSs best fit for the problem.",Erratum,pro49
pap654,7bcfe45fbdccbfc4667540b3c54b4aff398d140c,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",What is social debt in software engineering?,"“Social debt” in software engineering informally refers to unforeseen project cost connected to a “suboptimal” development community. The causes of suboptimal development communities can be many, ranging from global distance to organisational barriers to wrong or uninformed socio-technical decisions (i.e., decisions that influence both social and technical aspects of software development). Much like technical debt, social debt impacts heavily on software development success. We argue that, to ensure quality software engineering, practitioners should be provided with mechanisms to detect and manage the social debt connected to their development communities. This paper defines and elaborates on social debt, pointing out relevant research paths. We illustrate social debt by comparison with technical debt and discuss common real-life scenarios that exhibit “sub-optimal” development communities.",Article,pro49
pap655,7ddbc19300ef58833a543d5f4fce4de008730a53,con102,Annual Haifa Experimental Systems Conference,Uncovering theories in software engineering,"There has been a growing interest in the role of theory within Software Engineering (SE) research. For several decades, researchers within the SE research community have argued that, to become a real engineering science, SE needs to develop stronger theoretical foundations. A few authors have proposed guidelines for constructing theories, building on insights from other disciplines. However, so far, much SE research is not guided by explicit theory, nor does it produce explicit theory. In this paper we argue that SE research does, in fact, show traces of theory, which we call theory fragments. We have adapted an analytical framework from the social sciences, named the Validity Network Schema (VNS), that we use to illustrate the role of theorizing in SE research. We illustrate the use of this framework by dissecting three well known research papers, each of which has had significant impact on their respective subdisciplines. We conclude this paper by outlining a number of implications for future SE research, and show how by increasing awareness and training, development of SE theories can be improved.",Erratum,pro102
pap656,1cf41a21d103549f411181a297cd7e22aac417ea,con50,International Workshop on Green and Sustainable Software,Green software engineering with agile methods,"The energy consumption of information and communication technology (ICT) is still increasing. Since several concepts regarding hardware solutions for Green IT exist, the contribution of software to Green IT is still not well investigated. This comprises the production and the usage impact of software on energy consumption. In our paper, we discuss this contribution. Especially, we present a model that integrates Green IT aspects into software engineering processes with agile methods in order to produce “greener” software from scratch.",Letter,pro50
pap657,be779149798a217e81a4d9deb46a68db218a22c2,jou155,Information and Software Technology,Systematic literature reviews in software engineering,,Conference paper,vol155
pap658,7f714a6def35b98aa94d1f6a8e0026d4e023ec08,jou156,Journal of Systems and Software,On the reliability of mapping studies in software engineering,,Article,vol156
pap659,396eab2434bcfd369b472fa494b62cee8465a2f4,con9,Big Data,"Educational software engineering: Where software engineering, education, and gaming meet","We define and advocate the subfield of educational software engineering (i.e., software engineering for education), which develops software engineering technologies (e.g., software testing and analysis, software analytics) for general educational tasks, going beyond educational tasks for software engineering. In this subfield, gaming technologies often play an important role together with software engineering technologies. We expect that researchers in educational software engineering would be among key players in the education domain and in the coming age of Massive Open Online Courses (MOOCs). Educational software engineering can and will contribute significant solutions to address various critical challenges in education especially MOOCs such as automatic grading, intelligent tutoring, problem generation, and plagiarism detection. In this position paper, we define educational software engineering and illustrate Pex for Fun (in short as Pex4Fun), one of our recent examples on leveraging software engineering and gaming technologies to address educational tasks on teaching and learning programming and software engineering skills.",Erratum,pro9
pap660,45559b74f535a218746b2e5d837f39f33b31d492,jou155,Information and Software Technology,How to design gamification? A method for engineering gamified software,,Article,vol155
pap661,35a125397e9ec06bd5260e7c2229a31befdc7c16,jou155,Information and Software Technology,Systematic reviews in software engineering: An empirical investigation,,Letter,vol155
pap662,ae19e603d5cc98b0347663752e98a704608c1d86,jou152,Empirical Software Engineering,Replications of software engineering experiments,,Conference paper,vol152
pap663,d48a7458182aeebf080d4bc2b0100e6d126c1b79,jou151,IEEE Transactions on Software Engineering,Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases,"Building on concepts drawn from control theory, self-adaptive software handles environmental and internal uncertainties by dynamically adjusting its architecture and parameters in response to events such as workload changes and component failures. Self-adaptive software is increasingly expected to meet strict functional and non-functional requirements in applications from areas as diverse as manufacturing, healthcare and finance. To address this need, we introduce a methodology for the systematic ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST). ENTRUST uses a combination of (1) design-time and runtime modelling and verification, and (2) industry-adopted assurance processes to develop trustworthy self-adaptive software and assurance cases arguing the suitability of the software for its intended application. To evaluate the effectiveness of our methodology, we present a tool-supported instance of ENTRUST and its use to develop proof-of-concept self-adaptive software for embedded and service-based systems from the oceanic monitoring and e-finance domains, respectively. The experimental results show that ENTRUST can be used to engineer self-adaptive software systems in different application domains and to generate dynamic assurance cases for these systems.",Conference paper,vol151
pap664,39f43bcf1995909dc3cf0d6383f7afabfdafac8a,jou156,Journal of Systems and Software,Cloud engineering is Search Based Software Engineering too,,Conference paper,vol156
pap665,391235fa417410096740c4a923f71a6711fa0915,jou49,ACM Computing Surveys,The state of the art in end-user software engineering,"Most programs today are written not by professional software developers, but by people with expertise in other domains working towards goals for which they need computational support. For example, a teacher might write a grading spreadsheet to save time grading, or an interaction designer might use an interface builder to test some user interface design ideas. Although these end-user programmers may not have the same goals as professional developers, they do face many of the same software engineering challenges, including understanding their requirements, as well as making decisions about design, reuse, integration, testing, and debugging. This article summarizes and classifies research on these activities, defining the area of End-User Software Engineering (EUSE) and related terminology. The article then discusses empirical research about end-user software engineering activities and the technologies designed to support them. The article also addresses several crosscutting issues in the design of EUSE tools, including the roles of risk, reward, and domain complexity, and self-efficacy in the design of EUSE tools and the potential of educating users about software engineering principles.",Conference paper,vol49
pap666,6cf06cf61c937688f5c816edb4504000c6a553e3,con51,Brazilian Symposium on Software Engineering,On challenges in engineering IoT software systems,"Contemporary software systems, such as the Internet of Things (IoT), Industry 4.0, and Smart Cities are new technology paradigms that offer challenges for their construction since they are calling into question our traditional form of developing software. They are a promising paradigm for the integration of devices and communications technologies. It is leading to a shift from the classical monolithic view of development where stakeholder receive a software product at the end (that we have been doing for decades), to software systems materialized through physical objects interconnected by networks and with embedded software to support daily activities. We need therefore to revisit our way of developing software systems and start to consider the particularities required by these new sorts of applications. This paper presents research toward the definition of a framework to support the systems engineering of IoT applications, where we evolved the Zachman's Framework as an alternative to the organization of this architecture. The activities were two folded to address this goal: a) we identified leading concerns of IoT applications, recovered from technical literature, practitioners and a Government Report, in different studies; b) we structured the IoT paradigm in different facets. These activities provided 14 significant concerns and seven facets that together represent the engineering challenges to be faced both by research and practice towards the advancement of IoT in practice.",Letter,pro51
pap667,6028656fb04af3859ae8eeb349ff4b0367db41f7,con33,International Conference on Automated Software Engineering,Software engineering metrics and models,,Erratum,pro33
pap668,c3910095b25a674e7154acd9c38d0af220026e31,jou155,Information and Software Technology,Systematic literature reviews in software engineering - A tertiary study,,Letter,vol155
pap669,01fd7b20b15872d7420d08032f7b09e599ca8bb3,con14,Hawaii International Conference on System Sciences,Software design,"From the Publisher: 
 
Based on a curriculum module originally written for the Software Engineering Institute at Carnegie Mellon University, this text provides students with an introduction to the role of design in software engineering. The book surveys a wide range of design methods and evaluates their strengths and weaknesses in various applications. The author adopts a neutral approach, concentrating on the role of design in software development creating a more effective tutorial text for students. 
Features 
Provides a balanced introduction to software design, reviewing the leading design methods, both formal and informal, from a neutral viewpoint. 
Describes and evaluates a wide range of different design methods, including JSP, SSA/SD, JSD, object-oriented and object- based design 
Focuses on design principles and strategies, which can be directly applied in practice.",Erratum,pro14
pap670,d23e48d6f0facaaad74707b6cc94c8b322d0a228,con17,International Conference on Statistical and Scientific Database Management,Software Engineering for Self-Adaptive Systems: A Second Research Roadmap,,Erratum,pro17
pap671,8f5d22051835e2f303efefd62329c100972c1fb5,con89,Conference on Uncertainty in Artificial Intelligence,Requirements Engineering: From System Goals to UML Models to Software Specifications,"Essential comprehensive coverage of the fundamentals of requirements engineering Requirements engineering (RE) deals with the variety of prerequisites that must be met by a software system within an organization in order for that system to produce stellar results. With that explanation in mind, this must-have book presents a disciplined approach to the engineering of high-quality requirements. Serving as a helpful introduction to the fundamental concepts and principles of requirements engineering, this guide offers a comprehensive review of the aim, scope, and role of requirements engineering as well as best practices and flaws to avoid. Shares state-of-the-art techniques for domain analysis, requirements elicitation, risk analysis, conflict management, and more Features in-depth treatment of system modeling in the specific context of engineering requirements Presents various forms of reasoning about models for requirements quality assurance Discusses the transitions from requirements to software specifications to software architecture In addition, case studies are included that complement the many examples provided in the book in order to show you how the described method and techniques are applied in practical situations.",Erratum,pro89
pap672,8f25aa2eca9966610bbdb8ed89867b121a155ba5,con91,Symposium on the Theory of Computing,Software Product Lines: Practices and Patterns,"Foreword. Preface. Acknowledgements. Dedication. Reader's Guide. I. SOFTWARE PRODUCT LINE FUNDAMENTALS. 1. Basic Ideas and Terms. What Is a Software Product Line? What Software Product Lines Are Not. Fortuitous Small-Grained Reuse. Single-System Development with Reuse. Just Component-Based Development. Just a Reconfigurable Architecture. Releases and Versions of Single Products. Just a Set of Technical Standards. A Note on Terminology. For Further Reading. Discussion Questions. 2. Benefits. Organizational Benefits. Individual Benefits. Benefits versus Costs. For Further Reading. Discussion Questions. 3. The Three Essential Activities. What Are the Essential Activities? Core Asset Development. Product Development. Management. All Three Together. For Further Reading. Discussion Questions. II. SOFTWARE PRODUCT LINE PRACTICE AREAS. Describing the Practice Areas. Starting versus Running a Product Line. Organizing the Practice Areas. 4. Software Engineering Practice Areas. Architecture Definition. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Architecture Evaluation. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Component Development. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. COTS Utilization. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Mining Existing Assets. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Requirements Engineering. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Software System Integration. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Testing. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Understanding Relevant Domains. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. 5. Technical Management Practice Areas. Configuration Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Data Collection, Metrics, and Tracking. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Make/Buy/Mine/Commission Analysis. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Process Definition. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Scoping. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Technical Planning. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Technical Risk Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Tool Support. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. 6. Organizational Management Practice Areas. Building a Business Case. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Customer Interface Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Developing an Acquisition Strategy. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Funding. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Launching and Institutionalizing. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Market Analysis. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Operations. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Organizational Planning. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Organizational Risk Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Structuring the Organization. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Technology Forecasting. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Training. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. III. PUTTING THE PRACTICE AREAS INTO ACTION. 7. Software Product Line Practice Patterns. The Value of Patterns. Software Product Line Practice Pattern Descriptions. The Curriculum Pattern. The Essentials Coverage Pattern. Each Asset Pattern. What to Build Pattern. Product Parts Pattern. Assembly Line Pattern. Monitor Pattern. Product Builder Pattern. Cold Start Pattern. In Motion Pattern. Process Pattern. Factory Pattern. Other Patterns. Practice Area Coverage. Discussion Questions. 8. Product Line Technical Probe. What Is the Product Line Technical Probe? Probe Interview Questions. Probe Participants. Probe Process. Using the Probe Results. Conducting a Mini Self-Probe. Discussion Questions. 9. Cummins Engine Company: Embracing the Future. Prologue. Company History. A Product Line of Engine Software. Getting off the Ground. An Organization Structured for Cooperation. Running the Product Line. Results. Lessons Learned. Epilogue. Practice Area Compendium. For Further Reading. Discussion Questions. 10. Control Channel Toolkit: A Software Product Line that Controls Satellites. Contextual Background. Organizational Profiles. Project History. Control Channels. Launching CCT. Developing a Business Case for CCT. Developing the Acquisition Strategy and Funding CCT. Structuring the CCT Organization. Organizational and Technical Planning. Operations. Engineering the CCT Core Assets. Domain Analysis. Architecture. Component Engineering. Testing: Application and Test Engineering. Sustainment Engineering: Product Line Evolution. Documentation. Managing the CCT Effort. Early Benefits from CCT. First CCT Product. Benefits beyond CCT Products. Lessons and Issues. Tool Support Is Inadequate. Domain Analysis Documentation Is Important. An Early Architecture Focus Is Best. Product Builders Need More Support. CCT Users Need Reuse Metrics. It Pays to Be Flexible, and Cross-Unit Teams Work. A Real Product Is a Benefit. Summary. For Further Reading. Discussion Questions. 11. Successful Software product Line Development in Small Organization. Introduction. The Early Years. The MERGER Software Product Line. Market Maker Software Product Line Practices. Architecture Definition. Component Development. Structuring (and Staffing) the Organization. Testing. Data Collection and Metrics. Launching and Institutionalizing the Product Line. Understanding the Market. Technology Forecasting. A Few Observations. Effects of Company Culture. Cost Issues. The Customer Paradox. Tool Support. Lessons Learned. Drawbacks. Conclusions: Software Product Lines in Small Organizations. For Further Reading. Discussion Questions. 12. Conclusions: Practices, Patterns and Payoffs. The Practices. The Patterns. The Success Factors. The Payoff. Finale. Glossary. Bibliography. Index.",Erratum,pro91
pap673,cf20d6130160843e5cd8ef8f5553d66a920bc6f3,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Global software engineering and agile practices: a systematic review,"Agile practices have received attention from industry as an alternative to plan‐driven software development approaches. Agile encourages, for example, small self‐organized collocated teams, whereas global software engineering (GSE) implies distribution across cultural, temporal, and geographical boundaries. Hence, combining them is a challenge. A systematic review was conducted to capture the status of combining agility with GSE. The results were limited to peer‐reviewed conference papers or journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. publication year, contribution type, research method). At the end, 81 papers were judged as primary for further analysis. The distribution of papers over the years indicated that GSE and Agile in combination has received more attention in the last 5 years. However, the majority of the existing research is industrial experience reports in which Agile practices were modified with respect to the context and situational requirements. The emergent need in this research area is suggested to be developing a framework that considers various factors from different perspectives when incorporating Agile in GSE. Practitioners may use it as a decision‐making basis in early phases of software development. Copyright © 2011 John Wiley & Sons, Ltd.",Erratum,pro85
pap674,291001b0585ecda5aa9b4ad39d435463bf03dc7d,con34,International Conference on Agile Software Development,Revisions to Software Engineering 2004: Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering,Software Engineering 2004: Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering (SE 2004) is one volume in a set of computing curricula adopted and supported by the ACM and the IEEE Computer Society. In order to keep the software engineering guidelines up to date the two professional societies established a review project in early 2011. This paper describes that review effort and plans to revise the guidelines over the next year and a half.,Erratum,pro34
pap675,1727443b1a5c14663f4b04e7c6f27c36ca1c1ba3,con44,International Conference Knowledge Engineering and Knowledge Management,"What works for whom, where, when, and why? On the role of context in empirical software engineering","Context is a central concept in empirical software engineering. It is one of the distinctive features of the discipline and it is an indispensable part of software practice. It is likely responsible for one of the most challenging methodological and theoretical problems: study-to-study variation in research findings. Still, empirical software engineering research is mostly concerned with attempts to identify universal relationships that are independent of how work settings and other contexts interact with the processes important to software practice. The aim of this paper is to provide an overview of how context affects empirical research and how empirical software engineering research can be better `contextualized' in order to provide a better understanding of what works for whom, where, when, and why. We exemplify the importance of context with examples from recent systematic reviews and offer recommendations on the way forward.",Erratum,pro44
pap676,ab2ec1a1b6fdb710b2782554086a4782d50fc2c8,con38,International Symposium on Empirical Software Engineering and Measurement,Recommended Steps for Thematic Synthesis in Software Engineering,"Thematic analysis is an approach that is often used for identifying, analyzing, and reporting patterns (themes) within data in primary qualitative research. 'Thematic synthesis' draws on the principles of thematic analysis and identifies the recurring themes or issues from multiple studies, interprets and explains these themes, and draws conclusions in systematic reviews. This paper conceptualizes the thematic synthesis approach in software engineering as a scientific inquiry involving five steps that parallel those of primary research. The process and outcome associated with each step are described and illustrated with examples from systematic reviews in software engineering.",Conference paper,pro38
pap677,7093ea1420fca2a2cf679073ae14b77f07d5fbe5,jou160,IEEE Software,Where's the Theory for Software Engineering?,"Darwin's theory of natural selection, Maxwell's equations, the theory of demand and supply; almost all established academic disciplines place great emphasis on what their core theory is. This is not, however, the case in software engineering. What is the reason behind the software engineering community's apparent indifference to a concept that is so important to so many others?",Letter,vol160
pap678,ebbafe78a53a2086420034f6b3ee230c3bfae815,con38,International Symposium on Empirical Software Engineering and Measurement,Dynamic adaptive Search Based Software Engineering,"Search Based Software Engineering (SBSE) has proved to be a very effective way of optimising software engineering problems. Nevertheless, its full potential as a means of dynamic adaptivity remains under explored. This paper sets out the agenda for Dynamic Adaptive SBSE, in which the optimisation is embedded into deployed software to create self-optimising adaptive systems. Dynamic Adaptive SBSE will move the research agenda forward to encompass both software development processes and the software products they produce, addressing the long-standing, and as yet largely unsolved, grand challenge of self-adaptive systems.",Erratum,pro38
pap679,276da75fba54fef4bfe20b6b8841f78933ed3d0e,jou152,Empirical Software Engineering,Special issue on repeatable results in software engineering prediction,,Letter,vol152
pap680,43cf42a20a9dd1749bf525fc62eebba4d4e1eb72,con31,International Conference on Evaluation & Assessment in Software Engineering,Sustainability in software engineering: A systematic literature review,"Background: Supporting sustainability in software engineering is becoming an active area of research. We want to contribute the first Systematic Literature Review(SLR) in this field to aid researchers who are motivated to contribute to that topic by providing a body of knowledge as starting point, because we know from own experience, this search can be tedious and time consuming. Aim: We aim to provide an overview of different aspects of sustainability in software engineering research with regard to research activity, investigated topics, identified limitations, proposed approaches, used methods, available studies, and considered domains. Method: The applied method is a SLR in five reliable and commonly-used databases according to the (quasi-standard) protocol by Kitchenham et al. [1]. We assessed the 100 first results of each database ordered by relevance with respect to the search query. Results: Of 500 classified publications, we regard 96 as relevant for our research questions. We sketch a taxonomy of their topics and domains, and provide lists of used methods and proposed approaches. Most of the excluded publications were ruled out because of an unfitting usage of terms within the search query. Conclusions: Currently, there is little research coverage on the different aspects of sustainability in software engineering while other disciplines are already more active. Future work includes extending the study by reviewing a higher number of publications, including dedicated journal and workshop searches, and snowballing.",Article,pro31
pap681,e8dbf08e81b7db05abfc0526af0e97f1e679c66f,con110,Very Large Data Bases Conference,The role of Artificial Intelligence in Software Engineering,"There has been a recent surge in interest in the application of Artificial Intelligence (AI) techniques to Software Engineering (SE) problems. The work is typified by recent advances in Search Based Software Engineering, but also by long established work in Probabilistic reasoning and machine learning for Software Engineering. This paper explores some of the relationships between these strands of closely related work, arguing that they have much in common and sets out some future challenges in the area of AI for SE.",Erratum,pro110
pap682,03992dcfe1943355229cae90b7b206a480220bad,jou152,Empirical Software Engineering,On the dataset shift problem in software engineering prediction models,,Article,vol152
pap683,35e06978cd069786ff7ab05ac29d6e2e28f1bad6,jou167,Advances in Software Engineering,Clustering Methodologies for Software Engineering,"The size and complexity of industrial strength software systems are constantly increasing. This means that the task of managing a large software project is becoming even more challenging, especially in light of high turnover of experienced personnel. Software clustering approaches can help with the task of understanding large, complex software systems by automatically decomposing them into smaller, easier-to-manage subsystems. The main objective of this paper is to identify important research directions in the area of software clustering that require further attention in order to develop more effective and efficient clustering methodologies for software engineering. To that end, we first present the state of the art in software clustering research. We discuss the clustering methods that have received the most attention from the research community and outline their strengths and weaknesses. Our paper describes each phase of a clustering algorithm separately. We also present the most important approaches for evaluating the effectiveness of software clustering.",Conference paper,vol167
pap684,b2cc04740a617190dd179ad46f32cd432efee528,con77,International Conference on Artificial Neural Networks,"Software Engineering: A Practitioner's Approach, 7Th Edition","As recognized, adventure as skillfully as experience about lesson, amusement, as well as deal can be gotten by just checking out a ebook software engineering a practitioner39s approach 7th edition then it is not directly done, you could take even more around this life, concerning the world. We allow you this proper as capably as simple artifice to acquire those all. We find the money for software engineering a practitioner39s approach 7th edition and numerous books collections from fictions to scientific research in any way. in the midst of them is this software engineering a practitioner39s approach 7th edition that can be your partner. Page Url",Erratum,pro77
pap685,29d2b8dba00379edccbdd2bda90de9762d0c1003,jou160,IEEE Software,Embracing the Engineering Side of Software Engineering,"The author provides, based on 20 years of research and industrial experience, his assessment of software engineering research. He then builds on such analysis to provide recommendations on how we need to change as a research community to increase our impact, gain credibility, and ultimately ensure the success and recognition of our young discipline. The gist of the author's message is that we need to become a true engineering discipline.",Conference paper,vol160
pap686,eb2036e0a40c00f450e9d983e0ce23e40cbf1f7e,jou168,Springer US,Basics of Software Engineering Experimentation,,Letter,vol168
pap687,bbb9a237eb0cb75812d05c2d6428253bb1627a56,con68,Experimental Software Engineering Network,Software architecture in practice,"From the Book: 
 
Our goals for the first edition were threefold. First, we wanted to show through authentic case studies actual examples of software architectures solving real-world problems. Second, we wanted to establish and show the strong connection between an architecture and an organization's business goals. And third, we wanted to explain the importance of software architecture in achieving the quality goals for a system. 
 
Our goals for this second edition are the same, but the passage of time since the writing of the first edition has brought new developments in the field and new understanding of the important underpinnings of software architecture. We reflect the new developments with new case studies and the new understanding both through new chapters and through additions to and elaboration of the existing chapters. 
 
Architecture analysis, design, reconstruction, and documentation have all had major developments since the first edition. Architecture analysis has developed into a mature field with industrial-strength methods. This is reflected by a new chapter about the architecture tradeoff analysis method (ATAM). The ATAM has been adopted by industrial organizations as a technique for evaluating their software architectures. 
 
Architecture design has also had major developments since the first edition. The capturing of quality requirements, the achievement of those requirements through small-scale and large-scale architectural approaches (tactics and patterns, respectively), and a design method that reflects knowledge of how to achieve qualities are all captured in various chapters. Three new chapters treat understanding quality requirements, achieving qualities, and theattribute driven design (ADD) method, respectively. 
 
Architecture reconstruction or reverse engineering is an essential activity for capturing undocumented architectures. It can be used as a portion of a design project, an analysis project, or to provide input into a decision process to determine what to use as a basis for reconstructing an existing system. In the first edition, we briefly mentioned a tool set (Dali) and its uses in the re-engineering context; in in this edition the topic merits its own chapter. 
 
Documenting software architectures is another topic that has matured considerably in the recent past. When the first edition was published, the Unified Modeling Language (UML) was just arriving on the scene. Now it is firmly entrenched, a reality reflected by all-new diagrams. But more important, an understanding of what kind of information to capture about an architecture, beyond what notation to use, has emerged. A new chapter covers architecture documentation. 
 
The understanding of the application of software architecture to enable organizations to efficiently produce a variety of systems based on a single architecture is summarized in a totally rewritten chapter on software product lines. The chapter reinforces the link between architecture and an organization's business goals, as product lines, based around a software architecture, can enable order-of-magnitude improvements in cost, quality, and time to market. 
 
In addition to the architectural developments, the technology for constructing distributed and Web-based systems has become prominent in today's economy. We reflect this trend by updating the World Wide Web chapter, by using Web-based examples for the ATAM chapter and the chapter on building systems from components, by replacing the CORBA case study with one on Enterprise JavaBeans (EJB), and by introducing a case study on a wireless EJB system designed to support wearable computers for maintenance technicians. 
 
Finally, we have added a chapter that looks more closely at the financial aspects of architectures. There we introduce a method--the CBAM--for basing architectural decisions on economic criteria, in addition to the technical criteria that we had focused on previously. 
 
As in the first edition, we use the architecture business cycle as a unifying motif and all of the case studies are described in terms of the quality goals that motivated the system design and how the architecture for the system achieves those quality goals. 
 
In this edition, as in the first, we were very aware that our primary audience is practitioners, so we focus on presenting material that has been found useful in many industrial applications, as well as what we expect practice to be in the near future. 
 
We hope that you enjoy reading it at least as much as we enjoyed writing it. 
 
 
0321154959P12162002",Erratum,pro68
pap688,482112e839684c25166f16ed91153683158da31e,con101,International Conference on Biometrics,in Computer Science and Software Engineering,"— Intrusion detection is an alternative to the situation of the security violation.Security mechanism of the network is necessary against the threat to the system. There are two types of intruders: external intruders, who are unauthorized users of the machines they attack, and internal intruders, who have permission to access the system with some restrictions. This paper describes a brief overview of various intrusion detection techniques such as fuzzy logic, neural network, pattern recognition methods, genetic algorithms and related techniques is presented. Among the several soft computing paradigms, fuzzy rule-based classifiers, decision trees, support vector machines, linear genetic programming is model fast and efficient intrusion detection systems.",Erratum,pro101
pap689,7e5933ff05db0f1edf6d98992d1a3bd31813b4c0,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,A Mapping Study on Requirements Engineering in Agile Software Development,"Agile software development (ASD) methods have gained popularity in the industry and been the subject of an increasing amount of academic research. Although requirements engineering (RE) in ASD has been studied, the overall understanding of RE in ASD as a phenomenon is still weak. We conducted a mapping study of RE in ASD to review the scientific literature. 28 articles on the topic were identified and analyzed. The results indicate that the definition of agile RE is vague. The proposed benefits from agile RE included lower process overheads, a better requirements understanding, a reduced tendency to over allocate development resources, responsiveness to change, rapid delivery of value, and improved customer relationships. The problematic areas of agile RE were the use of customer representatives, the user story requirements format, the prioritization of requirements, growing technical debt, tacit requirements knowledge, and imprecise effort estimation. We also report proposed solutions to the identified problems.",Conference paper,pro39
pap690,e5dc4cae90bfe566c757028e42d77b013fe6f331,con31,International Conference on Evaluation & Assessment in Software Engineering,Software Engineering Game: Software Engineering Game,"The goal of this paper is to explore and evaluate the utility of different strategies used in educational games. These strategies include creating immersion and promote learning through visual gratification, feedback, scoring, reasoning, and cognitively demanding environments. A game prototype, based on the tactics and strategies outlined in the literature study, has been implemented and is subject for testing on students. The tests aims to determine how well different strategies would do in an actual implementation of a game.",Erratum,pro31
pap691,22201608e353530edff3ca503ccf7dad5df66efa,jou155,Information and Software Technology,A Process Framework for Global Software Engineering Teams,,Article,vol155
pap692,193ebbc6b5a11f3ed5fb67266d5b89832521e818,jou156,Journal of Systems and Software,Context-oriented programming: A software engineering perspective,,Conference paper,vol156
pap693,ef51193388ebcc0ab19a6200e469fb6666e42d3c,jou151,IEEE Transactions on Software Engineering,A Methodology for Collecting Valid Software Engineering Data,"An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.",Letter,vol151
pap694,dca45bd363820bce269a176a1ecde7e1885e2ea6,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",B4: experience with a globally-deployed software defined wan,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",Article,pro52
pap695,4c880ad1186754be17e1907d0265b450ab309b48,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Software Metrics : A Rigorous and Practical Approach,"From the Publisher: 
The Second Edition of Software Metrics provides an up-to-date, coherent, and rigorous framework for controlling, managing, and predicting software development processes. With an emphasis on real-world applications, Fenton and Pfleeger apply basic ideas in measurement theory to quantify software development resources, processes, and products. The book offers an accessible and comprehensive introduction to software metrics, now an essential component of software engineering for both classroom and industry. Software Metrics features extensive case studies from Hewlett Packard, IBM, the U.S. Department of Defense, Motorola, and others, in addition to worked examples and exercises. The Second Edition includes up-to-date material on process maturity and measurement, goal-question-metric, planning a metrics program, measurement in practice, experimentation, empirical studies, ISO9216, and metric tools.",Erratum,pro85
pap696,f2f72555b1bf716ccd8ab91a9441ea6d4864119d,con12,The Compass,Software engineering issues for mobile application development,"This paper provides an overview of important software engineering research issues related to the development of applications that run on mobile devices. Among the topics are development processes, tools, user interface design, application portability, quality, and security.",Erratum,pro12
pap697,301311f883cb3df1b1c00077ddf5f2fc0ed2f4f8,con55,Workshop on Learning from Authoritative Security Experiment Results,Traffic engineering in software defined networks,"Software Defined Networking is a new networking paradigm that separates the network control plane from the packet forwarding plane and provides applications with an abstracted centralized view of the distributed network state. A logically centralized controller that has a global network view is responsible for all the control decisions and it communicates with the network-wide distributed forwarding elements via standardized interfaces. Google recently announced [5] that it is using a Software Defined Network (SDN) to interconnect its data centers due to the ease, efficiency and flexibility in performing traffic engineering functions. It expects the SDN architecture to result in better network capacity utilization and improved delay and loss performance. The contribution of this paper is on the effective use of SDNs for traffic engineering especially when SDNs are incrementally introduced into an existing network. In particular, we show how to leverage the centralized controller to get significant improvements in network utilization as well as to reduce packet losses and delays. We show that these improvements are possible even in cases where there is only a partial deployment of SDN capability in a network. We formulate the SDN controller's optimization problem for traffic engineering with partial deployment and develop fast Fully Polynomial Time Approximation Schemes (FPTAS) for solving these problems. We show, by both analysis and ns-2 simulations, the performance gains that are achievable using these algorithms even with an incrementally deployed SDN.",Erratum,pro55
pap698,f60e0b81f1cf178175e0d726aa8824123c981500,con37,International Symposium on Search Based Software Engineering,On Parameter Tuning in Search Based Software Engineering,,Article,pro37
pap699,a6a9e207f7b80ae66d31ff449aaaca5144152161,jou160,IEEE Software,The Guide to the Software Engineering Body of Knowledge,"Reporting on the SWEBOK project, the authors-who represent the project's editorial team-discuss the three-phase plan to characterize a body of knowledge, a vital step toward developing software engineering as a profession.",Letter,vol160
pap700,c4518592ff763d6746a197c2c5f3df2c4044d13d,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Guide to the Software Engineering Body of Knowledge,data types Sorting and searching parallel and distributed algorithms 3. [AR] Computer Architecture,Erratum,pro54
pap701,ccd4e0cf89be4f0a1b8e979308695df7172985fe,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Human-Centered Software Engineering - Integrating Usability in the Software Development Lifecycle,,Erratum,pro42
pap702,fbfbc0dd1d6501c8cf7aecfe38d3a0149b22d37a,con31,International Conference on Evaluation & Assessment in Software Engineering,Motivation in software engineering: A systematic review update,"Background/Aim - Given the relevance and importance that the understanding of motivation has gained in the field of software engineering, this work was carried out in order to update the results of a literature review carried out in 2006 on motivation in software engineering. Method - Based on guidelines for this specific type of study, we replicated the original study protocol. Results - The combination of manual and automatic searches retrieved 6,534 papers, of which 53 relevant papers were selected for data extraction and analysis. Conclusions - Studies address motivation using several viewpoints and approaches and, even though the number of researches increased in this area, the overall understanding of what actually motivates software engineers does not seem to have significantly advanced in the last five years.",Letter,pro31
pap703,5a30fd3718835c500d1833492d6cd833c959d155,con96,Interspeech,Non-Functional Requirements in Software Engineering,,Erratum,pro96
pap704,bdfb23a874d6222d4a800b3379348d784ff4f43d,con33,International Conference on Automated Software Engineering,Ecological inference in empirical software engineering,"Software systems are decomposed hierarchically, for example, into modules, packages and files. This hierarchical decomposition has a profound influence on evolvability, maintainability and work assignment. Hierarchical decomposition is thus clearly of central concern for empirical software engineering researchers; but it also poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration and productivity? At the level of files? packages? or modules? How does the level of study affect the truth, meaning, and relevance of the findings? In other fields it has been found that choosing the wrong level might lead to misleading or fallacious results. Choosing a proper level, for study, is thus vitally important for empirical software engineering research; but this issue hasn't thus far been explicitly investigated. We describe the related idea of ecological inference and ecological fallacy from sociology and epidemiology, and explore its relevance to empirical software engineering; we also present some case studies, using defect and process data from 18 open source projects to illustrate the risks of modeling at an aggregation level in the context of defect prediction, as well as in hypothesis testing.",Conference paper,pro33
pap705,4399bc50a26f10e6bb253ecb82db15f7202452b9,con27,International Conference on Contemporary Computing,Software Engineering: A Practitioner's Approach (McGraw-Hill Series in Computer Science),,Erratum,pro27
pap706,b0b6b137612efa8209a5394870974c8362eaf3e7,con53,Workshop on Web 2.0 for Software Engineering,"Towards understanding twitter use in software engineering: preliminary findings, ongoing challenges and future questions",There has been some research conducted around the motivation for the use of Twitter and the value brought by micro-blogging tools to individuals and business environments. This paper builds on our understanding of how the phenomenon affects the population which birthed the technology: Software Engineers. We find that the Software Engineering community extensively leverages Twitter's capabilities for conversation and information sharing and that use of the tool is notably different between distinct Software Engineering groups. Our work exposes topics for future research and outlines some of the challenges in exploring this type of data.,Conference paper,pro53
pap707,7430c82d279f7cbc28556e8086e7cc60fef7f69f,jou152,Empirical Software Engineering,The role of non-exact replications in software engineering experiments,,Article,vol152
pap708,013c70a1f66f7c251490d771678ee747535bbe3d,con2,International Conference on Software Engineering,Toward sustainable software engineering: NIER track,"Current software engineering practices have significant effects on the environment. Examples include e-waste from computers made obsolete due to software upgrades, and changes in the power demands of new versions of software. Sustainable software engineering aims to create reliable, long-lasting software that meets the needs of users while reducing environmental impacts. We conducted three related research efforts to explore this area. First, we investigated the extent to which users thought about the environmental impact of their software usage. Second, we created a tool called GreenTracker, which measures the energy consumption of software in order to raise awareness about the environmental impact of software usage. Finally, we explored the indirect environmental effects of software in order to understand how software affects sustainability beyond its own power consumption. The relationship between environmental sustainability and software engineering is complex; understanding both direct and indirect effects is critical to helping humans live more sustainably.",Article,pro2
pap709,8de121442c5df6ebd1d93c132086c80ae7613b06,con48,ACM Symposium on Applied Computing,Handbook of software reliability engineering,Technical foundations introduction software reliability and system reliability the operational profile software reliability modelling survey model evaluation and recalibration techniques practices and experiences best current practice of SRE software reliability measurement experience measurement-based analysis of software reliability software fault and failure classification techniques trend analysis in validation and maintenance software reliability and field data analysis software reliability process assessment emerging techniques software reliability prediction metrics software reliability and testing fault-tolerant SRE software reliability using fault trees software reliability process simulation neural networks and software reliability. Appendices: software reliability tools software failure data set repository.,Erratum,pro48
pap710,df28611f5b9990d12844d5c85bd2994419a397bf,jou155,Information and Software Technology,Research synthesis in software engineering: A tertiary study,,Conference paper,vol155
pap711,9ae68e232cdff0fa0bffcd92a405e9573f7a5893,con8,Frontiers in Education Conference,Component-Based Software Engineering: Putting the Pieces Together,"Software components are increasingly central to efficient, cost-effective software development. In this book, the world's leading experts on component software development come together to present the field's state of the art, and to offer new insights into the key challenges of component architecture and reuse. With original contributions by leaders such as Ivar Jacobson, Martin Griss, Len Bass, Paul Clements, Don Reifer, and Will Tracz, this carefully edited book is the first word on components: a tool for helping practitioners get the most out of all their component-based resources. It offers new insight for deciding whether and how to implement component-based development strategies; as well as a clear understanding of the obstacles to successful component development, and best practices responses.",Erratum,pro8
pap712,e74f28e66a2aa715fff2cf60158b177c45130fdc,jou155,Information and Software Technology,Identifying relevant studies in software engineering,,Letter,vol155
pap713,05882b3376dddab5066015b46704bb7bfe3946cc,jou160,IEEE Software,Recommendation Systems for Software Engineering,"Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering (RSSEs) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.",Letter,vol160
pap714,3caedff0a82950046730bce6f8d85aec46cf2e8c,jou152,Empirical Software Engineering,Empirical evidence in global software engineering: a systematic review,,Conference paper,vol152
pap715,c9382574e6a868fe45aa9cc09d19c0f5fadbd652,con78,Neural Information Processing Systems,Evidence-based software engineering,"Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.",Erratum,pro78
pap716,24b11e369c41137e30ed026061bcfc66855e6832,con55,Workshop on Learning from Authoritative Security Experiment Results,Guide to Advanced Empirical Software Engineering,,Erratum,pro55
pap717,96b21ef42f94dc8f7ff7860c066a00d284f7362c,jou156,Journal of Systems and Software,Bridging metamodels and ontologies in software engineering,,Conference paper,vol156
pap718,87cc9fb5129781b74750d88f83472f2cd644ca60,con54,Conference of the Centre for Advanced Studies on Collaborative Research,An examination of software engineering work practices,"This paper presents work practice data of the daily activities of software engineers. Four separate studies are presented; one looking longitudinally at an individual SE; two looking at a software engineering group; and one looking at company-wide tool usage statistics. We also discuss the advantages in considering work practices in designing tools for software engineers, and include some requirements for a tool we have developed as a result of our studies.",Conference paper,pro54
pap719,367a101c220e5af0f02d61068a50fbb43edf4a1d,con55,Workshop on Learning from Authoritative Security Experiment Results,"Search Based Software Engineering: Techniques, Taxonomy, Tutorial",,Letter,pro55
pap720,8f79b5b359e5d49bdd2d53299c395e45815e4ffc,jou152,Empirical Software Engineering,Qualitative research in software engineering,,Conference paper,vol152
pap721,8adfb836c1af62800be790e3353f7d28a2943192,jou152,Empirical Software Engineering,Curating GitHub for engineered software projects,,Conference paper,vol152
pap722,deb5ed293e69c2a1fa59c670f8d7813c427bb3b8,con15,Pacific Symposium on Biocomputing,Ontologies for Software Engineering and Software Technology,,Erratum,pro15
pap723,dd8af788d55e95b9b4132bde4c7a5b2a7bb4dfbc,con15,Pacific Symposium on Biocomputing,A Comparison Between Five Models Of Software Engineering,"This research deals with a vital and important issue in computer world. It is concerned with the software management processes that examine the area of software development through the development models, which are known as software development life cycle. It represents five of the development models namely, waterfall, Iteration, V-shaped, spiral and Extreme programming. These models have advantages and disadvantages as well. Therefore, the main objective of this research is to represent different models of software development and make a comparison between them to show the features and defects of each model.",Erratum,pro15
pap724,1c5b80259a8b8b4d4f286a86ab2d5ea983ee0db5,con59,Annual Workshop of the Psychology of Programming Interest Group,Agile Practices in Global Software Engineering - A Systematic Map,"This paper presents the results of systematically reviewing the current research literature on the use of agile practices and lean software development in global software engineering (GSE). The primary purpose is to highlight under which circumstances they have been applied efficiently. Some common terms related to agile practices (e.g. scrum, extreme programming) were considered in formulating the search strings, along with a number of alternatives for GSE such as offshoring, outsourcing, and virtual teams. The results were limited to peer-reviewed conference papers/journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. research type, distribution). The analysis revealed that in most cases agile practices were modified with respect to the context and situational requirements. This indicates the need for future research on how to integrate all experiences and practices in a way to assist practitioners when setting up non-collocated agile projects.",Erratum,pro59
pap725,bdc141c6bd4b9454c06d57744e43ca9eaa5c657a,con56,International Conference on Software Engineering and Knowledge Engineering,Validity Threats in Empirical Software Engineering Research - An Initial Survey,"In judging the quality of a research study it is very important to consider threats to the validity of the study and the results. This is particularly important for empirical research where there is often a multitude of possible threats. With a growing focus on empirical research methods in software engineering it is important that there is a consensus in the community on this importance, that validity analysis is done by every researcher and that there is common terminology and support on how to do and report it. Even though there are previous relevant results they have primarily focused on quantitative research methods and in particular experiments. Here we look at the existing advice and guidelines and then perform a review of 43 papers published in the ESEM conference in 2009 and analyse the validity analysis they include and which threats and strategies for overcoming them that were given by the authors. Based on this analysis we then discuss what is working well and less well in validity analysis of empirical software engineering research and present recommendations on how to better support validity analysis in the future.",Conference paper,pro56
pap726,f4aae8bc0362133ad862f2160394cabc83607470,con62,Australian Software Engineering Conference,The impact of social media on software engineering practices and tools,"Today's generation of software developers frequently make use of social media, either as an adjunct or integrated into a wide range of tools ranging from code editors and issue trackers, to IDEs and web-based portals. The role of social media usage in software engineering is not well understood, and yet the use of these mechanisms influences software development practices. In this position paper, we advocate for research that strives to understand the benefits, risks and limitations of using social media in software development at the team, project and community levels. Guided by the implications of current tools and social media features, we propose a set of pertinent research questions around community involvement, project coordination and management, as well as individual software development activities. Answers to these questions will guide future software engineering tool innovations and software development team practices.",Erratum,pro62
pap727,185b75fa4b4b0096cbb957a4d7a2aa4438618f10,jou160,IEEE Software,Collaboration Tools for Global Software Engineering,"Software engineering involves people collaborating to develop better software. Collaboration is challenging, especially across time zones and without face-to-face meetings. We therefore use collaboration tools all along the product life cycle to let us work together, stay together, and achieve results together. This article summarizes experiences and trends chosen from recent IEEE International Conference on Global Software Engineering (IGSCE) conferences.",Conference paper,vol160
pap728,59d3ec40b4f17ed94dc5ae510c316ac511915031,con31,International Conference on Evaluation & Assessment in Software Engineering,On Searching Relevant Studies in Software Engineering,"BACKGROUND: Systematic Literature Review (SLR) has become an important research methodology in software engineering since 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is quite time consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need of a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. 
 
OBJECTIVE: The main objective of the research reported in this paper is to improve the search step of doing SLRs in SE by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. 
 
OUTCOMES: We have systematically selected and analytically studied a large number of papers to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLR, we have devised a systematic approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard', which consists of collection of known studies and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. We report the case study and its finding to demonstrate that the approach is able to improve the rigor of search process in an SLR, and can serves as the supplements to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using several case studies with varying topics in software engineering.",Letter,pro31
pap729,a3c99ea03a9d27f7f37ae7f3962e817b24abd3e1,con38,International Symposium on Empirical Software Engineering and Measurement,Synthesizing evidence in software engineering research,"Synthesizing the evidence from a set of studies that spans many countries and years, and that incorporates a wide variety of research methods and theoretical perspectives, is probably the single most challenging task of performing a systematic review. In this paper, we perform a tertiary review to assess the types and methods of research synthesis in systematic reviews in software engineering. Almost half of the 31 studies included in our review did not contain any synthesis; of the ones that did, two thirds performed a narrative or a thematic synthesis. The results show that, despite the focus on systematic reviews, there is, currently, limited attention to research synthesis in software engineering. This needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice.",Letter,pro38
pap730,6ef662fc9b318531ef3add75f8f8b6c0d0f5b11e,con41,Asia-Pacific Software Engineering Conference,Collaborative Software Engineering: Challenges and Prospects,,Erratum,pro41
pap731,219f3c0c931113cd83f92b18e0868cd69c47544a,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Global Software Engineering: A Software Process Approach,,Erratum,pro42
pap732,1a1aa3a659e8d21318414e822fa0008410f487bf,jou157,Requirements Engineering,Assessing traceability of software engineering artifacts,,Letter,vol157
pap733,e23e287baf50b5b9a19774de6d6fb356b6bac212,con101,International Conference on Biometrics,Software intelligence: the future of mining software engineering data,"Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence (SI) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name SI as an inspiration from the Business Intelligence (BI) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, SI offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. SI should support decision-making processes throughout the lifetime of a software system not just during its development phase.
 The vision of SI has yet to become a reality that would enable software engineering research to have a strong impact on modern software practice. Nevertheless, recent advances in the Mining Software Repositories (MSR) field show great promise and provide strong support for realizing SI in the near future. This position paper summarizes the state of practice and research of SI, and lays out future research directions for mining software engineering data to enable SI.",Erratum,pro101
pap734,108f91acc309d1b31336124bec48657a7736737b,jou151,IEEE Transactions on Software Engineering,How Reliable Are Systematic Reviews in Empirical Software Engineering?,"BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they may do so in different ways. This provides evidence that, in this context at least, the systematic review is a robust research method.",Conference paper,vol151
pap735,907e5aabc168ca7c4887b154a5f35025b3d4a491,jou152,Empirical Software Engineering,Applying empirical software engineering to software architecture: challenges and lessons learned,,Article,vol152
pap736,25fbf9843b617149db02bbc6a5c50f7939892593,con18,International Conference on Exploring Services Science,Collaborative Software Engineering: Concepts and Techniques,,Erratum,pro18
pap737,282b27fcdc35737d4edcc0bc54fe9560c68e1011,con57,International Workshop on Agent-Oriented Software Engineering,Agent-Oriented Software Engineering: The State of the Art,,Conference paper,pro57
pap738,69c24b9aa48b5ba9b2a06e7a0572e0c1308cb4f4,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine","On ""Software engineering""","Software engineers work on multidisciplinary teams to identify and develop software solutions and to maintain software intensive systems of all sizes. The focus of this program is on the rigorous engineering practices necessary to build, maintain, and protect modern software intensive systems. Consistent with this focus, the software engineering baccalaureate program consists of a rigorous curriculum of science, math, computer science, and software engineering courses.",Erratum,pro73
pap739,675052b39e572b46753ace4293cc1ae7c1e75a0d,con5,Technical Symposium on Computer Science Education,"To appear in: Knowledge Engineering Review, 1996 Software Agents: An Overview",,Erratum,pro5
pap740,e2ee8df90829f8fe41bf1a1ebc233113885c124a,con2,International Conference on Software Engineering,The Current State and Future of Search Based Software Engineering,"This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.",Erratum,pro2
pap741,d28585d2ec27b611f46125a56bc52a1d93fada98,con67,IEEE International Software Metrics Symposium,Software engineering in an uncertain world,"In this paper, we argue that the reality of today's software systems requires us to consider uncertainty as a first-class concern in the design, implementation, and deployment of those systems. We further argue that this induces a paradigm shift, and a number of research challenges that must be addressed.",Erratum,pro67
pap742,3713667bb3ce322c3bb078f4499017697a07282e,con88,European Conference on Computer Vision,Social media for software engineering,"Social media has changed the way that people collaborate and share information. In this paper, we highlight its impact for enabling new ways for software teams to form and work together. Individuals will self-organize within and across organizational boundaries. Grassroots software development communities will emerge centered around new technologies, common processes and attractive target markets. Companies consisting of lone individuals will able to leverage social media to conceive of, design, develop, and deploy successful and profitable product lines. A challenge for researchers who are interested in studying, influencing, and supporting this shift in software teaming is to make sure that their research methods protect the privacy and reputation of their stakeholders.",Erratum,pro88
pap743,15daae4c253eaeb6b1194e1fe6230ac3e394ae5a,con61,International Conference on Predictive Models in Software Engineering,"Object-Oriented Software Engineering Using UML, Patterns, and Java","This widely used book teaches practical object-oriented software engineering with the key real world tools UML, design patterns and Java. This step-by-step approach allows the reader to address complex and changing problems with practical and state-of-the-art solutions. This book uses examples from real systems and examines the interaction between such techniques as UML, Java-based technologies, design patterns, rationale, configuration management, and quality control. It also discusses project management related issues and their impacts. A valuable book for development engineers, software engineers, consulting engineers, software architects, product managers, project leaders, and knowledge managers.",Erratum,pro61
pap744,7f62cfa3b879ca9b8d3f736f2340f7a6c2c207da,jou169,Artificial Intelligence,On agent-based software engineering,,Conference paper,vol169
pap745,e79f82406d26581587b4b99112c3fd0c952659be,con81,International Conference on Learning Representations,Software Engineering for Self-Adaptive Systems [outcome of a Dagstuhl Seminar],,Erratum,pro81
pap746,3621eac63003c54f20ab387f6d4b5fbfd0624cfd,con33,International Conference on Automated Software Engineering,Global Software Engineering: The Future of Socio-technical Coordination,"Globally-distributed projects are rapidly becoming the norm for large software systems, even as it becomes clear that global distribution of a project seriously impairs critical coordination mechanisms. In this paper, I describe a desired future for global development and the problems that stand in the way of achieving that vision. I review research and lay out research challenges in four critical areas: software architecture, eliciting and communicating requirements, environments and tools, and orchestrating global development. I conclude by noting the need for a systematic understanding of what drives the need to coordinate and effective mechanisms for bringing it about.",Erratum,pro33
pap747,4d4c071d51252c8007f3dc64feeabe4434a6c83f,jou155,Information and Software Technology,Software engineering research for computer games: A systematic review,,Conference paper,vol155
pap748,7f11d4761b2bc41c8d261855607a724c9377a157,con55,Workshop on Learning from Authoritative Security Experiment Results,Guide to the Software Engineering Body of Knowledge (SWEBOK) and the Software Engineering Education Knowledge (SEEK) - a preliminary mapping,"This paper is the result of a workshop held in Montreal in October 2002 during the Software Technology and Practice Conference (STEP 2002). The purpose of the paper is to present a preliminary mapping of two related but distinct software engineering body of knowledge initiatives, and also a list of proposals to improve them: the guide to the Software Engineering Body of Knowledge (SWEBOK) and the Software Engineering Education [body of] Knowledge (SEEK). The SWEBOK guide is aimed at identifying and describing the body of knowledge of a software engineering professional who has an undergraduate degree and four years of experience. The intended audiences of the SWEBOK Guide include industry, academia and policy-making organizations. The SEEK is aimed at delimiting the knowledge that professionals teaching software engineering agree is necessary for anyone to obtain an undergraduate degree in this field. The mapping shows that, though there are no major ""school of thought"" divergences between the two bodies of knowledge, there are a number of differences in the details of each breakdown in terms of vocabulary, level of detail, decomposition approach and topics encompassed.",Erratum,pro55
pap749,f74d664f0c3451bd4c31cf973a549a6dc00897c6,con94,Vision,\{PROMISE\} Repository of empirical software engineering data,,Erratum,pro94
pap750,9acba85c78ed1767e47e1c3346f22557eb82eefb,con33,International Conference on Automated Software Engineering,"Software Engineering: A Practitionerʼs Approach, 7/e","ion—data, procedure, control Architecture—the overall structure of the software Patterns—”conveys the essence” of a proven design solution Separation of concerns—any complex problem can be more easily handled if it is subdivided into pieces Modularity—compartmentalization of data and function Hiding—controlled interfaces Functional independence—single-minded function and low coupling Refinement—elaboration of detail for all abstractions Aspects—a mechanism for understanding how global requirements affect design Refactoring—a reorganization technique that simplifies the design OO design concepts—Appendix II Design Classes—provide design detail that will enable analysis classes to be implemented",Erratum,pro33
pap751,6763a821d8f6f204256b57767f6d8350075c175b,con19,International Conference on Conceptual Structures,Search Based Software Engineering: A Comprehensive Analysis and Review of Trends Techniques and Applications,"In the past five years there has been a dramatic increase in work on Search Based Software Engineering (SBSE), an approach to software engineering in which search based optimisation algorithms are used to address problems in Software Engineering. SBSE has been applied to problems throughout the Software Engineering lifecycle, from requirements and project planning to maintenance and re-engineering. The approach is attractive because it offers a suite of adaptive automated and semi-automated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives. This paper provides a review and classification of literature on SBSE. The paper identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.",Erratum,pro19
pap752,c0d9a396c6d3881dfcb0d1cd28b2433f83faf9ca,con16,International Conference on Data Science and Advanced Analytics,Property-Based Software Engineering Measurement,"Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could generate a consensus in the software engineering community and provide a means for better communication among researchers, better guidelines for analysts, and better evaluation methods for commercial static analyzers for practitioners. We propose a mathematical framework which is generic, because it is not specific to any particular software artifact, and rigorous, because it is based on precise mathematical concepts. We use this framework to propose definitions of several important measurement concepts (size, length, complexity, cohesion, coupling). It does not intend to be complete or fully objective; other frameworks could have been proposed and different choices could have been made. However, we believe that the formalisms and properties we introduce are convenient and intuitive. This framework contributes constructively to a firmer theoretical ground of software measurement.",Erratum,pro16
pap753,3da0f2dedae35fdacfe6d5c828c29e3ff7d315df,jou170,IET Software,Progress on approaches to software defect prediction,"Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014-April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction.",Letter,vol170
pap754,a1b51ff5cfb974fdef34386bed5c5844ba7a8dcf,con50,International Workshop on Green and Sustainable Software,Managing the software process,Foreword. Preface. I. SOFTWARE PROCESS MATURITY. A Software Maturity Framework. The Principles of Software Process Change. Software Process Assessment. The Initial Process. II. THE REPEATABLE PROCESS. Managing Software Organizations. The Project Plan. Software Configuration Management-Part 1: Software Quality Assurance. III. THE DEFINED PROCESS. Software Standards. Software Inspections. Software Testing. Software Configuration Management (Continued). Defining the Software Process. The Software Engineering Process Group IV. THE MANAGED PROCESS. Data Gathering and Analysis. Managing Software Quality. V. THE OPTIMIZING PROCESS. Defect Prevention. Automating The Software Process. Contracting for Software. Conclusion. Appendices. Index. 0201180952T04062001,Erratum,pro50
pap755,7deeb94777af3649a24402de1b0573294560bd56,jou152,Empirical Software Engineering,Sentiment Polarity Detection for Software Development,,Conference paper,vol152
pap756,3a5a9920d3220ab47a5c39e8c49da86b54d08386,con38,International Symposium on Empirical Software Engineering and Measurement,"Model-Driven Software Development: Technology, Engineering, Management","Part I: Introduction. 1. Introduction. 2. MDSD - Basic Ideas and Terminology. 3. Case Study: A Typical Web Application. 4. Concept Formation. 5. Classification. Part II: Domain Architectures. 6. Metamodeling. 7. MDSD-Capable Target Architectures. 8. Building Domain Architectures. 9. Code Generation Techniques. 10. Model Transformation Techniques. 11. MDSD Tools: Roles, Architecture, Selection Criteria, and Pointers. 12. The MDA Standard. Part III: Processes and Engineering. 13. MDSD Process Building Blocks and Best Practices. 14. Testing. 15. Versioning. 16. Case Study: Embedded Component Infrastructures. 17. Case Study: An Enterprise System. Part IV: Management. 18. Decision Support. 1.9 Organizational Aspects. 20. Adoption Strategies for MDSD. References. Index.",Erratum,pro38
pap757,90b8cfa993357cccd94d05a4317342892516731b,jou171,Computer,Data Mining for Software Engineering,"To improve software productivity and quality, software engineers are increasingly applying data mining algorithms to various software engineering tasks. However, mining SE data poses several challenges. The authors present various algorithms to effectively mine sequences, graphs, and text from such data.",Conference paper,vol171
pap758,5248768746e145728bd44067a359e4801c8fb0e6,jou172,Information Systems,A software engineering approach to ontology building,,Letter,vol172
pap759,75917fc7a6959793abe36da6ec678bd9518207cd,jou68,IEEE Transactions on Services Computing,Ontology Classification for Semantic-Web-Based Software Engineering,"The semantic Web is the second generation of the Web, which helps sharing and reusing data across application, enterprise, and community boundaries. Ontology defines a set of representational primitives with which a domain of knowledge is modeled. The main purpose of the semantic Web and ontology is to integrate heterogeneous data and enable interoperability among disparate systems. Ontology has been used to model software engineering knowledge by denoting the artifacts that are designed or produced during the engineering process. The semantic Web allows publishing reusable software engineering knowledge resources and providing services for searching and querying. This paper classifies the ontologies developed for software engineering, reviews the current efforts on applying the semantic Web techniques on different software engineering aspects, and presents the benefits of their applications. We also foresee the possible future research directions.",Conference paper,vol68
pap760,3b1228cee8e74fa231ad730c405bc1b1e1cc174f,con100,International Conference on Automatic Face and Gesture Recognition,Software Engineering Challenges in Game Development,"In Software Engineering (SE), video game development is unique yet similar to other software endeavors. It is unique in that it combines the work of teams covering multiple disciplines (art, music, acting, programming, etc.), and that engaging game play is sought after through the use of prototypes and iterations. With that, game development is faced with challenges that can be addressed using traditional SE practices. The industry needs to adopt sound SE practices for their distinct needs such as managing multimedia assets and finding the “fun” in game play. The industry must take on the challenges by evolving SE methods to meet their needs. This work investigates these challenges and highlights engineering practices to mitigate these challenges.",Erratum,pro100
pap761,831bacac789fad53065575efbe87cc25a06913e7,con7,International Symposium on Intelligent Data Analysis,Software Engineering Best Practices,"Proven techniques for software development success 
 
In this practical guide, software-quality guru Capers Jones reveals best practices for ensuring software development success by illustrating the engineering methods used by the most successful large software projects at leading companies such as IBM, Microsoft, Sony, and EDS. 
 
Software Engineering Best Practices covers estimating and planning; requirements analysis; change control; quality control; progress and cost tracking; and maintenance and support after delivery. Agile development, extreme programming, joint application design (JAD), six-sigma for software, and other methods are discussed. 
 
Table of contents 
Chapter 1. Introduction and Definitions of Software Best Practices;Chapter 2. Overview of 50 Software Best Practices;Chapter 3. A Preview of Software Development and Maintenance in 2049;Chapter 4. How Software Personnel Learn New Skills;Chapter 5. Software Team Organization and Specialization;Chapter 6. Project Management and Software Engineering;Chapter 7. Requirements, Business Analysis, Architecture, Enterprise Architecture, and Design;Chapter 8. Programming and Code Development;Chapter 9. Software Quality: The Key to Successful Software Engineering;Index",Erratum,pro7
pap762,761bf03e086ce887a44dc5e44aefb0de05b4a47f,con38,International Symposium on Empirical Software Engineering and Measurement,Systematic literature reviews in software engineering: Preliminary results from interviews with researchers,"Systematic Literature Reviews (SLRs) have been gaining significant attention from software engineering researchers since 2004. Several researchers have reported their experiences of and lessons learned from applying systematic reviews to different subject matters in software engineering. However, there has been no attempt at independently exploring experiences and perceptions of the practitioners of systematic reviews in order to gain an in-depth understanding of various aspects of systemic reviews as a new research methodology in software engineering. We assert that there is a need of evidence-based body of knowledge about the application of systematic reviews in software engineering. To address this need, we have started an empirical research program that aims to contribute to the growing body of knowledge about systematic reviews in software engineering. This paper reports the design, logistics, and results of the first phase empirical study carried out in this program. The results provide interesting insights into different aspects of systematic reviews based on the analysis of the data gathered from 17 interviewees with varying levels of knowledge of and experiences in systematic reviews. The findings from this study are expected to contribute to the existing knowledge about using systematic reviews and help further improve the state-of-the-practice of this research methodology in software engineering.",Conference paper,pro38
pap763,99fd6bd546a08f2a31f840c847b3669a5b458c3c,jou155,Information and Software Technology,Motivation in Software Engineering: A systematic literature review,,Conference paper,vol155
pap764,4bddb173c165b2cd33e255e427e29422b87d54de,jou3,IEEE Transactions on Knowledge and Data Engineering,Development of a Software Engineering Ontology for Multisite Software Development,"This paper aims to present an ontology model of software engineering to represent its knowledge. The fundamental knowledge relating to software engineering is well described in the textbook entitled Software Engineering by Sommerville that is now in its eighth edition (2004) and the white paper, Software Engineering Body of Knowledge (SWEBOK), by the IEEE (203) upon which software engineering ontology is based. This paper gives an analysis of what software engineering ontology is, what it consists of, and what it is used for in the form of usage example scenarios. The usage scenarios presented in this paper highlight the characteristics of the software engineering ontology. The software engineering ontology assists in defining information for the exchange of semantic project information and is used as a communication framework. Its users are software engineers sharing domain knowledge as well as instance knowledge of software engineering.",Article,vol3
pap765,eece42f4d4bb7143643baa66cf6132b1c9e4915d,con38,International Symposium on Empirical Software Engineering and Measurement,Context in industrial software engineering research,"In order to draw valid conclusions when aggregating evidence it is important to describe the context in which industrial studies were conducted. This paper structures the context for empirical industrial studies and provides a checklist. The aim is to aid researchers in making informed decisions concerning which parts of the context to include in the descriptions. Furthermore, descriptions of industrial studies were surveyed.",Article,pro38
pap766,30e741a0330cdcaf6a6466eaca2f09c8bd604b57,jou151,IEEE Transactions on Software Engineering,A survey of controlled experiments in software engineering,"The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.",Article,vol151
pap767,d300348e57198af44ede8f70e3eabe30eb013dac,jou160,IEEE Software,Knowledge management in software engineering,"Software organizations' main assets are not plants, buildings, or expensive machines. A software organization's main asset is its intellectual capital, as it is in sectors such asconsulting, law, investment banking, and advertising. The major problem with intellectual capital is that it has legs and walks home every day. At the same rate experience walks out the door, inexperience walks in the door. Whether or not many software organizations admit it, they face the challenge ofsustaining the level of competence needed to win contracts and fulfill undertakings.",Conference paper,vol160
pap768,a01c4fc02389cd3b8b442e124018fc2cabfedad9,con108,International Conference on Information Integration and Web-based Applications & Services,Using the inverted classroom to teach software engineering,"An inverted classroom is a teaching environment that mixes the use of technology with hands-on activities. In an inverted classroom, typical in-class lecture time is replaced with laboratory and in-class activities. Outside class time, lectures are delivered over some other medium such as video on-demand. In a three credit hour course for instance, contact hours are spent having students actively engaged in learning activities. Outside of class, students are focused on viewing 3-6 hours of lectures per week. Additional time outside of class is spent completing learning activities. In this paper we present the inverted classroom model in the context of a software engineering curriculum. The paper motivates the use of the inverted classroom and suggests how different courses from the Software Engineering 2004 Model Curriculum Volume can incorporate the use of the inverted classroom. In addition, we present the results of a pilot course that utilized the inverted classroom model at Miami University and describe courses that are currently in process of piloting its use.",Erratum,pro108
pap769,fbcffae04423ea97cb5463cd9d0794d780708746,jou160,IEEE Software,Trends in Embedded Software Engineering,"Software's importance in the development of embedded systems has been growing rapidly over the last 20 years. Because of current embedded systems' complexity, they require sophisticated engineering methods for systematically developing high-quality software. Embedded software development differs from IT system development in several ways. For example, IT systems developers can use standard hardware and software platforms and don't face the resource requirements that embedded systems developers must take into account. To meet embedded software's extrafunctional requirements, embedded systems development is shifting from programming to model-driven development. Another important trend is the emphasis on the quality assurance of safety-related systems.",Conference paper,vol160
pap770,79651cbd4dc289d7e052885c4f904125da2a6d8a,jou155,Information and Software Technology,Models of motivation in software engineering,,Letter,vol155
pap771,2c417261c7b8bf5f01603717b4e71b1a65d9c2ea,jou168,Springer US,Reverse Engineering,,Article,vol168
pap772,2ae398028f82555931c8c504886ec5857640c59e,jou173,Journal of Software,Context and Adaptivity in Pervasive Computing Environments: Links with Software Engineering and Ontological Engineering,"In this article we present a review of selected literature of context-aware pervasive computing while integrating theory and practice from various disciplines in order to construct a theoretical grounding and a technical follow-up path for our future research. This paper is not meant to provide an extensive review of the literature, but rather to integrate and extend fundamental and promising theoretical and technical aspects found in the literature. Our purpose is to use the constructed theory and practice in order to enable anywhere and anytime adaptive e-learning environments. We particularly elaborate on context, adaptivity, context-aware systems, ontologies and software development issues. Furthermore, we represent our view point for context-aware pervasive application development particularly based on higher abstraction where ontologies and semantic web activities, also web itself, are of crucial.",Letter,vol173
pap773,d6b65ff0ec1bc4ef4cb32bd2335f91efdd53ec77,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Challenges in Model-Driven Software Engineering,,Article,pro58
pap774,2d69014c0d601c202f0c33dde2f646e8f31183c5,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",The Future of Empirical Methods in Software Engineering Research,"We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.",Erratum,pro73
pap775,9cb85d24f7819281b286347e083c7fcf8fd2fdcb,con46,Software Product Lines Conference,The State of the Art in End-User Software Engineering,___ Most programs today are written not by professional software developers,Erratum,pro46
pap776,073c37ddc266657a9bc21ebad7abc97991237539,jou160,IEEE Software,The Rise and Evolution of Agile Software Development,"Agile software development has dominated the second half of the past 50 years of software engineering. Retrospectives, one of the most common agile practices, enables reflection on past performance, discussion of current progress, and charting forth directions for future improvement. Because of agile’s burgeoning popularity as the software development model of choice and a significant research subdomain of software engineering, it demands a retrospective of its own. This article provides a historical overview of agile’s main focus areas and a holistic synthesis of its trends, their evolution over the past two decades, agile’s current status, and, forecast from these, agile’s likely future. This article is part of a theme issue on software engineering’s 50th anniversary.",Article,vol160
pap777,dfbc169d20897be26a005a08c29da7a1df5cd163,con104,Biometrics and Identity Management,Reporting Experiments in Software Engineering,,Erratum,pro104
pap778,f7644baad4c2c9bf0a7a2b1107209d2b9495aaff,con59,Annual Workshop of the Psychology of Programming Interest Group,Using Mapping Studies in Software Engineering,"Background: A mapping study provides a systematic and objective procedure for identifying the nature and extent of the empirical study data that is available to answer a particular research question. Such studies can also form a useful preliminary step for PhD study. Aim: We set out to assess how effective such studies have been when used for software engineering topics, and to identify the specific challenges that they present. Method: We have conducted an informal review of a number of mapping studies in software engineering, describing their main characteristics and the forms of analysis employed. Results: We examine the experiences and outcomes from six mapping studies, of which four are published. From these we note a recurring theme about the problems of classification and a preponderance of ‘gaps’ in the set of empirical studies. Conclusions: We identify our challenges as improving classification guidelines, encouraging better reporting of primary studies, and argue for identifying some ’empirical grand challenges’ for software engineering as a focus for the community.",Article,pro59
pap779,00fc19647ca1aa9a3910abe7cf3414af2f811e64,con106,International Conference on Mobile Data Management,Survey of multi-objective optimization methods for engineering,,Erratum,pro106
pap780,4aa7b33d161d7fbb0ce4aad088cd664eae90d5a7,con110,Very Large Data Bases Conference,Thermodynamics : An Engineering Approach,"Basic concepts of thermodynamics properties of pure substances the first law of thermodynamics - closed systems, control volumes the second law of thermodynamics entropy - a measure of disorder energy - a measure of work potential gas power cycles vapour and combined power cycles refrigeration cycles thermodynamics property gas mixtures gas vapour mixtures and air conditioning chemical reactions chemical and phase equilibrium thermodynamics of high-speed fluid flow property tables and charts (SI units, English units) about the software.",Erratum,pro110
pap781,8c4702b08e7bd5b2e7582812ae754cc605bb6c3d,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Building Theories in Software Engineering,,Erratum,pro73
pap782,1edc7f14653fa2d89343b62bb7254297b107bff3,jou152,Empirical Software Engineering,The role of replications in Empirical Software Engineering,,Conference paper,vol152
pap783,4abd729f079a650b0ed14aa06f25b2f1f3611e32,con38,International Symposium on Empirical Software Engineering and Measurement,Strength of evidence in systematic reviews in software engineering,"Systematic reviews are only as good as the evidence they are based on. It is important, therefore, that users of systematic reviews know how much confidence they can place in the conclusions and recommendations arising from such reviews. In this paper we present an overview of some of the most influential systems for assessing the quality of individual primary studies and for grading the overall strength of a body of evidence. We also present an example of the use of such systems based on a systematic review of empirical studies of agile software development. Our findings suggest that the systems used in other disciplines for grading the strength of evidence for and reporting of systematic reviews, especially those that take account of qualitative and observational studies are of particular relevance for software engineering.",Letter,pro38
pap784,8aa6361fc9b7876cddb3bb103a419cd8443adf60,con60,European Conference on Software Process Improvement,A Software Engineering Lifecycle Standard for Very Small Enterprises,,Article,pro60
pap785,9af3e5ec703f44ce80d1e0271474419c12c72122,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Rationale-Based Software Engineering,,Erratum,pro58
pap786,294df07a2eaaa9eb39d3404c145ef429f84b56ae,con61,International Conference on Predictive Models in Software Engineering,Data sets and data quality in software engineering,"OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.",Conference paper,pro61
pap787,f96d53e1132476a86f8b5a4ed2c9cd07345c9da3,con69,Formal Concept Analysis,A survey of social software engineering,"Software engineering is a complex socio-technical activity, due to the need for discussing and sharing knowledge among team members. This has raised the need for effective ways of sharing ideas, knowledge, and artifacts among groups and their members. The social aspect of software engineering process also demands computer support to facilitate the development by means of collaborative tools, applications and environments. In this paper, we present a survey of relevant works from psychology, mathematics and computer science studies. The combination of these fields provides the required infrastructure for engineering social and collaborative applications as well as the software engineering process. We also discuss possible solutions for the encountered shortcomings, and how they can improve software development.",Erratum,pro69
pap788,ecb8f480e369346d899b6c2ec935bea898fa499a,con91,Symposium on the Theory of Computing,08031 -- Software Engineering for Self-Adaptive Systems: A Research Road Map,"Software's ability to adapt at run-time to changing user needs, system intrusions or faults, changing operational environment, and resource variability has been proposed as a means to cope with the complexity of today's software-intensive systems. Such self-adaptive systems can configure and reconfigure themselves, augment their functionality, continually optimize themselves, protect themselves, and recover themselves, while keeping most of their complexity hidden from the user and administrator. In this paper, we present research road map for software engineering of self-adaptive systems focusing on four views, which we identify as essential: requirements, modelling, engineering, and assurances.",Erratum,pro91
pap789,377219289cc085e1d3553f9c8d0ab85143ee4809,con2,International Conference on Software Engineering,Proceedings of the 30th international conference on Software engineering,"Welcome to the 30th International Conference on Software Engineering in Leipzig, Germany. On behalf of the entire organizing committee, we are very happy to welcome you to ICSE which returns to Germany for the third time after Munich (1978) and Berlin (1996). It is in fact a very timely return, because the term software engineering was coined exactly 40 years ago in Garmisch-Partenkirchen, a beautifully located tourist spot in the Bavarian Alps. This year's ICSE is celebrating the 40th anniversary by a number of special events. 
 
Certainly, one highlight of the conference will be a series of sessions commemorating the Garmisch event. We are very lucky that some of the Garmisch pioneers accepted our invitation to talk not only about their memories from the past but also share their view on the current state of software engineering with the audience and the currently active researchers. 
 
The theme of the conference -- ""Driving World Business"" -- underlines the fact that during these 40 years software has become a major driver of many if not all technological and business systems and applications. In contrast to previous ICSEs this year's conference has a special dedicated set of experience sessions focusing on special application areas, namely automotive, healthcare and telecommunications where software and sophisticated software engineering practices definitely play a major role in business development. This is complemented by a group of distinguished keynote speakers from different parts of the world who give an account of the state-of-the-practice and state-of-research in the different fields. Lori Clarke from the University of Massachusetts talks about formal verification approaches to improve safety in medical systems and applications. Sam Adams from IBM discusses aspects of end user programming to improve the effective use of these systems which are usually operated by non computer experts. Finally, Herbert Hanselmann from dSPACE, a major German automotive supplier in the car industry, building control units and test and simulation environments, focuses on the role and benefits of model based design. 
 
As usual ICSE, as the ACM/IEEE flagship conference in the field, offers a diverse program of research, education, and practice-oriented content that will engage Software Engineers from around the world. The three day core of the meeting is anchored by the research paper track. The research paper track received 370 submissions, and after a very rigorous and thorough review process, the program committee selected 56 for inclusion in the program. The other tracks at ICSE cover Software Engineering education, provide an opportunity to see formal demonstrations of Software Engineering tools, and hear about experiences in applying Software Engineering methods in an industrial context. Each of these tracks had an independent organizing and program committee. Similarly the theme specific experience paper tracks were organized by distinguished experts from both academia and industry to make sure that experience and research are equally well reflected in the program. Surrounding the core program, ICSE 2008 offers a diverse collection of 25 workshops, 16 tutorials, and 4 co-located meetings. 
 
In addition to a rich technical program at ICSE, we hope you take the time to enjoy Leipzig and the conference social program. ICSE attendees will enjoy a reception in the world-renowned Gewandhaus and a performance of the legendary Gewandhaus Orchestra.",Conference paper,pro2
pap790,ac2235b6e3af4a74670c3d31f922692784bd7063,jou152,Empirical Software Engineering,Evaluating guidelines for reporting empirical software engineering studies,,Conference paper,vol152
pap791,afdfec4b9a0a65ac246b96f747b8aa07dda642c1,jou151,IEEE Transactions on Software Engineering,Problem Oriented Software Engineering: Solving the Package Router Control Problem,"Problem orientation is gaining interest as a way of approaching the development of software intensive systems, and yet, a significant example that explores its use is missing from the literature. In this paper, we present the basic elements of Problem Oriented Software Engineering (POSE), which aims at bringing both nonformal and formal aspects of software development together in a single framework. We provide an example of a detailed and systematic POSE development of a software problem: that of designing the controller for a package router. The problem is drawn from the literature, but the analysis presented here is new. The aim of the example is twofold: to illustrate the main aspects of POSE and how it supports software engineering design and to demonstrate how a nontrivial problem can be dealt with by the approach.",Letter,vol151
pap792,a9b42c3b3a22861ebd7289621b9e36be1b31699c,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Replication's Role in Software Engineering,,Erratum,pro47
pap793,fe627947c9f0467e9e06fedb6b35ee8dc77c17c8,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Towards individualized software engineering: empirical studies should collect psychometrics,"Even though software is developed by humans, research in software engineering primarily focuses on the technologies, methods and processes they use while disregarding the importance of the humans themselves. In this paper we argue that most studies in software engineering should give much more weight to human factors. In particular empirical software engineering studies involving human developers should always consider collecting psychometric data on the humans involved. We focus on personality as one important psychometric factor and present initial results from an empirical study investigating correlations between personality and attitudes to software engineering processes and tools. We discuss what are currently hindering a more wide-spread use of psychometrics and how overcoming these hurdles could lead to a more individualized software engineering.",Letter,pro49
pap794,57317f7b349f1d02153e54603e3988376ef4724f,jou163,IEEE Annals of the History of Computing,A Brief History of Software Engineering,"This personal perspective on the art of programming begins with a look at the state of programming from about 1960, and it follows programming's development through the present day. The article examines key contributions to the field of software engineering and identifies major obstacles, which persist even today.",Letter,vol163
pap795,60cef66236eee4cc7c124abfa04d27cbc379362f,con60,European Conference on Software Process Improvement,Missing Data in Software Engineering,,Erratum,pro60
pap796,2f9f3311be9d707bdcb0eb2d995d711f9138e614,con1,International Conference on Human Factors in Computing Systems,Metamodelling for software engineering,"Interestingly, metamodelling for software engineering that you really wait for now is coming. It's significant to wait for the representative and beneficial books to read. Every book that is provided in better way and utterance will be expected by many peoples. Even you are a good reader or not, feeling to read this book will always appear when you find it. But, when you feel hard to find it as yours, what to do? Borrow to your friends and don't know when to give back it to her or him.",Erratum,pro1
pap797,fb927d0f1148c23c86fbf0daa64650b1b1014efa,con72,Bioinformatics and Computational Biology,The software model checker B last : Applications to software engineering,,Erratum,pro72
pap798,f4df7f8cd5e5a2f00010856eaa8e5a2223b68203,con2,International Conference on Software Engineering,A view of 20th and 21st century software engineering,"George Santayana's statement, ""Those who cannot remember the past are condemned to repeat it,"" is only half true. The past also includes successful histories. If you haven't been made aware of them, you're often condemned not to repeat their successes.In a rapidly expanding field such as software engineering, this happens a lot. Extensive studies of many software projects such as the Standish Reports offer convincing evidence that many projects fail to repeat past successes.This paper tries to identify at least some of the major past software experiences that were well worth repeating, and some that were not. It also tries to identify underlying phenomena influencing the evolution of software engineering practices that have at least helped the author appreciate how our field has gotten to where it has been and where it is.A counterpart Santayana-like statement about the past and future might say, ""In an era of rapid change, those who repeat the past are condemned to a bleak future."" (Think about the dinosaurs, and think carefully about software engineering maturity models that emphasize repeatability.)This paper also tries to identify some of the major sources of change that will affect software engineering practices in the next couple of decades, and identifies some strategies for assessing and adapting to these sources of change. It also makes some first steps towards distinguishing relatively timeless software engineering principles that are risky not to repeat, and conditions of change under which aging practices will become increasingly risky to repeat.",Letter,pro2
pap799,95a48bcb1e7c5c6875f7ce30b7bcbdf215c3df1a,con37,International Symposium on Search Based Software Engineering,Software Engineering for Automotive Systems: A Roadmap,"The first pieces of software were introduced into cars in 1976. By 2010, premium class vehicles are expected to contain one gigabyte of on-board software. We present research challenges in the domain of automotive software engineering.",Erratum,pro37
pap800,daebea219735324da56cb5c181aee67193c7ea55,con62,Australian Software Engineering Conference,Ontology-Based Software Engineering- Software Engineering 2.0,"This paper describes the use of ontologies in different aspects of software engineering. This use of ontologies varies from support for software developers at multiple sites to the use of an ontology to provide semantics in different categories of software, particularly on the Web. The world's first and only software engineering ontology and a project management ontology in conjunction with a domain ontology are used to provide support for software development that is taking place at multiple sites. Ontologies are used to provide semantics to deal with heterogeneity in the representation of multiple information sources, enable the selection and composition of web services and grid resources, provide the shared knowledge base for multiagent systems, provide semantics and structure for trust and reputation systems and privacy based systems and codification of shared knowledge within different domains in business, science, manufacturing, engineering and utilities. They, therefore, bring a new paradigm to software engineering through the use of semantics as a central mechanism which will revolutionize the way software is developed and consumed in the future leading to the development of software as a service bringing about the dawn of software engineering 2.0.",Conference paper,pro62
pap801,f7cdedbebc51c913f7a7b566bc1eac296df296ad,con19,International Conference on Conceptual Structures,Challenges in automotive software engineering,"Developing and integrating automotive embedded software is a complex undertaking. The software is large. It is developed by many contributors. It is distributed over many control units connected by a variety of in-vehicle buses. Often much of the equipment or functions in a car are optional and regulatory requirements also vary between markets, leading to large combinatorial variations of software features. Targets running the software have to be cheap. Errors can be extremely expensive. New software and system features are demanded by the market and also by governmental regulations. Model-based design (MBD) of functional behaviour has been a big help in the recent past on the one hand, and on the other hand has by itself created new complexity by allowing relatively quick development of ever more features, especially when combined with autocoding. All this creates new challenges that did not exist a few years ago when feature development was slow. Major new challenges now are to tame all the complexity, get a system view on top of the individual functions, and to leverage executable system models to put more comprehensive testing into early phases of a development. Tools are required which really help those engineers and software developers. Their needs may not ask for a lot of computer science glamour. They can be quite basic and sophisticated concepts from computer science may find it difficult to find acceptance outside some niches. This presentation will outline the achievements, the current challenges and will point to upcoming tools and approaches that help meeting those challenges.",Erratum,pro19
pap802,dd153ebd44a07dde2259c22d43bb9cd18db44d2a,con64,British Computer Society Conference on Human-Computer Interaction,Modelling and Simulation in Materials Science and Engineering Visualization and analysis of atomistic simulation data with OVITO – the Open Visualization Tool,"The Open Visualization Tool (OVITO) is a new 3D visualization software designed for post-processing atomistic data obtained from molecular dynamics or Monte Carlo simulations. Unique analysis, editing and animations functions are integrated into its easy-to-use graphical user interface. The software is written in object-oriented C++, controllable via Python scripts and easily extendable through a plug-in interface. It is distributed as open-source software and can be downloaded from the website http://ovito.sourceforge.net/. (Some figures in this article are in colour only in the electronic version)",Erratum,pro64
pap803,3efa8779c215c6d8131d34ca0f9dc9d08e8665ba,con105,British Machine Vision Conference,Collaboration in Software Engineering: A Roadmap,"Software engineering projects are inherently cooperative, requiring many software engineers to coordinate their efforts to produce a large software system. Integral to this effort is developing shared understanding surrounding multiple artifacts, each artifact embodying its own model, over the entire development process. This focus on model- oriented collaboration embedded within a larger process is what distinguishes collaboration research in software engineering from broader collaboration research, which tends to address artifact-neutral coordination technologies and toolkits. This article first presents a list of goals for software engineering collaboration, then surveys existing collaboration support tools in software engineering. The survey covers both tools that focus on a single artifact or stage in the development process (requirements support tools, UML collaboration tools), and tools that support the representation and execution of an entire software process. Important collaboration standards are also described. Several possible future directions for collaboration in software engineering are presented, including tight integration between web and desktop development environments, broader participation by customers and end users in the entire development process, capturing argumentation surrounding design rationale, and use of massively multiplayer online (MMO) game technology as a collaboration medium. The article concludes by noting a problem in performing research on collaborative systems, that of assessing how well certain artifacts, models, and embedded processes work, and whether they are better than other approaches.",Erratum,pro105
pap804,f038d65d3971c4be5529aadd7d89d5529cef54bf,con108,International Conference on Information Integration and Web-based Applications & Services,Software Engineering Foundations: A Software Science Perspective,"To deal with the difficulties inherent in large-scale software development, the foundations of software engineering are yet to be explored. This comprehensive text is the first book to cover the theoretical and empirical foundations of software engineering. It provides a framework of software engineering methodologies and covers a wide range of foundations such as philosophy, informatics, and engineering economics. Self-contained and requiring only basic programming experience, this book is filled with in-depth comments, annotated references, real-world problems and heuristic questions. Software Engineering Foundations is an important book for software engineers and students alike.",Erratum,pro108
pap805,421306786efd93f82a69f9acacc44a4863a86aba,jou151,IEEE Transactions on Software Engineering,A Systematic Review of Theory Use in Software Engineering Experiments,"Empirically based theories are generally perceived as foundational to science. However, in many disciplines, the nature, role and even the necessity of theories remain matters for debate, particularly in young or practical disciplines such as software engineering. This article reports a systematic review of the explicit use of theory in a comprehensive set of 103 articles reporting experiments, from of a total of 5,453 articles published in major software engineering journals and conferences in the decade 1993-2002. Of the 103 articles, 24 use a total of 40 theories in various ways to explain the cause-effect relationship(s) under investigation. The majority of these use theory in the experimental design to justify research questions and hypotheses, some use theory to provide post hoc explanations of their results, and a few test or modify theory. A third of the theories are proposed by authors of the reviewed articles. The interdisciplinary nature of the theories used is greater than that of research in software engineering in general. We found that theory use and awareness of theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering. Several articles comment explicitly on the lack of relevant theory. We call for an increased awareness of the potential benefits of involving theory, when feasible. To support software engineering researchers who wish to use theory, we show which of the reviewed articles on which topics use which theories for what purposes, as well as details of the theories' characteristics",Letter,vol151
pap806,56cdab6279de1c69907c38f29cb603ee87be3950,con63,International Colloquium on Theoretical Aspects of Computing,Component-Based Software Engineering,,Article,pro63
pap807,658a89f31cd662e78aa0c4236b090894f07c0ba2,jou155,Information and Software Technology,A systematic review of effect size in software engineering experiments,,Letter,vol155
pap808,7f13651629224af4959a6e3ac888e26cdbbce6ca,jou35,British Journal of Educational Technology,An application of games-based learning within software engineering,"For some time now, computer games have played an important role in both children and adults' leisure activities. While there has been much written on the negative aspects of computer games, it has also been recognised that they have potential advantages and benefits. There is no doubt that computer games are highly engaging and incorporate features that are extremely compelling. It is these highly engaging features of computer games that have attracted the interests of educationalists. The use of games-based learning has been growing for many years now; however, within software engineering, there is still a dearth of empirical evidence to support this approach. In this paper, we examine the literature on the use of computer games to teach software engineering concepts and describe a computer game we have been developing to teach these concepts.",Article,vol35
pap809,932365acebbedd0be8da6245d537f031a38e7523,con102,Annual Haifa Experimental Systems Conference,The Focus Group Method as an Empirical Tool in Software Engineering,,Erratum,pro102
pap810,2a511f103dc93dd383d4630f0eccbefd24083272,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Software engineering and formal methods,The answer to software reliability concerns may lie in formal methods.,Erratum,pro85
pap811,ded5e02b541000ab011e2b9df0fdbc0655a4939a,con34,International Conference on Agile Software Development,Value-Based Software Engineering,"Reading is a hobby to open the knowledge windows. Besides, it can provide the inspiration and spirit to face this life. By this way, concomitant with the technology development, many companies serve the e-book or book in soft file. The system of this book of course will be much easier. No worry to forget bringing the value based software engineering book. You can open the device and get the book by on-line.",Erratum,pro34
pap812,53e0cea00e0653faba15d91f9b5673576af65967,con22,Grid Computing Environments,Software Engineering Data Collection for Field Studies,,Erratum,pro22
pap813,d4b0b86296dd492c870f144b32fda50aaa059f82,jou156,Journal of Systems and Software,Software Engineering Using RATionale,,Conference paper,vol156
pap814,4e6f0d57c71f61062fd2a7beaf04cfde5ebe47a1,jou152,Empirical Software Engineering,The role of replications in empirical software engineering—a word of warning,,Conference paper,vol152
pap815,807abec1d67585e64efa2c4a097325ceee69e2c1,con77,International Conference on Artificial Neural Networks,"Empirical Software Engineering Issues. Critical Assessment and Future Directions, International Workshop, Dagstuhl Castle, Germany, June 26-30, 2006. Revised Papers",,Erratum,pro77
pap816,e39bdae014acddf3c9dd47a24ddee0bfbec0ad7d,con40,Conference on Software Engineering Education and Training,Comprehensive Evaluation of an Educational Software Engineering Simulation Environment,"Software engineering educational approaches are often evaluated only anecdotally, or in informal pilot studies. We describe a more comprehensive approach to evaluating a software engineering educational technique (SimSE, a graphical, interactive, customizable, game-based software engineering simulation environment). Our method for evaluating SimSE went above and beyond anecdotal experience and approached evaluation from a number of different angles through a family of studies designed to assess SimSE's effectiveness and guide its development. In this paper, we demonstrate the insights and lessons that can be gained when using such a multi-angled evaluation approach. Our hope is that, from this paper, educators will: (1) learn ideas about how to more comprehensively evaluate their own approaches, and (2) be provided with evidence about the educational effectiveness of SimSE.",Article,pro40
pap817,c4740faa4eb65276f82ddb3ad2734588b1ca3721,con40,Conference on Software Engineering Education and Training,Increased Retention of Early Computer Science and Software Engineering Students Using Pair Programming,"An important problem faced by many Computer Science and Software Engineering programs is declining enrollment. In an effort to reverse that trend at Mississippi State University, we have instituted pair programming for the laboratory exercises in the introductory programming course. This paper describes a study performed to analyze whether using pair programming would increase retention. An important goal of this study was not only to measure increased retention, but to provide insight into why retention increased or decreased. The results of the study showed that retention significantly increased for those students already majoring in Computer Science, Software Engineering, or Computer Engineering. In addition, survey results indicated that the students viewed many aspects of pair programming to be very beneficial to their learning experience.",Article,pro40
pap818,da93f65bcceb9ebbd31d41f10167a7780e1fc901,jou160,IEEE Software,A Software Chasm: Software Engineering and Scientific Computing,"Some time ago, a chasm opened between the scientific-computing community and the software engineering community. Originally, computing meant scientific computing. Today, science and engineering applications are at the heart of software systems such as environmental monitoring systems, rocket guidance systems, safety studies for nuclear stations, and fuel injection systems. Failures of such health-, mission-, or safety-related systems have served as examples to promote the use of software engineering best practices. Yet, the bulk of the software engineering community's research is on anything but scientific-application software. This chasm has many possible causes. In this article, we look at the impact of one particular contributor in industry.",Conference paper,vol160
pap819,558d8f66074cc791b5a6504354ac1280e3a2ebf1,con2,International Conference on Software Engineering,Mining Software Engineering Data,"Software engineering data (such as code bases, exe- cution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project¿s status, progress, and evolution. Using well- established data mining techniques, practitioners and re- searchers can explore the potential of this valuable data in order to better manage their projects and to produce higher-quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining Soft- ware Engineering (SE) data, discusses challenges associ- ated with mining SE data, highlights SE data mining suc- cess stories, and outlines future research directions. Partic- ipants will acquire knowledge and skills needed to perform research or conduct practice in the field and to integrate data mining techniques in their own research or practice.",Article,pro2
pap820,3cecc99827849f1f991a4dd9d3abbdd8f63f8aa9,con2,International Conference on Software Engineering,Good Practices for Educational Software Engineering Projects,"Recent publications indicate the importance of software engineering in the computer science curriculum. In this paper, we present the final part of software engineering education at University of Groningen in the Netherlands and Vaxjo University in Sweden, where student teams perform an industrial software development project. It furthermore presents the main educational problems encountered in such real-life projects and explains how this international course addresses these problems. The main contribution of this paper is a set of seven good practices for project based software engineering education.",Letter,pro2
pap821,c4d1376a5652a4c06e88287b38f4e40469b8743f,con26,Decision Support Systems,The Future of Software Engineering,,Erratum,pro26
pap822,bb83408e1e4116a37bb5b14a045b0bb49b02145d,con64,British Computer Society Conference on Human-Computer Interaction,Software Design and Architecture The once and future focus of software engineering,"The design of software has been a focus of software engineering research since the field's beginning. This paper explores key aspects of this research focus and shows why design will remain a principal focus. The intrinsic elements of software design, both process and product, are discussed: concept formation, use of experience, and means for representation, reasoning, and directing the design activity. Design is presented as being an activity engaged by a wide range of stakeholders, acting throughout most of a system's lifecycle, making a set of key choices which constitute the application's architecture. Directions for design research are outlined, including: (a) drawing lessons, inspiration, and techniques from design fields outside of computer science, (b) emphasizing the design of application ""character"" (functionality and style) as well as the application's structure, and (c) expanding the notion of software to encompass the design of additional kinds of intangible complex artifacts.",Erratum,pro64
pap823,cd888f88f8227ceebe1a551de4fc9cdf6bee6e54,con64,British Computer Society Conference on Human-Computer Interaction,Agile human-centered software engineering,"We seek to close the gap between software engineering (SE) and human-computer interaction (HCI) by indicating interdisciplinary interfaces throughout the different phases of SE and HCI lifecycles. As agile representatives of SE, Extreme Programming (XP) and Agile Modeling (AM) contribute helpful principles and practices for a common engineering approach. We present a cross-discipline user interface design lifecycle that integrates SE and HCI under the umbrella of agile development. Melting IT budgets, pressure of time and the demand to build better software in less time must be supported by traveling as light as possible. We did, therefore, choose not just to mediate both disciplines. Following our surveys, a rather radical approach best fits the demands of engineering organizations.",Article,pro64
pap824,f91d47bbd10afc8fff7bc73b0fa5ce63dc975e1c,con101,International Conference on Biometrics,Enhancing software engineering education using teaching aids in 3-D online virtual worlds,"Three-dimensional online virtual worlds such as second life support avatar-based communications, a wide spectrum of online activities, and development of various in-world teaching and learning tools. We have experimented with second life in two computer science classes, one at Ohio University, the other at the University of Mary Washington, to enhance software engineering education. We used Second Life as an innovative collaboration and communication tool both in and outside classroom to help facilitate teamwork and interactions among student project team members. Second Life was also used as the virtual office for instructors and teaching assistants to answer students' questions during office hours. In addition, we developed two multi-player online software engineering educational games in second life, one based on the Groupthink software specification exercise developed at M.I.T., and the other based on the SimSE game (a 2-D single player game) developed at UC Irvine. By playing these two games, students learned fundamentals of software specification activities and principles of software development processes. In the paper, we will share our experience of using second life in two software engineering classes, and discuss its pros and cons based on the data collected from student surveys.",Erratum,pro101
pap825,dca4ed6eb0c45fc33fd73af15b52c736351490a4,con65,IEEE International Conference on Software Engineering and Formal Methods,Problem Oriented Software Engineering: A design-theoretic framework for software engineering,"A key challenge for software engineering is to learn how to reconcile the formal world of the machine and its software with the non-formal real world. In this paper, we discuss elements of problem oriented software engineering (POSE), an approach that brings both non- formal and formal aspects of software development together in a single theoretical framework for software engineering design. POSE presents development as the representation and step-wise transformation of software problems. It allows for the identification and clarification of system requirements, the understanding and structuring of the problem world, the structuring and specification of a hardware/software machine that can ensure satisfaction of the requirements in the problem world, and the construction of adequacy arguments, convincing both to developers and to customers, users and other interested parties, that the system will provide what is needed. Examples are used throughout the paper to illustrate how formal and non-formal descriptions are reconciled under POSE.",Conference paper,pro65
pap826,f88ce0c94f9e7ce8e126e250c91c93332238c37f,con79,IEEE Annual Symposium on Foundations of Computer Science,What Every Engineer Should Know about Software Engineering,"THE PROFESSION OF SOFTWARE ENGINEERING Introduction Software Engineering as an Engineering Profession Standards and Certifications Misconceptions about Software Engineering Further Reading SOFTWARE PROPERTIES, PROCESSES, AND STANDARDS Introduction Characteristics of Software Software Processes and Methodologies Software Standards Further Reading SOFTWARE REQUIREMENTS SPECIFICATION Introduction Requirements Engineering Concepts Requirements Specifications Requirements Elicitation Requirements Modeling Requirements Documentation Recommendations on Requirements Further Reading DESIGNING SOFTWARE Introduction Software Design Concepts Software Design Modeling Pattern-Based Design Design Documentation Further Reading BUILDING SOFTWARE Introduction Programming Languages Software Construction Tools Becoming a Better Code Developer Further Reading SOFTWARE QUALITY ASSURANCE Introduction Quality Models and Standards Software Testing Metrics Fault Tolerance Maintenance and Reusability Further Reading MANAGING SOFTWARE PROJECTS AND SOFTWARE ENGINEERS Introduction Software Engineers Are People Too Project Management Basics Tracking and Reporting Progress Software Cost Estimation Project Cost Justification Risk Management Further Reading THE FUTURE OF SOFTWARE ENGINEERING Introduction Open Source Outsourcing and Offshoring Global Software Development Further Reading APPENDIX A: SOFTWARE REQUIREMENTS FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM (REV. 01.01.00) Introduction Overall Description Specific Requirements References APPENDIX B: SOFTWARE DESIGN FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM (REV. 01.01.00) Introduction Overall Description Design Decomposition References APPENDIX C: OBJECT MODELS FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM INDEX",Erratum,pro79
pap827,40f08c6412a287e6708496d8ae1a10c2f9e72a21,jou160,IEEE Software,Evidence-Based Software Engineering for Practitioners,"Software managers and practitioners often must make decisions about what technologies to employ on their projects. They might be aware of problems with their current development practices (for example, production bottlenecks or numerous defect reports from customers) and want to resolve them. Or, they might have read about a new technology and want to take advantage of its promised benefits. However, practitioners can have difficulty making informed decisions about whether to adopt a new technology because there's little objective evidence to confirm its suitability, limits, qualities, costs, and inherent risks. This can lead to poor decisions about technology adoption. Software engineers might make incorrect decisions about adopting new techniques it they don't consider scientific evidence about the techniques' efficacy. They should consider using procedures similar to ones developed for evidence-based medicine. Software companies are often under pressure to adopt immature technologies because of market and management pressures. We suggest that practitioners consider evidence-based software engineering as a mechanism to support and improve their technology adoption decisions.",Conference paper,vol160
pap828,b5527bcf459fb62f280a969ce9c75fc743411f10,con16,International Conference on Data Science and Advanced Analytics,Systematic Review in Software Engineering,,Erratum,pro16
pap829,286495f04fbea22693e45957186c919e6e09b8cc,con9,Big Data,Writing good software engineering research papers,"Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to XSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.",Erratum,pro9
pap830,38e26d517cc5a1e609a90391d35c3f25ee84089f,con25,IEEE International Parallel and Distributed Processing Symposium,Value-based software engineering: reinventing,"The Value-Based Software Engineering (VBSE) agenda described in the preceding article has the objectives of integrating value considerations into current and emerging software engineering principles and practices, and of developing an overall framework in which they compatibly reinforce each other. In this paper, we provide a case study illustrating some of the key VBSE practices, and focusing on a particular anomaly in the monitoring and control area: the ""Earned Value Management System."" This is a most useful technique for monitoring and controlling the cost, schedule, and progress of a complex project. But it has absolutely nothing to say about the stakeholder value of the system being developed. The paper introduces an example order-processing software project, and shows how the use of Benefits Realization Analysis, stake-holder value proposition elicitation and reconciliation, and business case analysis provides a framework for stakeholder-earned-value monitoring and control.",Erratum,pro25
pap831,0cbfd89fa22878037f75612200b2417b47110da6,con11,European Conference on Modelling and Simulation,"Software engineering, 8th Edition",,Erratum,pro11
pap832,d966e2d2c2fcbacfb6d67964aa999e3efceff204,jou174,Advanced Engineering Informatics,Scientific research ontology to support systematic review in software engineering,,Letter,vol174
pap833,071e7c5701b73563117316880df4a31fa141f9ab,con60,European Conference on Software Process Improvement,Software Engineering with Reusable Components,,Erratum,pro60
pap834,fb170f5d7a9fe71b2f47ba6ad969ef410e402943,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Software Engineering (7th Edition),,Erratum,pro42
pap835,c575eb25feb0f06d2702fcf7751f6b4f61b892ee,con2,International Conference on Software Engineering,The Emerging Role of Data Scientists on Software Development Teams,"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.",Conference paper,pro2
pap836,4c6b43f5c68b82bdb814312b019af561eac5c6bc,con41,Asia-Pacific Software Engineering Conference,A Perspective on the Future of Middleware-based Software Engineering,"Middleware is a software layer that stands between the networked operating system and the application and provides well known reusable solutions to frequently encountered problems like heterogeneity, interoperability, security, dependability. Further, with networks becoming increasingly pervasive, middleware appears as a major building block for the development of future software systems. Starting with the impact of pervasive networking on computing models, manifested by now common grid and ubiquitous computing, this paper surveys related challenges for the middleware and related impact on the software development. Indeed, future applications will need to cope with advanced non-functional properties such as context- awareness and mobility, for which adequate middleware support must be devised together with accompanying software development notations, methods and tools. This leads us to introduce our view on next generation middleware, considering both technological advances in the networking area but also the need for closer integration with software engineering best practices, to ultimately suggest middleware-based software processes.",Erratum,pro41
pap837,57a2f619fd822762df18f545dc20aab867e08a81,con2,International Conference on Software Engineering,Proceedings of the 25th International Conference on Software Engineering,"Welcome to the silver anniversary of the International Conference on Software Engineering! Managing the complexity of modern software systems is without question a grand challenge. It is therefore fitting that, inspired by the view of Mount Hood in Portland, Oregon, the theme for this year's meeting is: Scaling New Heights.ICSE, the premier conference for software engineering, brings together world leaders in software engineering research, practice, and education to present and discuss the most recent advances, trends, and concerns in this ever expanding and critical field. This year's Software Engineering Week, May 3 to 10, 2003, consists of the main ICSE conference and an interesting array of tutorials, workshops, and related events associated or co-located with ICSE.",Letter,pro2
pap838,37b7f67b3a71d144ea1f0295a8899f75146255a5,con2,International Conference on Software Engineering,Software engineering for security: a roadmap,"Is there such a thing anymore as a software system that doesn’t need to be secure? Almost every softwarecontrolled system faces threats from potential adversaries, from Internet-aware client applications running on PCs, to complex telecommunications and power systems accessible over the Internet, to commodity software with copy protection mechanisms. Software engineers must be cognizant of these threats and engineer systems with credible defenses, while still delivering value to customers. In this paper, we present our perspectives on the research issues that arise in the interactions between software engineering and security.",Letter,pro2
pap839,a646c876e9bec9923bb374e886c89c458b248f8f,con96,Interspeech,Abstract Research in software engineering: an analysis of the literature,"In this paper, we examine the state of software engineering (SE) research from the point of view of the following research questions: 1. What topics do SE researchers address? 2. What research approaches do SE researchers use? 3. What research methods do SE researchers use? 4. On what reference disciplines does SE research depend? 5. At what levels of analysis do SE researchers conduct research? To answer those questions, we examined 369 papers in six leading research journals in the SE ®eld, answering those research questions for each paper. From that examination, we conclude that SE research is diverse regarding topic, narrow regarding research approach and method, inwardly-focused regarding reference discipline, and technically focused (as opposed to behaviorally focused) regarding level of analysis. We pass no judgment on the SE ®eld as a result of these ®ndings. Instead, we present them as groundwork for future SE research efforts. q 2002 Published by Elsevier Science B.V.",Erratum,pro96
pap840,11e2c3bfe1dd68446180f17e476addc947dad095,con56,International Conference on Software Engineering and Knowledge Engineering,Applications of Ontologies in Software Engineering,"The emerging field of semantic web technologies promises new stimulus for Software Engineering research. However, since the underlying concepts of the semantic web have a long tradition in the knowledge engineering field, it is sometimes hard for software engineers to overlook the variety of ontology-enabled approaches to Software Engineering. In this paper we therefore present some examples of ontology applications throughout the Software Engineering lifecycle. We discuss the advantages of ontologies in each case and provide a framework for classifying the usage of ontologies in Software Engineering.",Erratum,pro56
pap841,bc77f5407e5dcd818d7d51c5b059735c4939699e,con43,IEEE International Conference on Software Maintenance and Evolution,Software engineering: a practitioner's approach (2nd ed.),,Erratum,pro43
pap842,f73d04c6def6327cba25959f1bd8fca9f45feb93,con2,International Conference on Software Engineering,Challenges in automotive software engineering,"The amount of software in cars grows exponentially. Driving forces of this development are cheaper and more powerful hardware and the demand for innovations by new functions. The rapid increase of software and software based functionality brings various challenges (see [21], [23], [25], [26]) for the automotive industries, for their organization, key competencies, processes, methods, tools, models, product structures, division of work, logistics, maintenance, and long term strategies. From a software engineering perspective, the automotive industry is an ideal and fascinating application domain for advanced techniques. Although the automotive industry may adopt general results and solutions from the software engineering body of knowledge gained in other domains, the specific constraints and domain specific requirements in the automotive industry ask for individual solutions and bring various challenges for automotive software engineering. In cars we find literally all interesting problems and challenging issues of software and systems engineering.",Article,pro2
pap843,aad9c139227cc970646486109f7b8485a60d5c07,con15,Pacific Symposium on Biocomputing,Rationale Management in Software Engineering,,Erratum,pro15
pap844,7e63e256311e956093f9bea27456bc4e7206325a,con46,Software Product Lines Conference,Software Engineering Metrics: What Do They Measure and How Do We Know?,"Construct validity is about the question, how we know that we're measuring the attribute that we think we're measuring? This is discussed in formal, theoretical ways in the computing literature (in terms of the representational theory of measurement) but rarely in simpler ways that foster application by practitioners. Construct validity starts with a thorough analysis of the construct, the attribute we are attempting to measure. In the IEEE Standard 1061, direct measures need not be validated. ""Direct"" measurement of an attribute involves a metric that depends only on the value of the attribute, but few or no software engineering attributes or tasks are so simple that measures of them can be direct. Thus, all metrics should be validated. The paper continues with a framework for evaluating proposed metrics, and applies it to two uses of bug counts. Bug counts capture only a small part of the meaning of the attributes they are being used to measure. Multidimensional analyses of attributes appear promising as a means of capturing the quality of the attribute in question. Analysis fragments run throughout the paper, illustrating the breakdown of an attribute or task of interest into sub-attributes for grouped study.",Erratum,pro46
pap845,4c4442001e000d1afaba8e50181e3fc9fa68f88d,con90,Computer Vision and Pattern Recognition,Object-oriented and classical software engineering,"From the Publisher: 
Classical and Object-Oriented Software Engineering is designed for an introductory software engineering course. This book provides an excellent introduction to software engineering fundamentals,covering both traditional and object-oriented techniques. 
Schach's unique organization and style makes it excellent for use in a classroom setting. It presents the underlying software engineering theory in Part I and follows it up with the more practical life-cycle material in Part II. Many software engineering books are more like reference books,which do not provide the appropriate fundamentals before inundating students with implementation details. 
In this edition,more practical material has been added to help students understand how to use what they are learning. This has been done through the use of ""How To"" boxes and greater implementation detail in the case study. Additionally,the new edition contains the references to the most current literature and includes an overview of extreme programmming. 
The website in this edition will be more extensive. It will include Solutions,PowerPoints that incorporate lecture notes,newly developed self-quiz questions,and source code for the term project and case study.",Erratum,pro90
pap846,e88f940ed5f93c698689b87f2f147b8f218a3cf8,con66,International Conference on Software Reuse,Concepts and Guidelines of Feature Modeling for Product Line Software Engineering,,Conference paper,pro66
pap847,bd24420218cac6f5039c7a40189568ff4fae6048,jou175,Journal of the American Society for Information Science,Software Engineering as Seen through Its Research Literature: A Study in Co-Word Analysis,"This empirical research demonstrates the effectiveness of content analysis to map the research literature of the software engineering discipline. The results suggest that certain research themes in software engineering have remained constant, but with changing thrusts. Other themes have arisen, matured, and then faded as major research topics, while still others seem transient or immature. Co-word analysis is the specific technique used. This methodology identifies associations among publication descriptors (indexing terms) from the ACM Computing Classification System and produces networks of descriptors that reveal these underlying patterns. This methodology is applicable to other domains with a supporting corpus of textual data. While this study utilizes index terms from a fixed taxonomy, that restriction is not inherent; the descriptors can be generated from the corpus. Hence, co-word analysis and the supporting software tools employed here can provide unique insights into any discipline's evolution.",Letter,vol175
pap848,690e1d57b9c5ca3e7e348fcad257767a4a2c1011,con67,IEEE International Software Metrics Symposium,Experiences from conducting semi-structured interviews in empirical software engineering research,"Many phenomena related to software development are qualitative in nature. Relevant measures of such phenomena are often collected using semi-structured interviews. Such interviews involve high costs, and the quality of the collected data is related to how the interviews are conducted. Careful planning and conducting of the interviews are therefore necessary, and experiences from interview studies in software engineering should consequently be collected and analyzed to provide advice to other researchers. We have brought together experiences from 12 software engineering studies, in which a total of 280 interviews were conducted. Four areas were particularly challenging when planning and conducting these interviews; estimating the necessary effort, ensuring that the interviewer had the needed skills, ensuring good interaction between interviewer and interviewees, and using the appropriate tools and project artifacts. The paper gives advice on how to handle these areas and suggests what information about the interviews should be included when reporting studies where interviews have been used in data collection. Knowledge from other disciplines is included. By sharing experience, knowledge about the accomplishments of software engineering interviews is increased and hence, measures of high quality can be achieved",Article,pro67
pap849,5c0cc40b9886c5ae8f6e09b9778bbeacfbf3ed2e,con110,Very Large Data Bases Conference,Software engineering: Theory and practice,A book that describes and applies Software Engineering methods according to the state of the art of the subject matter; it is integrated with a proper exemplification and evolves as each topic is introduced. The material is designed for undergraduates of the initial course of Software Engineering and/or professionals who wish to improve their techniques for the development of software systems.,Erratum,pro110
pap850,b2f8ea4b7195153124c9c099c45cb1e9054674ea,con2,International Conference on Software Engineering,Empirical studies of software engineering: a roadmap,"In this article we summarize the strengths and weaknesses of empirical research in software engineering. We argue that in order to improve the current situation we must create better studies and draw more credible interpretations from them. We finally present a roadmap for this improvement, which includes a general structure for software empirical studies and concrete steps for achieving these goals: designing better studies, collecting data more effectively, and involving others in our empirical enterprises.",Conference paper,pro2
pap851,4d6191fb1c4bcfef93ba7ce9a46f71f04b11e41b,con20,ACM Conference on Economics and Computation,Changing the paradigm of software engineering,"Software evolution, iterative, and agile development represent a fundamental departure from the previous waterfall-based paradigm of software engineering.",Erratum,pro20
pap852,6157c688e166cc34cdc5e421e69424a4167c0dbf,con91,Symposium on the Theory of Computing,Bayesian Analysis of Empirical Software Engineering Cost Models,"Many parametric software estimation models have evolved in the last two decades (L.H. Putnam and W. Myers, 1992; C. Jones, 1997; R.M. Park et al., 1992). Almost all of these parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in the paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost models, COCOMO II. It describes the use of a pragmatic 10 percent weighted average approach that was used for the first publicly available calibrated version (S. Chulani et al., 1998). It then moves on to show how a more sophisticated Bayesian approach can be used to alleviate some of the problems faced by multiple regression. It compares and contrasts the two empirical approaches, and concludes that the Bayesian approach was better and more robust than the multiple regression approach.",Erratum,pro91
pap853,20d68fe79beb869193133d1cc850c22b53fcd5ad,con68,Experimental Software Engineering Network,Empirical Research Methods in Software Engineering,,Conference paper,pro68
pap854,1f55fcd9bfc08f66723683071c45718b1eda9f56,con12,The Compass,DESMET: a methodology for evaluating software engineering methods and tools,"DESMET was a DTI-backed project with the goal of developing and validating a methodology for evaluating software engineering methods and tools. The project identified nine methods of evaluation and a set of criteria to help evaluators select an appropriate method. Detailed guidelines were developed for three important evaluation methods: formal experiments, quantitative case studies and feature analysis evaluations. This article describes the way the DESMET project used the DESMET methodology both to evaluate the methodology itself and to provide direct assistance to the commercial organisations using it.",Erratum,pro12
pap855,008f3f4af65402cb9a552de3191cc8dfceaa3a34,jou160,IEEE Software,Software-engineering research revisited,"The author discusses three major changes that he suggests are occurring as a result of the software engineering industry adopting the industry-as-laboratory approach, in which researchers identify problems through close involvement with industrial projects and create and evaluate solutions in an almost indivisible research activity. This approach emphasizes what people actually do or can do in practice, rather than what is possible in principle. The three changes are a greater reliance on empirical definition of problems, an emphasis on real case studies, and a greater emphasis on contextual issues.<<ETX>>",Letter,vol160
pap856,491d79b6ae6e9c96d43b9c030e78c3647fb4af60,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",Software engineering - principles and practice,,Erratum,pro87
pap857,49be9d3df3b1e40f3f01443c38e9f92a643eaa0d,con68,Experimental Software Engineering Network,Using the focus group method in software engineering: obtaining practitioner and user experiences,"This paper reflects on three cases where the focus group method was used to obtain feedback and experiences from software engineering practitioners and application users. The focus group method and its background are presented, the method's weaknesses and strengths are discussed, and guidelines are provided for how to use the method in the software engineering context. Furthermore, the results of the three studies conducted are highlighted and the paper concludes in a discussion on the applicability of the method for this type of research. In summary, the focus group method is a cost-effective and quick empirical research approach for obtaining qualitative insights and feedback from practitioners. It can be used in several phases and types of research. However, a major limitation of the method is that it is useful only in studying concepts that can be understood by participants in a limited time. We also recommend that in the software engineering context, the method should be used with sufficient empirical rigor.",Erratum,pro68
pap858,41ddb771b92cdbc9a2d38d800a126fd60202438e,con4,Conference on Innovative Data Systems Research,Principles of software engineering management,"From the Publisher: 
This book is designed to help software engineers and project managers to understand and solve the problems involved in developing complex software systems. It provides practical guidelines and tools for managing the technical and organizational aspects of software engineering projects.",Erratum,pro4
pap859,12e4dad15a6b1056e3eb1446299b90d074731647,con105,British Machine Vision Conference,Facts and fallacies of software engineering,"There's a problem with those facts—and, as you might imagine, those fallacies. Many of these fundamentally important facts are learned by a software engineer, but over the short lifespan of the software field, all too many of them have been forgotten. While reading Facts and Fallacies of Software Engineering, you may experience moments of ""Oh, yes, I had forgotten that,"" alongside some ""Is that really true?"" thoughts.",Erratum,pro105
pap860,1d5c22408f0203d3b052b1aeac7083d9d5529427,con66,International Conference on Software Reuse,Encyclopedia of Software Engineering,"From the Publisher: 
Encompasses the field of software development process--from design to transpiration to testing and everything in between. Includes all functional disciplines, software tools and languages associated with software engineering of large and/or complex projects. Organized alphabetically--every major area contains an overview article that defines the topic. Each sub-discipline has a specific article covering history, current practice, practical data and projections about future practice.",Erratum,pro66
pap861,948351f610bbaeaa1ca7db39eee56eb8146adbb0,jou176,Autonomous Agents and Multi-Agent Systems,Challenges and Research Directions in Agent-Oriented Software Engineering,,Letter,vol176
pap862,286c57dd231dcac718f556f76ece89db7571d380,con84,Workshop on Interdisciplinary Software Engineering Research,A software engineering framework for context-aware pervasive computing,"There is growing interest in the use of context-awareness as a technique for developing pervasive computing applications that are flexible, adaptable, and capable of acting autonomously on behalf of users. However, context-awareness introduces various software engineering challenges, as well as privacy and usability concerns. In this paper, we present a conceptual framework and software infrastructure that together address known software engineering challenges, and enable further practical exploration of social and usability issues by facilitating the prototyping and fine-tuning of context-aware applications.",Erratum,pro84
pap863,cd5a4236691953cb024fdc34f496511d2ab555a5,jou177,Systems Engineering,Some future trends and implications for systems and software engineering processes,"In response to the increasing criticality of software within systems and the increasing demands being put onto 21st century systems, systems and software engineering processes will evolve significantly over the next two decades. This paper identifies eight relatively surprise‐free trends—the increasing interaction of software engineering and systems engineering; increased emphasis on users and end value; increased emphasis on systems and software dependability; increasingly rapid change; increasing global connectivity and need for systems to interoperate; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy systems and software integration; and computational plenty. It also identifies two “wild card” trends: increasing software autonomy and combinations of biology and computing. It then discusses the likely influences of these trends on systems and software engineering processes between now and 2025, and presents an emerging scalable spiral process model for coping with the resulting challenges and opportunities of developing 21st century software‐intensive systems and systems of systems. © 2006 Wiley Periodicals, Inc. Syst Eng 9: 1–19, 2006",Conference paper,vol177
pap864,78303d3ff0420644dc995458a7cb60fb09030a4e,con69,Formal Concept Analysis,A Survey of Formal Concept Analysis Support for Software Engineering Activities,,Letter,pro69
pap865,e349c69e9fca46f3bd08b44d396fdee647d9053d,con108,International Conference on Information Integration and Web-based Applications & Services,Categories for software engineering,,Erratum,pro108
pap866,59f9629329fea4aef68e69321e5305a0aeebe614,con93,International Conference on Computational Logic,Metrics and Models in Software Quality Engineering,"From the Publisher: 
Author Biography: 
Dr. Stephen H. Kan, an ASQC Certified Quality Engineer, is responsible for IBM Rochester's software quality strategy and plans, quality assessment, software measurements, and statistical analysis. He has been the software quality focal point for the software system of the AS/400 computer since its initial release in 1988.",Erratum,pro93
pap867,72876f852bdc5a80a9a2ab3cb71768a519c4deb5,jou178,Journal of Software and Systems Modeling,"Models in software engineering – an introduction
",,Article,vol178
pap868,7ae824e3cd361937a3514a22983d9f2fcac3f81d,con72,Bioinformatics and Computational Biology,Software engineering concepts,"Software engineering concepts , Software engineering concepts , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",Erratum,pro72
pap869,44c013666919e101aba7d57a72aba9267929919c,con33,International Conference on Automated Software Engineering,Software engineering with Ada,"From the Publisher: 
Grady Booch, a renowned authority in software development, and Doug Bryan combined their Ada programming and software engineering expertise for the new edition of this best-selling book. Their up-to-date introduction to Ada programming provides a foundation for using the language with software engineering and object-oriented design. Programmers will find Software Engineering with Ada, Third Edition to be a complete reference for creating large-scale Ada systems and understanding the software engineering aspects of these systems. Features of the third edition include techniques for combining object-oriented design principles and software engineering to maximize the potential of Ada; extensive examples of small-sized code that will benefit new Ada programmers; six chapters devoted to design; five new large-scale programming exercises that build upon the software engineering principles developed in the design chapters; design projects on topics such as environment monitoring, database systems, and generic tree packages; an introduction to up-to-date object-oriented design methodology; and a new appendix on the Ada 9X program.",Erratum,pro33
pap870,c50bb783b466f0a32f1c70a0d2398549690ee2d7,con11,European Conference on Modelling and Simulation,The \{PROMISE\} Repository of Software Engineering Databases.,,Erratum,pro11
pap871,379ed4f7d399dcd94ae54442eac1923c52fd0209,jou11,Communications of the ACM,End-user software engineering,"End-user programming has become the most common form of programming in use today [2], but there has been little investigation into the dependability of the programs end users create. This is problematic because the dependability of these programs can be very important; in some cases, errors in end-user programs, such as formula errors in spreadsheets, have cost millions of dollars. (For example, see www.theregister.co.uk/content/67/31298.html or panko.cba.hawaii.edu/ssr/Mypapers/whatknow.htm.) We have been investigating ways to address this problem by developing a software engineering paradigm viable for end-user programming, an approach we call end-user software engineering.",Letter,vol11
pap872,33725ed48b124e6897860dcb1eb5ac7e06fe0611,con46,Software Product Lines Conference,Methodologies and software engineering for agent systems : the agent-oriented software engineering handbook,Concepts and Abstractions of Agent-Oriented Software Engineering.- Agent-Based Abstractions for Software Development.- On the Use of Agents as Components of Software Systems.- A Survey on Agent-Oriented Oriented Software Engineering Research.- Methodologies for Agent-Based Systems Development.- The Gaia Methodology.- The Tropos Methodology.- The MaSE Methodology.- A Comparative Evaluation of Agent-Oriented Methodologies.- Special-Purpose Methodologies.- The ADELFE Methodology.- The Message Methodology.- The SADDE Methodology.- The Prometheus Methodology.- Tools and Infrastructures for Agent-Oriented Software Engineering.- The AUML Approach.- FIPA-Compliant Agent Infrastructures.- Coordination Infrastructures in the Engineering of Multiagent Systems.- Non Traditional Approaches to Agent-Oriented Software Engineering.- Engineering Amorphous Computing Systems.- Making Self-Organising Adaptive Multiagent Systems Work.- Engineering Swarming Systems.- Online Engineering and Open Computational Systems.- Emerging Trends and Perspectives.- Agents for Ubiquitous Computing.- Agents and the Grid.- Roadmap of Agent-Oriented Software Engineering.,Erratum,pro46
pap873,f43cba75815b7e9d079286341d15c014982b7823,con77,International Conference on Artificial Neural Networks,An experimental card game for teaching software engineering,"The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small ""toy"" software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning, and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in realworld software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game.",Erratum,pro77
pap874,ed96e3b61e6f6a682fee07f8aa8cdf246fab44b6,con2,International Conference on Software Engineering,The use of program dependence graphs in software engineering,"This paper describes a language-independent program representation-the program dependence graph-and discusses how program dependence graphs, together with operations such as program slicing, can provide the basis for powerful programmmg tools that address important software-engineering problems, such as understanding what an existing program does and how it works, understanding the differences between several versions of a program, and creating new programs by combining pieces of old pro- grams. The paper primarily surveys work in this area that has been carried out at the University of Wisconsin during the past five years.",Article,pro2
pap875,c039b9a4080428eca9a685c2cfe269f4b8a4a5e7,con76,IEEE International Conference on Tools with Artificial Intelligence,Conducting realistic experiments in software engineering,"An important goal of most empirical software engineering research is the transfer of research results to industrial applications. Two important obstacles for this transfer are the lack of control of variables of case studies, i.e., the lack of explanatory power, and the lack of realism of controlled experiments. While it may be difficult to increase the explanatory power of case studies, there is a large potential for increasing the realism of controlled software engineering experiments. To convince industry about the validity and applicability of the experimental results, the tasks, subjects and the environments of the experiments should be as realistic as practically possible. Such experiments are, however, more expensive than experiments involving students, small tasks and pen-and-paper environments. Consequently, a change towards more realistic experiments requires a change in the amount of resources spent on software engineering experiments. This paper argues that software engineering researchers should apply for resources enabling expensive and realistic software engineering experiments similar to how other researchers apply for resources for expensive software and hardware that are necessary for their research. The paper describes experiences from recent experiments that varied in size from involving one software professional for 5 days to 130 software professionals, from 9 consultancy companies, for one day each.",Erratum,pro76
pap876,1f1e25367457f67d74ea2a3b497645898c5962a8,con92,Human Language Technology - The Baltic Perspectiv,Using benchmarking to advance research: a challenge to software engineering,"Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.",Erratum,pro92
pap877,f96f0815deeeb244942a90ada99a8d669b178323,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Reformulating software engineering as a search problem,"Metaheuristic techniques such as genetic algorithms, simulated annealing and tabu search have found wide application in most areas of engineering. These techniques have also been applied in business, financial and economic modelling. Metaheuristics have been applied to three areas of software engineering: test data generation, module clustering and cost/effort prediction, yet there remain many software engineering problems which have yet to be tackled using metaheuristics. It is surprising that metaheuristics have not been more widely applied to software engineering; many problems in software engineering are characterised by precisely the features which make metaheuristics search applicable. In the paper it is argued that the features which make metaheuristics applicable for engineering and business applications outside software engineering also suggest that there is great potential for the exploitation of metaheuristics within software engineering. The paper briefly reviews the principal metaheuristic search techniques and surveys existing work on the application of metaheuristics to the three software engineering areas of test data generation, module clustering and cost/effort prediction. It also shows how metaheuristic search techniques can be applied to three additional areas of software engineering: maintenance/evolution system integration and requirements scheduling. The software engineering problem areas considered thus span the range of the software development process, from initial planning, cost estimation and requirements analysis through to integration, maintenance and evolution of legacy systems. The aim is to justify the claim that many problems in software engineering can be reformulated as search problems, to which metaheuristic techniques can be applied. The goal of the paper is to stimulate greater interest in metaheuristic search as a tool of optimisation of software engineering problems and to encourage the investigation and exploitation of these technologies in finding near optimal solutions to the complex constraint-based scenarios which arise so frequently in software engineering.",Erratum,pro39
pap878,100437655ade3a6467d5100b7c23eb0e36050d83,con72,Bioinformatics and Computational Biology,Component-based software engineering - new challenges in software development,"The primary role of component-based software engineering is to address the development of systems as an assembly of parts (components), the development of parts as reusable entities, and the maintenance and upgrading of systems by customising and replacing such parts. This requires established methodologies and tool support covering the entire component and system lifecycle including technological, organisational, marketing, legal, and other aspects. The traditional disciplines from software engineering need new methodologies to support component-based development.",Erratum,pro72
pap879,0f59e1a63f26ae5861075571fabf5fda0373a7db,con46,Software Product Lines Conference,"The role of experimentation in software engineering: past, current, and future",Software engineering needs to follow the model of other physical sciences and develop an experimental paradigm for the field. This paper proposes the approach towards developing an experimental component of such a paradigm. The approach is based upon a quality improvement paradigm that addresses the role of experimentation and process improvement in the content of industrial development. The paper outlines a classification scheme for characterizing such experiments.,Erratum,pro46
pap880,10885f36ed1cd87e735e25b32e4bdec9994b4ed7,con2,International Conference on Software Engineering,Software engineering and middleware: a roadmap,"The construction of a large class of distributed systems can be simplified by leveraging middleware, which is layered between network operating systems and application components. Middleware resolves heterogeneity, and facilitates communication and coordination of distributed components. Existing middleware products enable software engineers to build systems that are distributed across a local-area network. State-of-the-art middleware research aims to push this boundary towards Internet-scale distribution, adaptive and reconfigurable middleware and middleware for dependable and wireless systems. The challenge for software engineering research is to devise notations, techniques, methods and tools for distributed system construction that systematically build and exploit the capabilities that middleware deliver. 1 I N T R O D U C T I O N Various commercial trends have lead to an increasing demand for distributed systems. Firstly, the number of mergers between companies is continuing to increase. The different divisions of a newly merged company have to deliver unified services to their customers and this usually demands an integration of their IT systems. The time available for delivery of such an integration is often so short that building a new system is not an option and therefore existing system components have to be integrated into a distributed system that appears as an integrating computing facility. Secondly, the time available for providing new services are decreasing. Often this can only be achieved if components are procured off-the-shelf and then integrated into a system rather than built from scratch. Components to be integrated may have incompatible requirements for their hardware and operating system platforms; they have to be deployed on different hosts, forcing the resulting system to be distributed. Finally, the Internet provides new opportunities to offer products and services to a vast number of potential customers. In this setting, it is difficult to estimate the scalability requirements. Permission to make digital or hard copies of all or part of this work lbr personal or classroom use is granted without fee provided that copies are not made or distributed tbr profit or commercial advantage and that copies bear this notice and the lull citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a tee. Future of Sofware Engineering Limerick Ireland Copyright ACM 2000 1-58113-253-0/00/6...$5.00 An e-commerce site that was designed to cope with a given number of transactions per day may suddenly find itself exposed to demand that is by orders of magnitude larger. The required scalability cannot usually be achieved by centralized or client-server architectures but demands a distributed system. Distributed systems can integrate legacy components, thus preserving investment, they can decrease the time to market, they can be scalable and tolerant against failures. The caveat, however, is that the construction of a truly distributed systems is considerably more difficult than building a centralized or client/server system. This is because there are multiple points of failure in a distributed system, system components need to communicate with each other through a network, which complicates communication and opens the door for security attacks. Middleware has been devised in order to conceal these difficulties from application engineers as much as possible; As they solve a real problem and simplify distributed system construction, middleware products are rapidly being adopted in industry [6]. In order to build distributed systems that meet the requirements, software engineers have to know what middleware is available, which one is best suited to the problems at hand, and how middleware can be used in the architecture, design and implementation of distributed systems. The principal contribution of this paper is an assessment of both, the state-of-the-practice that current middleware products offer and the state-of-the-art in middleware research. Software engineers increasingly use middleware to build distributed systems. Any research into distributed software engineering that ignores this trend will only have limited impact. We, therefore, analyze the influence that the increasing use of middleware should have on the software engineering research agenda. We argue that requirements engineering techniques are needed that focus on non-functional requirements, as these influence the selection and use of middleware. We identify that software architecture research should produce methods that guide engineers towards selecting the right middleware and employing it so that it meets a set of non-functional requirements. We then highlight that the use of middleware is not transparent for system design and that design methods are needed that address this issue.",Article,pro2
pap881,33419d151b854d6268d5608cffdf25a8395d4967,jou171,Computer,Value-Based Software Engineering: A Case Study,"The information technology field's accelerating rate of change makes feedback control essential for organizations to sense, evaluate, and adapt to changing value propositions in their competitive marketplace. Although traditional project feedback control mechanisms can manage the development efficiency of stable projects in well-established value situations, they do little to address the project's actual value, and can lead to wasteful misuse of an organization's scarce resources. The value-based approach to software development integrates value considerations into current and emerging software engineering principles and practices, while developing an overall framework in which these techniques compatibly reinforce each other.",Letter,vol171
pap882,c96136368d2103900cd27da628e6b9aade3bbefb,con19,International Conference on Conceptual Structures,Search Based Software Engineering,,Letter,pro19
pap883,7fb861b7fbe87d5ad2e62306286c383a9f2a1573,jou160,IEEE Software,Reflections on software engineering education,"The ""engineering"" focus in software engineering education leaves instructors vulnerable to several traps. It also misleads students as to SE's essential human and social dimensions. Here, the author discusses how this limited conception of SE contributes to five assumptions that can trap SE educators: (i) an SE course needs an industrial project. (ii) SE is like other branches of engineering. (iii) Planning in SE is poorly done relative to other fields. (iv) The user interface is part of low-level design. (v) SWEBOK represents the state of the practice",Letter,vol160
pap884,44c03dc9b3b637e6aa45368733fc7bab8ef4189e,con2,International Conference on Software Engineering,On the success of empirical studies in the international conference on software engineering,"Critiques of the quantity and quality of empirical evaluations in software engineering have existed for quite some time. However such critiques are typically not empirically evaluated. This paper fills this gap by empirically analyzing papers published by ICSE, the prime research conference on Software Engineering. We present quantitative and qualitative results of a quasi-random experiment of empirical evaluations over the lifetime of the conference. Our quantitative results show the quantity of empirical evaluation has increased over 29 ICSE proceedings but we still have room to improve the soundness of empirical evaluations in ICSE proceedings. Our qualitative results point to specific areas of improvement in empirical evaluations.",Article,pro2
pap885,7c9fd54e37cb362ece082906c8b8f0787ff4bdcc,con17,International Conference on Statistical and Scientific Database Management,Using Ontologies in Software Engineering and Technology,,Erratum,pro17
pap886,57e648f1cea23e2d11625990de2510ddbede39d4,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Status of Empirical Research in Software Engineering,,Erratum,pro98
pap887,907dac144f6435d882c84e8d3626b8e27d4355a3,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,An Initial Theory of Value-Based Software Engineering,,Erratum,pro58
pap888,3fad976398c0aef498d064ff6428ec761cd3a1e9,con10,Americas Conference on Information Systems,Essentials of software engineering,"Updated with new case studies and content, the fully revised Third Edition of Essentials of Software Engineering offers a comprehensive, accessible, and concise introduction to core topics and methodologies of software development. Designed for undergraduate students in introductory courses, the text covers all essential topics emphasized by the IEEE Computer Society-sponsored Software Engineering Body of Knowledge (SWEBOK). In-depth coverage of key issues, combined with a strong focus on software quality, makes Essentials of Software Engineering, Third Edition the perfect text for students entering the fast-growing and lucrative field of software development. The text includes thorough overviews of programming concepts, system analysis and design, principles of software engineering, development and support processes, methodologies, and product management. The revised and updated Third Edition includes all-new sections on SCRUM and HTML-Script-SQL Design Examples, as well as expanded discussions of User-Interface Design, Flow of Interactions, Cognitive Models, and other UI Design issues. Covering all phases of the software production lifecycle and emphasizing quality throughout, Essentials of Software Engineering is a superb resource for students of software engineering. Key Features: Revised and fully updated throughout, with all-new sections on SCRUM and HTML-Script-SQL Design Examples, as well as expanded discussions of other central topics Provides coverage of all essential topics emphasized by SWEBOK Covers essential topics required for students to complete individual and team projects in an affordable and accessible paperback format. Contains an all-new Appendix with examples of Essential Software Development Plan (SDP), Essential Software Requirements Specifications (SRS), Essential Software Design, and Essential Test Plan",Erratum,pro10
pap889,f4f5136d20a905ff52c36de24afb1f7b61e1c5df,jou160,IEEE Software,SE2004: Recommendations for Undergraduate Software Engineering Curricula,"Universities throughout the world have established undergraduate programs in software engineering, which complement existing programs in computer science and computer engineering. To provide guidance in designing an effective curriculum, the IEEE Computer Society and the ACM have developed the Software Engineering 2004 (SE2004) set of recommendations. The SE2004 document guides universities and colleges regarding the knowledge they should teach in undergraduate software engineering programs. It also provides sample courses and curriculum patterns. SE2004 begins with an overview of software engineering, explaining how it is both a computing and an engineering discipline. It then outlines the principles that drove the document's development and describes expected student outcomes. Next, SE2004 details the knowledge that universities and colleges should teach, known as SEEK (software engineering education knowledge), in a software engineering program. These recommendations are followed by general pedagogical guidelines, sample courses, and sample curriculum patterns",Conference paper,vol160
pap890,6ad63dc45d5e9d9f0a6fc2abd9f3999b532ee9e5,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Value-Based Software Engineering: Overview and Agenda,,Erratum,pro82
pap891,dacd22e7d9a77fc98fdd62a5b11b89993c972378,con101,International Conference on Biometrics,Integrating Security and Software Engineering: Advances and Future Visions,A Sample of Contents: Integrating Security and Software Engineering A Methodology to Develop Secure Systems Using Patterns Extending Security in Agile Software Development Methods Access Control Specification in UML.,Erratum,pro101
pap892,9c243366103777ce1d029164bcc769f4f4231308,jou171,Computer,The unspoken revolution in software engineering,In this article the author describes the outsourcing aspects of software engineering. The author finds outsourcing so fascinating partly because it serves as a magnifier and revelator of just about everything in software engineering. The development of offshoring also raises a new challenge for those of us entrusted with educating future software professionals in the industrialized world.,Conference paper,vol171
pap893,cbd95a8750014a8c86bd9beb8f2ccacab85a2227,con37,International Symposium on Search Based Software Engineering,The Role of Controlled Experiments in Software Engineering Research,,Erratum,pro37
pap894,9ac8eda3552e02e23ca63715bffe7fcb98d5b4c9,jou179,International Journal on Software Tools for Technology Transfer (STTT),What makes good research in software engineering?,,Article,vol179
pap895,968b8b3162b62e4c91cf4244ca9bf6f42c2256b5,con92,Human Language Technology - The Baltic Perspectiv,Conducting on-line surveys in software engineering,"One purpose of empirical software engineering is to enable an understanding of factors that influence software development. Surveys are an appropriate empirical strategy to gather data from a large population (e.g., about methods, tools, developers, companies) and to achieve an understanding of that population. Although surveys are quite often performed, for example, in social sciences and marketing research, they are underrepresented in empirical software engineering research, which most often uses controlled experiments and case studies. Consequently, also the methodological support how to perform such studies in software engineering is rather low. However, with the increasing pervasion of the Internet it is possible to perform surveys easily and cost-effectively over Internet pages (i.e., on-line), while at the same time the interest in performing surveys is growing. The purpose of this paper is twofold. First we want to arise the awareness of on-line surveys and discuss methods how to perform these in the context of software engineering. Second, we report our experience in performing on-line surveys in the form of lessons learned and guidelines.",Erratum,pro92
pap896,1c96d058b293b1da80d103f31d5a14bc6a738ab8,con70,International Conference on Graph Transformation,Tutorial Introduction to Graph Transformation: A Software Engineering Perspective,,Letter,pro70
pap897,482dd72ce24ec08528bd97cbbe93d239a377fb0a,jou151,IEEE Transactions on Software Engineering,Experimentation in software engineering,"A framework is presented for analyzing most of the experimental work performed in software engineering over the past several years. The framework of experimentation consists of four categories corresponding to phases of the experimentation process: definition, planning, operation, and interpretation. A variety of experiments are described within the framework and their contribution to the software engineering discipline is discussed. Some recommendations for the application of the experimental process in software engineering are included.",Conference paper,vol151
pap898,e9415d9a6e0065b46acd99ba4ff8b89bd1435fc8,con33,International Conference on Automated Software Engineering,Machine Learning and Software Engineering,,Erratum,pro33
pap899,e6165f19a32c649e637ccaaf704e7bbb179c5823,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,"Object Oriented Software Engineering, Conquering Complex and Changing Systems","From the Publisher: 
This book is based on object-oriented techniques applied to software engineering. Employing the latest technologies such as UML, Patterns, and Java, Bernd Bruegge and Allen H. Dutoit offer a cohesive, class-tested presentation of object-oriented software engineering in a step-by-step format based on ten years of teaching and real-world software engineering experience. This text teaches practical experience in developing complex software appropriate for software engineering project courses, as well as industry R & D practitioners. The reader benefits from timely exposure to state-of-the-art tools and methods. 
 
Unlike other texts based on the teaching premise of multiple classes or developing multiple systems, this book focuses on techniques and applications in a reasonably complex environment, such as multi-team development projects including 20 to 60 participants. The book is based on concrete examples from real applications such as accident management, emissions modeling, facility management, and centralized traffic control. 
 
Provides an integrated communication infrastructure for distributed development 
Shows the state of the art in Software Engineering: UML, Java, Design Patterns, Distributed Development, and Multiproject Management 
Illustrates how the reader learns to develop in a distributed team with hands-on experience on real system development problems 
Offers a CD-ROM containing the materials used in courses taught by the authors-problem statements, requirement analysis documents, system design documents, test manuals, prototypes, and all the artifacts produced during the development of a facility management system 
Presents Companion Website (www.prenhall.com/bruegge) with supplemental material such as problem statements, requirement analysis documents, system design documents, test manuals, and solutions to exercises",Erratum,pro58
pap900,ec9196b9edd46c5c59124ca776ca71538655fbc6,con103,IEEE International Conference on Multimedia and Expo,Empirical Data Modeling in Software Engineering Using Radical Basis Functions,"Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.",Erratum,pro103
pap901,05acb25fccb689cb74b347929f425a6934e284bc,con55,Workshop on Learning from Authoritative Security Experiment Results,Sustainability Design and Software: The Karlskrona Manifesto,"Sustainability has emerged as a broad concern for society. Many engineering disciplines have been grappling with challenges in how we sustain technical, social and ecological systems. In the software engineering community, for example, maintainability has been a concern for a long time. But too often, these issues are treated in isolation from one another. Misperceptions among practitioners and research communities persist, rooted in a lack of coherent understanding of sustainability, and how it relates to software systems research and practice. This article presents a cross-disciplinary initiative to create a common ground and a point of reference for the global community of research and practice in software and sustainability, to be used for effectively communicating key issues, goals, values and principles of sustainability design for software-intensive systems.The centrepiece of this effort is the Karlskrona Manifesto for Sustainability Design, a vehicle for a much needed conversation about sustainability within and beyond the software community, and an articulation of the fundamental principles underpinning design choices that affect sustainability. We describe the motivation for developing this manifesto, including some considerations of the genre of the manifesto as well as the dynamics of its creation. We illustrate the collaborative reflective writing process and present the current edition of the manifesto itself. We assess immediate implications and applications of the articulated principles, compare these to current practice, and suggest future steps.",Erratum,pro55
pap902,a6f3a7288fc5f5e976427118abd354c8f602d9d4,con105,British Machine Vision Conference,Thinking on the Development of Software Engineering Technology,"The paper gives some thinking according to the following four aspects: 1) from the law of things development, revealing the development history of software engineering technology; 2) from the point of software natural characteristic, analyzing the construction of every abstraction layer of virtual machine; 3) from the point of software development, proposing the research content of software engineering discipline, and research the pattern of industrialized software production; 4) based on the appearance of Internet technology, exploring the development trend of software technology.",Erratum,pro105
pap903,14cae0857b066adfd72137481296cc9f81741d2a,con100,International Conference on Automatic Face and Gesture Recognition,"Software engineering education in the era of outsourcing, distributed development, and open source software: challenges and opportunities",,Erratum,pro100
pap904,74ea36e76ae8a208336d3cf79ce1482245264363,con71,Annual Conference on Innovation and Technology in Computer Science Education,Teaching software engineering through game design,"Many projects currently used in Software Engineering curricula lack both the ""fun factor"" needed to engage students, as well as the practical realism of engineering projects that include other computer science disciplines such as Software Engineering, Networks, or Human Computer Interaction. This paper reports on our endeavor to enhance interest and retention in an existing Software Engineering curriculum through the use of computer game-based projects. Specifically, a set of game-centric, project-based modules have been developed that enable students to: (1) actively participate in the different phases of the software lifecycle taking a single project from requirement elicitation to testing and maintenance; (2) expose students to real issues in project and team management over the course of a 2-semester project; and at the same time (3) introduce students to the different aspects of computer game design. Preliminary results suggest the merits of our approach, showing improved class participation and performance.",Letter,pro71
pap905,3a0535cf9abb931646e07cdf875a4ae081e0dafb,con8,Frontiers in Education Conference,Tool Integration in Software Engineering Environments,"This article presents doctoral research on tool int egration within software engineering environments. Tool int egration concerns the techniques used to form coalitions of to ls that provide an environment supporting some, or all, act ivities within the software engineering process. Some inte res ing phenomena have been observed, such as the ad hoc na ture of tool integration in one particular software enginee ring company. This observation is at variance to the com m n perception of widespread integration suggested by t ool vendors and some previous academic literature. Ini tial results suggest that integration must be implemented for bu siness reasons, not for its own sake.",Erratum,pro8
pap906,7044cf26d6f6ffe4804e0aaf7c18d6c6da5e3766,con11,European Conference on Modelling and Simulation,The challenges of software engineering education,"We discuss the technical skills that a software engineer should possess. We take the viewpoint of a school of engineering and put the software engineer's education in the wider context of engineering education. We stress both the common aspects that crosscut all engineering fields and the specific issues that pertain to software engineering. We believe that even in a continuously evolving field like software, education should emphasize principles and recognize what are the stable and long-lasting design concepts. Even though the more mundane technological solutions cannot be ignored, the students should be equipped with skills that allow them to dominate the evolution of technology.",Erratum,pro11
pap907,9ae81c7d36be6cd4c541e824f6886b33bae43b20,con65,IEEE International Conference on Software Engineering and Formal Methods,Models in software engineering - an introduction,,Erratum,pro65
pap908,27aa2b0aa6cfb97d2ac4044b28421700371e6027,jou152,Empirical Software Engineering,On the application of measurement theory in software engineering,,Article,vol152
pap909,4fe3be9b66f6a71aefe9963ec14cb56c62dca4f2,con45,International Conference on Global Software Engineering,A risk management framework for software engineering practice,"Formal risk analysis and management in software engineering is still an emerging part of project management. We provide a brief introduction to the concepts of risk management for software development projects, and then an overview of a new risk management framework. Risk management for software projects is intended to minimize the chances of unexpected events, or more specifically to keep all possible outcomes under tight management control. Risk management is also concerned with making judgments about how risk events are to be treated, valued, compared and combined. The ProRisk management framework is intended to account for a number of the key risk management principles required for managing the process of software development. It also provides a support environment to operationalize these management tasks.",Erratum,pro45
pap910,40f5cafb4033f952930bd05bff2559eae3f367da,jou166,"Software, Practice & Experience",Agents in object‐oriented software engineering,"Software engineers of multi‐agent systems (MASs) are faced with different concerns such as autonomy, adaptation, interaction, collaboration, learning, and mobility, which are essentially different from classical concerns addressed in object‐oriented software engineering. MAS developers, however, have relied mostly on object‐oriented design techniques and programming languages, such as Java. This often leads to a poor separation of MAS concerns and in turn to the production of MASs that are difficult to maintain and reuse. This paper discusses software engineering approaches for MASs, and presents a new method for integrating agents into object‐oriented software engineering from an early stage of design. The proposed approach encourages the separate handling of MAS concerns, and provides a disciplined scheme for their composition. Our proposal explores the benefits of aspect‐oriented software development for the incorporation of agents into object‐oriented systems. We also illustrate our aspect‐oriented approach through the Portalware multi‐agent system, a Web‐based environment for the development of e‐commerce portals. Copyright © 2004 John Wiley & Sons, Ltd.",Conference paper,vol166
pap911,49900e8f8328adfee4818fa4ee959c2529966636,con62,Australian Software Engineering Conference,Knowledge management in software engineering - describing the process,"The management of knowledge and experience are key means by which systematic software development and process improvement occur. Within the domain of software engineering (SE), quality continues to remain an issue of concern. Although remedies such as fourth generation programming languages, structured techniques and object-oriented technology have been promoted, a ""silver bullet"" has yet to be found. Knowledge management (KM) gives organisations the opportunity to appreciate the challenges and complexities inherent in software development. We report on two case studies that investigate KM in SE at two IT organisations. Structured interviews were conducted, with the assistance of a qualitative questionnaire. The results were used to describe current practices for KM in SE, to investigate the nature of KM activities in these organisations, and to explain the impact of leadership, technology, culture and measurement as enablers of the KM process for SE.",Erratum,pro62
pap912,25831b14ff3201ba595ee47c9347b576a34b15cd,jou152,Empirical Software Engineering,Knowledge-Sharing Issues in Experimental Software Engineering,,Conference paper,vol152
pap913,3a6eff8c5e72c0ac2fa6b6d72214aebbec0ed996,con56,International Conference on Software Engineering and Knowledge Engineering,Reuse and Productivity in Integrated Computer-Aided Software Engineering: An Empirical Study,"Growing competition in the investment banking industry has given rise to increasing demand for high functionality software applications that can be developed in a short period of time. Yet delivering such applications creates a bottleneck in software development activities. This dilemma can be addressed when firms shift to development methods that emphasize software reusability. This article examines the productivity implications of object and repository-based integrated computer-aided software engineering (ICASE) software development in the context of a major investment bank's information systems strategy. The strategy emphasizes software reusability. Our empirical results, based on data from 20 projects that delivered software for the bank's New Trades Processing Architecture (NTPA), indicate an order of magnitude gain in software development productivity and the importance of reuse as a driver in realizing this result. In addition, results are presented on the extent of the learning that occurred over a two-year period after ICASE was introduced, and on the influence of the link between application characteristics and the ICASE tool set in achieving development performance. This work demonstrates the viability of the firm's IS strategy and offers new ideas for code reuse and software development productivity measurement that can be applied in development environments that emphasize reuse.",Erratum,pro56
pap914,2588db39ca2c65c389de64ed18d1c5da4ee1d34c,con45,International Conference on Global Software Engineering,Iterative Software Engineering for Multiagent Systems: The MASSIVE Method,"Agents, Multiagent Systems and Software Engineering.- Basic Concepts in Software Engineering.- The Conceptual Framework of Massive.- Massive Views.- Further Case Studies.- Conclusion.- Toolkits for Agent-Based Applications.- Basic Problem Solving Capabilities of TCS Agents.- Protoz Specification of the Contract-Net Protocol.",Erratum,pro45
pap915,64ed48837049f3b6915df37de688a7f85d2cf8b2,con51,Brazilian Symposium on Software Engineering,Replication of Software Engineering Experiments,"Carrying out empirical studies is slowly becoming widely held to be of importance by the software engineering community. A view perhaps less widely held is that experiments should be replicated externally to both verify and generalise the original results. This paper serves a number of purposes. The need for external replications is established and the role of replication in experimental software engineering is discussed. Without the con rming power of external replications, results in experimental software engineering should only provisionally be accepted, if at all. The paper then draws heavily on the authors' experiences in externally replicating three software engineering experiments ([Kor86, PVB95, KL96])to provide guidance on three, relatively neglected areas: the improving of experimental `recipes' during replication to either focus or generalise results, the use of alternative data analysis techniques, speci cally rule induction, to seek alternative explanations when the results of replications di er, and packaging of experiments for replication. The facilitation of replication through properly constructed replication packages places formidable but largely unrewarding demands on the original experimenter. Yet without appropriately constructed and documented replication packages systematic and cumulative empirical investigation will be di cult to achieve. It is demonstrated that a modest generalisation of the characterisation scheme, or framework, proposed by Lott and Rombach [LR96], will serve well as the basis for the reporting, packaging and recipe improvement of software engineering experiments for the purposes of external replication. It also demonstrates the insights that can be achieved from seeking explanations beyond those provided by purely statistical analyses.",Erratum,pro51
pap916,6281e057089d356a119b75e5f75e93f3c5096622,con110,Very Large Data Bases Conference,Supporting impact analysis and change propagation in software engineering environments,"Impact analysis and change propagation are among the major issues of software change management. In this paper, we introduce an approach to providing impact analysis and change propagation support as an integral part of software engineering environments, so that they can be applied during both software development and maintenance. In this approach, the activities are carried out on the original representation of software artifacts in the environment, rather than on a separate system model extracted. This very fact enables automated direct support for the process of actually carrying out and propagating the changes. Dependences and properties of software artifacts are used for impact analysis to achieve increased flexibility and expressiveness. Both impact analysis and change propagation are a combination of guided user intervention and automatic processing based on codified change patterns and propagation rules.",Erratum,pro110
pap917,99e946ef6e9cfe65469750c7c69883d7cbbd6eda,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Managing Software Engineering Knowledge,,Erratum,pro82
pap918,d4162fd444fc1a287a9469366a712091074a5290,jou171,Computer,Software Engineering: Problems and Perspectives,"As software applications become more complex, software engineering will evolve. Specification languages, rapid prototyping, complexity metrics, and maintenance techniques will be its most significant products. Computer users first became aware of a software crisis 15 years ago. Software projects were being delivered far behind schedule, quality was poor, and maintenance was expensive. And as more complex software applications were found, programmers fell further behind the demand and their results were of poorer quality. The high demand and comparatively low productivity drove software costs up. In the US in 1980, software cost approximately $40 billion, or two percent of the gross national product. I Dolotta estimates that by the year 1985, the cost of software will be approximately 8.5 percent of the GNP,2 while Steel points to 13 percent by 1990.3 And in recognition of the importance of software engineering, the Department of Defense began a software initiative and has plans for establishing a software engineering institute. Software engineering is a relatively new discipline. It seeks to devise techniques for software development.",Conference paper,vol171
pap919,a7230b20e195a00de416c367a1e6a56a250bf43a,con20,ACM Conference on Economics and Computation,End-user software engineering with assertions in the spreadsheet paradigm,"There has been little research on end-user program development beyond the activity of programming. Devising ways to address additional activities related to end-user program development may be critical, however, because research shows that a large proportion of the programs written by end users contain faults. Toward this end, we have been working on ways to provide formal ""software engineering"" methodologies to end-user programmers. This paper describes an approach we have developed for supporting assertions in end-user software, focusing on the spreadsheet paradigm. We also report the results of a controlled experiment, with 59 end-user subjects, to investigate the usefulness of this approach. Our results show that the end users were able to use the assertions to reason about their spreadsheets, and that doing so was tied to both greater correctness and greater efficiency.",Erratum,pro20
pap920,e0a3c4c0b4dd64c6c5247ff16b79f52cf0b5d6cb,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Software Engineering for Adaptive Hypermedia Applications,"The Programming and Software Engineering Research Group of the Institute of Computer Science of the Ludwig-Maximilians University of Munich is focusing on software engineering for hypermedia and Web applications in general and, particularly, for adaptive applications. One main goal of the software engineering discipline is to find techniques that support the development process of software applications. Our goal is to find, between others, appropriate analysis and design techniques that support development and authoring of adaptive hypermedia and Web applications. General object-oriented software engineering approaches, such as the Unified Process (Jacobson, Booch & Rumbaugh, 1999) or specific methodologies for hypermedia like RMM (Isakowitz, Stohr & Balasubramanian, 1995), OOHDM (Schwabe & Rossi, 1998), and HFPM (Olsina, 1998) are not sufficient. They do not cover aspects relevant to personalization, i.e. user modeling and adaptation issues. A significant contribution in this field is AHAM (De Bra, Houben & Wu, 1999). AHAM is an application model for adaptive hypermedia that describe such applications from the authors’ point of view. We propose the UML-based Web Engineering approach (UWE) (Koch, 2000 & Koch et. al, 2001). UWE includes a design method for adaptive hypermedia applications and a development process for such applications. UWE is a systematic and object-oriented – in this way they differ from AHAM – design and development approach. We propose an integrated methodology for object-oriented development of adaptive hypermedia (Web) applications by presenting an extension to the Unified Modeling Language (UML). As basis for the software engineering approach we have developed the Munich Reference Model, i.e. a Dexter-based reference model which is formally specified using UML and OCL (Koch, 2000).",Erratum,pro47
pap921,2621ca6891f5d369c340b470b11d35a0c2a5c6be,con79,IEEE Annual Symposium on Foundations of Computer Science,A software engineering experiment in software component generation,"The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.",Erratum,pro79
pap922,88417de5d03126d0fb0ed93a9af995f24bfc6d73,jou160,IEEE Software,Diffusing software-engineering methods,"The problems that surround software technology transfer are examined. It is suggested that for software engineers to understand these problems, they need to identify a conceptual framework that can provide a systematic understanding of diffusion processes and to identify priority areas through which the software-engineering community can effect the successful diffusion of its innovations. E.M. Roger's framework for diffusion of innovations (The Diffusion of Innovations, Free Press, NY, 1983) is discussed, and two ways to use it, as a descriptive model or as a prescriptive model, are described. A case study is presented to illustrate the arguments.<<ETX>>",Letter,vol160
pap923,c8c10e47cc3ae9e6a67e2f30eb99696a5ea02cf5,jou3,IEEE Transactions on Knowledge and Data Engineering,A Knowledge-Based Environment for Modeling and Simulating Software Engineering Processes,"The design and representation schemes used in constructing a prototype computational environment for modeling and simulating multiagent software engineering processes are described. This environment is called the articulator. An overview of the articulator's architecture identifying five principal components is provided. Three of the components, the knowledge metamodel, the software process behavior simulator, and a knowledge base querying mechanism, are detailed and examples are included. The conclusion reiterates what is unique to this approach in applying knowledge engineering techniques to the problems of understanding the statics and dynamics of complex software engineering processes. >",Conference paper,vol3
pap924,0dc01dd9dcb0f2be2332d4f6a7b492847a3e653b,jou180,Software Engineering Journal,"Software engineering, the software process and their support","Computers are being applied more and more widely, penetrating ever deeper into the very fabric of society. Mankind is becoming increasingly dependent on the availability of software and its continuing validity. To achieve this consistently and reliably, in an operational domain that is forever changing, requires disciplined execution of the software development and evolution process and its effective management. That is the goal of advanced software engineering [1]. This paper summarises basic concepts of software engineering and of the software development process. This leads to a principle of uncertainty, analysis of its implications for the software development process, an overview of computer-assisted software engineering (CASE) and brief comments on the societal relevance of these topics. For researchers in the field and practitioners familiar with individual concepts, issues and specific solutions, the paper provides a unifying framework, a basis for conceptual advance. Those without a significant practical software engineering background and experienced graduate students will extend general familiarity with fresh insights, new concepts and additional detail. Undergraduate and graduate students without significant experience may treat the paper as an introductory text.",Letter,vol180
pap925,1f534ffbb7645df74cebd534554afa5c9cf8f77b,jou160,IEEE Software,Wisdom: A Software Engineering Method for Small Software Development Companies,"Wisdom is a new software engineering method addressing the specific needs of small teams that develop and maintain interactive systems. Because Wisdom defines a process, notation, and project philosophy, it can smoothly be applied in small companies leveraging on their communication, speed, and flexibility.",Letter,vol160
pap926,18a28e73f39a58194da82e559227809f0e0260dc,con29,ACM-SIAM Symposium on Discrete Algorithms,A practical approach of teaching Software Engineering,"In today's software industry a software engineer is not only expected to successfully cope with technical challenges, but also to deal with non-technical issues arising from difficult project situations. These issues typically include understanding the customer's domain and requirements, working in a team, organizing the division of work, and coping with time pressure and hard deadlines. Thus, in our opinion teaching Software Engineering, (SE) not only requires studying theory using text books, but also providing students with the experience of typical non-technical issues in a software project. This article reports experiences with the concept of a course focusing on providing practical know-how.",Erratum,pro29
pap927,2d8e4ed1d86114e3adcf4085e8acaf03c8872c99,jou160,IEEE Software,Software Engineering: Community and Culture,"Ideas and techniques from the social sciences can improve the theory and practice of the software engineering discipline. To illustrate the contributions this cross-pollination has made, the authors focus on the nature of paradigms and software quality management systems. Their studies underscore the importance of community in how new technical ideas become accepted, how despite software engineers too often prefer polemic to evidence, and the primacy given to the local guru that transcends formal organizational structures.",Article,vol160
pap928,8e34ac04a2de79ecd784f8374de20ecd9746cdcf,con104,Biometrics and Identity Management,Software Engineering Measurement,THE GOALS OF SOFTWARE ENGINEERING MEASUREMENT Software Engineering Measurement The Rationale for Effective Measurement Measurement across the Life Cycle Model Reasonable and Attainable Goals for Software Measurement Summary THE CONDUCT OF SCIENTIFIC INVESTIGATIONS The Principals of Scientific Investigation Measurement Measurement Issues Measurement Standards Principles of Experimentation MEASURING SOFTWARE DEVELOPMENT Measurement Domains Modeling: Mapping among Measurement Domains The Process of Software Measurement Summary VALIDATION OF SOFTWARE MEASURES Understanding What Is Being Measured Criterion-Oriented Validity Content Validity Construct Validity Empirical Validity Reliability STATIC SOFTWARE MEASUREMENT Introduction Primitive Measures of Source Code Measures of Software Quality Summary DERIVED SOFTWARE MEASURES Introduction Software Science Metrics Sources of Variation The Principal Components of Measurement Principal Components Analysis as a Validation Tool Discovering New Sources of Variation Domain Metrics A Unitary Measure of Software Complexity Summary MODELING WITH METRICS Introduction Simple Linear Regression Non-Linear Models Problems Associated with Multicollinearity Regression as a Metric Validation Tool Canonical Correlation MEASURING SOFTWARE EVOLUTION Introduction Measuring Evolving Software Measuring Changes to Modules across Builds Summary SOFTWARE SPECIFICATION AND DESIGN Introduction Software Operational Requirements Specification Software Functional Requirements Specification Software Module Requirements Specification A Formal Description of Program Operation Configuration Control for the Requirements Measuring Software Design Alternatives Maintainability DYNAMIC SOFTWARE MEASUREMENT Introduction A Stochastic Description of Program Operation The Profiles of Software Dynamics Estimates for Profiles Code Instrumentation Instrumenting for the Profiles Partial Complexity A Measure of Cohesion Entropy Testability Revisited THE MEASUREMENT OF SOFTWARE TESTING ACTIVITY Introduction Static and Dynamic Measurement A Metaphor for Test Activity Measurement Based Testing Fractional Measures Introduction to Statistical Testing SOFTWARE AVAILABILITY Introduction Software Reliability Availability Security Maintainability IMPLEMENTING A SOFTWARE MEASUREMENT PLAN The Software Measurement Process Building a Measurement Process Measurement Process Improvement Institutionalizing Measurement Process Improvement A Network Based Measurement System IMPLEMENTING A SOFTWARE RESEARCH PLAN What Is Software Research? Implementing a Research Plan Defining Software Research Objectives Budgeting for Software Research Research Pays APPENDIXES REVIEW OF MATHEMATICAL FUNDAMENTALS Matrix Algebra Some Notions of Probability Discrete Probability Distributions Continuous Probability Distributions Statistics Tests of Hypotheses Introduction to Modeling A STANDARD FOR THE MEASUREMENT OF C PROGRAMMING LANGUAGE ATTRIBUTES Introduction Compiler Directives Style and Statement Metrics Lexical Metrics Control Flowgraph Metrics Coupling Metrics Definitions Tokens,Erratum,pro104
pap929,4e5f14131db1dada5eab8c5d4bf1317e611bb107,con89,Conference on Uncertainty in Artificial Intelligence,A Pattern Recognition Approach for Software Engineering Data Analysis,"In order to plan, control, and evaluate the software development process, one needs to collect and analyze data in a meaningful way. Classical techniques for such analysis are not always well suited to software engineering data. A pattern recognition approach for analyzing software engineering data, called optimized set reduction (OSR), that addresses many of the problems associated with the usual approaches is described. Methods are discussed for using the technique for prediction, risk management, and quality evaluation. Experimental results are provided to demonstrate the effectiveness of the technique for the particular application of software cost estimation. >",Erratum,pro89
pap930,161928af38d0323a1821e8ac42c3494a15ff1e40,con5,Technical Symposium on Computer Science Education,Ethical Issues in Empirical Studies of Software Engineering,"The popularity of empirical methods in software engineering research is on the rise. Surveys, experiments, metrics, case studies, and field studies are examples of empirical methods used to investigate both software engineering processes and products. The increased application of empirical methods has also brought about an increase in discussions about adapting these methods to the peculiarities of software engineering. In contrast, the ethical issues raised by empirical methods have received little, if any, attention in the software engineering literature. This article is intended to introduce the ethical issues raised by empirical research to the software engineering research community and to stimulate discussion of how best to deal with these ethical issues. Through a review of the ethical codes of several fields that commonly employ humans and artifacts as research subjects, we have identified major ethical issues relevant to empirical studies of software engineering. These issues are illustrated with real empirical studies of software engineering.",Erratum,pro5
pap931,c68fbf44dd0cf27d6620584e89783046c7863c71,con104,Biometrics and Identity Management,Distributed component technologies and their software engineering implications,"In this state-of-the-art report, we review advances in distributed component technologies, such as the Enterprise JavaBeans (EJB) specification and the CORBA component model (CCM). We assess the state of industrial practice in the use of distributed components. We show several architectural styles for whose implementation distributed components have been used successfully. We review the use of iterative and incremental development processes and the notion of a model-driven architecture. We then assess the state of the art in research into novel software engineering methods and tools for the modelling, reasoning and deployment of distributed components. The open problems identified during this review result in the formulation of a research agenda that will contribute to the systematic engineering of distributed systems based on component technologies.",Erratum,pro104
pap932,19aee12e61ca1fe953b7e85433b4a6274f341678,con104,Biometrics and Identity Management,Predicate Logic for Software Engineering,"The interpretations of logical expressions found in most introductory textbooks are not suitable for use in software engineering applications because they do not deal with partial functions. More advanced papers and texts deal with partial functions in a variety of complex ways. This paper proposes a very simple change to the classic interpretation of predicate expressions, one that defines their value for all values of all variables, yet is almost identical to the standard definitions. It then illustrates the application of this interpretation in software documentation. >",Erratum,pro104
pap933,ce71e163e181f22231db9bc7eb2c8a0f8ae6976e,jou181,Annals of Software Engineering,"Process-Centered Software Engineering Environments, A Brief History and Future Challenges",,Conference paper,vol181
pap934,56615aff0dda1a5ddb51344cba2734906edfc88c,con108,International Conference on Information Integration and Web-based Applications & Services,A method for assessing the software engineering capability of contractors,"This document provides guidelines and procedures for assessing the ability of potential DoD contractors to develop software in accordance with modem software engineering methods. It includes spl-:ific questions and a method for evaluating the results. ,I General Introduction The purpose of this document is to facilitate objective and consistent assessments of the ability of potential DoD contractors to dovelop software in accordance with modem software engineering methods. Such assessments would be conducted either In the pre-solicitation qualification process, in the formal source selection process, or both. While this doc~ument Is Intended to guide the assessment of a contractor's overall software engineering capability, it can also be valuable in the assessment ."", a specific project team's software engineering capability. Alternatively, this document can be used as an aid to software development organizations in conducting an internal as issment cf their own softvare engineering capability. The document is designed to help An assessment team define the highest priorldy steps for the Improvement of an organization's capability. Because an understanding of proper software engineering practice is only now developing, standard, well-accepted measures do not yet exist. The assessment questions listed in the body of this .)•cum,'nt are phrased so that an affirmative answer indicates that an organization has a desirable characteristic. Some of the questions pertain to advanced concepts of software engineering that may not yet be sufficiently refined or disseminated to be incorporated in a contractor's standard practice; therefore, not all assessment questions need be answered affirmatively for an organization to be considered to have a modern software engineering capability. The capability of a contractor to perform software engineering has been divided into three areas: 1. organization and resource management 2. software engine.)rlng process and its management 3. tools and technology. The qualities that the questions assess are different for each of these areas acnd are described in the introductions to the questions for each area. 093087 SEt Assessment Methodology A full assessment of software engineering capability' Includes some evaluation of the experience level of the software development personnel. Addendum A contains suggested questions for use In this evaluation.",Erratum,pro108
pap935,615909cc602358945c2fe6a48ef4037ac4a38798,con27,International Conference on Contemporary Computing,Software engineering risk management,"Welcome to Software Engineering Risk Management (SERIM). As a professional associated with the development of software, you are well aware that the software development process can truly be a jungle, filled with hazards that lie in wait to sabotage your projects. These hazards (risks) are numerous and often complex. The purpose of this application is to help you find a safer path through this jungle by assessing risk factors, analyzing risks from several different perspectives, and developing focused action plans to manage risks before they sabotage your projects. I have used the mathematics of probability to design the formulas to help you assess and manage risks in the complex software development environment (Complete information on the SERIM ModelOs equations is included in this application.)",Erratum,pro27
pap936,6d52e9f70c3495df590df52392aa892905e11682,con48,ACM Symposium on Applied Computing,Agent-oriented software engineering,"The ATAL workshops focus on the links between the theory and practice of intelligent agents. One aspect of this, which is steadily growing in importance, is the idea of agent technology as a software engineering paradigm. Previous ATAL workshops have had special tracks on programming languages for agent-oriented development, and methodologies for agent system development. ATAL-99 aims to build on this experience by focussing on the wider issues of agents as a software engineering paradigm.",Erratum,pro48
pap937,8427d8c6425fe00b7c66f383dabe0e69745bd1f1,jou158,Lecture Notes in Computer Science,Experimental Software Engineering Issues: Critical Assessment and Future Directions,,Article,vol158
pap938,c74b1a9ebb1ddfa1149722716e09f6d6d185d5aa,con100,International Conference on Automatic Face and Gesture Recognition,Process-centered Software Engineering Environments,"From the Publisher: 
Process-Centered Software Engineering Environments (PSEEs) represent a new generation of software engineering environments in which the processes used to produce and maintain software products are explicitly modeled in the environment. PSEEs hold the exciting promise of enabling a significant increase in both software productivity and quality. The book presents a comprehensive picture of this emerging technology while highlighting the key concepts and issues. The first chapter introduces some of the basic concepts and developments behind PSEEs and discusses the unifying role it plays in combining project management, software engineering, and process engineering. The second chapter reviews related process modeling and representation concepts, terminology, and issues. Chapter 3 analyzes the features of some example PSEEs and Chapter 4 takes an inside look at the implementation of these features by describing specific design choices made by researchers. The last chapter discusses the evolution of PSEEs to accommodate practical issues in actual work settings and to play a more significant role in the software life cycle. The text is a collection of influential papers that will bring the newcomer quickly up to speed on this fast-moving field. For the researcher, the issues described in the text present a challenge to be conquered and directions to pursue. For the practitioner, they represent benefits that may be gained in the application of PSEEs in the work environment.",Erratum,pro100
pap939,97613fe1ff2bf6883cfe80196c576eee10ef7ada,con4,Conference on Innovative Data Systems Research,"Component-based software engineering: technologies, development frameworks, and quality assurance schemes","Component-based software development approach is based on the idea to develop software systems by selecting appropriate off-the-shelf components and then to assemble them with a well-defined software architecture. Because the new software development paradigm is very different from the traditional approach, quality assurance (QA) for component-based software development is a new topic in the software engineering community. In this paper, we survey current component-based software technologies, describe their advantages and disadvantages, and discuss the features they inherit. We also address QA issues for component-based software. As a major contribution, we propose a QA model for component-based software which covers component requirement analysis, component development, component certification, component customization, and system architecture design, integration, testing and maintenance.",Erratum,pro4
pap940,be27ac5208ec96e21969e855c4a35e0ce6ce578b,jou151,IEEE Transactions on Software Engineering,Developing interactive information systems with the User Software Engineering methodology,"User software engineering (USE) is a methodology, supported by automated tools, for the systematic development of interactive information systems. The USE methodology gives particular attention to effective user involvement in the early stages of the software development process, concentrating on external design and the use of rapidly created and modified prototypes of the user interface. The USE methodology is supported by an integrated set of graphically based tools. The USE methodology and the tools that support it are described.",Conference paper,vol151
pap941,a4006f42f842f299a66b6613bf145367b62dd771,con72,Bioinformatics and Computational Biology,Cleanroom software engineering for zero-defect software,"Cleanroom software engineering is a theory-based, team-oriented process for developing very high quality software under statistical control. Cleanroom combines formal methods of object-based box structure specification and design, function-theoretic correctness verification, and statistical usage testing for quality certification to produce software that has zero defects with high probability. The process of cleanroom development and certification is carried out incrementally. Interface and design errors are rare because at each stage the harmonious operation of future increments at the next level of refinement is predefined by increments already in execution. The cleanroom process is being successfully applied in IBM and other applications. Quality results from several cleanroom projects are summarized.<<ETX>>",Erratum,pro72
pap942,5ff6034f558caafbe7a7aef80a42cd0fadd051e2,jou160,IEEE Software,An Experience in Collaborative Software Engineering Education,"Large-scale software development requires the interaction of specialists from different fields who must communicate their decisions and coordinate their activities. As global software development becomes mainstream, software engineers face new challenges for which they have received little or no training. To help a new generation of software developers better understand the industry's globalization and familiarize them with distributed, collaborative development, we designed a course entitled the Distributed Software Engineering Laboratory. In the class, pairs of students from different countries work as a virtual organization overseeing the whole software development process. We describe the lessons we have learned in this course and propose a framework useful in dealing with some of the difficulties participants face.",Conference paper,vol160
pap943,a8a9839b171c8ff0c27b4e53f7ca7abd5662d154,con12,The Compass,Software Engineering Project Management,"From the Publisher: 
The 2nd edition of Thayer's popular, bestselling book presents a top-down practical view of managing a successful software engineering project. The book builds on a framework for project managements activities based on the planning, organizing, staffing, directing, and controlling model. Thayer provides information designed to help readers understand and successfully perform the unique role of a project manager. 400 pp. Pub: 8/97.",Erratum,pro12
pap944,29131c413a3a49368b429865b0966b5daa257ae5,con6,Annual Conference on Genetic and Evolutionary Computation,Agent-Oriented Software Engineering for Internet Applications,,Erratum,pro6
pap945,be416b6798c1a3216105fb319fb160704fd6a770,con90,Computer Vision and Pattern Recognition,Towards a software engineering approach to Web site development,"The World Wide Web (WWW) has become ""the"" global infrastructure for delivering information and services. The demands and expectations of information providers and consumers are pushing WWW technology towards higher-level quality of presentation, including active contents and improved usability of the hypermedia distributed infrastructure. This technological evolution, however, is not supported by adequate Web design methodologies. Web site development is usually carried out without following a well-defined process and lacks suitable tool support. In addition, Web technologies are quite powerful but rather low-level and their semantics is often left largely unspecified. As a consequence, understanding the conceptual structure of a complex Web site and managing its evolution are complex and difficult tasks. The approach we advocate here is based on sound software engineering principles. The Web site development process goes through requirements analysis, design, and implementation in a high-level language. We define an object-oriented modeling framework, called WOOM, which provides constructs and abstractions for a high-level implementation of a Web site. An important feature of WOOM is that it clearly separates the data that are presented through the site from the context in which the user accesses such data. This feature not only enhances separation of concerns in the design stage, but also favors its subsequent evolution. The paper provides a view of the approach and of its current prototype implementation.",Erratum,pro90
pap946,9ba73feb6685945e88bebb19468b0177d4cae1a8,con26,Decision Support Systems,in Software Engineering,"Software engineers work on multidisciplinary teams to identify and develop software solutions and to maintain software intensive systems of all sizes. The focus of this program is on the rigorous engineering practices necessary to build, maintain, and protect modern software intensive systems. Consistent with this focus, the software engineering baccalaureate program consists of a rigorous curriculum of science, math, computer science, and software engineering courses.",Erratum,pro26
pap947,5bf3c1fa92728cd2f83885cc40ebea3b050f21da,con75,Intelligent Systems in Molecular Biology,"Software Engineering: Design, Reliability, and Management","Software engineering: design, reliability, and management , Software engineering: design, reliability, and management , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",Erratum,pro75
pap948,ebb97dbeae429f40767d07038541eb76e92ecb4b,con43,IEEE International Conference on Software Maintenance and Evolution,Automated Theorem Proving in Software Engineering,,Erratum,pro43
pap949,2e15acf6fcf62d034e52caffcc1b4b74c00c3d70,con62,Australian Software Engineering Conference,Formulation and preliminary test of an empirical theory of coordination in software engineering,"Motivated by evidence that coordination and dependencies among engineering decisions in a software project are key to better understanding and better methods of software creation, we set out to create empirically testable theory to characterize and make predictions about coordination of engineering decisions. We demonstrate that our theory is capable of expressing some of the main ideas about coordination in software engineering, such as Conway's law and the effects of information hiding in modular design. We then used software project data to create measures and test two hypotheses derived from our theory. Our results provide preliminary support for our formulations.",Erratum,pro62
pap950,a22c336bbd84abdb02de3b19f7dc549a0ee2d95f,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",A survey of Agent-Oriented Software Engineering,"Agent-Oriented Software Engineering is the one of the most recent contributions to the field of Software Engineering. It has several benefits compared to existing development approaches, in particular the ability to let agents represent high-level abstractions of active entities in a software system. This paper gives an overview of recent research and industrial applications of both general high-level methodologies and on more specific design methodologies for industry-strength software engineering.",Erratum,pro52
pap951,d9a427231e2912cdf1f4b43a1a89443ba5f219d2,jou181,Annals of Software Engineering,Software engineering programmes are not computer science programmes,,Article,vol181
pap952,50e47cc44bbf3c766f48e37ba640f94acaa11210,con57,International Workshop on Agent-Oriented Software Engineering,Patterns in Agent-Oriented Software Engineering,,Conference paper,pro57
pap953,2f92f51d26a6fcea9f3bc237990bc35e55f0995e,con2,International Conference on Software Engineering,Software engineering: a roadmap,"This paper provides a roadmap for software engineering. It 
identifies the principal research challenges being faced by 
the discipline and brings together the threads derived from 
the key research specialisations within software 
engineering. The paper draws heavily on the roadmaps 
covering specific areas of software engineering research 
collected in this volume.",Conference paper,pro2
pap954,36951d0cfb7aae0ef2ab77404e7d0dee75ac997e,con108,International Conference on Information Integration and Web-based Applications & Services,Software Pioneers: Contributions to Software Engineering,,Erratum,pro108
pap955,1a0e21c3bde997153a59f800ce2babe0779fffd3,con91,Symposium on the Theory of Computing,Automotive software engineering,"Information technology has become the driving force of innovation in many areas of technology and also in cars. Embedded software controls the functions of cars, supports and assists the driver and realizes systems for information and entertainment. Software in automobiles is today one of the great challenges for software engineering. On modem cars we find all issues of software systems in a nutshell. It is a challenge for software and systems engineering.",Erratum,pro91
pap956,84d7bb903cff3a748c92e7b67f1d0f35a5e04cd7,con1,International Conference on Human Factors in Computing Systems,Software engineering - a holistic view,"The software engineering a holistic view that we provide for you will be ultimate to give preference. This reading book is your chosen book to accompany you when in your free time, in your lonely. This kind of book can help you to heal the lonely and get or add the inspirations to be more inoperative. Yeah, book as the widow of the world can be very inspiring manners. As here, this book is also created by an inspiring author that can make influences of you to do more.",Erratum,pro1
pap957,4754b60280d2ced3d0e757226d416957ed144646,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Software engineering issues for ubiquitous computing,"In the last decade, we have experienced the advent of the paradigm of ubiquitous computing, with the goal of making computational services so pervasive throughout an environment that they become transparent to the human user. Research in ubiquitous computing raises many challenging issues for computer science in general, but successful research in ubiquitous computing requires the deployment of applications that can survive everyday use, and this in itself presents a great software engineering challenge. In our experience, we have found three features common across many ubiquitous computing applications-transparent interfaces that provide appropriate alternatives to the desktop bound traditional graphical user interface, the ability to modify behavior of a application based on knowledge of its context of use, and the ability to capture live experiences for later recall. Building ubiquitous computing applications with these features raises software engineering problems in toolkit design, software structuring for separation of concerns and component integration. We clarify these problems and discuss our approaches towards their solution.",Erratum,pro58
pap958,9941b0548c85c86e063c3b17d80077574ecfb726,con53,Workshop on Web 2.0 for Software Engineering,Object-oriented software engineering - practical software development using UML and Java,1. Software and Software Engineering. 2. Review of Object Orientation and Java. 3. Basing Software Development on Reusable Technology. 4. Developing Requirements. 5. Modelling with Classes. 6. Using Design Patterns. 7. Focusing on Users and Their Tasks. 8. Modelling Interactions and Behaviour. 9. Architecting and Designing Software. 10. Testing and Inspecting to Ensure High Quality. 11. Managing the Software Process. 12. Review. Appendix A: Summary of UML Notation used in this Book. Appendix B: Summary of the Documentation Formats Recommended in this Book. Appendix C: System Descriptions. Appendix D: Answers to Selected Exercises. Glossary. Index.,Erratum,pro53
pap959,1a706faa1effe9e161d124a68f9fcb1dcde31053,con1,International Conference on Human Factors in Computing Systems,A Mature Profession of Software Engineering.,"Abstract : A model is presented that allows the characterization of the maturity of a profession in terms of eight infrastructure components: initial professional education, accreditation, skills development, certification, licensing, professional development, a code of ethics, and a professional society. Several mature professions are examined to provide examples of the nature of these components. The current states of the components of software engineering are described, and predictions are made for the evolution of those components as the profession matures.",Erratum,pro1
pap960,91b866c4a949bb5de3fb75475d629566cc224204,con9,Big Data,Assessing process-centered software engineering environments,"Process-centered software engineering environments (PSEEs) are the most recent generation of environments supporting software development activities. They exploit an representation of the process (called the process model that specifies how to carry out software development activities, the roles and tasks of software developers, and how to use and control software development tools. A process model is therefore a vehicle to better understand and communicate the process. If it is expressed in a formal notation, it can be used to support a variety of activities such as process analysis, process simulation, and process enactment. PSEEs provide automatic support for these activities. They exploit languages based on different paradigms, such as Petri nets and rule-based systems. They include facilities to edit and analyze process models. By enacting the process model, a PSEE provides a variety of services, such as assistance for software developers, automation of routine tasks, invocation and control of software development tools, and enforcement of mandatory rules and practices. Several PSEEs have been developed, both as research projects and as commercial products. The initial deployment and exploitation of this technology have made it possible to produce a significant amount of experiences, comments, evaluations, and feedback. We still lack, however, consistent and comprehensive assessment methods that can be used to collect and organize this information. This article aims at contributing to the definition of such methods, by providing a systematic comparison grid and by accomplishing an initial evaluation of the state of the art in the field. This evaluation takes into account the systems that have been developed by the authors in the past five years, as well as the main characteristics of other well-known environments",Erratum,pro9
pap961,614ad6dac9a02863c47b434999c8a09ed5ed49f9,con96,Interspeech,Software engineering with ada,"Now, we come to offer you the right catalogues of book to open. software engineering with ada is one of the literary work in this world in suitable to be reading material. That's not only this book gives reference, but also it will show you the amazing benefits of reading a book. Developing your countless minds is needed; moreover you are kind of people with great curiosity. So, the book is very appropriate for you.",Erratum,pro96
pap962,2f470c6516cf4210f906410c14611dc03e51fe3c,con35,IEEE Working Conference on Mining Software Repositories,Computer-Aided Software Engineering in a distributed workstation environment,"Computer-Aided Software Engineering environments are becoming essential for complex software projects, just as CAD systems have become essential for complex hardware projects. DSEE, the DOMAIN Software Engineering Environment, is a distributed, production quality, software development environment that runs on Apollo workstations. DSEE provides source code control, configuration management, release control, advice management, task management, and user-defined dependency tracking with automatic notification.
 DSEE incorporates some of the best ideas from existing systems. This paper describes DSEE, contrasts it other systems, and discusses some of the technical issues involved in the construction of a highly-reliable, safe, efficient, and distributed development environment.",Erratum,pro35
pap963,27d062db677355d1f98d280e61ea8c068dcb0a0d,con48,ACM Symposium on Applied Computing,A Ranking of Software Engineering Measures Based on Expert Opinion,"This research proposes a framework based on expert opinion elicitation, developed to select the software engineering measures which are the best software reliability indicators. The current research is based on the top 30 measures identified in an earlier study conducted by Lawrence Livermore National Laboratory. A set of ranking criteria and their levels were identified. The score of each measure for each ranking criterion was elicited through expert opinion and then aggregated into a single score using multiattribute utility theory. The basic aggregation scheme selected was a linear additive scheme. A comprehensive sensitivity analysis was carried out. The sensitivity analysis included: variation of the ranking criteria levels, variation of the weights, variation of the aggregation schemes. The top-ranked measures were identified. Use of these measures in each software development phase can lead to a more reliable quantitative prediction of software reliability.",Erratum,pro48
pap964,7319d1e88da5cd1c707a71ce4cdc3e8c5c05b97c,con3,Knowledge Discovery and Data Mining,Reformulating software engineering as a search,"Metaheuristic techniques such as genecc algorithms, simulated annealing and tabu search have found wide application in most areas of engineering. These techniques have also been applied in business, financial and economic modelling. Metaheuristics have been applied to three areas of software engineering: test data generation, module clustering and cost/effort prediction, yet there remain many software engineering problems which have yet to be tackled using metaheuristics. It is surprising that metaheuristics have not been more widely applied to software engineering; many problems in software engineering are characterised by precisely the features which make metaheuristics search applicable. In the paper it is argued that the features which make metaheuristics applicable for engineering and business applications outside software engineering also suggest that there is great potential for the exploitation of metaheuristics within software engineering. The paper briefly reviews the principal metahenristic search techniques and surveys existing work on the application of metaheuristics to the three software engineering areas of test data generation, module clustering and cost/effort prediction. It also shows how metaheuristic search techniques can be applied to three additional areas of software engineering: maintenance/evolution system integration and requirements scheduling. The soft- ware engineering problem areas considered thus span the range of the software development process, from initial planning, cost estimation and requirements analysis through to integration, maintenance and evolution of legacy systems. The aim is to justify the claim that many problems in software engineering can be reformulated as search problems. to which metaheuristic techniques can be applied. The goal of the paper is to stimulate greater interest in metaheuristic search as a tool of optimisation of software engineering problems and to encourage the investigation and exploitation of these technologies in finding near optimal solutions to the complex constraint-based scenarios which arise so frequently in software engineering.",Erratum,pro3
pap965,ee225bb6b97ece25146f9cdfe0279370d79a72c9,con94,Vision,CASE productivity perceptions of software engineering professionals,Computer-aided software engineering (CASE) is moving into the problem-solving domain of the systems analyst. The authors undertook a study to investigate the various functional and behavioral aspects of CASE and determine the impact it has over manual methods of software engineering productivity.,Erratum,pro94
pap966,ae7e51e8255946b0801d060ec456c5c4d6813132,con96,Interspeech,Component Metadata for Software Engineering Tasks,,Erratum,pro96
pap967,c39fe2bade1123e7d6742e572e7261c84c12e601,con29,ACM-SIAM Symposium on Discrete Algorithms,Representing Software Engineering Models: The TAME Goal Oriented Approach,"A methodology and a knowledge representation and reasoning framework for top-down goal-oriented characterization, modeling, and execution of software engineering activities is presented. A prototype system (ES-TAME) which demonstrates the underlying knowledge representation and reasoning principles is described. ES-TAME provides an object-oriented metamodel concept that provides support for tailorable and reusable software engineering models (SEMs). It provides the basic mechanisms, functions, and attributes for all the other models. It is based on interobject relationships, dynamic viewpoints, and selective inheritance in addition to traditional object-oriented mechanisms. Descriptive SEMs include representations for basic software engineering activities. They are controlled and made operational by active GQM (goal-question-metric paradigm) models which are built by a systematic mechanism for defining and evaluating project and corporate goals and using measurement to provide feedback in real-time. >",Erratum,pro29
pap968,717163cd60fa89b405423bf319bcf5126a8275e2,jou181,Annals of Software Engineering,A stakeholder win–win approach to software engineering education,,Conference paper,vol181
pap969,f5d39747f15bb4476d3de01420326f9884d11ff3,con89,Conference on Uncertainty in Artificial Intelligence,Software engineering code of ethics,"T he Board of Governors of the IEEE Computer Society established a steering committee in May 1993 for evaluating, planning, and coordinating actions related to establishing software engineering as a profession. In that same year the ACM Council endorsed the establishment of a Commission on Software Engineering. By January 1994, both societies formed a joint steering committee “to establish the appropriate set(s) of standards for professional practice of software engineering upon which industrial decisions, professional certification, and educational curricula can be based.” To accomplish these tasks they made the following recommendations: ACM and the IEEE Computer Society join forces to create a code of professional practices within our industry. Now, we ask for your comments.",Erratum,pro89
pap970,a20f2ec6201f50ac3d0dfed57400f1b707dd951d,con68,Experimental Software Engineering Network,Practical Experiences in the Design and Conduct of Surveys in Empirical Software Engineering,,Conference paper,pro68
pap971,377cc56f8258a44cb7ab38172e8161ba69766650,con2,International Conference on Software Engineering,The software engineering laboratory - an operational software experience factory,"For 15 years, the Software Engineering Laboratory (SEL) has been carrying out studies and experiments for the purpose of understand- ing, assessing, and improving software and software processes within a production software development environment at the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). The SEL comprises three major organizations: NASA/GSFC, Flight Dynamics Division University of Maryland, Department of Computer Science Computer Sciences Corporation, Flight Dynamics Technology Group - These organizations have jointly carried out several hundred software studies, producing hundreds of reports, papers, and documents, all of which de scribe some aspect of the software engineering technology that has been analyzed in the flight dynamics environment at NASA. The studies range from small, controlled experiments (such as analyzing the effectiveness of code readingversus that of functional testing) tolarge, multiple- project studies (such as assessing the impacts of Ada on a production environment). The organization's driving goal is to improve the software process continually, so that sustained improvement may be observed in the resulting products. This paper discusses the SEL as a functioning example of an operational software experience factory and summarizes the characteristics of and major lessons learned from 15 years of SEL operations.",Article,pro2
pap972,6acda628eedf8317d0be2f4b4e6025c29ca05233,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Has twenty-five years of empirical software engineering made a difference?,"Our activities in software engineering typically fall into one of three categories, (1) to invent new phenomena, (2) to understand existing phenomena, and (3) to facilitate inspirational education. This paper explores the place of empirical software engineering in the first two of these activities. In this exploration evidence is drawn from the empirical literature in the areas of software inspections and software cost modelling and estimation. This research is then compared with the literature published in the Journal of Empirical Software Engineering. This evidence throws light on aspects of theory derivation, experimental methods and analysis, and also the challenges that we face as empirical software engineering evolves into the future.",Erratum,pro54
pap973,7fd3d0a1d17a134ad1bbbf753451c227477db1e5,con55,Workshop on Learning from Authoritative Security Experiment Results,Process Models in Software Engineering,"Software systems come and go through a series of passages that account for their inception, initial development, productive operation, upkeep, and retirement from one generation to another. This article categorizes and examines a number of methods for describing or modeling how software systems are developed. It begins with background and definitions of traditional software life cycle models that dominate most textbook discussions and current software development practices. This is followed by a more comprehensive review of the alternative models of software evolution that are of current use as the basis for organizing software engineering projects and technologies.",Erratum,pro55
pap974,193071cfbadc10719ee44ff4cb5f2982fb1ee467,con23,International Conference on Open and Big Data,Advances in Software Engineering and Knowledge Engineering,"The papers collected in this book were invited by the editors as tutorial courses or keynote speeches for the Fourth International Conference on Software Engineering and Knowledge Engineering. The book offers wide coverage of the main topics involved with the specifications, prototyping, development and maintenance of software systems and knowledge-based systems. The main issues in the area of software engineering and knowledge engineering are addressed and for each analyzed topic the corresponding research state is reported.",Erratum,pro23
pap975,ad32fcfaee0a8a42f47f431738cd89c1868c36ef,con2,International Conference on Software Engineering,Software engineering for real-time: a roadmap,"The next ten years will see distributed real-time computer systems replacing many mechanical and hydraulic control systems in high-dependability applications. In these applications a failure in the temporal domain can be as critical as a failure in the value domain. This paper discusses some of the technology trends that explain why distributed embedded real-time systems for highdependability applications will move into the mainstream. It then investigates the new requirements that must be addressed by the software engineering process. Two of the most important requirements are the design for composability and the systematic validation of highdependability distributed real-time systems. In the last two sections, these issues of composability and validation are treated in some detail.",Article,pro2
pap976,8e22c6ffc70744031171237c6471587a4331d660,jou171,Computer,Toward computer-supported concurrent software engineering,"An experimental software engineering environment called the flexible environment for collaborative software engineering (Flecse), which supports concurrent software engineering, is discussed. Flecse features tools designed to surmount collaboration problems that software engineers are increasingly encountering. The implementation of five important themes of concurrent software engineering in Flecse tools, concepts, life cycles, integration, and sharing, is examined.<<ETX>>",Conference paper,vol171
pap977,d14eaffd38f00a1c6c6018d1df093abfb536bb15,jou181,Annals of Software Engineering,Internet Software Engineering: A Different Class of Processes,,Letter,vol181
pap978,8b589bc4a9f00221c67e002cc8550cf8715fec09,con66,International Conference on Software Reuse,Outline of a Paradigm Change in Software Engineering,,Erratum,pro66
pap979,a71d66e4a6ef674ede43de952cbc90f26cb10cfd,jou181,Annals of Software Engineering,Experimental design and analysis in software engineering,,Article,vol181
pap980,eb545656fb93b04d9c1acae8a21207ea1fa95f5f,con75,Intelligent Systems in Molecular Biology,A survey of the relevance of computer science and software engineering education,"We describe a study of 168 software professionals to determine how relevant their education has been to their careers. Starting with a list of 57 topics, we asked the participants to indicate how much they learned in university, how much they know now, how useful the material has been and whether they would like to learn more. We conclude from the results that certain software engineering topics should be given more emphasis, while the emphasis on certain mathematics topics should be changed.",Erratum,pro75
pap981,3a33a419b902517a8ec4e8d8b1e32a0772742672,con95,IEEE International Conference on Computer Vision,A critique of diffusion theory as a managerial framework for understanding adoption of software engineering innovations,"The authors provide a brief overview of classical diffusion theory and suggest the potential applicability of this theory to problems related to predicting the adoption of technological innovations, including those related to software engineering. They critically evaluate the theory, identifying elements that must be extended and modified before it can be applied to technology transition, in general, and software engineering, specifically. They offer suggestions on ways in which these limitations might be overcome.<<ETX>>",Erratum,pro95
pap982,8e95c6989b2521760bd5a4dd08c65c803f92c62b,con4,Conference on Innovative Data Systems Research,Software Engineering Processes: Principles and Applications,Fundamentals of the Software Engineering Process Introduction A Unified Framework of the Software Engineering Process Process Algebra Process-Based Software Engineering Software Engineering Process System Modeling The CMM Model The ISO 9001 Model The BOOTSTRAP Model The ISO/IEC 15504 (SPICE) Model The Software Engineering Process Reference Model: SEPRM Software Engineering Process System Analysis Benchmarking the SEPRM Processes Comparative Analysis of Current Process Models Transformation of Capability Levels Between Current Process Models Software Engineering Process Establishment Software Process Establishment Methodologies An Extension of ISO/IEC TR 15504 Model Software Engineering Process Assessment Software Process Assessment Methodologies Software Process Assessment Supporting Tools Software Engineering Process Improvement Software Process Improvement Methodologies Case Studies in Software Process Improvement Review And Perspectives Bibliography Appendices Index,Erratum,pro4
pap983,6c658528361dce6ed8a3b2f7fc2411ca86c78cec,con89,Conference on Uncertainty in Artificial Intelligence,A flexible transaction model for software engineering,"It is generally recognized that the classical transaction model, providing atomicity and serializability, is too strong for certain application areas since it unnecessarily restricts concurrency. The author is concerned with supporting cooperative work in multiuser design environments, particularly teams of programmers cooperating to develop and maintain software systems. An extended transaction model that meets the special requirements of software engineering projects is presented, possible implementation techniques are described, and a number of issues regarding the incorporation of such a model into multiuser software development environments are discussed.<<ETX>>",Erratum,pro89
pap984,51c5b153032d9d4fe190dedcebc9546c7ffc46bd,con65,IEEE International Conference on Software Engineering and Formal Methods,Classical and Object Oriented Software Engineering,"From the Publisher: 
Classical and Object-Oriented Software Engineering is designed for an introductory software engineering course. This book provides an excellent introduction to software engineering fundamentals,covering both traditional and object-oriented techniques. 
Schach's unique organization and style makes it excellent for use in a classroom setting. It presents the underlying software engineering theory in Part I and follows it up with the more practical life-cycle material in Part II. Many software engineering books are more like reference books,which do not provide the appropriate fundamentals before inundating students with implementation details. 
In this edition,more practical material has been added to help students understand how to use what they are learning. This has been done through the use of ""How To"" boxes and greater implementation detail in the case study. Additionally,the new edition contains the references to the most current literature and includes an overview of extreme programmming. 
The website in this edition will be more extensive. It will include Solutions,PowerPoints that incorporate lecture notes,newly developed self-quiz questions,and source code for the term project and case study.",Erratum,pro65
pap985,dd096e01562e32aecb99a7cfa943af042866b562,con2,International Conference on Software Engineering,Software engineering tools and environments: a roadmap,"Tools and environments to aid developers in producing software have existed, in one form or another, since the early days of computer programming. They are becoming increasingly crucial as the demand for software increases, time-to-market decreases, and diversity and complexity grow beyond anything imagined a few decades ago. In this paper, we briefly review some of the history of tools and environments in software engineering, and then discuss some key challenges that we believe the field faces over the next decade.",Conference paper,pro2
pap986,ad7227d76df78c0807e4d14d0f7b18040ea091c2,con2,International Conference on Software Engineering,Database system support for software engineering,"The activity of Computer-Aided Software Engineering (CASE) generates much data. Although there are few, if any, database system products with features that attend to the requirements of CASE environments, there is a flurry of research work in the database field to develop technology to meet these requirements. The purpose of this paper is to raise the level of awareness in the software engineering community about this research on database systems for design applications. It describes technical problems and proposed solutions, and provides an extensive bibliography.",Article,pro2
pap987,cea798850b3331f0f37e0c763e3f22cc3d72ec9b,jou160,IEEE Software,Software Engineering Programs Are Not Computer Science Programs,"Software Engineering programs have become a source of contention in many universities. Computer Science departments, many of which have used that phrase to describe individual courses for decades, claim SE as part of their discipline. Yet some engineering faculties claim it as a new specialty among the engineering disciplines. This article discusses the differences between traditional CS programs and most engineering programs, and argues that we need SE programs that follow the traditional engineering approach to professional education.",Article,vol160
pap988,102a737e06904566914c52ee2f2cd182cb98542a,jou182,Proceedings of the IEEE,Measurement and experimentation in software engineering,"The contributions of measurement and experimentation to the state of the art in software engineering are reviewed. The role of measurement in developing theoretical models is discussed, and concerns for reliability and validity are stressed. Current approaches to measuring software characteristics are presented as examples. In particular, software complexity metrics related to control flow, module interconnectedness, and Halstead's Software Science are discussed. The use of experimental methods in evaluating cause-effect relationships is also discussed. Example programs of experimental research which investigated conditional statements and control flow are reviewed. The conclusion argues that many advances in software engineering will be related to improvements in the measurement and experimental evaluation of software techniques and practices.",Letter,vol182
pap989,edc534e2fb855269db5c6a202412171cd2c2a242,jou160,IEEE Software,Toward a Discipline of Software Engineering,"Despite rapid changes in computing and software development, some fundamental ideas have remained constant. This article describes eight such concepts that together constitute a viable foundation for a software engineering discipline: abstraction, analysis and design methods and notations, user interface prototyping, modularity and architecture, software life cycle and process, reuse, metrics, and automated support.",Article,vol160
pap990,ea747e976790e3e91eb1f1f293d48f09cc53e999,con2,International Conference on Software Engineering,The State Of Software Engineering Practice: A Preliminary Report,"This is the first in a series of SEI reports to provide periodic updates on the state of software engineering practice in the DoD software community. The SEI has developed, and is refining, a process framework and assessment methodology for characterizing the processes used by software organizations to develop and evolve software products. This report provides a brief overview of the process framework and assessment ap- proach, describes assessment results obtained to date, and dis- cusses implications of the current state of the practice for both customers and suppliers of DoD software.",Conference paper,pro2
pap991,b1bdae74be47faeeb04492c05ec0f4a75c25e430,con24,International Conference on Data Technologies and Applications,Agent orientation in software engineering,"Agent-Oriented Software Engineering (AOSE) is rapidly emerging in response to urgent needs in both software engineering and agent-based computing. While these two disciplines coexisted without remarkable interaction until some years ago, today there is rich and fruitful interaction among them and various approaches are available that bring together techniques, concepts and ideas from both sides. This article offers a guide to the broad body of literature on AOSE. The guide, which is intended to be of value to both researchers and practitioners, is structured according to key issues and key topics that arise when dealing with AOSE: methods and frameworks for requirements engineering, analysis, design, and implementation; languages for programming, communication and coordination and ontology specification; and development tools and platforms.",Erratum,pro24
pap992,54a26878f0c5fe82582466e7c788cf6ddf100599,con2,International Conference on Software Engineering,Using a behavioral theory of program comprehension in software engineering,"A theory is presented of how a programmer goes about understanding a program. The theory is based on a representation of knowledge about programs as a succession of knowledge domains which bridge between the problem domain and the executing program. A hypothesis and verify process is used by programmers to reconstruct these domains when they seek to understand a program.
 The theory is useful in several ways in software engineering: It makes accurate predictions about the effectiveness of documentation; it can be used to systematically evaluate and critique other claims about documentation, and it may even be a useful guideline to a programmer in actually constructing documentation.",Article,pro2
pap993,652d75d2571c7945cc378933f56797780b5b87c9,con100,International Conference on Automatic Face and Gesture Recognition,Modeling Articulation Work in Software Engineering Processes,"Current software process modeling techniques do not generally support articulation work. Articulation work is the diagnosis, recovery and resumption of development activities that unexpectedly fail. It is an integral part of software process enactment since software processes can sometimes fail or breakdown. This paper presents a knowledge-based model of articulation work in software engineering processes. I t uses empirically-grounded heuristics t o address three problems in articulation work: diagnosing failed development activities, determining appropriate recovery, and resuming software processes. We first investigate the role and importance of articulation work with respect t o planned software development activities. We then outline a knowledge-based model of articulation work. The model has been implemented in a knowledgebased software process modeling environment called the Articulator. Combining the available software process modeling techniques and the model of articulation leads to a better foundation in process improvement and evolution.",Erratum,pro100
pap994,f8efcdd2055e0772e5b1ba2132104fec96c4ec20,con110,Very Large Data Bases Conference,Computational intelligence in software engineering,"The paper provides a unified view of computational intelligence in the context of software engineering. Technologies such as fuzzy sets, neural and evolutionary computing useful in software development are considered. The links between software engineering and computational intelligence are identified. An illustration is given in terms of a fuzzy software quality model.",Erratum,pro110
pap995,9599aa996302c3ece477edc4122f0dee5e78f0ca,con43,IEEE International Conference on Software Maintenance and Evolution,Research synthesis in software engineering: a case for meta-analysis,"The use of meta-analytic techniques to summarize empirical software engineering research results is illustrated using a set of 5 published experiments from the literature. The intent of the analysis is to guide future work in this area through objective summarization of the literature to date. A focus on effect magnitude, in addition to statistical significance is championed, and the reader is provided with an illustration of simple methods for computing effect magnitudes.",Erratum,pro43
pap996,363677f867ae75808b5e1a9371b81bdfa648e726,con104,Biometrics and Identity Management,The software engineering impacts of cultural factors on multi-cultural software development teams,"This paper is based on our experiences in trying to apply software engineering practices to development projects staffed by developers from three distinct cultures; Japan, India, and the United States. The development of commercial software products has always been difficult. The standard balancing act that occurs between features, schedules, and resources is at the core of the difficulty. We found that cultural differences also had a large impact on our software engineering work Much has been written and said about software engineering methods that can be applied to development projects to reduce and control these core difficulties. Methods that were thought to be ""best practices"" turned out to be ineffective or very difficult to implement. Our understanding of the possible root causes for these difficulties greatly increased when we began to study some of the cultural dynamics within the team. This paper describes our observations in terms of how these cultural factors impacted the software engineering techniques used on the projects.",Erratum,pro104
pap997,40f3a037c0c75c7e11f02f97c284dc58f0e9fb61,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Perspectives in Software Engineering,"Software engineering refers to the process of creating software systems. It applies loosely to techniques which reduce high software cost and complexity while increasing reliability and mochfiability. This paper outlines the procedures used in the development of computer software, emphasizing large-scale software development, and pmpomtmg areas where problems exist and solutions have been proposed Solutions from both the management and the programmer points of vtew are then given for many of these problem areas.",Erratum,pro52
pap998,fe492c64031d000119298b656c61c672cfac1dd9,con92,Human Language Technology - The Baltic Perspectiv,Design rationale for software engineering: a survey,"The authors provide an introduction to design rationale and why it is important in software engineering. They look at the recent history of argumentation methods. They survey a number of the major systems developed for the support of design rationale, comparing their features and discussing their differences. They look at advantages and disadvantages of the various approaches to design rationale with special attention paid to how they can be used in the process software engineering. They conclude with a discussion of some open issues which are important for the inclusion of design rationale systems in the software engineering process.<<ETX>>",Erratum,pro92
pap999,51e5da45a734e8a6dae37e2fb0af7d7014d278b2,con46,Software Product Lines Conference,Value-based software engineering (VBSE),,Conference paper,pro46
pap1000,57e7a7323f58a35f5e2cc33bf17d4ac9cdcafdd4,jou183,Nature Protocols,Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources,,Article,vol183
pap1001,0517a517d92b56e99ea93f96db5c7e2bcb22ebeb,con51,Brazilian Symposium on Software Engineering,BIOINFORMATICS APPLICATIONS NOTE,"Motivation: Modern anatomical and developmental studies often require high-resolution imaging of large specimens in three dimensions (3D). Confocal microscopy produces high-resolution 3D images, but is limited by a relatively small ﬁeld of view compared with the size of large biological specimens. Therefore, motorized stages that move the sample are used to create a tiled scan of the whole specimen. The physical coordinates provided by the microscope stage are not precise enough to allow direct reconstruction (Stitching) of the whole image from individual image stacks. Results: To optimally stitch a large collection of 3D confocal images, we developed a method that, based on the Fourier Shift Theorem, computes all possible translations between pairs of 3D images, yielding the best overlap in terms of the cross-correlation measure and subsequently ﬁnds the globally optimal conﬁguration of the whole group of 3D images. This method avoids the propagation of errors by consecutive registration steps. Additionally, to compensate the brightness differences between tiles, we apply a smooth, non-linear intensity transition between the overlapping images. Our stitching approach is fast, works on 2D and 3D images, and for small image sets does not require prior knowledge about the tile conﬁguration. Availability: The implementation of this method is available as an ImageJ plugin distributed as a part of the Fiji project ( F iji i s j ust I mageJ : http://paciﬁc.mpi-cbg.de/).",Erratum,pro51
pap1002,fd495d6cf7c3169bc58550fdf32be6e16e2800f8,jou5,Genome Biology,Bioconductor: open software development for computational biology and bioinformatics,,Conference paper,vol5
pap1003,fa60b6806050255a77699bd0f9f5d824884c5162,jou106,Nucleic Acids Research,Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists,"Functional analysis of large gene lists, derived in most cases from emerging high-throughput genomic, proteomic and bioinformatics scanning approaches, is still a challenging and daunting task. The gene-annotation enrichment analysis is a promising high-throughput strategy that increases the likelihood for investigators to identify biological processes most pertinent to their study. Approximately 68 bioinformatics enrichment tools that are currently available in the community are collected in this survey. Tools are uniquely categorized into three major classes, according to their underlying enrichment algorithms. The comprehensive collections, unique tool classifications and associated questions/issues will provide a more comprehensive and up-to-date view regarding the advantages, pitfalls and recent trends in a simpler tool-class level rather than by a tool-by-tool approach. Thus, the survey will help tool designers/developers and experienced end users understand the underlying algorithms and pertinent details of particular tool categories/tools, enabling them to make the best choices for their particular research interests.",Conference paper,vol106
pap1004,90485e0ce54c1ad12a2d01362a007ab107d71063,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Biopython: freely available Python tools for computational molecular biology and bioinformatics,"Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning. Availability: Biopython is freely available, with documentation and source code at www.biopython.org under the Biopython license. Contact: All queries should be directed to the Biopython mailing lists, see www.biopython.org/wiki/_Mailing_listspeter.cock@scri.ac.uk.",Erratum,pro82
pap1005,7b1a7f8fcab2d1671cd907c9bafa81cb784bac1c,con69,Formal Concept Analysis,"Introducing the Bacterial and Viral Bioinformatics Resource Center (BV-BRC): a resource combining PATRIC, IRD and ViPR","The National Institute of Allergy and Infectious Diseases (NIAID) established the Bioinformatics Resource Center (BRC) program to assist researchers with analyzing the growing body of genome sequence and other omics-related data. In this report, we describe the merger of the PAThosystems Resource Integration Center (PATRIC), the Influenza Research Database (IRD) and the Virus Pathogen Database and Analysis Resource (ViPR) BRCs to form the Bacterial and Viral Bioinformatics Resource Center (BV-BRC) https://www.bv-brc.org/. The combined BV-BRC leverages the functionality of the bacterial and viral resources to provide a unified data model, enhanced web-based visualization and analysis tools, bioinformatics services, and a powerful suite of command line tools that benefit the bacterial and viral research communities.",Erratum,pro69
pap1006,06aad24ac4ad4312ca190d0c16fe0df7f474844c,con60,European Conference on Software Process Improvement,BMC Bioinformatics Methodology article Statistical significance for hierarchical clustering in genetic association and microarray expression studies,,Erratum,pro60
pap1007,2fabff697cd0ec343490f89e8e6587200d97f378,con23,International Conference on Open and Big Data,"Sangerbox: A comprehensive, interaction‐friendly clinical bioinformatics analysis platform",,Erratum,pro23
pap1008,63c4adf203e2e0f9895ff82485a0a701cf3cb650,jou0,Nature Biotechnology,The nf-core framework for community-curated bioinformatics pipelines,,Letter,vol0
pap1009,98c25683fc8d6446448b734b1bcf08e1457f8d85,con66,International Conference on Software Reuse,A review of feature selection techniques in bioinformatics,"Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.",Erratum,pro66
pap1010,7389b6ebbf36f4d869a02e305e2ef52ad2c92264,con9,Big Data,Applications of transformer-based language models in bioinformatics: a survey,"Abstract Summary The transformer-based language models, including vanilla transformer, BERT and GPT-3, have achieved revolutionary breakthroughs in the field of natural language processing (NLP). Since there are inherent similarities between various biological sequences and natural languages, the remarkable interpretability and adaptability of these models have prompted a new wave of their application in bioinformatics research. To provide a timely and comprehensive review, we introduce key developments of transformer-based language models by describing the detailed structure of transformers and summarize their contribution to a wide range of bioinformatics research from basic sequence analysis to drug discovery. While transformer-based applications in bioinformatics are diverse and multifaceted, we identify and discuss the common challenges, including heterogeneity of training data, computational expense and model interpretability, and opportunities in the context of bioinformatics research. We hope that the broader community of NLP researchers, bioinformaticians and biologists will be brought together to foster future research and development in transformer-based language models, and inspire novel bioinformatics applications that are unattainable by traditional methods. Supplementary information Supplementary data are available at Bioinformatics Advances online.",Erratum,pro9
pap1011,1ff4bd599b950218f0517fb76ee49ad0599e1c53,jou184,Journal of Molecular Biology,A Completely Reimplemented MPI Bioinformatics Toolkit with a New HHpred Server at its Core.,,Article,vol184
pap1012,eb41f630c7fbd1b41a4fbb41dc4b1bffa8e1cf95,con11,European Conference on Modelling and Simulation,BIOINFORMATICS ORIGINAL PAPER,"Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A ﬁrst generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. Results: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows–Wheeler Transform (BWT), to efﬁciently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ∼ 10–20 × faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.",Erratum,pro11
pap1013,625bdb87e5750faf96fd0c59cb274ff9dda9cbb3,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Snakemake - a scalable bioinformatics workflow engine,,Erratum,pro13
pap1014,a41d8c4eddf4054ef080c7edec21b39c492892ee,jou0,Nature Biotechnology,"Nanopore sequencing technology, bioinformatics and applications",,Conference paper,vol0
pap1015,8002fefd7bcc66c23a8d49aa9a7ae8d4a9885ad3,con5,Technical Symposium on Computer Science Education,"Expasy, the Swiss Bioinformatics Resource Portal, as designed by its users","Abstract The SIB Swiss Institute of Bioinformatics (https://www.sib.swiss) creates, maintains and disseminates a portfolio of reliable and state-of-the-art bioinformatics services and resources for the storage, analysis and interpretation of biological data. Through Expasy (https://www.expasy.org), the Swiss Bioinformatics Resource Portal, the scientific community worldwide, freely accesses more than 160 SIB resources supporting a wide range of life science and biomedical research areas. In 2020, Expasy was redesigned through a user-centric approach, known as User-Centred Design (UCD), whose aim is to create user interfaces that are easy-to-use, efficient and targeting the intended community. This approach, widely used in other fields such as marketing, e-commerce, and design of mobile applications, is still scarcely explored in bioinformatics. In total, around 50 people were actively involved, including internal stakeholders and end-users. In addition to an optimised interface that meets users' needs and expectations, the new version of Expasy provides an up-to-date and accurate description of high-quality resources based on a standardised ontology, allowing to connect functionally-related resources.",Erratum,pro5
pap1016,c684ff184c369265d57ccbb1b7c220d27396cc11,con28,International Conference Geographic Information Science,"VEuPathDB: the eukaryotic pathogen, vector and host bioinformatics resource center","Abstract The Eukaryotic Pathogen, Vector and Host Informatics Resource (VEuPathDB, https://veupathdb.org) represents the 2019 merger of VectorBase with the EuPathDB projects. As a Bioinformatics Resource Center funded by the National Institutes of Health, with additional support from the Welllcome Trust, VEuPathDB supports >500 organisms comprising invertebrate vectors, eukaryotic pathogens (protists and fungi) and relevant free-living or non-pathogenic species or hosts. Designed to empower researchers with access to Omics data and bioinformatic analyses, VEuPathDB projects integrate >1700 pre-analysed datasets (and associated metadata) with advanced search capabilities, visualizations, and analysis tools in a graphic interface. Diverse data types are analysed with standardized workflows including an in-house OrthoMCL algorithm for predicting orthology. Comparisons are easily made across datasets, data types and organisms in this unique data mining platform. A new site-wide search facilitates access for both experienced and novice users. Upgraded infrastructure and workflows support numerous updates to the web interface, tools, searches and strategies, and Galaxy workspace where users can privately analyse their own data. Forthcoming upgrades include cloud-ready application architecture, expanded support for the Galaxy workspace, tools for interrogating host-pathogen interactions, and improved interactions with affiliated databases (ClinEpiDB, MicrobiomeDB) and other scientific resources, and increased interoperability with the Bacterial & Viral BRC.",Erratum,pro28
pap1017,7d7735582cfa14efb00d967e9af4d725579e8746,con107,Chinese Conference on Biometric Recognition,The PATRIC Bioinformatics Resource Center: expanding data and analysis capabilities,"The PathoSystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center funded by the National Institute of Allergy and Infectious Diseases (https://www.patricbrc.org). PATRIC supports bioinformatic analyses of all bacteria with a special emphasis on pathogens, offering a rich comparative analysis environment that provides users with access to over 250 000 uniformly annotated and publicly available genomes with curated metadata. PATRIC offers web-based visualization and comparative analysis tools, a private workspace in which users can analyze their own data in the context of the public collections, services that streamline complex bioinformatic workflows and command-line tools for bulk data analysis. Over the past several years, as genomic and other omics-related experiments have become more cost-effective and widespread, we have observed considerable growth in the usage of and demand for easy-to-use, publicly available bioinformatic tools and services. Here we report the recent updates to the PATRIC resource, including new web-based comparative analysis tools, eight new services and the release of a command-line interface to access, query and analyze data.",Erratum,pro107
pap1018,766d3c1f67728737a255bd88e272f2bacb92d6e9,con92,Human Language Technology - The Baltic Perspectiv,Protein Sequence Analysis Using the MPI Bioinformatics Toolkit,"The MPI Bioinformatics Toolkit (https://toolkit.tuebingen.mpg.de) provides interactive access to a wide range of the best‐performing bioinformatics tools and databases, including the state‐of‐the‐art protein sequence comparison methods HHblits and HHpred. The Toolkit currently includes 35 external and in‐house tools, covering functionalities such as sequence similarity searching, prediction of sequence features, and sequence classification. Due to this breadth of functionality, the tight interconnection of its constituent tools, and its ease of use, the Toolkit has become an important resource for biomedical research and for teaching protein sequence analysis to students in the life sciences. In this article, we provide detailed information on utilizing the three most widely accessed tools within the Toolkit: HHpred for the detection of homologs, HHpred in conjunction with MODELLER for structure prediction and homology modeling, and CLANS for the visualization of relationships in large sequence datasets. © 2020 The Authors.",Erratum,pro92
pap1019,2bb698b9c5dc6647943eaa7240b4edbe3f93aab1,con72,Bioinformatics and Computational Biology,Structural Bioinformatics,,Article,pro72
pap1020,0ff76dd78e47f4534ee148b644f1f2707bc70df5,con74,IEEE International Conference on Information Reuse and Integration,"Improvements to PATRIC, the all-bacterial Bioinformatics Database and Analysis Resource Center","The Pathosystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center (https://www.patricbrc.org). Recent changes to PATRIC include a redesign of the web interface and some new services that provide users with a platform that takes them from raw reads to an integrated analysis experience. The redesigned interface allows researchers direct access to tools and data, and the emphasis has changed to user-created genome-groups, with detailed summaries and views of the data that researchers have selected. Perhaps the biggest change has been the enhanced capability for researchers to analyze their private data and compare it to the available public data. Researchers can assemble their raw sequence reads and annotate the contigs using RASTtk. PATRIC also provides services for RNA-Seq, variation, model reconstruction and differential expression analysis, all delivered through an updated private workspace. Private data can be compared by ‘virtual integration’ to any of PATRIC's public data. The number of genomes available for comparison in PATRIC has expanded to over 80 000, with a special emphasis on genomes with antimicrobial resistance data. PATRIC uses this data to improve both subsystem annotation and k-mer classification, and tags new genomes as having signatures that indicate susceptibility or resistance to specific antibiotics.",Erratum,pro74
pap1021,028539afb9c5f115163fff3500e266fb29a7d8ee,con107,Chinese Conference on Biometric Recognition,BMC Bioinformatics BioMed Central,,Erratum,pro107
pap1022,c67f7d629c4500bd47056b03bdfaf2df2f3c9346,jou106,Nucleic Acids Research,Human Splicing Finder: an online bioinformatics tool to predict splicing signals,"Thousands of mutations are identified yearly. Although many directly affect protein expression, an increasing proportion of mutations is now believed to influence mRNA splicing. They mostly affect existing splice sites, but synonymous, non-synonymous or nonsense mutations can also create or disrupt splice sites or auxiliary cis-splicing sequences. To facilitate the analysis of the different mutations, we designed Human Splicing Finder (HSF), a tool to predict the effects of mutations on splicing signals or to identify splicing motifs in any human sequence. It contains all available matrices for auxiliary sequence prediction as well as new ones for binding sites of the 9G8 and Tra2-β Serine-Arginine proteins and the hnRNP A1 ribonucleoprotein. We also developed new Position Weight Matrices to assess the strength of 5′ and 3′ splice sites and branch points. We evaluated HSF efficiency using a set of 83 intronic and 35 exonic mutations known to result in splicing defects. We showed that the mutation effect was correctly predicted in almost all cases. HSF could thus represent a valuable resource for research, diagnostic and therapeutic (e.g. therapeutic exon skipping) purposes as well as for global studies, such as the GEN2PHEN European Project or the Human Variome Project.",Article,vol106
pap1023,cc384cff27d8a609f89f9e915c26bf31c39749a1,con0,International Conference on Machine Learning,Deep learning in bioinformatics,"In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.",Erratum,pro0
pap1024,19fd00f8540a5728a21593b2e62e4f9a8abf74d6,jou75,Frontiers in Genetics,Graph Neural Networks and Their Current Applications in Bioinformatics,"Graph neural networks (GNNs), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, GNNs have also become an important tool in bioinformatics. In this research, a systematic survey of GNNs and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used GNN models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by GNNs: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although GNNs have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that GNNs are potentially an excellent method that solves various biological problems in bioinformatics research.",Article,vol75
pap1025,c21252cac5b6c33012d2a071433f79ab0beeb0fb,con108,International Conference on Information Integration and Web-based Applications & Services,"BIOINFORMATICS Bioinformatics Advance Access published November 12, 2005 The SWISS-MODEL Workspace: A web-based environment for protein structure homology modelling",,Erratum,pro108
pap1026,f7cc8ded65046ba1364e16d8318d8bf5a128f2b2,con107,Chinese Conference on Biometric Recognition,Bioinformatics and the Metaverse: Are We Ready?,"COVID-19 forced humanity to think about new ways of working globally without physically being present with other people, and eXtended Reality (XR) systems (defined as Virtual Reality, Augmented Reality and Mixed Reality) offer a potentially elegant solution. Previously seen as mainly for gaming, commercial and research institutions are investigating XR solutions to solve real world problems from training, simulation, mental health, data analysis, and studying disease progression. More recently large corporations such as Microsoft and Meta have announced they are developing the Metaverse as a new paradigm to interact with the digital world. This article will look at how visualization can leverage the Metaverse in bioinformatics research, the pros and cons of this technology, and what the future may hold.",Erratum,pro107
pap1027,bc54798db4962ec9ba6a5e87be3dce156454cfef,jou185,Journal of Open Source Software,Augur: a bioinformatics toolkit for phylogenetic analyses of human pathogens,"Summary and statement of need The analysis of human pathogens requires a diverse collection of bioinformatics tools. These tools include standard genomic and phylogenetic software and custom software developed to handle the relatively numerous and short genomes of viruses and bacteria. Researchers increasingly depend on the outputs of these tools to infer transmission dynamics of human diseases and make actionable recommendations to public health officials (Black et al., 2020; Gardy et al., 2015). In order to enable real-time analyses of pathogen evolution, bioinformatics tools must scale rapidly with the number of samples and be flexible enough to adapt to a variety of questions and organisms. To meet these needs, we developed Augur, a bioinformatics toolkit designed for phylogenetic analyses of human pathogens.",Article,vol185
pap1028,13226c692dd6908c64555fb095c4ee968a539a83,con96,Interspeech,Trends in the development of miRNA bioinformatics tools,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that regulate gene expression via recognition of cognate sequences and interference of transcriptional, translational or epigenetic processes. Bioinformatics tools developed for miRNA study include those for miRNA prediction and discovery, structure, analysis and target prediction. We manually curated 95 review papers and ∼1000 miRNA bioinformatics tools published since 2003. We classified and ranked them based on citation number or PageRank score, and then performed network analysis and text mining (TM) to study the miRNA tools development trends. Five key trends were observed: (1) miRNA identification and target prediction have been hot spots in the past decade; (2) manual curation and TM are the main methods for collecting miRNA knowledge from literature; (3) most early tools are well maintained and widely used; (4) classic machine learning methods retain their utility; however, novel ones have begun to emerge; (5) disease-associated miRNA tools are emerging. Our analysis yields significant insight into the past development and future directions of miRNA tools.",Erratum,pro96
pap1029,219bee9fde1303f423fac52ff373f19552c60d53,jou186,Experimental and Molecular Medicine,Single-cell RNA sequencing technologies and bioinformatics pipelines,,Article,vol186
pap1030,5c511ed1011c42a92fe1c4c5fe1fbc7190a5fde9,jou187,Nature reviews genetics,Piercing the dark matter: bioinformatics of long-range sequencing and mapping,,Letter,vol187
pap1031,d68656133b19d0cae8fbd5d6c032445050f2b068,con7,International Symposium on Intelligent Data Analysis,IEEE International Conference on Bioinformatics and Biomedicine ANALYSIS OF MULTIPLEX GENE EXPRESSION MAPS OBTAINED BY VOXELATION,,Erratum,pro7
pap1032,265da04a678d6a857c43d30761027d093633e5f2,con97,ACM SIGMOD Conference,"Graph representation learning in bioinformatics: trends, methods and applications","Graph is a natural data structure for describing complex systems, which contains a set of objects and relationships. Ubiquitous real-life biomedical problems can be modeled as graph analytics tasks. Machine learning, especially deep learning, succeeds in vast bioinformatics scenarios with data represented in Euclidean domain. However, rich relational information between biological elements is retained in the non-Euclidean biomedical graphs, which is not learning friendly to classic machine learning methods. Graph representation learning aims to embed graph into a low-dimensional space while preserving graph topology and node properties. It bridges biomedical graphs and modern machine learning methods and has recently raised widespread interest in both machine learning and bioinformatics communities. In this work, we summarize the advances of graph representation learning and its representative applications in bioinformatics. To provide a comprehensive and structured analysis and perspective, we first categorize and analyze both graph embedding methods (homogeneous graph embedding, heterogeneous graph embedding, attribute graph embedding) and graph neural networks. Furthermore, we summarize their representative applications from molecular level to genomics, pharmaceutical and healthcare systems level. Moreover, we provide open resource platforms and libraries for implementing these graph representation learning methods and discuss the challenges and opportunities of graph representation learning in bioinformatics. This work provides a comprehensive survey of emerging graph representation learning algorithms and their applications in bioinformatics. It is anticipated that it could bring valuable insights for researchers to contribute their knowledge to graph representation learning and future-oriented bioinformatics studies.",Erratum,pro97
pap1033,740c502c5596a570abfcffc1eefea160c2d6112c,jou147,Scientific Reports,BATMAN-TCM: a Bioinformatics Analysis Tool for Molecular mechANism of Traditional Chinese Medicine,,Article,vol147
pap1034,d0da9ce3ca989bce2579b64be9aed518265a8994,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Bioinformatics and Computational Biology Solutions Using R and Bioconductor,,Erratum,pro52
pap1035,3d4c709be74c6926b220f4cf7d8adf50082f3886,con66,International Conference on Software Reuse,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,Letter,pro66
pap1036,1e05e8c73a099b7ce11d4764706ba657c0e0d37b,con40,Conference on Software Engineering Education and Training,The bioinformatics toolbox for circRNA discovery and analysis,"Abstract Circular RNAs (circRNAs) are a unique class of RNA molecule identified more than 40 years ago which are produced by a covalent linkage via back-splicing of linear RNA. Recent advances in sequencing technologies and bioinformatics tools have led directly to an ever-expanding field of types and biological functions of circRNAs. In parallel with technological developments, practical applications of circRNAs have arisen including their utilization as biomarkers of human disease. Currently, circRNA-associated bioinformatics tools can support projects including circRNA annotation, circRNA identification and network analysis of competing endogenous RNA (ceRNA). In this review, we collected about 100 circRNA-associated bioinformatics tools and summarized their current attributes and capabilities. We also performed network analysis and text mining on circRNA tool publications in order to reveal trends in their ongoing development.",Erratum,pro40
pap1037,4050c80ba36c6f86b0684a59be70182557b0770c,jou188,Nature Machine Intelligence,Ensemble deep learning in bioinformatics,,Letter,vol188
pap1038,12467886cc2ab7aa5b0108001144ffaa3e67ded0,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Deep learning-based clustering approaches for bioinformatics,"Abstract Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.",Erratum,pro73
pap1039,fe540ba862a10e06c8b75b751745e8b2d6161c73,con93,International Conference on Computational Logic,Perseus: A Bioinformatics Platform for Integrative Analysis of Proteomics Data in Cancer Research.,,Erratum,pro93
pap1040,32dc2a2ea7d1735b3c0cf5f782200453215fe6cd,con35,IEEE Working Conference on Mining Software Repositories,The EMBL-EBI bioinformatics web and programmatic tools framework,"Since 2009 the EMBL-EBI Job Dispatcher framework has provided free access to a range of mainstream sequence analysis applications. These include sequence similarity search services (https://www.ebi.ac.uk/Tools/sss/) such as BLAST, FASTA and PSI-Search, multiple sequence alignment tools (https://www.ebi.ac.uk/Tools/msa/) such as Clustal Omega, MAFFT and T-Coffee, and other sequence analysis tools (https://www.ebi.ac.uk/Tools/pfa/) such as InterProScan. Through these services users can search mainstream sequence databases such as ENA, UniProt and Ensembl Genomes, utilising a uniform web interface or systematically through Web Services interfaces (https://www.ebi.ac.uk/Tools/webservices/) using common programming languages, and obtain enriched results with novel visualisations. Integration with EBI Search (https://www.ebi.ac.uk/ebisearch/) and the dbfetch retrieval service (https://www.ebi.ac.uk/Tools/dbfetch/) further expands the usefulness of the framework. New tools and updates such as NCBI BLAST+, InterProScan 5 and PfamScan, new categories such as RNA analysis tools (https://www.ebi.ac.uk/Tools/rna/), new databases such as ENA non-coding, WormBase ParaSite, Pfam and Rfam, and new workflow methods, together with the retirement of depreciated services, ensure that the framework remains relevant to today's biological community.",Erratum,pro35
pap1041,3f7c26ec2a6a02123d34f9aaf278f698110b20b0,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",Venn diagrams in bioinformatics,"Venn diagrams are widely used tools for graphical depiction of the unions, intersections and distinctions among multiple datasets, and a large number of programs have been developed to generate Venn diagrams for applications in various research areas. However, a comprehensive review comparing these tools has not been previously performed. In this review, we collect Venn diagram generators (i.e. tools for visualizing the relationships of input lists within a Venn diagram) and Venn diagram application tools (i.e. tools for analyzing the relationships between biological data and visualizing them in a Venn diagram) to compare their functional capacity as follows: ability to generate high-quality diagrams; maximum datasets handled by each program; input data formats; output diagram styles and image output formats. We also evaluate the picture beautification parameters of the Venn diagram generators in terms of the graphical layout and briefly describe the functional characteristics of the most popular Venn diagram application tools. Finally, we discuss the challenges in improving Venn diagram application tools and provide a perspective on Venn diagram applications in bioinformatics. Our aim is to assist users in selecting suitable tools for analyzing and visualizing user-defined datasets.",Erratum,pro87
pap1042,013e313763388dd82a916c890a070aea5162b24c,con110,Very Large Data Bases Conference,BMC Bioinformatics BioMed Central Methodology article,,Erratum,pro110
pap1043,6966d2b49fa885897e0b2e18ce144831c55f3e5f,jou88,bioRxiv,"Deep learning in bioinformatics: introduction, application, and perspective in big data era","Deep learning, which is especially formidable in handling big data, has achieved great success in various fields, including bioinformatics. With the advances of the big data era in biology, it is foreseeable that deep learning will become increasingly important in the field and will be incorporated in vast majorities of analysis pipelines. In this review, we provide both the exoteric introduction of deep learning, and concrete examples and implementations of its representative applications in bioinformatics. We start from the recent achievements of deep learning in the bioinformatics field, pointing out the problems which are suitable to use deep learning. After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures. After that, we provide eight examples, covering five bioinformatics research directions and all the four kinds of data type, with the implementation written in Tensorflow and Keras. Finally, we discuss the common issues, such as overfitting and interpretability, that users will encounter when adopting deep learning methods and provide corresponding suggestions. The implementations are freely available at https://github.com/lykaust15/Deep_learning_examples.",Conference paper,vol88
pap1044,bfada4dd6e30ed53835b0ab326f16140922ebf31,con89,Conference on Uncertainty in Artificial Intelligence,ExPASy: SIB bioinformatics resource portal,"ExPASy (http://www.expasy.org) has worldwide reputation as one of the main bioinformatics resources for proteomics. It has now evolved, becoming an extensible and integrative portal accessing many scientific resources, databases and software tools in different areas of life sciences. Scientists can henceforth access seamlessly a wide range of resources in many different domains, such as proteomics, genomics, phylogeny/evolution, systems biology, population genetics, transcriptomics, etc. The individual resources (databases, web-based and downloadable software tools) are hosted in a ‘decentralized’ way by different groups of the SIB Swiss Institute of Bioinformatics and partner institutions. Specifically, a single web portal provides a common entry point to a wide range of resources developed and operated by different SIB groups and external institutions. The portal features a search function across ‘selected’ resources. Additionally, the availability and usage of resources are monitored. The portal is aimed for both expert users and people who are not familiar with a specific domain in life sciences. The new web interface provides, in particular, visual guidance for newcomers to ExPASy.",Erratum,pro89
pap1045,424b9225684cf54ef6d3939e2a4d7514bc90e67e,jou189,Protein Science,The Bio3D packages for structural bioinformatics,"Bio3D is a family of R packages for the analysis of biomolecular sequence, structure, and dynamics. Major functionality includes biomolecular database searching and retrieval, sequence and structure conservation analysis, ensemble normal mode analysis, protein structure and correlation network analysis, principal component, and related multivariate analysis methods. Here, we review recent package developments, including a new underlying segregation into separate packages for distinct analysis, and introduce a new method for structure analysis named ensemble difference distance matrix analysis (eDDM). The eDDM approach calculates and compares atomic distance matrices across large sets of homologous atomic structures to help identify the residue wise determinants underlying specific functional processes. An eDDM workflow is detailed along with an example application to a large protein family. As a new member of the Bio3D family, the Bio3D‐eddm package supports both experimental and theoretical simulation‐generated structures, is integrated with other methods for dissecting sequence‐structure–function relationships, and can be used in a highly automated and reproducible manner. Bio3D is distributed as an integrated set of platform independent open source R packages available from: http://thegrantlab.org/bio3d/.",Conference paper,vol189
pap1046,fef2bd94ea0ece037c7f983b9d51a6a305482f07,jou190,International Journal of Molecular Sciences,Bioinformatics Methods for Mass Spectrometry-Based Proteomics Data Analysis,"Recent advances in mass spectrometry (MS)-based proteomics have enabled tremendous progress in the understanding of cellular mechanisms, disease progression, and the relationship between genotype and phenotype. Though many popular bioinformatics methods in proteomics are derived from other omics studies, novel analysis strategies are required to deal with the unique characteristics of proteomics data. In this review, we discuss the current developments in the bioinformatics methods used in proteomics and how they facilitate the mechanistic understanding of biological processes. We first introduce bioinformatics software and tools designed for mass spectrometry-based protein identification and quantification, and then we review the different statistical and machine learning methods that have been developed to perform comprehensive analysis in proteomics studies. We conclude with a discussion of how quantitative protein data can be used to reconstruct protein interactions and signaling networks.",Letter,vol190
pap1047,80e86c8ca16da71d7ff279619d7b72c82a14f564,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,A new bioinformatics analysis tools framework at EMBL–EBI,"The EMBL-EBI provides access to various mainstream sequence analysis applications. These include sequence similarity search services such as BLAST, FASTA, InterProScan and multiple sequence alignment tools such as ClustalW, T-Coffee and MUSCLE. Through the sequence similarity search services, the users can search mainstream sequence databases such as EMBL-Bank and UniProt, and more than 2000 completed genomes and proteomes. We present here a new framework aimed at both novice as well as expert users that exposes novel methods of obtaining annotations and visualizing sequence analysis results through one uniform and consistent interface. These services are available over the web and via Web Services interfaces for users who require systematic access or want to interface with customized pipe-lines and workflows using common programming languages. The framework features novel result visualizations and integration of domain and functional predictions for protein database searches. It is available at http://www.ebi.ac.uk/Tools/sss for sequence similarity searches and at http://www.ebi.ac.uk/Tools/msa for multiple sequence alignments.",Erratum,pro58
pap1048,671fdddfa9debef365e19d0e2b61a8b6ca7b451e,jou49,ACM Computing Surveys,Homomorphic Encryption for Machine Learning in Medicine and Bioinformatics,"Machine learning and statistical techniques are powerful tools for analyzing large amounts of medical and genomic data. On the other hand, ethical concerns and privacy regulations prevent free sharing of this data. Encryption techniques such as fully homomorphic encryption (FHE) enable evaluation over encrypted data. Using FHE, machine learning models such as deep learning, decision trees, and Naive Bayes have been implemented for privacy-preserving applications using medical data. These applications include classifying encrypted data and training models on encrypted data. FHE has also been shown to enable secure genomic algorithms, such as paternity and ancestry testing and privacy-preserving applications of genome-wide association studies. This survey provides an overview of fully homomorphic encryption and its applications in medicine and bioinformatics. The high-level concepts behind FHE and its history are introduced, and details on current open-source implementations are provided. The state of fully homomorphic encryption for privacy-preserving techniques in machine learning and bioinformatics is reviewed, along with descriptions of how these methods can be implemented in the encrypted domain.",Letter,vol49
pap1049,224a97657b897bbae36494025e4aeef5bb18d155,con78,Neural Information Processing Systems,Programmatic access to bioinformatics tools from EMBL-EBI update: 2017,"Abstract Since 2009 the EMBL-EBI provides free and unrestricted access to several bioinformatics tools via the user's browser as well as programmatically via Web Services APIs. Programmatic access to these tools, which is fundamental to bioinformatics, is increasingly important as more high-throughput data is generated, e.g. from proteomics and metagenomic experiments. Access is available using both the SOAP and RESTful approaches and their usage is reviewed regularly in order to ensure that the best, supported tools are available to all users. We present here an update describing the latest enhancement to the Job Dispatcher APIs as well as the governance under it.",Erratum,pro78
pap1050,36789799e464aa7465125ff8e778939843a0e89b,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Taverna: a tool for the composition and enactment of bioinformatics workflows,"MOTIVATION
In silico experiments in bioinformatics involve the co-ordinated use of computational tools and information repositories. A growing number of these resources are being made available with programmatic access in the form of Web services. Bioinformatics scientists will need to orchestrate these Web services in workflows as part of their analyses.


RESULTS
The Taverna project has developed a tool for the composition and enactment of bioinformatics workflows for the life sciences community. The tool includes a workbench application which provides a graphical user interface for the composition of workflows. These workflows are written in a new language called the simple conceptual unified flow language (Scufl), where by each step within a workflow represents one atomic task. Two examples are used to illustrate the ease by which in silico experiments can be represented as Scufl workflows using the workbench application.",Erratum,pro85
pap1051,38080784dcde5f8bdb86af9a186b47117db37133,jou191,Journal of Clinical Medicine,Bioinformatics and Computational Tools for Next-Generation Sequencing Analysis in Clinical Genetics,"Clinical genetics has an important role in the healthcare system to provide a definitive diagnosis for many rare syndromes. It also can have an influence over genetics prevention, disease prognosis and assisting the selection of the best options of care/treatment for patients. Next-generation sequencing (NGS) has transformed clinical genetics making possible to analyze hundreds of genes at an unprecedented speed and at a lower price when comparing to conventional Sanger sequencing. Despite the growing literature concerning NGS in a clinical setting, this review aims to fill the gap that exists among (bio)informaticians, molecular geneticists and clinicians, by presenting a general overview of the NGS technology and workflow. First, we will review the current NGS platforms, focusing on the two main platforms Illumina and Ion Torrent, and discussing the major strong points and weaknesses intrinsic to each platform. Next, the NGS analytical bioinformatic pipelines are dissected, giving some emphasis to the algorithms commonly used to generate process data and to analyze sequence variants. Finally, the main challenges around NGS bioinformatics are placed in perspective for future developments. Even with the huge achievements made in NGS technology and bioinformatics, further improvements in bioinformatic algorithms are still required to deal with complex and genetically heterogeneous disorders.",Article,vol191
pap1052,a6409a9192845e3f321edb0c147d263d9fad783c,jou75,Frontiers in Genetics,Recent Advances of Deep Learning in Bioinformatics and Computational Biology,"Extracting inherent valuable knowledge from omics big data remains as a daunting problem in bioinformatics and computational biology. Deep learning, as an emerging branch from machine learning, has exhibited unprecedented performance in quite a few applications from academia and industry. We highlight the difference and similarity in widely utilized models in deep learning studies, through discussing their basic structures, and reviewing diverse applications and disadvantages. We anticipate the work can serve as a meaningful perspective for further development of its theory, algorithm and application in bioinformatic and computational biology.",Article,vol75
pap1053,602ab24e8134815630da74c746405b3693836d6e,jou192,Journal of medical systems,A Survey of Data Mining and Deep Learning in Bioinformatics,,Article,vol192
pap1054,d26f49d123b114ba7ec83164bd4edca15d398677,jou193,Briefings in Bioinformatics,Sequence clustering in bioinformatics: an empirical study.,"Sequence clustering is a basic bioinformatics task that is attracting renewed attention with the development of metagenomics and microbiomics. The latest sequencing techniques have decreased costs and as a result, massive amounts of DNA/RNA sequences are being produced. The challenge is to cluster the sequence data using stable, quick and accurate methods. For microbiome sequencing data, 16S ribosomal RNA operational taxonomic units are typically used. However, there is often a gap between algorithm developers and bioinformatics users. Different software tools can produce diverse results and users can find them difficult to analyze. Understanding the different clustering mechanisms is crucial to understanding the results that they produce. In this review, we selected several popular clustering tools, briefly explained the key computing principles, analyzed their characters and compared them using two independent benchmark datasets. Our aim is to assist bioinformatics users in employing suitable clustering tools effectively to analyze big sequencing data. Related data, codes and software tools were accessible at the link http://lab.malab.cn/∼lg/clustering/.",Article,vol193
pap1055,1279fe26020af034fd6f16b04a1c23427c127db3,con15,Pacific Symposium on Biocomputing,Data-driven advice for applying machine learning to bioinformatics problems,"As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms. Here we contribute a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers. We present a number of statistical and visual comparisons of algorithm performance and quantify the effect of model selection and algorithm tuning for each algorithm and dataset. The analysis culminates in the recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems.",Letter,pro15
pap1056,6ece27a1817be86fd7019fdeb503309917ebe2d5,con65,IEEE International Conference on Software Engineering and Formal Methods,BIOINFORMATICS APPLICATIONS,"Summary: The HyPhy package is designed to provide a ﬂexible and uniﬁed platform for carrying out likelihood-based analyses on multiple alignments of molecular sequence data, with the emphasis on studies of rates and patterns of sequence evolution.",Erratum,pro65
pap1057,b115f2ea812a607b1939f284e70258e6fb0ddadc,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Bioinformatics and Computational Biology Solutions Using R and Bioconductor,"Full four-color book.

Some of the editors created the Bioconductor project and Robert Gentleman is one of the two originators of R.

All methods are illustrated with publicly available data, and a major section of the book is devoted to fully worked case studies.

Code underlying all of the computations that are shown is made available on a companion website, and readers can reproduce every number, figure, and table on their own computers.",Erratum,pro42
pap1058,9ade9dace9c2351280b8be050b69ad2abe5e7d9d,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",A brief history of bioinformatics,"It is easy for today's students and researchers to believe that modern bioinformatics emerged recently to assist next-generation sequencing data analysis. However, the very beginnings of bioinformatics occurred more than 50 years ago, when desktop computers were still a hypothesis and DNA could not yet be sequenced. The foundations of bioinformatics were laid in the early 1960s with the application of computational methods to protein sequence analysis (notably, de novo sequence assembly, biological sequence databases and substitution models). Later on, DNA analysis also emerged due to parallel advances in (i) molecular biology methods, which allowed easier manipulation of DNA, as well as its sequencing, and (ii) computer science, which saw the rise of increasingly miniaturized and more powerful computers, as well as novel software better suited to handle bioinformatics tasks. In the 1990s through the 2000s, major improvements in sequencing technology, along with reduced costs, gave rise to an exponential increase of data. The arrival of 'Big Data' has laid out new challenges in terms of data mining and management, calling for more expertise from computer science into the field. Coupled with an ever-increasing amount of bioinformatics tools, biological Big Data had (and continues to have) profound implications on the predictive power and reproducibility of bioinformatics results. To overcome this issue, universities are now fully integrating this discipline into the curriculum of biology students. Recent subdisciplines such as synthetic biology, systems biology and whole-cell modeling have emerged from the ever-increasing complementarity between computer science and biology.",Erratum,pro73
pap1059,0d67f9364948d1d48f3c6cb9b88e60afd5f6d460,con99,North American Chapter of the Association for Computational Linguistics,The MPI bioinformatics Toolkit as an integrative platform for advanced protein sequence and structure analysis,"The MPI Bioinformatics Toolkit (http://toolkit.tuebingen.mpg.de) is an open, interactive web service for comprehensive and collaborative protein bioinformatic analysis. It offers a wide array of interconnected, state-of-the-art bioinformatics tools to experts and non-experts alike, developed both externally (e.g. BLAST+, HMMER3, MUSCLE) and internally (e.g. HHpred, HHblits, PCOILS). While a beta version of the Toolkit was released 10 years ago, the current production-level release has been available since 2008 and has serviced more than 1.6 million external user queries. The usage of the Toolkit has continued to increase linearly over the years, reaching more than 400 000 queries in 2015. In fact, through the breadth of its tools and their tight interconnection, the Toolkit has become an excellent platform for experimental scientists as well as a useful resource for teaching bioinformatic inquiry to students in the life sciences. In this article, we report on the evolution of the Toolkit over the last ten years, focusing on the expansion of the tool repertoire (e.g. CS-BLAST, HHblits) and on infrastructural work needed to remain operative in a changing web environment.",Erratum,pro99
pap1060,10b40befe5942e997f88ab40bac1acd931147893,con8,Frontiers in Education Conference,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",Erratum,pro8
pap1061,fd69bd5b948ff001ae87f6bfa06693398965a9c2,con96,Interspeech,Influenza Research Database: An integrated bioinformatics resource for influenza virus research,"The Influenza Research Database (IRD) is a U.S. National Institute of Allergy and Infectious Diseases (NIAID)-sponsored Bioinformatics Resource Center dedicated to providing bioinformatics support for influenza virus research. IRD facilitates the research and development of vaccines, diagnostics and therapeutics against influenza virus by providing a comprehensive collection of influenza-related data integrated from various sources, a growing suite of analysis and visualization tools for data mining and hypothesis generation, personal workbench spaces for data storage and sharing, and active user community support. Here, we describe the recent improvements in IRD including the use of cloud and high performance computing resources, analysis and visualization of user-provided sequence data with associated metadata, predictions of novel variant proteins, annotations of phenotype-associated sequence markers and their predicted phenotypic effects, hemagglutinin (HA) clade classifications, an automated tool for HA subtype numbering conversion, linkouts to disease event data and the addition of host factor and antiviral drug components. All data and tools are freely available without restriction from the IRD website at https://www.fludb.org.",Erratum,pro96
pap1062,04b71445fca3e98d4f572284a15ccb7d6c7cb917,jou194,Journal of Molecular Cell Biology,Modern deep learning in bioinformatics,"Deep learning (DL) has shown explosive growth in its application to bioinformatics and has demonstrated thrillingly promising power to mine the complex relationship hidden in large-scale biological and biomedical data. A number of comprehensive reviews have been published on such applications, ranging from high-level reviews with future perspectives to those mainly serving as tutorials. These reviews have provided an excellent introduction to and guideline for applications of DL in bioinformatics, covering multiple types of machine learning (ML) problems, different DL architectures, and ranges of biological/biomedical problems. However, most of these reviews have focused on previous research, whereas current trends in the principled DL field and perspectives on their future developments and potential new applications to biology and biomedicine are still scarce. We will focus on modern DL, the ongoing trends and future directions of the principled DL field, and postulate new and major applications in bioinformatics.",Article,vol194
pap1063,2392f7f8b64c2016bf57957d51f32b7cef2acbcd,jou195,SpringerPlus,An overview of topic modeling and its current applications in bioinformatics,,Letter,vol195
pap1064,6be2e6dca6c66f3ad74eb01c168e3c104119f41c,jou196,Medicinal chemistry,Impacts of bioinformatics to medicinal chemistry.,"Facing the explosive growth of biological sequence data, such as those of protein/peptide and DNA/RNA, generated in the post-genomic age, many bioinformatical and mathematical approaches as well as physicochemical concepts have been introduced to timely derive useful informations from these biological sequences, in order to stimulate the development of medical science and drug design. Meanwhile, because of the rapid penetrations from these disciplines, medicinal chemistry is currently undergoing an unprecedented revolution. In this minireview, we are to summarize the progresses by focusing on the following six aspects. (1) Use the pseudo amino acid composition or PseAAC to predict various attributes of protein/peptide sequences that are useful for drug development. (2) Use pseudo oligonucleotide composition or PseKNC to do the same for DNA/RNA sequences. (3) Introduce the multi-label approach to study those systems where the constituent elements bear multiple characters and functions. (4) Utilize the graphical rules and ""wenxiang"" diagrams to analyze complicated biomedical systems. (5) Recent development in identifying the interactions of drugs with its various types of target proteins in cellular networking. (6) Distorted key theory and its application in developing peptide drugs.",Conference paper,vol196
pap1065,e128b25148504cc21e4cf8c6426cddc566e8107c,jou197,Current Bioinformatics,Review of the Applications of Deep Learning in Bioinformatics,"

Rapid advances in biological research over recent years have significantly enriched
biological and medical data resources. Deep learning-based techniques have been successfully
utilized to process data in this field, and they have exhibited state-of-the-art performances even on
high-dimensional, nonstructural, and black-box biological data. The aim of the current study is to
provide an overview of the deep learning-based techniques used in biology and medicine and their
state-of-the-art applications. In particular, we introduce the fundamentals of deep learning and then
review the success of applying such methods to bioinformatics, biomedical imaging, biomedicine,
and drug discovery. We also discuss the challenges and limitations of this field, and outline possible
directions for further research.
",Conference paper,vol197
pap1066,8ace7572803409d4dcb4c0baceccbf61c0e94bcb,jou198,BMC Microbiology,A comparison of sequencing platforms and bioinformatics pipelines for compositional analysis of the gut microbiome,,Conference paper,vol198
pap1067,e6a5bcdb576f2b0d965a9f71f24514553966716b,jou199,Journal of Infectious Diseases,"Next Generation Sequencing and Bioinformatics Methodologies for Infectious Disease Research and Public Health: Approaches, Applications, and Considerations for Development of Laboratory Capacity.","Next generation sequencing (NGS) combined with bioinformatics has successfully been used in a vast array of analyses for infectious disease research of public health relevance. For instance, NGS and bioinformatics approaches have been used to identify outbreak origins, track transmissions, investigate epidemic dynamics, determine etiological agents of a disease, and discover novel human pathogens. However, implementation of high-quality NGS and bioinformatics in research and public health laboratories can be challenging. These challenges mainly include the choice of the sequencing platform and the sequencing approach, the choice of bioinformatics methodologies, access to the appropriate computation and information technology infrastructure, and recruiting and retaining personnel with the specialized skills and experience in this field. In this review, we summarize the most common NGS and bioinformatics workflows in the context of infectious disease genomic surveillance and pathogen discovery, and highlight the main challenges and considerations for setting up an NGS and bioinformatics-focused infectious disease research public health laboratory. We describe the most commonly used sequencing platforms and review their strengths and weaknesses. We review sequencing approaches that have been used for various pathogens and study questions, as well as the most common difficulties associated with these approaches that should be considered when implementing in a public health or research setting. In addition, we provide a review of some common bioinformatics tools and procedures used for pathogen discovery and genome assembly, along with the most common challenges and solutions. Finally, we summarize the bioinformatics of advanced viral, bacterial, and parasite pathogen characterization, including types of study questions that can be answered when utilizing NGS and bioinformatics.",Letter,vol199
pap1068,632df9a1308ec631143f2897f79eeaf04208319d,con109,International Society for Music Information Retrieval Conference,Metabolomics technology and bioinformatics for precision medicine,"Precision medicine is rapidly emerging as a strategy to tailor medical treatment to a small group or even individual patients based on their genetics, environment and lifestyle. Precision medicine relies heavily on developments in systems biology and omics disciplines, including metabolomics. Combination of metabolomics with sophisticated bioinformatics analysis and mathematical modeling has an extreme power to provide a metabolic snapshot of the patient over the course of disease and treatment or classifying patients into subpopulations and subgroups requiring individual medical treatment. Although a powerful approach, metabolomics have certain limitations in technology and bioinformatics. We will review various aspects of metabolomics technology and bioinformatics, from data generation, bioinformatics analysis, data fusion and mathematical modeling to data management, in the context of precision medicine.",Erratum,pro109
pap1069,cd8156fc9f17146b39dfaf47fcd20f1e3ab70791,jou182,Proceedings of the IEEE,Manual for Using Homomorphic Encryption for Bioinformatics,"Biological data science is an emerging field facing multiple challenges for hosting, sharing, computing on, and interacting with large data sets. Privacy regulations and concerns about the risks of leaking sensitive personal health and genomic data add another layer of complexity to the problem. Recent advances in cryptography over the last five years have yielded a tool, homomorphic encryption, which can be used to encrypt data in such a way that storage can be outsourced to an untrusted cloud, and the data can be computed on in a meaningful way in encrypted form, without access to decryption keys. This paper introduces homomorphic encryption to the bioinformatics community, and presents an informal “manual” for using the Simple Encrypted Arithmetic Library (SEAL), which we have made publicly available for bioinformatic, genomic, and other research purposes.",Letter,vol182
pap1070,a885fc8bcd9f398acdfeb1d64c37b523063b585e,jou200,Neurocomputing,A novel features ranking metric with application to scalable visual and bioinformatics data classification,,Letter,vol200
pap1071,02f42cf7470fbe514c6426e63631fc1ab5a2e350,jou201,Analytical and Bioanalytical Chemistry,Bioinformatics and peptidomics approaches to the discovery and analysis of food-derived bioactive peptides,,Conference paper,vol201
pap1072,d7e1225452deed203a2f92d7cca29bcde27866e5,jou75,Frontiers in Genetics,A Review of Bioinformatics Tools for Bio-Prospecting from Metagenomic Sequence Data,"The microbiome can be defined as the community of microorganisms that live in a particular environment. Metagenomics is the practice of sequencing DNA from the genomes of all organisms present in a particular sample, and has become a common method for the study of microbiome population structure and function. Increasingly, researchers are finding novel genes encoded within metagenomes, many of which may be of interest to the biotechnology and pharmaceutical industries. However, such “bioprospecting” requires a suite of sophisticated bioinformatics tools to make sense of the data. This review summarizes the most commonly used bioinformatics tools for the assembly and annotation of metagenomic sequence data with the aim of discovering novel genes.",Letter,vol75
pap1073,b8efd44766f246fb38ec62658591dbb92e2e8aa9,jou81,BMC Bioinformatics,Current trend and development in bioinformatics research,,Conference paper,vol81
pap1074,0cc0a1ba893e35adf01342ed8f91bfa0ad940416,jou202,Journal of Biomedical Informatics,An overview of bioinformatics tools for epitope prediction: Implications on vaccine development,,Conference paper,vol202
pap1075,042328a210e2a2301bf1bd81b238f8cc3bd82ba5,con68,Experimental Software Engineering Network,Performance measures in evaluating machine learning based bioinformatics predictors for classifications,,Erratum,pro68
pap1076,cca5e74bf5303d9504e78bc58d61d9fe653d160d,jou75,Frontiers in Genetics,The Road to Metagenomics: From Microbiology to DNA Sequencing Technologies and Bioinformatics,"The study of microorganisms that pervade each and every part of this planet has encountered many challenges through time such as the discovery of unknown organisms and the understanding of how they interact with their environment. The aim of this review is to take the reader along the timeline and major milestones that led us to modern metagenomics. This new and thriving area is likely to be an important contributor to solve different problems. The transition from classical microbiology to modern metagenomics studies has required the development of new branches of knowledge and specialization. Here, we will review how the availability of high-throughput sequencing technologies has transformed microbiology and bioinformatics and how to tackle the inherent computational challenges that arise from the DNA sequencing revolution. New computational methods are constantly developed to collect, process, and extract useful biological information from a variety of samples and complex datasets, but metagenomics needs the integration of several of these computational methods. Despite the level of specialization needed in bioinformatics, it is important that life-scientists have a good understanding of it for a correct experimental design, which allows them to reveal the information in a metagenome.",Conference paper,vol75
pap1077,e67a2993f38c125539a17c16791a6c73dbcc586d,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,A novel hierarchical selective ensemble classifier with bioinformatics application,,Erratum,pro21
pap1078,42a277905d54c0eb13a755e578852d4d52cb55eb,con40,Conference on Software Engineering Education and Training,Review of feature selection techniques in bioinformatics,"In this paper we want to explain what feature selection is and what it is used for. We discuss methods for getting those features and present them in the context of classification that is using only supervised learning. Our goal is to present applications of these techniques in bioinformatics and match them with the appropriate type of method to use. We will also talk about how to deal with small sample sizes and problems related with it. Finally, we will present some areas in bioinformatics that can be improved using feature selection methods.",Erratum,pro40
pap1079,e785892cf00aea8b58e638994b31a366d31b794e,con61,International Conference on Predictive Models in Software Engineering,Encyclopedia of Bioinformatics and Computational Biology: ABC of Bioinformatics,,Erratum,pro61
pap1080,adb9e9014f3ac6fb55ca0b792ad964a23fe6da63,jou88,bioRxiv,CLIMB (the Cloud Infrastructure for Microbial Bioinformatics): an online resource for the medical microbiology community,"The increasing availability and decreasing cost of high-throughput sequencing has transformed academic medical microbiology, delivering an explosion in available genomes while also driving advances in bioinformatics. However, many microbiologists are unable to exploit the resulting large genomics datasets because they do not have access to relevant computational resources and to an appropriate bioinformatics infrastructure. Here, we present the Cloud Infrastructure for Microbial Bioinformatics (CLIMB) facility, a shared computing infrastructure that has been designed from the ground up to provide an environment where microbiologists can share and reuse methods and data. DATA SUMMARY The paper describes a new, freely available public resource and therefore no data has been generated. The resource can be accessed at http://www.climb.ac.uk. Source code for software developed for the project can be found at http://github.com/MRC-CLIMB/ I/We confirm all supporting data, code and protocols have been provided within the article or through supplementary data files. IMPACT STATEMENT Technological advances mean that genome sequencing is now relatively simple, quick, and affordable. However, handling large genome datasets remains a significant challenge for many microbiologists, with substantial requirements for computational resources and expertise in data storage and analysis. This has led to fragmentary approaches to software development and data sharing that reduce the reproducibility of research and limits opportunities for bioinformatics training. Here, we describe a nationwide electronic infrastructure that has been designed to support the UK microbiology community, providing simple mechanisms for accessing large, shared, computational resources designed to meet the bioinformatic needs of microbiologists.",Letter,vol88
pap1081,e2991da06cec4ca1f8f9727279674cfccdeee93c,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,A global perspective on evolving bioinformatics and data science training needs,"Abstract Bioinformatics is now intrinsic to life science research, but the past decade has witnessed a continuing deficiency in this essential expertise. Basic data stewardship is still taught relatively rarely in life science education programmes, creating a chasm between theory and practice, and fuelling demand for bioinformatics training across all educational levels and career roles. Concerned by this, surveys have been conducted in recent years to monitor bioinformatics and computational training needs worldwide. This article briefly reviews the principal findings of a number of these studies. We see that there is still a strong appetite for short courses to improve expertise and confidence in data analysis and interpretation; strikingly, however, the most urgent appeal is for bioinformatics to be woven into the fabric of life science degree programmes. Satisfying the relentless training needs of current and future generations of life scientists will require a concerted response from stakeholders across the globe, who need to deliver sustainable solutions capable of both transforming education curricula and cultivating a new cadre of trainer scientists.",Erratum,pro82
pap1082,fd23cfb37f4f69932edce56a13578b60b029db3a,jou203,Molecular Ecology Resources,PipeCraft: Flexible open‐source toolkit for bioinformatics analysis of custom high‐throughput amplicon sequencing data,"High‐throughput sequencing methods have become a routine analysis tool in environmental sciences as well as in public and private sector. These methods provide vast amount of data, which need to be analysed in several steps. Although the bioinformatics may be applied using several public tools, many analytical pipelines allow too few options for the optimal analysis for more complicated or customized designs. Here, we introduce PipeCraft, a flexible and handy bioinformatics pipeline with a user‐friendly graphical interface that links several public tools for analysing amplicon sequencing data. Users are able to customize the pipeline by selecting the most suitable tools and options to process raw sequences from Illumina, Pacific Biosciences, Ion Torrent and Roche 454 sequencing platforms. We described the design and options of PipeCraft and evaluated its performance by analysing the data sets from three different sequencing platforms. We demonstrated that PipeCraft is able to process large data sets within 24 hr. The graphical user interface and the automated links between various bioinformatics tools enable easy customization of the workflow. All analytical steps and options are recorded in log files and are easily traceable.",Letter,vol203
pap1083,8a8ec7b22ded7e455b1d156d0a57f3eba9cf38b0,jou204,Methods,Feature selection methods for big data bioinformatics: A survey from the search perspective.,,Conference paper,vol204
pap1084,8c5c522a0d02b0487929412be41eb385ff1eb991,jou190,International Journal of Molecular Sciences,Aptamer Bioinformatics,"Aptamers are short nucleic acid sequences capable of specific, high-affinity molecular binding. They are isolated via SELEX (Systematic Evolution of Ligands by Exponential Enrichment), an evolutionary process that involves iterative rounds of selection and amplification before sequencing and aptamer characterization. As aptamers are genetic in nature, bioinformatic approaches have been used to improve both aptamers and their selection. This review will discuss the advancements made in several enclaves of aptamer bioinformatics, including simulation of aptamer selection, fragment-based aptamer design, patterning of libraries, identification of lead aptamers from high-throughput sequencing (HTS) data and in silico aptamer optimization.",Article,vol190
pap1085,a0672457c2687759fe88a44541cc53c81738c0da,con57,International Workshop on Agent-Oriented Software Engineering,The development and application of bioinformatics core competencies to improve bioinformatics training and education,"Bioinformatics is recognized as part of the essential knowledge base of numerous career paths in biomedical research and healthcare. However, there is little agreement in the field over what that knowledge entails or how best to provide it. These disagreements are compounded by the wide range of populations in need of bioinformatics training, with divergent prior backgrounds and intended application areas. The Curriculum Task Force of the International Society of Computational Biology (ISCB) Education Committee has sought to provide a framework for training needs and curricula in terms of a set of bioinformatics core competencies that cut across many user personas and training programs. The initial competencies developed based on surveys of employers and training programs have since been refined through a multiyear process of community engagement. This report describes the current status of the competencies and presents a series of use cases illustrating how they are being applied in diverse training contexts. These use cases are intended to demonstrate how others can make use of the competencies and engage in the process of their continuing refinement and application. The report concludes with a consideration of remaining challenges and future plans.",Erratum,pro57
pap1086,d4d6bef7e3df07197a9882a89f140a394e3c6a5c,jou2,Genome Research,A cloud-compatible bioinformatics pipeline for ultrarapid pathogen identification from next-generation sequencing of clinical samples,"Unbiased next-generation sequencing (NGS) approaches enable comprehensive pathogen detection in the clinical microbiology laboratory and have numerous applications for public health surveillance, outbreak investigation, and the diagnosis of infectious diseases. However, practical deployment of the technology is hindered by the bioinformatics challenge of analyzing results accurately and in a clinically relevant timeframe. Here we describe SURPI (“sequence-based ultrarapid pathogen identification”), a computational pipeline for pathogen identification from complex metagenomic NGS data generated from clinical samples, and demonstrate use of the pipeline in the analysis of 237 clinical samples comprising more than 1.1 billion sequences. Deployable on both cloud-based and standalone servers, SURPI leverages two state-of-the-art aligners for accelerated analyses, SNAP and RAPSearch, which are as accurate as existing bioinformatics tools but orders of magnitude faster in performance. In fast mode, SURPI detects viruses and bacteria by scanning data sets of 7–500 million reads in 11 min to 5 h, while in comprehensive mode, all known microorganisms are identified, followed by de novo assembly and protein homology searches for divergent viruses in 50 min to 16 h. SURPI has also directly contributed to real-time microbial diagnosis in acutely ill patients, underscoring its potential key role in the development of unbiased NGS-based clinical assays in infectious diseases that demand rapid turnaround times.",Article,vol2
pap1087,aca8d8cc3fec56b9b41dc1853a75c1f0c0bd7d91,con57,International Workshop on Agent-Oriented Software Engineering,Network Inference and Reconstruction in Bioinformatics,,Erratum,pro57
pap1088,620542a56bbf1a466213c029d93212b65915e7c5,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Data Mining in Bioinformatics,,Erratum,pro54
pap1089,68459ed4cff7692699d60218c1dc579cfe5e24c5,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Ieee/acm Transactions on Computational Biology and Bioinformatics 1 Biclustering Algorithms for Biological Data Analysis: a Survey,"A large number of clustering approaches have been proposed for the analysis of gene expression data obtained from microarray experiments. However, the results of the application of standard clustering methods to genes are limited. These limited results are imposed by the existence of a number of experimental conditions where the activity of genes is uncorrelated. A similar limitation exists when clustering of conditions is performed. For this reason, a number of algorithms that perform simultaneous clustering on the row and column dimensions of the gene expression matrix has been proposed to date. This simultaneous clustering, usually designated by biclustering, seeks to find sub-matrices, that is subgroups of genes and subgroups of columns, where the genes exhibit highly correlated activities for every condition. This type of algorithms has also been proposed and used in other fields, such as information retrieval and data mining. In this comprehensive survey, we analyze a large number of existing approaches to biclustering, and classify them in accordance with the type of biclusters they can find, the patterns of biclusters that are discovered, the methods used to perform the search and the target applications.",Erratum,pro49
pap1090,4fa6bcd51bc3ff76e1026ea3d7415deb0370b325,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Challenges and Opportunities of Amazon Serverless Lambda Services in Bioinformatics,"Currently, several factors are moving biomedical research towards a (big) data-centred science. This yields new challenges for computer science solutions when dealing with bioinformatics applications. Among others, efficient storage, preprocessing, integration and analysis of omics and clinical data, result in a bottleneck on the analysis pipeline. This may be faced using cloud technology. This paper discusses the challenges and opportunities of deploying bioinformatics applications using the Amazon Serverless Lambda services. First experiments show that serverless computing is useful for this particular bioinformatics high-throughput application, because it simplifies resource management.",Article,pro73
pap1091,bf4e29b5b8afe5aa7dbf091f11d488a9b34c7e90,con78,Neural Information Processing Systems,The Impact of Bioinformatics on Vaccine Design and Development,"Vaccines are the pharmaceutical products that offer the best cost‐benefit ratio in the pre‐ vention or treatment of diseases. In that a vaccine is a pharmaceutical product, vaccine development and production are costly and it takes years for this to be accomplished. Several approaches have been applied to reduce the times and costs of vaccine develop‐ ment, mainly focusing on the selection of appropriate antigens or antigenic structures, carriers, and adjuvants. One of these approaches is the incorporation of bioinformatics methods and analyses into vaccine development. This chapter provides an overview of the application of bioinformatics strategies in vaccine design and development, supply‐ ing some successful examples of vaccines in which bioinformatics has furnished a cutting edge in their development. Reverse vaccinology, immunoinformatics, and structural vac ‐ cinology are described and addressed in the design and development of specific vaccines against infectious diseases caused by bacteria, viruses, and parasites. These include some emerging or re‐emerging infectious diseases, as well as therapeutic vaccines to fight can‐ cer, allergies, and substance abuse, which have been facilitated and improved by using bioinformatics tools or which are under development based on bioinformatics strategies. antigenic B‐cell (IEDB) and CTL epitopes (NetCTL.1.2 server). They determined, by in silico studies, surface accessibility, surface flexibility, hydrophilicity, homology modeling (MODELLER ver. 9.12, CHARMM, WhatIF, PROCHECK, Verify 3D), and structure‐based epitope prediction for E protein, NS3, and NS5. They performed molecular docking of the ZIKV‐E protein with HLA‐A0201, of the ZIKV‐NS3 protein with HLA‐B2705, and of the ZIKV‐NS5 protein with HLA‐C0801 (PatchDock rigid‐body docking server, FireDock server). these",Erratum,pro78
pap1092,75c52a70ec235ceb8a522a1e7e884c9792211d47,jou88,bioRxiv,Bioinformatics core competencies for undergraduate life sciences education,"Bioinformatics is becoming increasingly central to research in the life sciences. However, despite its importance, bioinformatics skills and knowledge are not well integrated in undergraduate biology education. This curricular gap prevents biology students from harnessing the full potential of their education, limiting their career opportunities and slowing genomic research innovation. To advance the integration of bioinformatics into life sciences education, a framework of core bioinformatics competencies is needed. To that end, we here report the results of a survey of life sciences faculty in the United States about teaching bioinformatics to undergraduate life scientists. Responses were received from 1,260 faculty representing institutions in all fifty states with a combined capacity to educate hundreds of thousands of students every year. Results indicate strong, widespread agreement that bioinformatics knowledge and skills are critical for undergraduate life scientists, as well as considerable agreement about which skills are necessary. Perceptions of the importance of some skills varied with the respondent’s degree of training, time since degree earned, and/or the Carnegie classification of the respondent’s institution. To assess which skills are currently being taught, we analyzed syllabi of courses with bioinformatics content submitted by survey respondents. Finally, we used the survey results, the analysis of syllabi, and our collective research and teaching expertise to develop a set of bioinformatics core competencies for undergraduate life sciences students. These core competencies are intended to serve as a guide for institutions as they work to integrate bioinformatics into their life sciences curricula. Significance Statement Bioinformatics, an interdisciplinary field that uses techniques from computer science and mathematics to store, manage, and analyze biological data, is becoming increasingly central to modern biology research. Given the widespread use of bioinformatics and its impacts on societal problem-solving (e.g., in healthcare, agriculture, and natural resources management), there is a growing need for the integration of bioinformatics competencies into undergraduate life sciences education. Here, we present a set of bioinformatics core competencies for undergraduate life scientists developed using the results of a large national survey and the expertise of our working group of bioinformaticians and educators. We also present results from the survey on the importance of bioinformatics skills and the current state of integration of bioinformatics into biology education.",Article,vol88
pap1093,963b6626180c10826dade32678fcf305a8f7bc15,con56,International Conference on Software Engineering and Knowledge Engineering,Bioinformatics Applications Note Sequence Analysis Cd-hit Suite: a Web Server for Clustering and Comparing Biological Sequences,"CD-HIT is a widely used program for clustering and comparing large biological sequence datasets. In order to further assist the CD-HIT users, we significantly improved this program with more functions and better accuracy, scalability and flexibility. Most importantly, we developed a new web server, CD-HIT Suite, for clustering a user-uploaded sequence dataset or comparing it to another dataset at different identity levels. Users can now interactively explore the clusters within web browsers. We also provide downloadable clusters for several public databases (NCBI NR, Swissprot and PDB) at different identity levels.",Erratum,pro56
pap1094,b48c077e8f368857a2453a5ac6a62108308dbe89,con30,PS,Systems Bioinformatics: increasing precision of computational diagnostics and therapeutics through network-based approaches,"Abstract Systems Bioinformatics is a relatively new approach, which lies in the intersection of systems biology and classical bioinformatics. It focuses on integrating information across different levels using a bottom-up approach as in systems biology with a data-driven top-down approach as in bioinformatics. The advent of omics technologies has provided the stepping-stone for the emergence of Systems Bioinformatics. These technologies provide a spectrum of information ranging from genomics, transcriptomics and proteomics to epigenomics, pharmacogenomics, metagenomics and metabolomics. Systems Bioinformatics is the framework in which systems approaches are applied to such data, setting the level of resolution as well as the boundary of the system of interest and studying the emerging properties of the system as a whole rather than the sum of the properties derived from the system’s individual components. A key approach in Systems Bioinformatics is the construction of multiple networks representing each level of the omics spectrum and their integration in a layered network that exchanges information within and between layers. Here, we provide evidence on how Systems Bioinformatics enhances computational therapeutics and diagnostics, hence paving the way to precision medicine. The aim of this review is to familiarize the reader with the emerging field of Systems Bioinformatics and to provide a comprehensive overview of its current state-of-the-art methods and technologies. Moreover, we provide examples of success stories and case studies that utilize such methods and tools to significantly advance research in the fields of systems biology and systems medicine.",Erratum,pro30
pap1095,4edf4880e93783901df1118ff174c839ffe2986f,jou88,bioRxiv,Enabling the democratization of the genomics revolution with a fully integrated web-based bioinformatics platform,"Continued advancements in sequencing technologies have fueled the development of new sequencing applications and promise to flood current databases with raw data. A number of factors prevent the seamless and easy use of these data, including the breadth of project goals, the wide array of tools that individually perform fractions of any given analysis, the large number of associated software/hardware dependencies, and the detailed expertise required to perform these analyses. To address these issues, we have developed an intuitive web-based environment with a wide assortment of integrated and cutting-edge bioinformatics tools. These preconfigured workflows provide even novice next-generation sequencing users with the ability to perform many complex analyses with only a few mouse clicks, and, within the context of the same environment, to visualize and further interrogate their results. This bioinformatics platform is an initial attempt at Empowering the Development of Genomics Expertise (EDGE) in a wide range of applications.",Article,vol88
pap1096,aa8cd9b6126da24d6e772a6973e82b5443537780,jou205,Current Topics in Medicinal Chemistry,Bioinformatics and Drug Discovery,"Bioinformatic analysis can not only accelerate drug target identification and drug candidate screening and refinement, but also facilitate characterization of side effects and predict drug resistance. High-throughput data such as genomic, epigenetic, genome architecture, cistromic, transcriptomic, proteomic, and ribosome profiling data have all made significant contribution to mechanism-based drug discovery and drug repurposing. Accumulation of protein and RNA structures, as well as development of homology modeling and protein structure simulation, coupled with large structure databases of small molecules and metabolites, paved the way for more realistic protein-ligand docking experiments and more informative virtual screening. I present the conceptual framework that drives the collection of these high-throughput data, summarize the utility and potential of mining these data in drug discovery, outline a few inherent limitations in data and software mining these data, point out news ways to refine analysis of these diverse types of data, and highlight commonly used software and databases relevant to drug discovery.",Article,vol205
pap1097,a164044fe875d7d9d23a9f569c5b2c82660add4e,jou206,Bioscience Reports,Bioinformatics in translational drug discovery,"Bioinformatics approaches are becoming ever more essential in translational drug discovery both in academia and within the pharmaceutical industry. Computational exploitation of the increasing volumes of data generated during all phases of drug discovery is enabling key challenges of the process to be addressed. Here, we highlight some of the areas in which bioinformatics resources and methods are being developed to support the drug discovery pipeline. These include the creation of large data warehouses, bioinformatics algorithms to analyse ‘big data’ that identify novel drug targets and/or biomarkers, programs to assess the tractability of targets, and prediction of repositioning opportunities that use licensed drugs to treat additional indications.",Letter,vol206
pap1098,1b54ecd153bcb9283bacf12d448a12ff871b51fc,con103,IEEE International Conference on Multimedia and Expo,BIOINFORMATICS APPLICATIONS NOTE,"Motivations: Spreadsheet-like tabular formats are ever more popular in the biomedical ﬁeld as a mean for experimental reporting. The problem of converting the graph of an experimental workﬂow into a table-based representation occurs in many such formats and is not easy to solve. Results: We describe graph2tab, a library that implements methods to realise such a conversion in a size-optimised way. Our solution is generic and can be adapted to speciﬁc cases of data exporters or data converters that need to be implemented. Availability and Implementation: The library source code and documentation are available at http://github.com/ISA-tools/ graph2tab. Contact",Erratum,pro103
pap1099,fbae68c4166d297cb4cc7b014906d559dfce484a,jou207,GigaScience,Bioinformatics applications on Apache Spark,"Abstract With the rapid development of next-generation sequencing technology, ever-increasing quantities of genomic data pose a tremendous challenge to data processing. Therefore, there is an urgent need for highly scalable and powerful computational systems. Among the state-of–the-art parallel computing platforms, Apache Spark is a fast, general-purpose, in-memory, iterative computing framework for large-scale data processing that ensures high fault tolerance and high scalability by introducing the resilient distributed dataset abstraction. In terms of performance, Spark can be up to 100 times faster in terms of memory access and 10 times faster in terms of disk access than Hadoop. Moreover, it provides advanced application programming interfaces in Java, Scala, Python, and R. It also supports some advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. We surveyed Spark-based applications used in next-generation sequencing and other biological domains, such as epigenetics, phylogeny, and drug discovery. The results of this survey are used to provide a comprehensive guideline allowing bioinformatics researchers to apply Spark in their own fields.",Letter,vol207
pap1100,1a8bfb3b927778f643ae612562e0b9b915f614d3,jou208,Journal of B.U.ON. : official journal of the Balkan Union of Oncology,Microarray bioinformatics in cancer- a review.,"Bioinformatics is one of the newest fields of biological research, and should be viewed broadly as the use of mathematical, statistical, and computational methods for the processing and analysis of biological data. Over the last decade, the rapid growth of information and technology in both ""genomics"" and ""omics"" eras has been overwhelming for the laboratory scientists to process experimental results. Traditional gene-by-gene approaches in research are insufficient to meet the growth and demand of biological research in understanding the true biology. The massive amounts of data generated by new technologies as genomic sequencing and microarray chips make the management of data and the integration of multiple platforms of high importance; this is then followed by data analysis and interpretation to achieve biological understanding and therapeutic progress. Global views of analyzing the magnitude of information are necessary and traditional approaches to lab work have steadily been changing towards a bioinformatics era. Research is moving from being restricted to a laboratory environment to working with computers in a ""virtual lab"" environment. The present review article shall put light on this emerging field and its applicability towards cancer research.",Conference paper,vol208
pap1101,841dbff787e715ffb5b4c6b5a9841e0a2da0f1a0,con59,Annual Workshop of the Psychology of Programming Interest Group,The European Bioinformatics Institute in 2016: Data growth and integration,"New technologies are revolutionising biological research and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastructure of the European Bioinformatics Institute (EMBL-EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of December 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two new resources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which allows users to run large analyses in a virtual environment next to EMBL-EBI's vast public data resources.",Erratum,pro59
pap1102,27464183a5d63c06d4a8fafb0ba4e61bbdb59b70,jou75,Frontiers in Genetics,Single-Cell Transcriptomics Bioinformatics and Computational Challenges,"The emerging single-cell RNA-Seq (scRNA-Seq) technology holds the promise to revolutionize our understanding of diseases and associated biological processes at an unprecedented resolution. It opens the door to reveal intercellular heterogeneity and has been employed to a variety of applications, ranging from characterizing cancer cells subpopulations to elucidating tumor resistance mechanisms. Parallel to improving experimental protocols to deal with technological issues, deriving new analytical methods to interpret the complexity in scRNA-Seq data is just as challenging. Here, we review current state-of-the-art bioinformatics tools and methods for scRNA-Seq analysis, as well as addressing some critical analytical challenges that the field faces.",Article,vol75
pap1103,13043cbc3317220e57f89b70344d9b3f63d3a347,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",Bioinformatics for precision oncology,"Abstract Molecular profiling of tumor biopsies plays an increasingly important role not only in cancer research, but also in the clinical management of cancer patients. Multi-omics approaches hold the promise of improving diagnostics, prognostics and personalized treatment. To deliver on this promise of precision oncology, appropriate bioinformatics methods for managing, integrating and analyzing large and complex data are necessary. Here, we discuss the specific requirements of bioinformatics methods and software that arise in the setting of clinical oncology, owing to a stricter regulatory environment and the need for rapid, highly reproducible and robust procedures. We describe the workflow of a molecular tumor board and the specific bioinformatics support that it requires, from the primary analysis of raw molecular profiling data to the automatic generation of a clinical report and its delivery to decision-making clinical oncologists. Such workflows have to various degrees been implemented in many clinical trials, as well as in molecular tumor boards at specialized cancer centers and university hospitals worldwide. We review these and more recent efforts to include other high-dimensional multi-omics patient profiles into the tumor board, as well as the state of clinical decision support software to translate molecular findings into treatment recommendations.",Erratum,pro87
pap1104,a066d6337183fba874dd6f90805df9178f00d847,con35,IEEE Working Conference on Mining Software Repositories,Designing a course model for distance-based online bioinformatics training in Africa: The H3ABioNet experience,"Africa is not unique in its need for basic bioinformatics training for individuals from a diverse range of academic backgrounds. However, particular logistical challenges in Africa, most notably access to bioinformatics expertise and internet stability, must be addressed in order to meet this need on the continent. H3ABioNet (www.h3abionet.org), the Pan African Bioinformatics Network for H3Africa, has therefore developed an innovative, free-of-charge “Introduction to Bioinformatics” course, taking these challenges into account as part of its educational efforts to provide on-site training and develop local expertise inside its network. A multiple-delivery–mode learning model was selected for this 3-month course in order to increase access to (mostly) African, expert bioinformatics trainers. The content of the course was developed to include a range of fundamental bioinformatics topics at the introductory level. For the first iteration of the course (2016), classrooms with a total of 364 enrolled participants were hosted at 20 institutions across 10 African countries. To ensure that classroom success did not depend on stable internet, trainers pre-recorded their lectures, and classrooms downloaded and watched these locally during biweekly contact sessions. The trainers were available via video conferencing to take questions during contact sessions, as well as via online “question and discussion” forums outside of contact session time. This learning model, developed for a resource-limited setting, could easily be adapted to other settings.",Erratum,pro35
pap1105,81a40d51130e6da8e4a006a0e2ca1691ed8b25fc,con70,International Conference on Graph Transformation,Protein Structural Bioinformatics: An Overview,,Erratum,pro70
pap1106,590e4cc7cffc06cb337a33a2d450bfd790bda848,jou81,BMC Bioinformatics,BART: bioinformatics array research tool,,Letter,vol81
pap1107,6696681640fb9c9dc5f81ea73a433f09bfe9886a,jou81,BMC Bioinformatics,Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support African genomics,,Conference paper,vol81
pap1108,78dbf5d24f1788ec47b723904e85dc68d74d77b7,jou209,Clinical Chemistry,Bioinformatics for clinical next generation sequencing.,"BACKGROUND
Next generation sequencing (NGS)-based assays continue to redefine the field of genetic testing. Owing to the complexity of the data, bioinformatics has become a necessary component in any laboratory implementing a clinical NGS test.


CONTENT
The computational components of an NGS-based work flow can be conceptualized as primary, secondary, and tertiary analytics. Each of these components addresses a necessary step in the transformation of raw data into clinically actionable knowledge. Understanding the basic concepts of these analysis steps is important in assessing and addressing the informatics needs of a molecular diagnostics laboratory. Equally critical is a familiarity with the regulatory requirements addressing the bioinformatics analyses. These and other topics are covered in this review article.


SUMMARY
Bioinformatics has become an important component in clinical laboratories generating, analyzing, maintaining, and interpreting data from molecular genetics testing. Given the rapid adoption of NGS-based clinical testing, service providers must develop informatics work flows that adhere to the rigor of clinical laboratory standards, yet are flexible to changes as the chemistry and software for analyzing sequencing data mature.",Letter,vol209
pap1109,ad02572df6c57604703ed35d6f7a161b14adf540,jou187,Nature reviews genetics,Applied bioinformatics for the identification of regulatory elements,,Conference paper,vol187
pap1110,17e6aba5cd7012b42a3851a6425513d92ac49fc2,jou81,BMC Bioinformatics,Reproducible bioinformatics project: a community for reproducible bioinformatics analysis pipelines,,Conference paper,vol81
pap1111,a541fb091828a8a52ab0bc5914c9f68ca89df3e5,jou2,Genome Research,"H3ABioNet, a sustainable pan-African bioinformatics network for human heredity and health in Africa","The application of genomics technologies to medicine and biomedical research is increasing in popularity, made possible by new high-throughput genotyping and sequencing technologies and improved data analysis capabilities. Some of the greatest genetic diversity among humans, animals, plants, and microbiota occurs in Africa, yet genomic research outputs from the continent are limited. The Human Heredity and Health in Africa (H3Africa) initiative was established to drive the development of genomic research for human health in Africa, and through recognition of the critical role of bioinformatics in this process, spurred the establishment of H3ABioNet, a pan-African bioinformatics network for H3Africa. The limitations in bioinformatics capacity on the continent have been a major contributory factor to the lack of notable outputs in high-throughput biology research. Although pockets of high-quality bioinformatics teams have existed previously, the majority of research institutions lack experienced faculty who can train and supervise bioinformatics students. H3ABioNet aims to address this dire need, specifically in the area of human genetics and genomics, but knock-on effects are ensuring this extends to other areas of bioinformatics. Here, we describe the emergence of genomics research and the development of bioinformatics in Africa through H3ABioNet.",Letter,vol2
pap1112,fa4240d02661b2fe4762ffdc557995e0f0216170,jou81,BMC Bioinformatics,Proceedings of the 16th Annual UT-KBRIN Bioinformatics Summit 2016: bioinformatics,,Article,vol81
pap1113,8263465748cb85e0945d5c530812b87f2b962bde,jou210,Microbes and Environments,Metagenomics and Bioinformatics in Microbial Ecology: Current Status and Beyond,"Metagenomic approaches are now commonly used in microbial ecology to study microbial communities in more detail, including many strains that cannot be cultivated in the laboratory. Bioinformatic analyses make it possible to mine huge metagenomic datasets and discover general patterns that govern microbial ecosystems. However, the findings of typical metagenomic and bioinformatic analyses still do not completely describe the ecology and evolution of microbes in their environments. Most analyses still depend on straightforward sequence similarity searches against reference databases. We herein review the current state of metagenomics and bioinformatics in microbial ecology and discuss future directions for the field. New techniques will allow us to go beyond routine analyses and broaden our knowledge of microbial ecosystems. We need to enrich reference databases, promote platforms that enable meta- or comprehensive analyses of diverse metagenomic datasets, devise methods that utilize long-read sequence information, and develop more powerful bioinformatic methods to analyze data from diverse perspectives.",Conference paper,vol210
pap1114,228de021449678a97656b6f3f367c4f45fb28dab,con19,International Conference on Conceptual Structures,Bioinformatics Original Paper Copasi—a Complex Pathway Simulator,"Motivation: Simulation and modeling is becoming a standard approach to understand complex biochemical processes. Therefore, there is a big need for software tools that allow access to diverse simulation and modeling methods as well as support for the usage of these methods. Results: Here, we present COPASI, a platform-independent and user-friendly biochemical simulator that offers several unique features. We discuss numerical issues with these features; in particular, the criteria to switch between stochastic and deterministic simulation methods, hybrid deterministic–stochastic methods, and the importance of random number generator numerical resolution in stochastic simulation. Availability: The complete software is available in binary (executable) for MS Windows, OS X, Linux (Intel) and Sun Solaris (SPARC), as well as the full source code under an open source license from http://www.",Erratum,pro19
pap1115,011625cef5e11fc55ce285faae1c1ca338d8d78a,con17,International Conference on Statistical and Scientific Database Management,Bioinformatics Original Paper Improved Scoring of Functional Groups from Gene Expression Data by Decorrelating Go Graph Structure,"Motivation: The result of a typical microarray experiment is a long list of genes with corresponding expression measurements. This list is only the starting point for a meaningful biological interpretation. Modern methods identify relevant biological processes or functions from gene expression data by scoring the statistical significance of prede-fined functional gene groups, e.g. based on Gene Ontology (GO). We develop methods that increase the explanatory power of this approach by integrating knowledge about relationships between the GO terms into the calculation of the statistical significance. Results: We present two novel algorithms that improve GO group scoring using the underlying GO graph topology. The algorithms are evaluated on real and simulated gene expression data. We show that both methods eliminate local dependencies between GO terms and point to relevant areas in the GO graph that remain undetected with state-of-the-art algorithms for scoring functional terms. A simulation study demonstrates that the new methods exhibit a higher level of detecting relevant biological terms than competing methods.",Erratum,pro17
pap1116,b779b41898931373f6e1f302b0a0d9a38313956a,con17,International Conference on Statistical and Scientific Database Management,Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics,"The random forest (RF) algorithm by Leo Breiman has become a standard data analysis tool in bioinformatics. It has shown excellent performance in settings where the number of variables is much larger than the number of observations, can cope with complex interaction structures as well as highly correlated variables and return measures of variable importance. This paper synthesizes 10 years of RF development with emphasis on applications to bioinformatics and computational biology. Special attention is paid to practical aspects such as the selection of parameters, available RF implementations, and important pitfalls and biases of RF and its variable importance measures (VIMs). The paper surveys recent developments of the methodology relevant to bioinformatics as well as some representative examples of RF applications in this context and possible directions for future research. © 2012 Wiley Periodicals, Inc.",Erratum,pro17
pap1117,8799948d9a7ddef5a90276ae807ff0442f35bab6,con3,Knowledge Discovery and Data Mining,Bioinformatics Applications Note Proteowizard: Open Source Software for Rapid Proteomics Tools Development,"The ProteoWizard software project provides a modular and extensible set of open-source, cross-platform tools and libraries. The tools perform proteomics data analyses; the libraries enable rapid tool creation by providing a robust, pluggable development framework that simplifies and unifies data file access, and performs standard proteomics and LCMS dataset computations. The library contains readers and writers of the mzML data format, which has been written using modern C++ techniques and design principles and supports a variety of platforms with native compilers. The software has been specifically released under the Apache v2 license to ensure it can be used in both academic and commercial projects. In addition to the library, we also introduce a rapidly growing set of companion tools whose implementation helps to illustrate the simplicity of developing applications on top of the ProteoWizard library. Availability: Cross-platform software that compiles using native compilers (i.e. GCC on Linux, MSVC on Windows and XCode on OSX) is available for download free of charge, at http://proteowizard.sourceforge.net. This website also provides code examples, and documentation. It is our hope the ProteoWizard project will become a standard platform for proteomics development; consequently, code use, contribution and further development are strongly encouraged. Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro3
pap1118,8fdfb72555c7ca65b00e0496f36c9639e19d351f,con93,International Conference on Computational Logic,In the loop: promoter–enhancer interactions and bioinformatics,"Enhancer–promoter regulation is a fundamental mechanism underlying differential transcriptional regulation. Spatial chromatin organization brings remote enhancers in contact with target promoters in cis to regulate gene expression. There is considerable evidence for promoter–enhancer interactions (PEIs). In the recent years, genome-wide analyses have identified signatures and mapped novel enhancers; however, being able to precisely identify their target gene(s) requires massive biological and bioinformatics efforts. In this review, we give a short overview of the chromatin landscape and transcriptional regulation. We discuss some key concepts and problems related to chromatin interaction detection technologies, and emerging knowledge from genome-wide chromatin interaction data sets. Then, we critically review different types of bioinformatics analysis methods and tools related to representation and visualization of PEI data, raw data processing and PEI prediction. Lastly, we provide specific examples of how PEIs have been used to elucidate a functional role of non-coding single-nucleotide polymorphisms. The topic is at the forefront of epigenetic research, and by highlighting some future bioinformatics challenges in the field, this review provides a comprehensive background for future PEI studies.",Erratum,pro93
pap1119,3a3256ab94a4af91c2ddaf2388e56d2dcaa4e5d6,con89,Conference on Uncertainty in Artificial Intelligence,Big Data Analytics in Bioinformatics: A Machine Learning Perspective,"Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. 
Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. 
However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.",Erratum,pro89
pap1120,d78797ea88f50994e97358fedbc9922baaffba8d,con45,International Conference on Global Software Engineering,Bioinformatics For Geneticists,"Thank you for reading bioinformatics for geneticists. As you may know, people have search numerous times for their favorite novels like this bioinformatics for geneticists, but end up in harmful downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some malicious virus inside their laptop. bioinformatics for geneticists is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library saves in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the bioinformatics for geneticists is universally compatible with any devices to read.",Erratum,pro45
pap1121,71d7233f4e490dcd1e221cf0d4b7584a6806b6b2,con31,International Conference on Evaluation & Assessment in Software Engineering,A selective review of robust variable selection with applications in bioinformatics,"A drastic amount of data have been and are being generated in bioinformatics studies. In the analysis of such data, the standard modeling approaches can be challenged by the heavy-tailed errors and outliers in response variables, the contamination in predictors (which may be caused by, for instance, technical problems in microarray gene expression studies), model mis-specification and others. Robust methods are needed to tackle these challenges. When there are a large number of predictors, variable selection can be as important as estimation. As a generic variable selection and regularization tool, penalization has been extensively adopted. In this article, we provide a selective review of robust penalized variable selection approaches especially designed for high-dimensional data from bioinformatics and biomedical studies. We discuss the robust loss functions, penalty functions and computational algorithms. The theoretical properties and implementation are also briefly examined. Application examples of the robust penalization approaches in representative bioinformatics and biomedical studies are also illustrated.",Erratum,pro31
pap1122,138b2db34defab5f8836851e4ddd0d3b7394e36b,con5,Technical Symposium on Computer Science Education,BIOINFORMATICS APPLICATIONS,"Summary: A tool to predict the effect that newly discovered genomic variants have on known transcripts is indispensible in prioritizing and categorizing such variants. In Ensembl, a web-based tool (the SNP Effect Predictor) and API interface can now functionally annotate variants in all Ensembl and Ensembl Genomes supported species. Availability: The Ensembl SNP Effect Predictor can be accessed via the Ensembl website at http://www.ensembl.org/. The Ensembl API (http://www.ensembl.org/info/docs/api/ api_installation.html for installation instructions) is open source software.",Erratum,pro5
pap1123,a56eefb5300ca03bbdddfec6153bafc9d91debb7,con1,International Conference on Human Factors in Computing Systems,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btm108 Genetics and population analysis GenABEL: an R library for genome-wide association analysis,,Erratum,pro1
pap1124,262758686200e7d847e5489fff55b6a5919fcb1d,con19,International Conference on Conceptual Structures,"Bioinformatics approaches, prospects and challenges of food bioactive peptide research",,Erratum,pro19
pap1125,c273e7be5e2c9a316efbd01687852bc3af364455,con100,International Conference on Automatic Face and Gesture Recognition,Bioinformatics in Africa: The Rise of Ghana?,"Until recently, bioinformatics, an important discipline in the biological sciences, was largely limited to countries with advanced scientific resources. Nonetheless, several developing countries have lately been making progress in bioinformatics training and applications. In Africa, leading countries in the discipline include South Africa, Nigeria, and Kenya. However, one country that is less known when it comes to bioinformatics is Ghana. Here, I provide a first description of the development of bioinformatics activities in Ghana and how these activities contribute to the overall development of the discipline in Africa. Over the past decade, scientists in Ghana have been involved in publications incorporating bioinformatics analyses, aimed at addressing research questions in biomedical science and agriculture. Scarce research funding and inadequate training opportunities are some of the challenges that need to be addressed for Ghanaian scientists to continue developing their expertise in bioinformatics.",Erratum,pro100
pap1126,26b3223735e316901bdadb831f012ab67b0456fe,jou133,PLoS ONE,Genomics Virtual Laboratory: A Practical Bioinformatics Workbench for the Cloud,"Background Analyzing high throughput genomics data is a complex and compute intensive task, generally requiring numerous software tools and large reference data sets, tied together in successive stages of data transformation and visualisation. A computational platform enabling best practice genomics analysis ideally meets a number of requirements, including: a wide range of analysis and visualisation tools, closely linked to large user and reference data sets; workflow platform(s) enabling accessible, reproducible, portable analyses, through a flexible set of interfaces; highly available, scalable computational resources; and flexibility and versatility in the use of these resources to meet demands and expertise of a variety of users. Access to an appropriate computational platform can be a significant barrier to researchers, as establishing such a platform requires a large upfront investment in hardware, experience, and expertise. Results We designed and implemented the Genomics Virtual Laboratory (GVL) as a middleware layer of machine images, cloud management tools, and online services that enable researchers to build arbitrarily sized compute clusters on demand, pre-populated with fully configured bioinformatics tools, reference datasets and workflow and visualisation options. The platform is flexible in that users can conduct analyses through web-based (Galaxy, RStudio, IPython Notebook) or command-line interfaces, and add/remove compute nodes and data resources as required. Best-practice tutorials and protocols provide a path from introductory training to practice. The GVL is available on the OpenStack-based Australian Research Cloud (http://nectar.org.au) and the Amazon Web Services cloud. The principles, implementation and build process are designed to be cloud-agnostic. Conclusions This paper provides a blueprint for the design and implementation of a cloud-based Genomics Virtual Laboratory. We discuss scope, design considerations and technical and logistical constraints, and explore the value added to the research community through the suite of services and resources provided by our implementation.",Conference paper,vol133
pap1127,640f1a4c0d7a8825611a872e11d232ba73a81559,jou211,"Genomics, Proteomics & Bioinformatics","Translational Bioinformatics: Past, Present, and Future",,Letter,vol211
pap1128,995bbef8ff7bbbb7d994e3bfeed1ac169356db5f,con101,International Conference on Biometrics,Survey of Natural Language Processing Techniques in Bioinformatics,"Informatics methods, such as text mining and natural language processing, are always involved in bioinformatics research. In this study, we discuss text mining and natural language processing methods in bioinformatics from two perspectives. First, we aim to search for knowledge on biology, retrieve references using text mining methods, and reconstruct databases. For example, protein-protein interactions and gene-disease relationship can be mined from PubMed. Then, we analyze the applications of text mining and natural language processing techniques in bioinformatics, including predicting protein structure and function, detecting noncoding RNA. Finally, numerous methods and applications, as well as their contributions to bioinformatics, are discussed for future use by text mining and natural language processing researchers.",Erratum,pro101
pap1129,ae2164e0358f21a22f47d01a58745f09ac0c4dea,jou212,Network Modeling Analysis in Health Informatics and Bioinformatics,"Big data analytics in bioinformatics: architectures, techniques, tools and issues",,Conference paper,vol212
pap1130,d05cf07b2cb746f812a7268dc842ff23bd38f928,con79,IEEE Annual Symposium on Foundations of Computer Science,Bioinformatics programs are 31-fold over-represented among the highest impact scientific papers of the past two decades,"MOTIVATION
To analyze the relative proportion of bioinformatics papers and their non-bioinformatics counterparts in the top 20 most cited papers annually for the past two decades.


RESULTS
When defining bioinformatics papers as encompassing both those that provide software for data analysis or methods underlying data analysis software, we find that over the past two decades, more than a third (34%) of the most cited papers in science were bioinformatics papers, which is approximately a 31-fold enrichment relative to the total number of bioinformatics papers published. More than half of the most cited papers during this span were bioinformatics papers. Yet, the average 5-year JIF of top 20 bioinformatics papers was 7.7, whereas the average JIF for top 20 non-bioinformatics papers was 25.8, significantly higher (P < 4.5 × 10(-29)). The 20-year trend in the average JIF between the two groups suggests the gap does not appear to be significantly narrowing. For a sampling of the journals producing top papers, bioinformatics journals tended to have higher Gini coefficients, suggesting that development of novel bioinformatics resources may be somewhat 'hit or miss'. That is, relative to other fields, bioinformatics produces some programs that are extremely widely adopted and cited, yet there are fewer of intermediate success.


CONTACT
jdwren@gmail.com


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro79
pap1131,d58ce51acf831b07613ca9394e86273115ba4dd3,jou81,BMC Bioinformatics,"Knowledge Discovery and interactive Data Mining in Bioinformatics - State-of-the-Art, future challenges and research directions",,Conference paper,vol81
pap1132,c464387a83a573c5154d551214628c760ff1a2b9,con11,European Conference on Modelling and Simulation,Unipro UGENE: a unified bioinformatics toolkit,"UNLABELLED
Unipro UGENE is a multiplatform open-source software with the main goal of assisting molecular biologists without much expertise in bioinformatics to manage, analyze and visualize their data. UGENE integrates widely used bioinformatics tools within a common user interface. The toolkit supports multiple biological data formats and allows the retrieval of data from remote data sources. It provides visualization modules for biological objects such as annotated genome sequences, Next Generation Sequencing (NGS) assembly data, multiple sequence alignments, phylogenetic trees and 3D structures. Most of the integrated algorithms are tuned for maximum performance by the usage of multithreading and special processor instructions. UGENE includes a visual environment for creating reusable workflows that can be launched on local resources or in a High Performance Computing (HPC) environment. UGENE is written in C++ using the Qt framework. The built-in plugin system and structured UGENE API make it possible to extend the toolkit with new functionality.


AVAILABILITY AND IMPLEMENTATION
UGENE binaries are freely available for MS Windows, Linux and Mac OS X at http://ugene.unipro.ru/download.html. UGENE code is licensed under the GPLv2; the information about the code licensing and copyright of integrated tools can be found in the LICENSE.3rd_party file provided with the source bundle.",Erratum,pro11
pap1133,460eba44832548ea3fec1ab942515d42735e5839,con81,International Conference on Learning Representations,proteins STRUCTURE O FUNCTION O BIOINFORMATICS Protonate3D: Assignment of ionization,,Erratum,pro81
pap1134,1c2ee9858ac6ebbfbb21f8a4a8f8ac8d281b97e1,jou213,Journal of Cellular Physiology,Big Data Bioinformatics,"Recent technological advances allow for high throughput profiling of biological systems in a cost‐efficient manner. The low cost of data generation is leading us to the “big data” era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this review, we introduce key concepts in the analysis of big data, including both “machine learning” algorithms as well as “unsupervised” and “supervised” examples of each. We note packages for the R programming language that are available to perform machine learning analyses. In addition to programming based solutions, we review webservers that allow users with limited or no programming background to perform these analyses on large data compendia. J. Cell. Physiol. 229: 1896–1900, 2014. © 2014 Wiley Periodicals, Inc.",Article,vol213
pap1135,81cc8e468ef4fc555b5bb89697f587b2e2969f97,jou214,Current Protocols in Human Genetics,Getting Started with Microbiome Analysis: Sample Acquisition to Bioinformatics,"Historically, in order to study microbes, it was necessary to grow them in the laboratory. It was clear though that many microbe communities were refractory to study because none of the members could be grown outside of their native habitat. The development of culture‐independent methods to study microbiota using high‐throughput sequencing of the 16S ribosomal RNA gene variable regions present in all prokaryotic organisms has provided new opportunities to investigate complex microbial communities. In this unit, the process for a microbiome analysis is described. Many of the components required for this process may already exist. A pipeline is described for acquisition of samples from different sites on the human body, isolation of microbial DNA, and DNA sequencing using the Illumina MiSeq sequencing platform. Finally, a new analytical workflow for basic bioinformatics data analysis, QWRAP, is described, which can be used by clinical and basic science investigators. Curr. Protoc. Hum. Genet. 82:18.8.1‐18.8.29. © 2014 by John Wiley & Sons, Inc.",Conference paper,vol214
pap1136,f3940978ecf60c554b654bd1afbb3ab03f660ff8,con51,Brazilian Symposium on Software Engineering,Bioinformatics Curriculum Guidelines: Toward a Definition of Core Competencies,"Rapid advances in the life sciences and in related information technologies necessitate the ongoing refinement of bioinformatics educational programs in order to maintain their relevance. As the discipline of bioinformatics and computational biology expands and matures, it is important to characterize the elements that contribute to the success of professionals in this field. These individuals work in a wide variety of settings, including bioinformatics core facilities, biological and medical research laboratories, software development organizations, pharmaceutical and instrument development companies, and institutions that provide education, service, and training. In response to this need, the Curriculum Task Force of the International Society for Computational Biology (ISCB) Education Committee seeks to define curricular guidelines for those who train and educate bioinformaticians. The previous report of the task force summarized a survey that was conducted to gather input regarding the skill set needed by bioinformaticians [1]. The current article details a subsequent effort, wherein the task force broadened its perspectives by examining bioinformatics career opportunities, surveying directors of bioinformatics core facilities, and reviewing bioinformatics education programs. 
 
The bioinformatics literature provides valuable perspectives on bioinformatics education by defining skill sets needed by bioinformaticians, presenting approaches for providing informatics training to biologists, and discussing the roles of bioinformatics core facilities in training and education. 
 
The skill sets required for success in the field of bioinformatics are considered by several authors: Altman [2] defines five broad areas of competency and lists key technologies; Ranganathan [3] presents highlights from the Workshops on Education in Bioinformatics, discussing challenges and possible solutions; Yale's interdepartmental PhD program in computational biology and bioinformatics is described in [4], which lists the general areas of knowledge of bioinformatics; in a related article, a graduate of Yale's PhD program reflects on the skills needed by a bioinformatician [5]; Altman and Klein [6] describe the Stanford Biomedical Informatics (BMI) Training Program, presenting observed trends among BMI students; the American Medical Informatics Association defines competencies in the related field of biomedical informatics in [7]; and the approaches used in several German universities to implement bioinformatics education are described in [8]. 
 
Several approaches to providing bioinformatics training for biologists are described in the literature. Tan et al. [9] report on workshops conducted to identify a minimum skill set for biologists to be able to address the informatics challenges of the “-omics” era. They define a requisite skill set by analyzing responses to questions about the knowledge, skills, and abilities that biologists should possess. The authors in [10] present examples of strategies and methods for incorporating bioinformatics content into undergraduate life sciences curricula. Pevzner and Shamir [11] propose that undergraduate biology curricula should contain an additional course, “Algorithmic, Mathematical, and Statistical Concepts in Biology.” Wingren and Botstein [12] present a graduate course in quantitative biology that is based on original, pathbreaking papers in diverse areas of biology. Johnson and Friedman [13] evaluate the effectiveness of incorporating biological informatics into a clinical informatics program. The results reported are based on interviews of four students and informal assessments of bioinformatics faculty. 
 
The challenges and opportunities relevant to training and education in the context of bioinformatics core facilities are discussed by Lewitter et al. [14]. Relatedly, Lewitter and Rebhan [15] provide guidance regarding the role of a bioinformatics core facility in hiring biologists and in furthering their education in bioinformatics. Richter and Sexton [16] describe a need for highly trained bioinformaticians in core facilities and provide a list of requisite skills. Similarly, Kallioniemi et al. [17] highlight the roles of bioinformatics core units in education and training. 
 
This manuscript expands the body of knowledge pertaining to bioinformatics curriculum guidelines by presenting the results from a broad set of surveys (of core facility directors, of career opportunities, and of existing curricula). Although there is some overlap in the findings of the surveys, they are reported separately, in order to avoid masking the unique aspects of each of the perspectives and to demonstrate that the same themes arise, even when different perspectives are considered. The authors derive from their surveys an initial set of core competencies and relate the competencies to three different categories of professions that have a need for bioinformatics training.",Erratum,pro51
pap1137,001eeb49fd35bc035d53da5575ff41e2752d0f32,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Chemo- and bioinformatics resources for in silico drug discovery from medicinal plants beyond their traditional use: a critical review.,"In silico approaches have been widely recognised to be useful for drug discovery. Here, we consider the significance of available databases of medicinal plants and chemo- and bioinformatics tools for in silico drug discovery beyond the traditional use of folk medicines. This review contains a practical example of the application of combined chemo- and bioinformatics methods to study pleiotropic therapeutic effects (known and novel) of 50 medicinal plants from Traditional Indian Medicine.",Erratum,pro13
pap1138,c03fd5d4d6521c6398b73ff242d97fff4630aac4,con57,International Workshop on Agent-Oriented Software Engineering,Bioinformatics Education—Perspectives and Challenges out of Africa,"The discipline of bioinformatics has developed rapidly since the complete sequencing of the first genomes in the 1990s. The development of many high-throughput techniques during the last decades has ensured that bioinformatics has grown into a discipline that overlaps with, and is required for, the modern practice of virtually every field in the life sciences. This has placed a scientific premium on the availability of skilled bioinformaticians, a qualification that is extremely scarce on the African continent. The reasons for this are numerous, although the absence of a skilled bioinformatician at academic institutions to initiate a training process and build sustained capacity seems to be a common African shortcoming. This dearth of bioinformatics expertise has had a knock-on effect on the establishment of many modern high-throughput projects at African institutes, including the comprehensive and systematic analysis of genomes from African populations, which are among the most genetically diverse anywhere on the planet. Recent funding initiatives from the National Institutes of Health and the Wellcome Trust are aimed at ameliorating this shortcoming. In this paper, we discuss the problems that have limited the establishment of the bioinformatics field in Africa, as well as propose specific actions that will help with the education and training of bioinformaticians on the continent. This is an absolute requirement in anticipation of a boom in high-throughput approaches to human health issues unique to data from African populations.",Erratum,pro57
pap1139,cd238467fdd77d04902d1ceafc9d4b590093f659,jou215,CBE - Life Sciences Education,A Survey of Scholarly Literature Describing the Field of Bioinformatics Education and Bioinformatics Educational Research,"This article provides an overview of the state of research in bioinformatics education in the years 1998 through 2013. It identifies current curricular approaches for integrating bioinformatics education, concepts and skills being taught, pedagogical approaches and methods of delivery, and educational research and evaluation results.",Conference paper,vol215
pap1140,001010b8b964a429f50a49a219c4ed4ae97a3400,con22,Grid Computing Environments,BMC Bioinformatics BioMed Central Methodology article,,Erratum,pro22
pap1141,cabf20b781705bc68322972385ab2c6a4dc6b871,jou81,BMC Bioinformatics,Verification and validation of bioinformatics software without a gold standard: a case study of BWA and Bowtie,,Article,vol81
pap1142,07ec7585d8a336a5f8da022b5b23d1a0aa38f522,jou158,Lecture Notes in Computer Science,Bioinformatics Research and Applications,,Letter,vol158
pap1143,03633f1e781670a023fb2cdc7e3cb0da6fd5591a,con76,IEEE International Conference on Tools with Artificial Intelligence,Challenges in RNA virus bioinformatics,"Abstract Motivation: Computer-assisted studies of structure, function and evolution of viruses remains a neglected area of research. The attention of bioinformaticians to this interesting and challenging field is far from commensurate with its medical and biotechnological importance. It is telling that out of >200 talks held at ISMB 2013, the largest international bioinformatics conference, only one presentation explicitly dealt with viruses. In contrast to many broad, established and well-organized bioinformatics communities (e.g. structural genomics, ontologies, next-generation sequencing, expression analysis), research groups focusing on viruses can probably be counted on the fingers of two hands. Results: The purpose of this review is to increase awareness among bioinformatics researchers about the pressing needs and unsolved problems of computational virology. We focus primarily on RNA viruses that pose problems to many standard bioinformatics analyses owing to their compact genome organization, fast mutation rate and low evolutionary conservation. We provide an overview of tools and algorithms for handling viral sequencing data, detecting functionally important RNA structures, classifying viral proteins into families and investigating the origin and evolution of viruses. Contact: manja@uni-jena.de Supplementary information: Supplementary data are available at Bioinformatics online. The references for this article can be found in the Supplementary Material.",Erratum,pro76
pap1144,2bc20367f1c7e16d23f2d2c78bfe38fb1450f5dd,con5,Technical Symposium on Computer Science Education,"Bioinformatics for Beginners: Genes, Genomes, Molecular Evolution, Databases and Analytical Tools","Bioinformatics for Beginners provides a coherent and friendly treatment of bioinformatics for any student or scientist within biology who has not routinely performed bioinformatic analysis.  
 
 The book discusses relevant principles needed to understand the theoretical underpinnings of bioinformatic analysis, and demonstrates with examples targeted analysis using freely available web-based software and publicly available databases. Eschewing non-essential information, the work focuses on principles and hands-on analysis and points to many further study options. 
 
 
 Avoids non-essential coverage yet fully describes the field for beginners - in approximately 200 pages of text 
 
 Explains the molecular basis of evolution to place bioinformatic analysis in biological context 
 
 Provides useful links to the vast resource of publicly available bioinformatic databases and analysis tools 
 
 Over 100 figures aid in concept discovery and illustration",Erratum,pro5
pap1145,8b1b5d208cc81fa7a194a843b8140f8f5d45ecb5,con77,International Conference on Artificial Neural Networks,"EDAM: an ontology of bioinformatics operations, types of data and identifiers, topics and formats","Motivation: Advancing the search, publication and integration of bioinformatics tools and resources demands consistent machine-understandable descriptions. A comprehensive ontology allowing such descriptions is therefore required. Results: EDAM is an ontology of bioinformatics operations (tool or workflow functions), types of data and identifiers, application domains and data formats. EDAM supports semantic annotation of diverse entities such as Web services, databases, programmatic libraries, standalone tools, interactive applications, data schemas, datasets and publications within bioinformatics. EDAM applies to organizing and finding suitable tools and data and to automating their integration into complex applications or workflows. It includes over 2200 defined concepts and has successfully been used for annotations and implementations. Availability: The latest stable version of EDAM is available in OWL format from http://edamontology.org/EDAM.owl and in OBO format from http://edamontology.org/EDAM.obo. It can be viewed online at the NCBO BioPortal and the EBI Ontology Lookup Service. For documentation and license please refer to http://edamontology.org. This article describes version 1.2 available at http://edamontology.org/EDAM_1.2.owl. Contact: jison@ebi.ac.uk",Erratum,pro77
pap1146,6901570083ecd16ceaf296de88819f0105d309aa,jou216,Analytical Chemistry,Bioinformatics: The Next Frontier of Metabolomics,"Bioinformatic tools are required to carry out essential functions such as statistical analyses and database functionalities. Now, they are also needed for one of the most difficult tasks, helping researchers decide which metabolites are the most biologically meaningful. This can be achieved through aiding the identification process, reducing feature redundancy, putting forward better candidates for tandem mass spectrometry (MS/MS), speeding up or automating the workflow, deconvolving the feature list through meta-analysis or multigroup analysis, or using stable isotopes and pathway mapping. This review thus focuses on the most recent and innovative bioinformatic advancements for identifying metabolites. 
 
A primary objective of metabolomics beyond biomarker discovery is to identify the most meaningful metabolites that correlate with disease pathogenesis or other perturbations of metabolism. Metabolites play important roles in biological pathways; their flux or differential regulation (dysregulation) can reveal novel insights into disease and environmental influences. Therefore, one of the most important goals of metabolomic analysis has been to assign metabolite identity so they can be used for further statistical and informed pathway analysis.1,2 Over the past few years, technologies for analyzing metabolites by untargeted or targeted metabolomics have undergone extensive improvements. Strides to establish the most efficient protocols for experimental design, sample extraction techniques, and data acquisition have paid off providing robust complex data sets.3−9 As more is being required of these data sets such as assigning identity and biological meaning to the features, bioinformatics is the area of metabolomics which is currently undergoing the most needed growth. 
 
It is often the case that metabolomic analysis results in a list of metabolites with low specificity for the disease or stimulus being studied (Figure ​(Figure1).1). Some of these metabolites seem to be dysregulated in a variety of diseases such as acylcarnitines10−13 and fatty acids.14−17 They may be more indicative of a perturbed systemic cause (appetite, physical activity, diurnal rhythm changes, etc..), sample contamination, or instrumental/bioinformatic noise, rather than a specific biomarker of disease. An example of this can be seen in the analysis of urinary biomarkers of ionizing radiation, where dicarboxylic acids were downregulated in the rat after radiation exposure. It was proven that this observation was actually caused by a decreased appetite after radiation exposure perturbing the β-oxidation pathway and not from radiation-induced cellular changes.18,19 Furthermore, dicarboxylic acids can leach out from plastics during the extraction process, further adding to the ambiguity of their role in ionizing radiation.20 
 
 
 
Figure 1 
 
Biomarkers that have high vs low disease specificity. 
 
 
 
As well as identifying the correct source of the biomarkers, it is also important to identify their physiological role and how to utilize them as therapeutic targets. This first has to start with the identification of the metabolite and is determined by filtering thresholds set by the user which is intrinsically biased. These thresholds include those for fold change and p-value, which are highly dependent on the experiment; in vitro experiments would exhibit lower variation between biological replicates than in vivo. The ease of identifying the metabolite is also determined by its concentration in the sample and previous annotation in metabolite databases. Filtering thresholds for metabolite intensity that are set too high may omit important biologically meaningful metabolites rather than noise. Furthermore, a metabolite that is novel or not curated in a database may not be taken into consideration based on the chemical knowledge of the researcher and what they deem as meaningful. 
 
In order to transform the complex list of identified metabolites into markers of disease, or assign what role they play, bioinformatic tools can aid in identifying the potential pathways that the metabolite may belong to. It is then that the researcher can use this knowledge surrounding the biology of the metabolite to probe the mechanism of the disease. Untargeted metabolomics has already been used in such a manner to find the source of neuropathic pain.21N,N-Dimethylsphingosine was dysregulated in a rat model of neuropathic pain, furthermore when dosed to control rats it induced mechanical hypersensitivity. This metabolite implicated the sphingomyelin-ceramide pathway as a potential therapeutic target. Antimetabolite inhibitors of enzymes in this pathway were tested and were able to ameliorate neuropathic pain (unpublished data). This study holds promise for other metabolomic studies to maximize the potential information contained within the data for finding therapeutics of disease rather than only providing lists of dysregulated metabolites.",Conference paper,vol216
pap1147,638342beccc1922285c81499e3e67fb65b32d6e8,con95,IEEE International Conference on Computer Vision,Random Forest for Bioinformatics,,Erratum,pro95
pap1148,458fadd0bb30f05386a9f5ab19bf977a889159d6,con62,Australian Software Engineering Conference,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Transcription factor (TF) ChIP-seq datasets have particular characteristics that provide unique challenges and opportunities for motif discovery. Most existing motif discovery algorithms do not scale well to such large datasets, or fail to report many motifs associated with cofactors of the ChIP-ed TF. Results: We present DREME, a motif discovery algorithm speciﬁcally designed to ﬁnd the short, core DNA-binding motifs of eukaryotic TFs, and optimized to analyze very large ChIP-seq datasets in minutes. Using DREME, we discover the binding motifs of the the ChIP-ed TF and many cofactors in mouse ES cell (mESC), mouse erythrocyte and human cell line ChIP-seq datasets. For example, in mESC ChIP-seq data for the TF Esrrb, we discover the binding motifs for eight cofactor TFs important in the maintenance of pluripotency. Several other commonly used algorithms ﬁnd at most two cofactor motifs in this same dataset. DREME can also perform discriminative motif discovery, and we use this feature to provide evidence that Sox2 and Oct4 do not bind in mES cells as an obligate heterodimer. DREME is much faster than many commonly used algorithms, scales linearly in dataset size, ﬁnds multiple, non-redundant motifs and reports a reliable measure of statistical signiﬁcance for each motif found. DREME is available as part of the MEME Suite of motif-based sequence analysis tools (http://meme.nbcr.net).",Erratum,pro62
pap1149,97d667dbff26423037bc23240579a42d39e68aee,jou133,PLoS ONE,The Role of Balanced Training and Testing Data Sets for Binary Classifiers in Bioinformatics,"Training and testing of conventional machine learning models on binary classification problems depend on the proportions of the two outcomes in the relevant data sets. This may be especially important in practical terms when real-world applications of the classifier are either highly imbalanced or occur in unknown proportions. Intuitively, it may seem sensible to train machine learning models on data similar to the target data in terms of proportions of the two binary outcomes. However, we show that this is not the case using the example of prediction of deleterious and neutral phenotypes of human missense mutations in human genome data, for which the proportion of the binary outcome is unknown. Our results indicate that using balanced training data (50% neutral and 50% deleterious) results in the highest balanced accuracy (the average of True Positive Rate and True Negative Rate), Matthews correlation coefficient, and area under ROC curves, no matter what the proportions of the two phenotypes are in the testing data. Besides balancing the data by undersampling the majority class, other techniques in machine learning include oversampling the minority class, interpolating minority-class data points and various penalties for misclassifying the minority class. However, these techniques are not commonly used in either the missense phenotype prediction problem or in the prediction of disordered residues in proteins, where the imbalance problem is substantial. The appropriate approach depends on the amount of available data and the specific problem at hand.",Article,vol133
pap1150,14d46d1d2f7cd38f68f9fc68726b45db57e327b9,con106,International Conference on Mobile Data Management,Bioinformatics: Sequence and Genome Analysis,Preface Chapter 1. Historical introduction and overview Chapter 2. Collecting and storing sequences in the laboratory Chapter 3. Alignment of pairs of sequences Chapter 4. Introduction to probability and statistical analysis of sequence alignments Chapter 5. Multiple sequence alignment Chapter 6. Sequence database searching for similar sequences Chapter 7. Phylogenetic prediction Chapter 8. Prediction of RNA secondary structure Chapter 9. Gene prediction and regulation Chapter 10. Protein classification and structure prediction Chapter 11. Genome analysis Chapter 12. Bioinformatics programming using Perl and Perl modules Chapter 13. Analysis of microarrays,Erratum,pro106
pap1151,83e21271aa794fd03ec49c059ec6fb831307a3b5,con4,Conference on Innovative Data Systems Research,ViPR: an open bioinformatics database and analysis resource for virology research,"The Virus Pathogen Database and Analysis Resource (ViPR, www.ViPRbrc.org) is an integrated repository of data and analysis tools for multiple virus families, supported by the National Institute of Allergy and Infectious Diseases (NIAID) Bioinformatics Resource Centers (BRC) program. ViPR contains information for human pathogenic viruses belonging to the Arenaviridae, Bunyaviridae, Caliciviridae, Coronaviridae, Flaviviridae, Filoviridae, Hepeviridae, Herpesviridae, Paramyxoviridae, Picornaviridae, Poxviridae, Reoviridae, Rhabdoviridae and Togaviridae families, with plans to support additional virus families in the future. ViPR captures various types of information, including sequence records, gene and protein annotations, 3D protein structures, immune epitope locations, clinical and surveillance metadata and novel data derived from comparative genomics analysis. Analytical and visualization tools for metadata-driven statistical sequence analysis, multiple sequence alignment, phylogenetic tree construction, BLAST comparison and sequence variation determination are also provided. Data filtering and analysis workflows can be combined and the results saved in personal ‘Workbenches’ for future use. ViPR tools and data are available without charge as a service to the virology research community to help facilitate the development of diagnostics, prophylactics and therapeutics for priority pathogens and other viruses.",Erratum,pro4
pap1152,eb9bb36f0a3d2d21157c0479aa784f12d996f237,con12,The Compass,BIOINFORMATICS APPLICATIONS,"Summary: De novo assembly tools play a main role in reconstructing genomes from next-generation sequencing (NGS) data and usually yield a number of contigs. Using paired-read sequencing data it is possible to assess the order, distance and orientation of contigs and combine them into so-called scaffolds . Although the latter process is a crucial step in ﬁnishing genomes",Erratum,pro12
pap1153,7b4c5f3a1fdcc582f63796d238cfef3ef0b73192,jou217,Nature Chemical Biology,A widespread self-cleaving ribozyme class is revealed by bioinformatics,,Conference paper,vol217
pap1154,fff1e960ab8471252f2ba4a188332ee04d51722a,con7,International Symposium on Intelligent Data Analysis,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm098 Databases and ontologies UniRef: comprehensive and non-redundant UniProt reference,,Erratum,pro7
pap1155,8f124016fda610e3e9147dd046fb306f219a8c85,jou218,Viruses,Coronavirus Genomics and Bioinformatics Analysis,"The drastic increase in the number of coronaviruses discovered and coronavirus genomes being sequenced have given us an unprecedented opportunity to perform genomics and bioinformatics analysis on this family of viruses. Coronaviruses possess the largest genomes (26.4 to 31.7 kb) among all known RNA viruses, with G + C contents varying from 32% to 43%. Variable numbers of small ORFs are present between the various conserved genes (ORF1ab, spike, envelope, membrane and nucleocapsid) and downstream to nucleocapsid gene in different coronavirus lineages. Phylogenetically, three genera, Alphacoronavirus, Betacoronavirus and Gammacoronavirus, with Betacoronavirus consisting of subgroups A, B, C and D, exist. A fourth genus, Deltacoronavirus, which includes bulbul coronavirus HKU11, thrush coronavirus HKU12 and munia coronavirus HKU13, is emerging. Molecular clock analysis using various gene loci revealed that the time of most recent common ancestor of human/civet SARS related coronavirus to be 1999–2002, with estimated substitution rate of 4×10−4 to 2×10−2 substitutions per site per year. Recombination in coronaviruses was most notable between different strains of murine hepatitis virus (MHV), between different strains of infectious bronchitis virus, between MHV and bovine coronavirus, between feline coronavirus (FCoV) type I and canine coronavirus generating FCoV type II, and between the three genotypes of human coronavirus HKU1 (HCoV-HKU1). Codon usage bias in coronaviruses were observed, with HCoV-HKU1 showing the most extreme bias, and cytosine deamination and selection of CpG suppressed clones are the two major independent biological forces that shape such codon usage bias in coronaviruses.",Conference paper,vol218
pap1156,cb5756420f7ed71e49847223fdaf17d3ea9a511c,jou219,Journal of chemical information and computer sciences,The Chemistry Development Kit (CDK): An Open-Source Java Library for Chemo-and Bioinformatics,"The Chemistry Development Kit (CDK) is a freely available open-source Java library for Structural Chemo-and Bioinformatics. Its architecture and capabilities as well as the development as an open-source project by a team of international collaborators from academic and industrial institutions is described. The CDK provides methods for many common tasks in molecular informatics, including 2D and 3D rendering of chemical structures, I/O routines, SMILES parsing and generation, ring searches, isomorphism checking, structure diagram generation, etc. Application scenarios as well as access information for interested users and potential contributors are given.",Article,vol219
pap1157,ac12c9b9e35e58b55d85a97c47886a7371c14afa,con69,Formal Concept Analysis,Data mining in bioinformatics using Weka,"UNLABELLED
The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.


AVAILABILITY
http://www.cs.waikato.ac.nz/ml/weka.",Erratum,pro69
pap1158,338d70443ddf32b5698ee9c90478f79e5b47d1f3,con100,International Conference on Automatic Face and Gesture Recognition,Survey of MapReduce frame operation in bioinformatics,"Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.",Erratum,pro100
pap1159,cb3ee9880c252c23f2b473d4055f26df2a6f75a9,jou81,BMC Bioinformatics,MEIGO: an open-source software suite based on metaheuristics for global optimization in systems biology and bioinformatics,,Article,vol81
pap1160,5beb01ad524e13e6f835dad34603219d32ed169a,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Bioinformatics Applications Note Sequence Analysis Datamonkey 2010: a Suite of Phylogenetic Analysis Tools for Evolutionary Biology,"Datamonkey is a popular web-based suite of phylog-enetic analysis tools for use in evolutionary biology. Since the original release in 2005, we have expanded the analysis options to include recently developed algorithmic methods for recombination detection, evolutionary fingerprinting of genes, codon model selection, co-evolution between sites, identification of sites, which rapidly escape host-immune pressure and HIV-1 subtype assignment. The traditional selection tools have also been augmented to include recent developments in the field. Here, we summarize the analyses options currently available on Datamonkey, and provide guidelines for their use in evolutionary biology.",Erratum,pro85
pap1161,1c0a72198b638f458f1f1ea0ebae05bef5ce005b,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Crowdsourcing for bioinformatics,"MOTIVATION
Bioinformatics is faced with a variety of problems that require human involvement. Tasks like genome annotation, image analysis, knowledge-base population and protein structure determination all benefit from human input. In some cases, people are needed in vast quantities, whereas in others, we need just a few with rare abilities. Crowdsourcing encompasses an emerging collection of approaches for harnessing such distributed human intelligence. Recently, the bioinformatics community has begun to apply crowdsourcing in a variety of contexts, yet few resources are available that describe how these human-powered systems work and how to use them effectively in scientific domains.


RESULTS
Here, we provide a framework for understanding and applying several different types of crowdsourcing. The framework considers two broad classes: systems for solving large-volume 'microtasks' and systems for solving high-difficulty 'megatasks'. Within these classes, we discuss system types, including volunteer labor, games with a purpose, microtask markets and open innovation contests. We illustrate each system type with successful examples in bioinformatics and conclude with a guide for matching problems to crowdsourcing solutions that highlights the positives and negatives of different approaches.",Erratum,pro21
pap1162,d07fea283ad69996b9d2b553e23dc61b872f18d6,jou220,Influenza and Other Respiratory Viruses,Influenza Research Database: an integrated bioinformatics resource for influenza research and surveillance,"Please cite this paper as: Squires et al. (2012) Influenza research database: an integrated bioinformatics resource for influenza research and surveillance. Influenza and Other Respiratory Viruses 6(6), 404–416.",Conference paper,vol220
pap1163,87135bf4f04ba4deff127cb016bd1c7a77524207,con31,International Conference on Evaluation & Assessment in Software Engineering,Metscape 2 bioinformatics tool for the analysis and visualization of metabolomics and gene expression data,"MOTIVATION
Metabolomics is a rapidly evolving field that holds promise to provide insights into genotype-phenotype relationships in cancers, diabetes and other complex diseases. One of the major informatics challenges is providing tools that link metabolite data with other types of high-throughput molecular data (e.g. transcriptomics, proteomics), and incorporate prior knowledge of pathways and molecular interactions.


RESULTS
We describe a new, substantially redesigned version of our tool Metscape that allows users to enter experimental data for metabolites, genes and pathways and display them in the context of relevant metabolic networks. Metscape 2 uses an internal relational database that integrates data from KEGG and EHMN databases. The new version of the tool allows users to identify enriched pathways from expression profiling data, build and analyze the networks of genes and metabolites, and visualize changes in the gene/metabolite data. We demonstrate the applications of Metscape to annotate molecular pathways for human and mouse metabolites implicated in the pathogenesis of sepsis-induced acute lung injury, for the analysis of gene expression and metabolite data from pancreatic ductal adenocarcinoma, and for identification of the candidate metabolites involved in cancer and inflammation.


AVAILABILITY
Metscape is part of the National Institutes of Health-supported National Center for Integrative Biomedical Informatics (NCIBI) suite of tools, freely available at http://metscape.ncibi.org. It can be downloaded from http://cytoscape.org or installed via Cytoscape plugin manager.


CONTACT
metscape-help@umich.edu; akarnovs@umich.edu


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro31
pap1164,a51badd5461c1650b2bfba7d0a5a89949186fb67,jou202,Journal of Biomedical Informatics,Bio2RDF: Towards a mashup to build bioinformatics knowledge systems,,Conference paper,vol202
pap1165,ab9fe0fe3b39e6f36dcf74a715e7adcd218325b2,con86,The Web Conference,BIOINFORMATICS REVIEW,,Erratum,pro86
pap1166,48a3c8c0a27802dfdca150a77cfa9bcd51381378,jou221,Human Mutation,ALSoD: A user‐friendly online bioinformatics tool for amyotrophic lateral sclerosis genetics,"Amyotrophic lateral sclerosis (ALS) is the commonest adult onset motor neuron disease, with a peak age of onset in the seventh decade. With advances in genetic technology, there is an enormous increase in the volume of genetic data produced, and a corresponding need for storage, analysis, and interpretation, particularly as our understanding of the relationships between genotype and phenotype mature. Here, we present a system to enable this in the form of the ALS Online Database (ALSoD at http://alsod.iop.kcl.ac.uk), a freely available database that has been transformed from a single gene storage facility recording mutations in the SOD1 gene to a multigene ALS bioinformatics repository and analytical instrument combining genotype, phenotype, and geographical information with associated analysis tools. These include a comparison tool to evaluate genes side by side or jointly with user configurable features, a pathogenicity prediction tool using a combination of computational approaches to distinguish variants with nonfunctional characteristics from disease‐associated mutations with more dangerous consequences, and a credibility tool to enable ALS researchers to objectively assess the evidence for gene causation in ALS. Furthermore, integration of external tools, systems for feedback, annotation by users, and two‐way links to collaborators hosting complementary databases further enhance the functionality of ALSoD. Hum Mutat 33:1345–1351, 2012. © 2012 Wiley Periodicals, Inc.",Letter,vol221
pap1167,65ef7aec70be56f5cb1ccac8e8ff9fa5c4811039,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies","IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB), Special Issue BSB 2012","This special issue of the IEEE/ACM Transactions on Computational Biology and Bioinformatic includes a selection of papers presented at the Seventh Brazilian Symposium on Bioinformatics (BSB 2012) held 15-17 August 2012 in Campo Grande (Mato Grosso do Sul), Brazil. BSB is an international symposium that covers all aspects of bioinformatics and computational biology. The symposium is organized by the special interest group in Computational Biology of the Brazilian Computer Society (SBC).",Erratum,pro49
pap1168,b3f6b5d741c028fad39a692ee40dfd613ed36089,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",A primer to frequent itemset mining for bioinformatics,"Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.",Erratum,pro42
pap1169,af65acb150fd0b3f11d296e7977416d4da9f4ee4,con9,Big Data,Structural Bioinformatics,"Improving the accuracy of the structure prediction of the third hypervariable loop of the heavy chains of antibodies ABSTRACT Motivation: Antibodies are able to recognize a wide range of antigens through their complementary determining regions formed by six hypervariable loops. Predicting the 3D structure of these loops is essential for the analysis and reengineering of novel antibodies with enhanced affinity and specificity. The canonical structure model allows high accuracy prediction for five of the loops. The third loop of the heavy chain, H3, is the hardest to predict because of its diversity in structure, length and sequence composition. Results: We describe a method, based on the Random Forest automatic learning technique, to select structural templates for H3 loops among a dataset of candidates. These can be used to predict the structure of the loop with a higher accuracy than that achieved by any of the presently available methods. The method also has the advantage of being extremely fast and returning a reliable estimate of the model quality.",Erratum,pro9
pap1170,9ae641778971fd6e0bdf3755ef4658d40c218def,con94,Vision,Flow Cytometry Bioinformatics,"Flow cytometry bioinformatics is the application of bioinformatics to flow cytometry data, which involves storing, retrieving, organizing, and analyzing flow cytometry data using extensive computational resources and tools. Flow cytometry bioinformatics requires extensive use of and contributes to the development of techniques from computational statistics and machine learning. Flow cytometry and related methods allow the quantification of multiple independent biomarkers on large numbers of single cells. The rapid growth in the multidimensionality and throughput of flow cytometry data, particularly in the 2000s, has led to the creation of a variety of computational analysis methods, data standards, and public databases for the sharing of results. Computational methods exist to assist in the preprocessing of flow cytometry data, identifying cell populations within it, matching those cell populations across samples, and performing diagnosis and discovery using the results of previous steps. For preprocessing, this includes compensating for spectral overlap, transforming data onto scales conducive to visualization and analysis, assessing data for quality, and normalizing data across samples and experiments. For population identification, tools are available to aid traditional manual identification of populations in two-dimensional scatter plots (gating), to use dimensionality reduction to aid gating, and to find populations automatically in higher dimensional space in a variety of ways. It is also possible to characterize data in more comprehensive ways, such as the density-guided binary space partitioning technique known as probability binning, or by combinatorial gating. Finally, diagnosis using flow cytometry data can be aided by supervised learning techniques, and discovery of new cell types of biological importance by high-throughput statistical methods, as part of pipelines incorporating all of the aforementioned methods. Open standards, data, and software are also key parts of flow cytometry bioinformatics. Data standards include the widely adopted Flow Cytometry Standard (FCS) defining how data from cytometers should be stored, but also several new standards under development by the International Society for Advancement of Cytometry (ISAC) to aid in storing more detailed information about experimental design and analytical steps. Open data is slowly growing with the opening of the CytoBank database in 2010 and FlowRepository in 2012, both of which allow users to freely distribute their data, and the latter of which has been recommended as the preferred repository for MIFlowCyt-compliant data by ISAC. Open software is most widely available in the form of a suite of Bioconductor packages, but is also available for web execution on the GenePattern platform.",Erratum,pro94
pap1171,2045eaca03cd0a6b72cd44f616998f30ed3cf4b5,con97,ACM SIGMOD Conference,BioJava: an open-source framework for bioinformatics in 2012,"Motivation: BioJava is an open-source project for processing of biological data in the Java programming language. We have recently released a new version (3.0.5), which is a major update to the code base that greatly extends its functionality. Results: BioJava now consists of several independent modules that provide state-of-the-art tools for protein structure comparison, pairwise and multiple sequence alignments, working with DNA and protein sequences, analysis of amino acid properties, detection of protein modifications and prediction of disordered regions in proteins as well as parsers for common file formats using a biologically meaningful data model. Availability: BioJava is an open-source project distributed under the Lesser GPL (LGPL). BioJava can be downloaded from the BioJava website (http://www.biojava.org). BioJava requires Java 1.6 or higher. All inquiries should be directed to the BioJava mailing lists. Details are available at http://biojava.org/wiki/BioJava:MailingLists Contact: andreas.prlic@gmail.com",Erratum,pro97
pap1172,25338f00a219e2f4b152ff8b62e8ca8aa6d4709f,con69,Formal Concept Analysis,BIOINFORMATICS APPLICATIONS NOTE,"Summary: NAViGaTOR is a powerful graphing application for the 2D and 3D visualization of biological networks. NAViGaTOR includes a rich suite of visual mark-up tools for manual and automated annotation, fast and scalable layout algorithms and OpenGL hardware acceleration to facilitate the visualization of large graphs. Publication-quality images can be rendered through SVG graphics export. NAViGaTOR supports community-developed data formats (PSI-XML, BioPax and GML), is platform-independent and is extensible through a plug-in architecture. Availability: NAViGaTOR is freely available to the research community from http://ophid.utoronto.ca/navigator/. Installers and documentation are provided for 32-and 64-bit Windows, Mac, Linux and Unix.",Erratum,pro69
pap1173,035256a8a6d8a73af2adb38245f4130daa1f0535,con53,Workshop on Web 2.0 for Software Engineering,Machine Learning in Bioinformatics,,Erratum,pro53
pap1174,b84cc17984f6f13bfadd2b61a605dabf9d9bfa8b,jou81,BMC Bioinformatics,An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics,,Article,vol81
pap1175,2316adc7f81213388c0f777dc2c68ff603d9b9e3,con15,Pacific Symposium on Biocomputing,Bioinformatics opportunities for identification and study of medicinal plants,"Plants have been used as a source of medicine since historic times and several commercially important drugs are of plant-based origin. The traditional approach towards discovery of plant-based drugs often times involves significant amount of time and expenditure. These labor-intensive approaches have struggled to keep pace with the rapid development of high-throughput technologies. In the era of high volume, high-throughput data generation across the biosciences, bioinformatics plays a crucial role. This has generally been the case in the context of drug designing and discovery. However, there has been limited attention to date to the potential application of bioinformatics approaches that can leverage plant-based knowledge. Here, we review bioinformatics studies that have contributed to medicinal plants research. In particular, we highlight areas in medicinal plant research where the application of bioinformatics methodologies may result in quicker and potentially cost-effective leads toward finding plant-based remedies.",Erratum,pro15
pap1176,dbce33c58b187318292e93e578d0d025be0f2156,jou197,Current Bioinformatics,Bioinformatics Tools for Mass Spectroscopy-Based Metabolomic Data Processing and Analysis,"Biological systems are increasingly being studied in a holistic manner, using omics approaches, to provide quantitative and qualitative descriptions of the diverse collection of cellular components. Among the omics approaches, metabolomics, which deals with the quantitative global profiling of small molecules or metabolites, is being used extensively to explore the dynamic response of living systems, such as organelles, cells, tissues, organs and whole organisms, under diverse physiological and pathological conditions. This technology is now used routinely in a number of applications, including basic and clinical research, agriculture, microbiology, food science, nutrition, pharmaceutical research, environmental science and the development of biofuels. Of the multiple analytical platforms available to perform such analyses, nuclear magnetic resonance and mass spectrometry have come to dominate, owing to the high resolution and large datasets that can be generated with these techniques. The large multidimensional datasets that result from such studies must be processed and analyzed to render this data meaningful. Thus, bioinformatics tools are essential for the efficient processing of huge datasets, the characterization of the detected signals, and to align multiple datasets and their features. This paper provides a state-of-the-art overview of the data processing tools available, and reviews a collection of recent reports on the topic. Data conversion, pre-processing, alignment, normalization and statistical analysis are introduced, with their advantages and disadvantages, and comparisons are made to guide the reader.",Article,vol197
pap1177,14522a6e2c9cae34d61bf41f9dc9773db04e5c55,jou81,BMC Bioinformatics,Bioinformatics analysis of the epitope regions for norovirus capsid protein,,Article,vol81
pap1178,378ca8eac56e3b0372bee1eb746cfe05b848ccbe,jou209,Clinical Chemistry,Proteomics and bioinformatics approaches for identification of serum biomarkers to detect breast cancer.,"BACKGROUND
Surface-enhanced laser desorption/ionization (SELDI) is an affinity-based mass spectrometric method in which proteins of interest are selectively adsorbed to a chemically modified surface on a biochip, whereas impurities are removed by washing with buffer. This technology allows sensitive and high-throughput protein profiling of complex biological specimens.


METHODS
We screened for potential tumor biomarkers in 169 serum samples, including samples from a cancer group of 103 breast cancer patients at different clinical stages [stage 0 (n = 4), stage I (n = 38), stage II (n = 37), and stage III (n = 24)], from a control group of 41 healthy women, and from 25 patients with benign breast diseases. Diluted serum samples were applied to immobilized metal affinity capture Ciphergen ProteinChip Arrays previously activated with Ni2+. Proteins bound to the chelated metal were analyzed on a ProteinChip Reader Model PBS II. Complex protein profiles of different diagnostic groups were compared and analyzed using the ProPeak software package.


RESULTS
A panel of three biomarkers was selected based on their collective contribution to the optimal separation between stage 0-I breast cancer patients and noncancer controls. The same separation was observed using independent test data from stage II-III breast cancer patients. Bootstrap cross-validation demonstrated that a sensitivity of 93% for all cancer patients and a specificity of 91% for all controls were achieved by a composite index derived by multivariate logistic regression using the three selected biomarkers.


CONCLUSIONS
Proteomics approaches such as SELDI mass spectrometry, in conjunction with bioinformatics tools, could greatly facilitate the discovery of new and better biomarkers. The high sensitivity and specificity achieved by the combined use of the selected biomarkers show great potential for the early detection of breast cancer.",Article,vol209
pap1179,5c0b263ad495870bedbf78e2553d83e88cc555d3,con26,Decision Support Systems,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btn081 Systems biology Network-constrained regularization and variable selection for analysis of genomic data,,Erratum,pro26
pap1180,de623ec9dbbd4cdb29925dd215cfe0c4399cdb6a,con96,Interspeech,Bioinformatics Applications Note Arrayqualitymetrics—a Bioconductor Package for Quality Assessment of Microarray Data,"The assessment of data quality is a major concern in microarray analysis. arrayQualityMetrics is a Bioconductor package that provides a report with diagnostic plots for one or two colour microarray data. The quality metrics assess reproducibility, identify apparent outlier arrays and compute measures of signal-to-noise ratio. The tool handles most current microarray technologies and is amenable to use in automated analysis pipelines or for automatic report generation, as well as for use by individuals. The diagnosis of quality remains, in principle, a context-dependent judgement, but our tool provides powerful, automated, objective and comprehensive instruments on which to base a decision. Availability: arrayQualityMetrics is a free and open source package, under LGPL license, available from the Bioconductor project at www.bioconductor.org. A users guide and examples are provided with the package. Some examples of HTML reports generated by arrayQualityMetrics can be found at http://www.microarray-quality.org Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro96
pap1181,d33bc1af4b65696b4d76bbac13684a8dc0ae13e0,con109,International Society for Music Information Retrieval Conference,Computational and Bioinformatics Frameworks for Next-Generation Whole Exome and Genome Sequencing,"It has become increasingly apparent that one of the major hurdles in the genomic age will be the bioinformatics challenges of next-generation sequencing. We provide an overview of a general framework of bioinformatics analysis. For each of the three stages of (1) alignment, (2) variant calling, and (3) filtering and annotation, we describe the analysis required and survey the different software packages that are used. Furthermore, we discuss possible future developments as data sources grow and highlight opportunities for new bioinformatics tools to be developed.",Erratum,pro109
pap1182,12ab5f1e159f71367ea967c4cb287a1dd5915518,jou222,Frontiers in Oncology,Current Challenges in the Bioinformatics of Single Cell Genomics,"Single cell genomics is a rapidly growing field with many new techniques emerging in the past few years. However, few bioinformatics tools specific for single cell genomics analysis are available. Single cell DNA/RNA sequencing data usually have low genome coverage and high amplification bias, which makes bioinformatics analysis challenging. Many current bioinformatics tools developed for bulk cell sequencing do not work well with single cell sequencing data. Here, we summarize current challenges in the bioinformatics analysis of single cell genomic DNA sequencing and single cell transcriptomes. These challenges include calling copy number variations, identifying mutated genes in tumor samples, reconstructing cell lineages, recovering low abundant transcripts, and improving the accuracy of quantitative analysis of transcripts. Development in single cell genomics bioinformatics analysis will promote the application of this technology to basic biology and medical research.",Article,vol222
pap1183,3e48eb32868c583d1616f077d24f15f49341dac4,con88,European Conference on Computer Vision,Milestones in graphical bioinformatics,"After reviewing the field of graphical bioinformatics, we have selected two dozen of the most significant publications that represent milestones of graphical bioinformatics. These publications can be viewed as forming the backbone of graphical bioinformatics, the branch of bioinformatics that initiates analysis of DNA, RNA, and proteins by considering various graphical representations of these sequences. Graphical bioinformatics, a division of bioinformatics that analyzes sequences of DNA, RNA, proteins, and proteomics maps by developing and using tools of discrete mathematics and graph theory in particular, has expanded since the year 2000, although pioneering contributions date back to Hamory (1983) and Jeffrey (1990). We chronologically follow the development of graphical bioinformatics, without assuming that readers are familiar with discrete mathematics or graph theory. Readers unfamiliar with graph theory may even have some advantage over those who have been only superficially exposed to graph theory, inview of wide misconceptions and misinformation about chemical graph theory among quantum chemists, physical chemists, and medicinal chemists in past decades. © 2013 Wiley Periodicals, Inc.",Erratum,pro88
pap1184,0a59d6ecaad02de3c8c8d57e81fff765e93a08ca,jou212,Network Modeling Analysis in Health Informatics and Bioinformatics,Role of bioinformatics and pharmacogenomics in drug discovery and development process,,Conference paper,vol212
pap1185,92c8e2c123838e82e1a3417d470cbb2286f15f9c,jou81,BMC Bioinformatics,bioNerDS: exploring bioinformatics’ database and software use through literature mining,,Article,vol81
pap1186,f950ef4d991c8a6109e174af09d3737796d4771a,jou223,Biochimica et Biophysica Acta,Bioinformatics tools for secretome analysis.,,Letter,vol223
pap1187,1c1f09c8a2edfb5089b802e207bee9575836bafe,con61,International Conference on Predictive Models in Software Engineering,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Multiple sequence alignment is of central importance to bioinformatics and computational biology. Although a large number of algorithms for computing a multiple sequence alignment have been designed, the efﬁcient computation of highly accurate multiple alignments is still a challenge. Results: We present MSAProbs , a new and practical multiple alignment algorithm for protein sequences. The design of MSAProbs is based on a combination of pair hidden Markov models and partition functions to calculate posterior probabilities. Furthermore, two critical bioinformatics techniques, namely weighted probabilistic consistency transformation and weighted proﬁle–proﬁle alignment, are incorporated to improve alignment accuracy. Assessed using the popular benchmarks: BAliBASE, PREFAB, SABmark and OXBENCH, MSAProbs achieves statistically signiﬁcant accuracy improvements over the existing top performing aligners, including ClustalW, MAFFT, MUSCLE, ProbCons and Probalign. Furthermore, MSAProbs is optimized for multi-core CPUs by employing a multi-threaded design, leading to a competitive execution time compared to other aligners. Availability: The source code of MSAProbs, written in C++, is freely and publicly available from http://msaprobs.sourceforge.net.",Erratum,pro61
pap1188,430fd08176f58140b6a0b239902cb6575bcfc565,con48,ACM Symposium on Applied Computing,Are graph databases ready for bioinformatics?,Contact: Lars.Juhl.Jensen@gmail.com,Erratum,pro48
pap1189,48a7652ee731507d556e2a3d09bd7a5786a0434a,con76,IEEE International Conference on Tools with Artificial Intelligence,BMC Bioinformatics Methodology article PAGE: Parametric Analysis of Gene Set Enrichment,,Erratum,pro76
pap1190,28bd4091dd7fa9e81142129734bde24355d85980,con77,International Conference on Artificial Neural Networks,BIOINFORMATICS ORIGINAL PAPER,"Motivation: The Basic Local Alignment Search Tool (BLAST) is one of the most widely used bioinformatics tools. The widespread impact of BLAST is reﬂected in over 53000 citations that this software has received in the past two decades, and the use of the word ‘blast’ as a verb referring to biological sequence comparison. Any improvement in the execution speed of BLAST would be of great importance in the practice of bioinformatics, and facilitate coping with ever increasing sizes of biomolecular databases. Results: Using a general-purpose graphics processing unit (GPU), we have developed GPU-BLAST, an accelerated version of the popular NCBI-BLAST. The implementation is based on the source code of NCBI-BLAST, thus maintaining the same input and output interface while producing identical results. In comparison to the sequential NCBI-BLAST, the speedups achieved by GPU-BLAST range mostly between 3 and 4. Availability: The source code of GPU-BLAST is freely available at http",Erratum,pro77
pap1191,3bc1a4b70d3f157139a048cb1ebe623464ce1b71,con64,British Computer Society Conference on Human-Computer Interaction,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Understanding key biological processes (bioprocesses) and their relationships with constituent biological entities and pharmaceutical agents is crucial for drug design and discovery. One way to harvest such information is searching the literature. However, bioprocesses are difﬁcult to capture because they may occur in text in a variety of textual expressions. Moreover, a bioprocess is often composed of a series of bioevents, where a bioevent denotes changes to one or a group of cells involved in the bioprocess. Such bioevents are often used to refer to bioprocesses in text, which current techniques, relying solely on specialized lexicons, struggle to ﬁnd. Results: This article presents a range of methods for ﬁnding bioprocess terms and events. To facilitate the study, we built a gold standard corpus in which terms and events related to angiogenesis , a key biological process of the growth of new blood vessels, were annotated. Statistics of the annotated corpus revealed that over 36% of the text expressions that referred to angiogenesis appeared as events. The proposed methods respectively employed domain-speciﬁc vocabularies, a manually annotated corpus and unstructured domain-speciﬁc documents. Evaluation results showed that, while a supervised machine-learning model yielded the best precision, recall and F1 scores, the other methods achieved reasonable performance and less cost to develop",Erratum,pro64
pap1192,14ed65f92e12909133c2514c836c8a317f68af47,con45,International Conference on Global Software Engineering,"BIOINFORMATICS Bioinformatics Advance Access published January 18, 2005 Online Predicted Human Interaction Database",,Erratum,pro45
pap1193,05e507f194e7c210b3c94440828669310e5956d1,con59,Annual Workshop of the Psychology of Programming Interest Group,BIOINFORMATICS ORIGINAL PAPER Sequence analysis,,Erratum,pro59
pap1194,9dfe93fbf8c835ce08c063e37af4ba80031ba7b9,jou158,Lecture Notes in Computer Science,Pattern Recognition in Bioinformatics,,Article,vol158
pap1195,041ab0031f655ff2d313db5ae7150a9d67e437e3,con110,Very Large Data Bases Conference,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btk010 Sequence analysis RNAshapes: an integrated RNA analysis package based on abstract shapes,,Erratum,pro110
pap1196,8f0a8ce2855a113a91053f3e25e95aa3c84cb2c6,con65,IEEE International Conference on Software Engineering and Formal Methods,Bioinformatics Applications Note Sequence Analysis the Uea Srna Workbench: a Suite of Tools for Analysing and Visualizing next Generation Sequencing Microrna and Small Rna Datasets,"RNA silencing is a complex, highly conserved mechanism mediated by small RNAs (sRNAs), such as microRNAs (miRNAs), that is known to be involved in a diverse set of biological functions including development, pathogen control, genome maintenance and response to environmental change. Advances in next generation sequencing technologies are producing increasingly large numbers of sRNA reads per sample at a fraction of the cost of previous methods. However, many bioinformatics tools do not scale accordingly, are cumbersome, or require extensive support from bioinformatics experts. Therefore, researchers need user-friendly, robust tools, capable of not only processing large sRNA datasets in a reasonable time frame but also presenting the results in an intuitive fashion and visualizing sRNA genomic features. Herein, we present the UEA sRNA workbench, a suite of tools that is a successor to the web-based UEA sRNA Toolkit, but in downloadable format and with several enhanced and additional features. Availability: The program and help pages are available at http://srna",Erratum,pro65
pap1197,668f58972329633c8fcdb45dd40959759fcf54cb,jou168,Springer US,Bioinformatics: Databases and Systems,,Article,vol168
pap1198,84534c6fa2dab4768617205b302bfbce9b3bb376,jou197,Current Bioinformatics,A Review of Ensemble Methods in Bioinformatics,"Ensemble learning is an intensively studies technique in machine learning and pattern recognition. Recent work in computational biology has seen an increasing use of ensemble learning methods due to their unique advantages in dealing with small sample size, high-dimensionality, and complexity data structures. The aim of this article is two-fold. First, it is to provide a review of the most widely used ensemble learning methods and their application in various bioinformatics problems, including the main topics of gene expression, mass spectrometry-based proteomics, gene-gene interaction identification from genome-wide association studies, and prediction of regulatory elements from DNA and protein sequences. Second, we try to identify and summarize future trends of ensemble methods in bioinformatics. Promising directions such as ensemble of support vector machine, meta-ensemble, and ensemble based feature selection are discussed.",Conference paper,vol197
pap1199,786bb2fba4f2dfda851f156bbbed5dd08b48554f,con111,International Conference on Image Analysis and Processing,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btm311 Genome analysis beadarray: R classes and methods for Illumina bead-based data,,Erratum,pro111
pap1200,786bb2fba4f2dfda851f156bbbed5dd08b48554f,con109,International Society for Music Information Retrieval Conference,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btm311 Genome analysis beadarray: R classes and methods for Illumina bead-based data,,Erratum,pro109
pap1201,bdc87c6da0d459d54e2808e26f9d91dac9bb81b8,con55,Workshop on Learning from Authoritative Security Experiment Results,BIOINFORMATICS ORIGINAL PAPER,"Background: Metagenomics is the study of the genomic content of an environmental sample of microbes. Advances in the through-put and cost-efﬁciency of sequencing technology is fueling a rapid increase in the number and size of metagenomic datasets being generated. Bioinformatics is faced with the problem of how to handle and analyze these datasets in an efﬁcient and useful way. One goal of these metagenomic studies is to get a basic understanding of the microbial world both surrounding us and within us. One major challenge is how to compare multiple datasets. Furthermore, there is a need for bioinformatics tools that can process many large datasets and are easy to use. Results: This article describes two new and helpful techniques for comparing multiple metagenomic datasets. The ﬁrst is a visualization technique for multiple datasets and the second is a new statistical method for highlighting the differences in a pairwise comparison. We have developed implementations of both methods that are suitable for very large datasets and provide these in Version 3 of our standalone metagenome analysis tool MEGAN. Conclusion: These new methods are suitable for the visual comparison of many large metagenomes and the statistical comparison of two metagenomes at a time. Nevertheless, more work needs to be done to support the comparative analysis of multiple metagenome datasets.",Erratum,pro55
pap1202,8f262d37490e1393123d2b3459f73dd06f27159a,jou81,BMC Bioinformatics,Cloud BioLinux: pre-configured and on-demand bioinformatics computing for the genomics community,,Conference paper,vol81
pap1203,07446e229b1dda8c42c7975a4c8b738beaaceb90,con75,Intelligent Systems in Molecular Biology,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Statistical phylogenetics is computationally intensive, resulting in considerable attention meted on techniques for parallelization. Codon-based models allow for independent rates of synonymous and replacement substitutions and have the potential to more adequately model the process of protein-coding sequence evolution with a resulting increase in phylogenetic accuracy. Unfortunately, due to the high number of codon states, computational burden has largely thwarted phylogenetic reconstruction under codon models, particularly at the genomic-scale. Here, we describe novel algorithms and methods for evaluating phylogenies under arbitrary molecular evolutionary models on graphics processing units (GPUs), making use of the large number of processing cores to efﬁciently parallelize calculations even for large state-size models. Results: We implement the approach in an existing Bayesian framework and apply the algorithms to estimating the phylogeny of 62 complete mitochondrial genomes of carnivores under a 60-state codon model. We see a near 90-fold speed increase over an optimized CPU-based computation and a > 140-fold increase over the currently available implementation, making this the ﬁrst practical use of codon models for phylogenetic inference over whole mitochondrial or microorganism genomes. Availability and implementation: Source code provided in BEAGLE: Broad-platform Evolutionary Analysis General Likelihood Evaluator, a cross-platform/processor library for phylogenetic likelihood computation (http://beagle-lib.googlecode.com/). We employ a BEAGLE-implementation using the Bayesian phylogenetics framework BEAST (http://beast.bio.ed.ac.uk/).",Erratum,pro75
pap1204,13e6f587d6c09f6e97e21926045fec25928df98f,con75,Intelligent Systems in Molecular Biology,BIOINFORMATICS APPLICATIONS,"Summary: R/DWD is an extensible package for classiﬁcation. It is built based on a recently developed powerful classiﬁcation method called distance weighted discrimination (DWD). DWD is related to, and has been shown to be superior to, the support vector machine in situations that are fundamental to bioinformatics, such as very high dimensional data. DWD has proven to be very useful for several fundamental bioinformatics tasks, including classiﬁcation, data visualization and removal of biases, such as batch effects. Earlier DWD implementations, however, relied on Matlab, which is not free and requires a license. The major contribution of the R/DWD package is an implementation that is completely in R and thus can be used without any requirements for licensing or software purchase. In addition, R/DWD also provides efﬁcient solvers for second-order-cone-programming and quadratic programming. Availability and implementation: The package is freely available from cran.r-project.org.",Erratum,pro75
pap1205,f42261d881dbac77870df168376bb2898cf46333,jou81,BMC Bioinformatics,4273π: Bioinformatics education on low cost ARM hardware,,Conference paper,vol81
pap1206,f0cb15fe57fc7c0df374e09c2f415b4f45a46ee1,jou224,Biology Direct,Bioinformatics clouds for big data manipulation,,Article,vol224
pap1207,ba69a521bad7a913a9ee82d1c2e49f41771d5c00,jou225,Infection and Immunity,PATRIC: the Comprehensive Bacterial Bioinformatics Resource with a Focus on Human Pathogenic Species,"ABSTRACT Funded by the National Institute of Allergy and Infectious Diseases, the Pathosystems Resource Integration Center (PATRIC) is a genomics-centric relational database and bioinformatics resource designed to assist scientists in infectious-disease research. Specifically, PATRIC provides scientists with (i) a comprehensive bacterial genomics database, (ii) a plethora of associated data relevant to genomic analysis, and (iii) an extensive suite of computational tools and platforms for bioinformatics analysis. While the primary aim of PATRIC is to advance the knowledge underlying the biology of human pathogens, all publicly available genome-scale data for bacteria are compiled and continually updated, thereby enabling comparative analyses to reveal the basis for differences between infectious free-living and commensal species. Herein we summarize the major features available at PATRIC, dividing the resources into two major categories: (i) organisms, genomes, and comparative genomics and (ii) recurrent integration of community-derived associated data. Additionally, we present two experimental designs typical of bacterial genomics research and report on the execution of both projects using only PATRIC data and tools. These applications encompass a broad range of the data and analysis tools available, illustrating practical uses of PATRIC for the biologist. Finally, a summary of PATRIC's outreach activities, collaborative endeavors, and future research directions is provided.",Letter,vol225
pap1208,75116430b3eb9890b3b1e5d6cc72816292f6ac97,con100,International Conference on Automatic Face and Gesture Recognition,Bioinformatics Applications Note Sequence Type Analysis and Recombinational Tests (start),"The 32-bit Windows application START is implemented using Visual Basic and C ++ and performs analyses to aid in the investigation of bacterial population structure using multilocus sequence data. These analyses include data summary, lineage assignment, and tests for recombination and selection. Multilocus Sequence Typing (MLST) is a nucleotide sequence-based typing method that indexes the variation present in bacterial housekeeping genes, where most of the variation is selectively neutral (Maiden et al., 1998). Internal fragments of seven housekeeping genes, approximately 450–500 bp in length, are sequenced and novel alleles are assigned with arbitrary numbers sequentially to provide an allelic profile of seven integers that defines the Sequence Type (ST) of each isolate. The technique is designed primarily for global or long-term epidemiology and surveillance, and has the advantage over other typing methods, such as genetic fingerprinting, of electronic portability and unambiguous characterization of isolates. MLST schemes have been developed for a range of bacterial pathogens and databases for these organisms can be interrogated at the MLST web-site (http://www.mlst.net/) thus facilitating rapid comparisons of isolates typed using the method. A further advantage of MLST is that it provides large quantities of data that may be analyzed by a number of evolutionary approaches to yield insights into the structure of bacterial populations and the selective pressures which act upon them. With the increasing availability of MLST data, the need for software to describe and analyze datasets has become apparent. Sequence Type Analysis and Recombinational Tests (START) was written to address this need through the inclusion of multiple analytical techniques in an easy-to-use and intuitive interface for Windows 95/98/NT/2000 operating systems. Techniques available within the START program are divided into four categories: data summary, lineage assignment , tests for recombination and tests for selection. Two input files are required for many of the tests—allelic profiles, consisting of isolate identifiers and allele numbers , and allele sequences. Profile data can be entered into the program directly from the keyboard, by pasting from the clipboard or by loading a tab-delimited text file while allele sequences need to be in FASTA format. The program utilizes an embedded web-browser for output, enabling easy formatting of tables and inclusion of diagrams generated by the lineage assignment algorithms using HTML, as well as printing and saving of results. Graphical output from analyses is produced in the form of Windows Metafiles (WMF) embedded within the page and these may be …",Erratum,pro100
pap1209,40e91b5591de9312d6945aab6ca9e9180879765a,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",BIOINFORMATICS APPLICATIONS NOTE,"Motivation: The complete sequencing of the human genome shows that only 1% of the entire genome encodes for proteins. The major part of the genome is made up of non-coding DNA, regulatory elements and junk DNA . Transcriptional regulation plays a central role in a multitude of critical cellular processes and responses, and it is a central force in the development and differentiation of multicellular organisms. Identifying regulatory elements is one of the major tasks in this challenge. To accomplish this task, we developed a solid and simple suite that allows direct access to genomic database and immediate result check. We introduce COMPASSS (COMplex PAttern of Sequence Search Software), a simple and effective tool for motif search in entire genomes. Motifs can be partially degenerated and interrupted by spacers of variable length. Results: We demonstrate through real biological data mining the simplicity and robustness of this tool. The test was performed on two well-known protein domains and a highly variable cis -acting element. COMPASSS successfully identiﬁes both protein domains and cis -acting semi-conserved elements. Availability",Erratum,pro42
pap1210,5e175febb415b131fbea5c19e7ef8081f7d91d16,con63,International Colloquium on Theoretical Aspects of Computing,Applications of artiﬁcial intelligence in bioinformatics: A review,"Artiﬁcial intelligence (AI) has increasingly gained attention in bioinformatics research and computational molecular biology. With the availability of different types of AI algorithms, it has become common for the researchers to apply the off-shelf systems to classify and mine their databases. At present, with various intelligent methods available in the literature, researchers are facing difﬁculties in choosing the best method that could be applied to a speciﬁc data set. Researchers need tools, which present the data in a comprehensible fashion, annotated with context, estimates of accuracy and explanation. This article aims to review the use of AI in the areas of bioinformatics and computational molecular biology (DNA sequencing). These areas have risen from the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic research. The underlying motivation for many of the bioinformatics and DNA sequencing approaches is the evolution of organisms and the complexity of working with erroneous data. This article also describes the kind of software programs which were developed by the research community in order to (1) search, classify and mine different available biological databases; (2) simulate biological experiments with and without errors.",Erratum,pro63
pap1211,faf5397cce428ad13484c29d4ee326900e469791,con108,International Conference on Information Integration and Web-based Applications & Services,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Current sequencing technologies produce a large number of erroneous reads. The sequencing errors present a major challenge in utilizing the data in de novo sequencing projects as assemblers have difﬁculties in dealing with errors. Results: We present Coral which corrects sequencing errors by forming multiple alignments. Unlike previous tools for error correction, Coral can utilize also bases distant from the error in the correction process because the whole read is present in the alignment. Coral is easily adjustable to reads produced by different sequencing technologies like Illumina Genome Analyzer and Roche/454 Life Sciences sequencing platforms because the sequencing error model can be deﬁned by the user. We show that our method is able to reduce the error rate of reads more than previous methods. Availability: The source code of Coral is freely available at http://www.cs.helsinki.ﬁ/u/lmsalmel/coral/.",Erratum,pro108
pap1212,4c435881bee138d5439d37b90df1a981e53a10ed,con79,IEEE Annual Symposium on Foundations of Computer Science,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm035 Structural bioinformatics Local structural disorder imparts plasticity on linear motifs,,Erratum,pro79
pap1213,649a219ed2a3e82e6975828a5461f4ebf0cdefdb,con106,International Conference on Mobile Data Management,BIOINFORMATICS ORIGINAL PAPER Genome analysis The UCSC Known Genes,,Erratum,pro106
pap1214,1fbac92306d62bd45a434cc88c6d84d70ab51df5,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",The Roots of Bioinformatics in Theoretical Biology,"From the late 1980s onward, the term “bioinformatics” mostly has been used to refer to computational methods for comparative analysis of genome data. However, the term was originally more widely defined as the study of informatic processes in biotic systems. In this essay, I will trace this early history (from a personal point of view) and I will argue that the original meaning of the term is re-emerging.",Erratum,pro42
pap1215,44f8d70302f963a79277161d2f3b60c61de376a6,con43,IEEE International Conference on Software Maintenance and Evolution,Bioinformatics challenges for personalized medicine,"Motivation: Widespread availability of low-cost, full genome sequencing will introduce new challenges for bioinformatics. Results: This review outlines recent developments in sequencing technologies and genome analysis methods for application in personalized medicine. New methods are needed in four areas to realize the potential of personalized medicine: (i) processing large-scale robust genomic data; (ii) interpreting the functional effect and the impact of genomic variation; (iii) integrating systems data to relate complex genetic interactions with phenotypes; and (iv) translating these discoveries into medical practice. Contact: russ.altman@stanford.edu Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro43
pap1216,a14f723938c3a39bebb67b2d561faddb1a61aa3b,con70,International Conference on Graph Transformation,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm486 Data and text mining Monte Carlo,,Erratum,pro70
pap1217,405cbbeff94d27c0e45ea18241bcc13edc16bf83,con44,International Conference Knowledge Engineering and Knowledge Management,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btk005 Gene expression,,Erratum,pro44
pap1218,62b9f2aa45be15ba72bbb441bd7c36f70be6249e,con69,Formal Concept Analysis,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm305 Gene expression Predicting survival from microarray data—a comparative study,,Erratum,pro69
pap1219,14b82cf9ffb945ea12ede2326997e5dc56611cc5,con80,International Conference on Advanced Computer Science Applications and Technologies,BIOINFORMATICS ORIGINAL PAPER,"Motivation: RNA editing is a phenomenon, which is responsible for the alteration of particular nucleotides in RNA sequences relative to their genomic templates. Recently, a large number of RNA editing instances in humans have been identiﬁed using bioinformatic screens and high-throughput experimental investigations utilizing next-generation sequencing technologies. However, the available data on RNA editing are not uniform and difﬁcult to access. Results: Here, we describe a new database DARNED (DAtabase of RNa EDiting) that provides centralized access to available published data related to RNA editing. RNA editing locations are mapped on the reference human genome. The current release of the database contains information on approximately 42000 human genome coordinates corresponding to RNA locations that undergo RNA editing, mostly involving adenosine-to-inosine (A-to-I) substitutions. The data can be queried using a range of genomic coordinates, their corresponding functional localization in RNA molecules [Exons, Introns, CoDing Sequence (CDS) and UnTranslated Regions (UTRs)] and information regarding tissue/organ/cell sources where RNA editing has been observed. It is also possible to obtain RNA editing information for a speciﬁc gene or an RNA molecule using corresponding accession numbers. Search results provide information on the number of expressed sequence tags (ESTs) supporting edited and genomic bases, functional localization of RNA editing and existence of known single nucleotide polymorphisms (SNPs). Editing data can be explored in UCSC and Ensembl genome browsers, in conjunction with additional data provided by these popular genome browsers. DARNED has been designed for researchers seeking information on RNA editing and for the developers of novel algorithms for its prediction.",Erratum,pro80
pap1220,3f8c59c988f407ccd7af99af4e4bd1b46ff7bdc5,con8,Frontiers in Education Conference,BIOINFORMATICS APPLICATIONS,"Motivation: The Automated Mass Spectral Deconvolution and Identiﬁcation System (AMDIS) is freeware extensively applied in metabolomics. However, datasets processed by AMDIS require extensive data correction, ﬁltering and reshaping to create reliable datasets for further downstream analysis. Performed manually, these processes are laborious and extremely time consuming. Furthermore, manual corrections increase the chance of human error and can introduce additional technical variability to the data. Thus, an automated pipeline for curating GC-MS data is urgently needed. Results: We present the Metab R package designed to automate the pipeline for analysis of metabolomics GC-MS datasets processed by AMDIS. Availability: The Metab package, the AMDIS library and the reference ion library are available at www.metabolomics.auckland .ac.nz/index.php/downloads.",Erratum,pro8
pap1221,41fd356bfd1a14e74ec52a463e822deaa920931a,con27,International Conference on Contemporary Computing,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm436 Sequence analysis,,Erratum,pro27
pap1222,560f2ece39727316cc80c5621b34e1330552da0d,con17,International Conference on Statistical and Scientific Database Management,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm302 Structural bioinformatics POODLE-L: a two-level SVM prediction system for reliably,,Erratum,pro17
pap1223,247aa54539ce20c8a5a8d33c5c65e05d39099576,con105,British Machine Vision Conference,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm097 Genome analysis IMEx: Imperfect Microsatellite Extractor,,Erratum,pro105
pap1224,851fb16e92cd27098a991b9c73f66b2dc54558cc,con110,Very Large Data Bases Conference,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm510 Systems biology,,Erratum,pro110
pap1225,4e80e351f21aca53e73ae725c1070bf7cc3fd013,con37,International Symposium on Search Based Software Engineering,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Many algorithms that integrate multiple functional association networks for predicting gene function construct a composite network as a weighted sum of the individual networks and then use the composite network to predict gene function. The weight assigned to an individual network represents the usefulness of that network in predicting a given gene function. However, because many categories of gene function have a small number of annotations, the process of assigning these network weights is prone to overﬁtting. Results: Here, we address this problem by proposing a novel approach to combining multiple functional association networks. In particular, we present a method where network weights are simultaneously optimized on sets of related function categories. The method is simpler and faster than existing approaches. Further, we show that it produces composite networks with improved function prediction accuracy using ﬁve example species (yeast, mouse, ﬂy, Esherichia coli and human). Availability: Networks and code are available from:",Erratum,pro37
pap1226,0abd2b0f53f701ecdb0190d34ca088f04fd322c5,con106,International Conference on Mobile Data Management,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Genome-wide association studies (GWAS) involving half a million or more single nucleotide polymorphisms (SNPs) allow genetic dissection of complex diseases in a holistic manner. The common practice of analyzing one SNP at a time does not fully realize the potential of GWAS to identify multiple causal variants and to predict risk of disease. Existing methods for joint analysis of GWAS data tend to miss causal SNPs that are marginally uncorrelated with disease and have high false discovery rates (FDRs). Results: We introduce GWASelect, a statistically powerful and computationally efﬁcient variable selection method designed to tackle the unique challenges of GWAS data. This method searches iteratively over the potential SNPs conditional on previously selected SNPs and is thus capable of capturing causal SNPs that are marginally correlated with disease as well as those that are marginally uncorrelated with disease. A special resampling mechanism is built into the method to reduce false positive ﬁndings. Simulation studies demonstrate that the GWASelect performs well under a wide spectrum of linkage disequilibrium patterns and can be substantially more powerful than existing methods in capturing causal variants while having a lower FDR. In addition, the regression models based on the GWASelect tend to yield more accurate prediction of disease risk than existing methods. The advantages of the GWASelect are illustrated with the Wellcome Trust Case-Control Consortium (WTCCC) data.",Erratum,pro106
pap1227,abea8be8f779ebc004c39cf83d2163164ce22e8e,con16,International Conference on Data Science and Advanced Analytics,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm093 Systems biology The impact of function perturbations in Boolean networks,,Erratum,pro16
pap1228,29f5526a5c4cc18d4996283db231a2da10b4e52c,con107,Chinese Conference on Biometric Recognition,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Recent large-scale chromatin states mapping efforts have revealed characteristic chromatin modiﬁcation signatures for various types of functional DNA elements. Given the important inﬂuence of chromatin states on gene regulation and the rapid accumulation of genome-wide chromatin modiﬁcation data, there is a pressing need for computational methods to analyze these data in order to identify functional DNA elements. However, existing computational tools do not exploit data transformation and feature extraction as a means to achieve a more accurate prediction. Results: We introduce a new computational framework for identifying functional DNA elements using chromatin signatures. The framework consists of a data transformation and a feature extraction step followed by a classiﬁcation step using time-delay neural network. We implemented our framework in a software tool CSI-ANN (chromatin signature identiﬁcation by artiﬁcial neural network). When applied to predict transcriptional enhancers in the ENCODE region, CSI-ANN achieved a 65.5% sensitivity and 66.3% positive predictive value, a 5.9% and 11.6% improvement, respectively, over the previously best approach. Availability and Implementation: CSI-ANN is implemented in Matlab. The source code is freely available at http://www.medicine",Erratum,pro107
pap1229,9dd25b46bec0bc06e6912c61f5283910d456bcc5,con9,Big Data,BIOINFORMATICS APPLICATIONS,"Motivation: Collecting millions of genetic variations is feasible with the advanced genotyping technology. With a huge amount of genetic variations data in hand, developing efﬁcient algorithms to carry out the gene–gene interaction analysis in a timely manner has become one of the key problems in genome-wide association studies (GWAS). Boolean operation-based screening and testing (BOOST), a recent work in GWAS, completes gene–gene interaction analysis in 2.5 days on a desktop computer. Compared with central processing units (CPUs), graphic processing units (GPUs) are highly parallel hardware and provide massive computing resources. We are, therefore, motivated to use GPUs to further speed up the analysis of gene–gene interactions. Results: We implement the BOOST method based on a GPU framework and name it GBOOST. GBOOST achieves a 40-fold speedup compared with BOOST. It completes the analysis of Wellcome Trust Case Control Consortium Type 2 Diabetes (WTCCC T2D) genome data within 1.34h on a desktop computer equipped with Nvidia GeForce GTX 285 display card. Availability: GBOOST code is available at http://bioinformatics.ust .hk/BOOST.html#GBOOST.",Erratum,pro9
pap1230,23686360c58e8c93d705557bff2b9778c613fcd2,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Fuzzy c-means clustering is widely used to identify cluster structures in high-dimensional datasets, such as those obtained in DNA microarray and quantitative proteomics experiments. One of its main limitations is the lack of a computationally fast method to set optimal values of algorithm parameters. Wrong parameter values may either lead to the inclusion of purely random ﬂuctuations in the results or ignore potentially important data. The optimal solution has parameter values for which the clustering does not yield any results for a purely random dataset but which detects cluster formation with maximum resolution on the edge of randomness. Results: Estimation of the optimal parameter values is achieved by evaluation of the results of the clustering procedure applied to randomized datasets. In this case, the optimal value of the fuzziﬁer follows common rules that depend only on the main properties of the dataset. Taking the dimension of the set and the number of objects as input values instead of evaluating the entire dataset allows us to propose a functional relationship determining the fuzziﬁer directly. This result speaks strongly against using a predeﬁned fuzziﬁer as typically done in many previous studies. Validation indices are generally used for the estimation of the optimal number of clusters. A comparison shows that the minimum distance between the centroids provides results that are at least equivalent or better than those obtained by other computationally more expensive indices. Contact",Erratum,pro39
pap1231,34a00d32156cd30ad24b850778b69136857077db,con41,Asia-Pacific Software Engineering Conference,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btn036 Systems biology,,Erratum,pro41
pap1232,e9b2904f1f3af9983bc89dc855b6767e4afe2d97,con9,Big Data,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm611 Sequence analysis,,Erratum,pro9
pap1233,868578dca60a78f579e1c260d355f671a31160bf,con55,Workshop on Learning from Authoritative Security Experiment Results,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm362 Systems Biology Robustness analysis and tuning of synthetic gene networks,,Erratum,pro55
pap1234,7bde34b2c7bfa6ee240f9b9d523419cfd87a563a,con17,International Conference on Statistical and Scientific Database Management,Current Protocols in Bioinformatics,"Enzyme Commission numbers that had been assigned to genes in MGI were annotated to GO terms based on the inclusion of EC#s within GO terms from the molecular function ontology. Details of this strategy can be found in Hill et al, Genomics (2001) 74:121-128. Title Gene Ontology Annotation by the MGI Curatorial Staff MGI Accession ID MGI:2152097",Erratum,pro17
pap1235,b3bfdd2e95f629c508c65f62c58ae0e89b0c6f86,con99,North American Chapter of the Association for Computational Linguistics,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Importance of accurate automatic protein function prediction is ever increasing in the face of a large number of newly sequenced genomes and proteomics data that are awaiting biological interpretation. Conventional methods have focused on high sequence similarity-based annotation transfer which relies on the concept of homology. However, many cases have been reported that simple transfer of function from top hits of a homology search causes erroneous annotation. New methods are required to handle the sequence similarity in a more robust way to combine together signals from strongly and weakly similar proteins for effectively predicting function for unknown proteins with high reliability. Results: We present the extended similarity group (ESG) method, which performs iterative sequence database searches and annotates a query sequence with Gene Ontology terms. Each annotation is assigned with probability based on its relative similarity score with the multiple-level neighbors in the protein similarity graph. We will depict how the statistical framework of ESG improves the prediction accuracy by iteratively taking into account the neighborhood of query protein in the sequence similarity space. ESG outperforms conventional PSI-BLAST and the protein function prediction (PFP) algorithm. It is found that the iterative search is effective in capturing multiple-domains in a query protein, enabling accurately predicting several functions which originate from different domains.",Erratum,pro99
pap1236,2b079c815d28f231be9cb744674fb1c35bc85e12,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm606 Sequence analysis,,Erratum,pro58
pap1237,a414ddea8c63da27a0336d81523823f0acde1e46,con48,ACM Symposium on Applied Computing,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm370 Systems biology,,Erratum,pro48
pap1238,869836518181cfeaeab81d9b8249c805d72e2dd5,con64,British Computer Society Conference on Human-Computer Interaction,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm612 Systems biology,,Erratum,pro64
pap1239,5f072c1a496195a3d94029d3912ea60aab5da516,con66,International Conference on Software Reuse,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm580 Systems biology,,Erratum,pro66
pap1240,59d6e76097eb9af3b5d70d6567f0aa2281d627b7,con71,Annual Conference on Innovation and Technology in Computer Science Education,BIOINFORMATICS ORIGINAL PAPER,"Motivation: It is far from trivial to select the most effective clustering method and its parameterization, for a particular set of gene expression data, because there are a very large number of possibilities. Although many researchers still prefer to use hierarchical clustering in one form or another, this is often sub-optimal. Cluster ensemble research solves this problem by automatically combining multiple data partitions from different clusterings to improve both the robustness and quality of the clustering result. However, many existing ensemble techniques use an association matrix to summarize sample-cluster co-occurrence statistics, and relations within an ensemble are encapsulated only at coarse level, while those existing among clusters are completely neglected. Discovering these missing associations may greatly extend the capability of the ensemble methodology for microarray data clustering. Results: The link-based cluster ensemble (LCE) method, presented here, implements these ideas and demonstrates outstanding performance. Experiment results on real gene expression and synthetic datasets indicate that LCE: (i) usually outperforms the existing cluster ensemble algorithms in individual tests and, overall, is clearly class-leading; (ii) generates excellent, robust performance across different types of data, especially with the presence of noise and imbalanced data clusters; (iii) provides a high-level data matrix that is applicable to many numerical clustering techniques; and (iv) is computationally efﬁcient for large datasets and gene clustering. Availability: Online supplementary and implementation are available at:",Erratum,pro71
pap1241,0229c8ba6346efd58a9cbdeb29b07c6ba0ccee4c,con8,Frontiers in Education Conference,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm001 Data and text mining,,Erratum,pro8
pap1242,968516d4ded428ee3ca1ab08ba03486c335a18c4,jou226,Journal of Inorganic Biochemistry,A bioinformatics view of zinc enzymes.,,Letter,vol226
pap1243,00c35e83fb6559356b73297aa5f1657998d539d6,con53,Workshop on Web 2.0 for Software Engineering,BIOINFORMATICS ORIGINAL PAPER,"Motivation: The advent of high-throughput sequencing (HTS) technologies has made it affordable to sequence many individuals’ genomes. Simultaneously the computational analysis of the large volumes of data generated by the new sequencing machines remains a challenge. While a plethora of tools are available to map the resulting reads to a reference genome, and to conduct primary analysis of the mappings, it is often necessary to visually examine the results and underlying data to conﬁrm predictions and understand the functional effects, especially in the context of other datasets. Results: We introduce Savant, the Sequence Annotation, Visualization and ANalysis Tool, a desktop visualization and analysis browser for genomic data. Savant was developed for visualizing and analyzing HTS data, with special care taken to enable dynamic visualization in the presence of gigabases of genomic reads and references the size of the human genome. Savant supports the visualization of genome-based sequence, point, interval and continuous datasets, and multiple visualization modes that enable easy identiﬁcation of genomic variants (including single nucleotide polymorphisms, structural and copy number variants), and functional genomic information (e.g. peaks in ChIP-seq data) in the context of genomic annotations.",Erratum,pro53
pap1244,fc8475059bf71726a46947e3f52cbf10bcb9d98f,con62,Australian Software Engineering Conference,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btl673 Genome analysis Identifying clusters of functionally related genes in genomes,,Erratum,pro62
pap1245,a5107c8185dbbc50de5354c8ef43961fcdaaff95,con30,PS,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btm100 Structural bioinformatics,,Erratum,pro30
pap1246,91b3cd1bbc671dbfa9b23aaedd3162383c890c65,con96,Interspeech,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm414,,Erratum,pro96
pap1247,bcf51eeed34a986a7f8bef1fafda6940ff82ff36,con20,ACM Conference on Economics and Computation,BIOINFORMATICS APPLICATIONS,"Summary: The Mirrortree server allows to graphically and interactively study the co-evolution of two protein families, and investigate their possible interactions and functional relationships in a taxonomic context. The server includes the possibility of starting from single sequences and hence it can be used by non-expert users. Availability and Implementation: The web server is freely available at http://csbg.cnb.csic.es/mtserver. It was tested in the main web browsers. Adobe Flash Player is required at the client side to perform the interactive assessment of co-evolution. Contact",Erratum,pro20
pap1248,f9d6f6e7dd6c8ff93e491a9d7888e8709c8a2ab6,con69,Formal Concept Analysis,BIOINFORMATICS APPLICATIONS,"Summary: Representing models of cellular processes or pathways in a graphically rich form facilitates interpretation of biological observations and generation of new hypotheses. Solving biological problems using large pathway datasets requires software that can combine data mapping, querying and visualization as well as providing access to diverse data resources on the Internet. ChiBE is an open source software application that features user-friendly multi-view display, navigation and manipulation of pathway models in BioPAX format. Pathway views are rendered in a feature-rich format, and may be laid out and edited with state-of-the-art visualization methods, including compound or nested structures for visualizing cellular compartments and molecular complexes. Users can easily query and visualize pathways through an integrated Pathway Commons query tool and analyze molecular proﬁles in pathway context.",Erratum,pro69
pap1249,c26f4eda3429478b8409e0a7b9b9d04f77ccddd6,con7,International Symposium on Intelligent Data Analysis,BIOINFORMATICS APPLICATIONS,"Summary: We present an updated version of the TFold software for pinpointing differentially expressed proteins in shotgun proteomics experiments. Given an FDR bound, the updated approach uses a theoretical FDR estimator to maximize the number of identiﬁcations that satisfy both a fold-change cutoff that varies with the t -test P -value as a power law and a stringency criterion that aims to detect lowly abundant proteins. The new version has yielded signiﬁcant improvements in sensitivity over the previous one. Availability: Freely available for academic use at http://pcarvalho. com/patternlab.",Erratum,pro7
pap1250,812a85dbe2ad77fe4f1a5f52166a51a3ad1ecd44,con81,International Conference on Learning Representations,"Bioinformatics Discovery Note Structural Bioinformatics the Product of C9orf72, a Gene Strongly Implicated in Neurodegeneration, Is Structurally Related to Denn Rab-gefs","Motivation: Fronto-temporal dementia (FTD) and amyotrophic lateral sclerosis (ALS, also called motor neuron disease, MND) are severe neurodegenerative diseases that show considerable overlap at the clinical and cellular level. The most common single mutation in families with FTD or ALS has recently been mapped to a non-coding repeat expansion in the uncharacterized gene C9ORF72. Although a plausible mechanism for disease is that aberrant C9ORF72 mRNA poisons splicing, it is important to determine the cellular function of C9ORF72, about which nothing is known. Results: Sensitive homology searches showed that C9ORF72 is a full-length distant homologue of proteins related to Differentially Expressed in Normal and Neoplasia (DENN), which is a GDP/GTP exchange factor (GEF) that activates Rab-GTPases. Our results suggest that C9ORF72 is likely to regulate membrane traffic in conjunction with Rab-GTPase switches, and we propose to name the gene and its product DENN-like 72 (DENNL72). Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro81
pap1251,1783e2da0f2a0bf97dd57c7b10e27b9d91116d23,jou227,Molecular & Cellular Proteomics,"Probing Native Protein Structures by Chemical Cross-linking, Mass Spectrometry, and Bioinformatics","Chemical cross-linking of reactive groups in native proteins and protein complexes in combination with the identification of cross-linked sites by mass spectrometry has been in use for more than a decade. Recent advances in instrumentation, cross-linking protocols, and analysis software have led to a renewed interest in this technique, which promises to provide important information about native protein structure and the topology of protein complexes. In this article, we discuss the critical steps of chemical cross-linking and its implications for (structural) biology: reagent design and cross-linking protocols, separation and mass spectrometric analysis of cross-linked samples, dedicated software for data analysis, and the use of cross-linking data for computational modeling. Finally, the impact of protein cross-linking on various biological disciplines is highlighted.",Article,vol227
pap1252,ce03622b85115d0ab2e18cfc787187d77c8d69f1,con20,ACM Conference on Economics and Computation,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm332 Data and text mining,,Erratum,pro20
pap1253,72d8fcdf2ea78bda79dc511010a52e47e8fa0448,con30,PS,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btn060 Systems biology,,Erratum,pro30
pap1254,12d8861eab0a6aabd1e1774a48ebbdc30a81b315,con53,Workshop on Web 2.0 for Software Engineering,"Proteins : Structure , Function , and Bioinformatics","After printing the PDF file, please read the page proofs carefully and: 1) indicate changes or corrections in the margin of the page proofs; 2) answer all queries (footnotes A,B,C, etc.) on the last page of the PDF proof; 3) proofread any tables and equations carefully; 4) check that any Greek, especially ""mu"", has translated correctly. Within 48 hours, please return the following to the address given below: 1) original PDF set of page proofs, 2) Reprint Order form, 3) Return fax form Return to: Your article will be published online via our EarlyView service within a few days of correction receipt. Your prompt attention to and return of page proofs is crucial to faster publication of your work. If you experience technical problems, please contact Doug Frank (",Erratum,pro53
pap1255,7131d7298f2d45dff6ba45e2afe1c0fe492f4a3b,con86,The Web Conference,BIOINFORMATICS ORIGINAL PAPER,"Motivation: The study of complex biological relationships is aided by large and high-dimensional data sets whose analysis often involves dimension reduction to highlight representative or informative directions of variation. In principle, information theory provides a general framework for quantifying complex statistical relationships for dimension reduction. Unfortunately, direct estimation of high-dimensional information theoretic quantities, such as entropy and mutual information (MI), is often unreliable given the relatively small sample sizes available for biological problems. Here, we develop and evaluate a hierarchy of approximations for high-dimensional information theoretic statistics from associated low-order terms, which can be more reliably estimated from limited samples. Due to a relationship between this metric and the minimum spanning tree over a graph representation of the system, we refer to these approximations as MIST (Maximum Information Spanning Trees). Results: The MIST approximations are examined in the context of synthetic networks with analytically computable entropies and using experimental gene expression data as a basis for the classiﬁcation of multiple cancer types. The approximations result in signiﬁcantly more accurate estimates of entropy and MI, and also correlate better with biological classiﬁcation error than direct estimation and another low-order approximation, minimum-redundancy–maximum-relevance (mRMR). Availability: Software to compute the entropy approximations described here is available as Supplementary Material.",Erratum,pro86
pap1256,13902bee3dbd6317221a78227964f49d21b541ae,con89,Conference on Uncertainty in Artificial Intelligence,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm446 Systems biology,,Erratum,pro89
pap1257,a54c831474d17c7b1c6880d2780dfa9e6ea06aa1,con51,Brazilian Symposium on Software Engineering,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm161 Data and text mining,,Erratum,pro51
pap1258,7af3e3b1cd19b3ac33936bee61769469603b263c,con94,Vision,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btm330 Structural bioinformatics,,Erratum,pro94
pap1259,24909689f8963be92b360e217274fecdd578ad2e,con50,International Workshop on Green and Sustainable Software,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btl666 Structural bioinformatics,,Erratum,pro50
pap1260,723cf90ef04282bca51a244eccb566c27d96d756,con111,International Conference on Image Analysis and Processing,Ieee/acm Transactions on Computational Biology and Bioinformatics,"— This paper proposes new algorithms for computing pairwise rearrangement scenarios that conserve the combina-torial structure of genomes. More precisely, we investigate the problem of sorting signed permutations by reversals without breaking common intervals. We describe a combinatorial framework for this problem that allows to characterize classes of signed permutations for which one can compute in polynomial time a shortest reversal scenario that conserves all common intervals. In particular we define a class of permutations for which this computation can be done in linear time with a very simple algorithm that does not rely on the classical Hannenhalli-Pevzner theory for sorting by reversals. We apply these methods to the computation of rearrangement scenarios between permutations obtained from 16 synteny blocks of the X chromosomes of the human, mouse and rat.",Erratum,pro111
pap1261,05e6a2f85eb02c25ed659d314b88e3b1c8f57c67,con105,British Machine Vision Conference,BMC Bioinformatics BioMed Central Methodology article,,Erratum,pro105
pap1262,346a4d94b4b695a9f0966ebbe05dbc6da73c6e96,con103,IEEE International Conference on Multimedia and Expo,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btl642 Structural bioinformatics A structural,,Erratum,pro103
pap1263,92555f78bf48e48605507485a0b8b0c7db18d5cc,con32,International Conference on Software Technology: Methods and Tools,International Journal of Knowledge Discovery in Bioinformatics,,Erratum,pro32
pap1264,10f20d35bba020c38f018b07606ff0b443df9993,con19,International Conference on Conceptual Structures,BIOINFORMATICS APPLICATIONS NOTE,"Motivation: Binding site identiﬁcation is a classical problem that is important for a range of applications, including the structure-based prediction of function, the elucidation of functional relationships among proteins, protein engineering and drug design. We describe an accurate method of binding site identiﬁcation, namely FTSite. This method is based on experimental evidence that ligand binding sites also bind small organic molecules of various shapes and polarity. The FTSite algorithm does not rely on any evolutionary or statistical information, but achieves near experimental accuracy: it is capable of identifying the binding sites in over 94% of apo proteins from established test sets that have been used to evaluate many other binding site prediction methods. Availability: FTSite is freely available as a web-based server at http://ftsite.bu.edu.",Erratum,pro19
pap1265,13dd596cd68b2aac2e86fbab2c2a7c168b424c47,con57,International Workshop on Agent-Oriented Software Engineering,BIOINFORMATICS APPLICATIONS NOTE,Summary: sORF ﬁnder is a program package for identifying small open reading frames (sORFs) with high-coding potential. This application allows the identiﬁcation of coding sORFs according to the nucleotide composition bias among coding sequences and the potential functional constraint at the amino acid level through evaluation of synonymous and non-synonymous substitution rates,Erratum,pro57
pap1266,2eeed5701d5b2f9770e12295e7cba2a65d1ec2fd,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Bioinformatics and Biology Insights,": A computational approach for identification and assessment of genomic sequence variability (GeneSV) is described. For a given nucleotide sequence, GeneSV collects information about the permissible nucleotide variability (changes that potentially preserve function) observed in corresponding regions in genomic sequences, and combines it with conservation/variability results from protein sequence and structure-based analyses of evaluated protein coding regions. GeneSV was used to predict effects (functional vs. non-functional) of 37 amino acid substitutions on the NS5 polymerase (RdRp) of dengue virus type 2 (DENV-2), 36 of which are not observed in any publicly available DENV-2 sequence. 32 novel mutants with single amino acid substitutions in the RdRp were generated using a DENV-2 reverse genetics system. In 81% (26 of 32) of predictions tested, GeneSV correctly predicted viability of introduced mutations. In 4 of 5 (80%) mutants with double amino acid substitutions proximal in structure to one another GeneSV was also correct in its predictions. Predictive capabilities of the developed system were illustrated on dengue RNA virus, but described in the manuscript a general approach to characterize real or theoretically possible variations in genomic and protein sequences can be applied to any organism.",Erratum,pro39
pap1267,646b21ce0b4c85c971c7fcf089138af0c76c6ec6,con57,International Workshop on Agent-Oriented Software Engineering,Proteins Structure O Function O Bioinformatics,Mapping a-helical induced folding within the intrinsically disordered C-terminal domain of the measles virus nucleoprotein by site-directed spin-labeling EPR spectroscopy AQ1,Erratum,pro57
pap1268,ce83b76497148dfc81c9d1700399bf23f06ed079,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",BIOINFORMATICS APPLICATIONS,"Summary: Multiple sequence alignment (MSA) is a central tool in most modern biology studies. However, despite generations of valuable tools, human experts are still able to improve automatically generated MSAs. In an effort to automatically identify the most reliable MSA for a given protein family, we propose a very simple protocol, named AQUA for ‘Automated quality improvement for multiple sequence alignments’. Our current implementation relies on two alignment programs (MUSCLE and MAFFT), one reﬁnement program (RASCAL) and one assessment program (NORMD), but other programs could be incorporated at any of the three steps. Availability: AQUA is implemented in Tcl/Tk and runs in command line on all platforms. The source code is available under the GNU GPL license. Source code, README and Supplementary data are available at http://www.bork.embl.de/Docu/AQUA.",Erratum,pro52
pap1269,25da19c11d3c58e787710628c80b77fd5e5ba140,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,BIOINFORMATICS APPLICATIONS NOTE,"Summary: Multiparent crosses of recombinant inbred lines provide opportunity to map markers and quantitative trait loci (QTL) with much greater resolution than is possible in biparental crosses. Realizing the full potential of these crosses requires computational tools capable of handling the increased statistical complexity of the analyses. R/mpMap provides a ﬂexible and extensible environment, which interfaces easily with other packages to satisfy this demand. Functions in the package encompass simulation, marker map construction, haplotype reconstruction and QTL mapping. We demonstrate the easy-to-use features of mpMap through a simulated data example.",Erratum,pro98
pap1270,20db5b7a716ca3d4c66626d87b45fcea4829f92d,con75,Intelligent Systems in Molecular Biology,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm250 Structural bioinformatics Persistent voids: a new structural metric for membrane fusion,,Erratum,pro75
pap1271,0f5e32f44c4486c53a7559179fc6c21456506016,con6,Annual Conference on Genetic and Evolutionary Computation,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm151 Systems biology,,Erratum,pro6
pap1272,32b02a8a3fc8558e32d121a05a7419f9b8cbe9fb,con50,International Workshop on Green and Sustainable Software,Bioinformatics and Biology Insights,Open Access: Full open access to this and thousands of other papers at,Erratum,pro50
pap1273,a854751c6ab172d922dae842074d06afacf063ae,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Bioinformatics Applications Note Structural Bioinformatics Fast Tomographic Reconstruction on Multicore Computers,"Tomo3D implements a multithreaded vectorized approach to tomographic reconstruction that takes full advantage of the computer power in modern multicore computers. Full resolution tomograms are generated at high speed on standard computers with no special system requirements. Tomo3D has the most common reconstruction methods implemented, namely weighted Back-projection (WBP) and simultaneous iterative reconstruction technique (SIRT). It proves to be competitive with current graphic processor unit solutions in terms of processing time, in the order of a few seconds with WBP or minutes with SIRT. The program is compatible with standard packages, which easily allows integration in the electron tomography workflow.",Erratum,pro85
pap1274,39e5b01e94d64b5b54e65f59d31a748ef4417663,con5,Technical Symposium on Computer Science Education,Bpipe: a tool for running and managing bioinformatics pipelines,"SUMMARY
Bpipe is a simple, dedicated programming language for defining and executing bioinformatics pipelines. It specializes in enabling users to turn existing pipelines based on shell scripts or command line tools into highly flexible, adaptable and maintainable workflows with a minimum of effort. Bpipe ensures that pipelines execute in a controlled and repeatable fashion and keeps audit trails and logs to ensure that experimental results are reproducible. Requiring only Java as a dependency, Bpipe is fully self-contained and cross-platform, making it very easy to adopt and deploy into existing environments.


AVAILABILITY AND IMPLEMENTATION
Bpipe is freely available from http://bpipe.org under a BSD License.",Erratum,pro5
pap1275,26207047ae0e437139f0b660fe21c33bdce0174c,jou81,BMC Bioinformatics,Provenance in bioinformatics workflows,,Article,vol81
pap1276,cbbdb4eeafb1f7ea1d7e4bb27352c9341ee5c662,con48,ACM Symposium on Applied Computing,Advances and Applications in Bioinformatics and Chemistry,"A rapid method for combined analysis of common and rare variants at the level of a region, gene, or pathway Abstract: Previously described methods for the combined analysis of common and rare variants have disadvantages such as requiring an arbitrary classification of variants or permutation testing to assess statistical significance. Here we propose a novel method which implements a weighting scheme based on allele frequencies observed in both cases and controls. Because the test is unbiased, scores can be analyzed with a standard t-test. To test its validity we applied it to data for common, rare, and very rare variants simulated under the null hypothesis. To test its power we applied it to simulated data in which association was present, including data using the observed allele frequencies of common and rare variants in NOD2 previously reported in cases of Crohn's disease and controls. The method produced results that conformed well to those expected under the null hypothesis. It demonstrated more power to detect association when rare and common variants were analyzed jointly, the power further increasing when rare variants were assigned higher weights. 20,000 analyses of a gene containing 62 variants could be performed in 80 minutes on a laptop. This approach shows promise for the analysis of data currently emerging from genome wide sequencing studies.",Erratum,pro48
pap1277,369687d0f95b26702db55075932f2b951d0f7736,con8,Frontiers in Education Conference,BIOINFORMATICS APPLICATIONS,"Motivation: BLAST users frequently expect to obtain homologous genes with certain similarity to their query genes. But what they get from BLAST searches are often collections of local alignments called high-scoring segment pairs (HSPs). On the other hand, most homology-based gene ﬁnders have been built using computation-intensive algorithms, without taking full advantage of BLAST searches that have been perfected over the last decades. Results: Here we report an efﬁcient algorithm, genBlastG that directly uses the HSPs reported by BLAST to deﬁne high-quality gene models.",Erratum,pro8
pap1278,31b6292728805572de28133c6eb02ed1d44ef211,con102,Annual Haifa Experimental Systems Conference,Bioinformatics and Biology Insights,"Background: Precise determination of conformational epitopes of neutralizing antibodies represents a key step in the rational design of novel vaccines. A powerful experimental method to gain insights on the physical chemical nature of conformational epitopes is the selection of linear peptides that bind with high affinities to a monoclonal antibody of interest by phage display technology. However, the structural characterization of conformational epitopes from these mimotopes is not straightforward, and in the past the interpretation of peptide sequences from phage display experiments focused on linear sequence analysis to find a consensus sequence or common sequence motifs. Results: We present a fully automated search method, EpiSearch that predicts the possible location of conformational epitopes on the surface of an antigen. The algorithm uses peptide sequences from phage display experiments as input, and ranks all surface exposed patches according to the frequency distribution of similar residues in the peptides and in the patch. We have tested the performance of the EpiSearch algorithm for six experimental data sets of phage display experiments, the human epidermal growth factor receptor-2 (HER-2/neu), the antibody mAb Bo2C11 targeting the C 2 domain of FVIII, antibodies mAb 17b and mAb b12 of the HIV envelope protein gp120, mAb 13b5 targeting HIV-1 capsid protein and 80R of the SARS coronavirus spike protein. In all these examples the conformational epitopes as determined by the X-ray crystal structures of the antibody-antigen complexes, were found within the highest scoring patches of EpiSearch, covering in most cases more than 50% residues of experimental observed conformational epitopes. Input options of the program include mapping of a single peptide or a set of peptides on the antigen structure, and the results of the calculation can be visualized on our interactive web server.",Erratum,pro102
pap1279,a4d5d41173ae2ebe0d2b142eb193f61e6ea16a2a,con84,Workshop on Interdisciplinary Software Engineering Research,Bioinformatics Applications Note Structure Prediction Meta Server,"The Structure Prediction Meta Server offers a convenient way for biologists to utilize various high quality structure prediction servers available worldwide. The meta server translates the results obtained from remote services into uniform format, which are consequently used to request a jury prediction from a remote consensus server Pcons. Availability: The structure prediction meta server is freely available at http://BioInfo.PL/meta/, some remote servers have however restrictions for non-academic users, which are respected by the meta server. Contact: leszek@bioinfo.pl Supplementary information: Results of several sessions of the CAFASP and LiveBench programs for assessment of performance of fold-recognition servers carried out via the meta server are available at http://BioInfo.PL/services. html. The Structure Prediction Meta Server is aimed to provide fast and convenient assignment of three-dimensional structure for the query protein based only on the provided amino acid sequence. The meta server is a framework for communication between several providers of structure prediction services and offers a convenient interface for the user. The user is asked to provide the amino acid sequence of the query protein, the name of the query protein or a reference name for the prediction job, and the e-mail address. The e-mail address is used only for notification about errors during the execution of the job. The query sequence and the reference name are placed in a process queue coupled to an SQL database engine. The meta server accepts only sequences, which have not been submitted before. The possibility to update the predictions for sequences, which were submitted previously will be introduced to the meta server in the future. The SQL database offers the possibility to find any previous jobs * To whom correspondence should be addressed. processed by the meta server using 'regular expressions' addressing fields like e-mail, job name and the host name, from which the job was initiated. The database currently holds about 3000 predictions. In the future a purging process will delete very old jobs. The meta server is only a set of programs aimed to process and manage biological data, while the predictive power of the service comes from remote prediction providers that collaborate with the central meta server. The meta server utilizes various, carefully evaluated services available from the community of the developers of structure prediction methods. The remote servers provide secondary (local) structure predictions as well as tertiary structure predictions and solvent accessibility. The local structure predictions are obtained from PsiPred (McGuffin …",Erratum,pro84
pap1280,04c8f7d1158759880e50a74371578ba12ee5a44f,con91,Symposium on the Theory of Computing,BIOINFORMATICS Classiﬁcation of Microarrays to Nearest Centroids,"Motivation: Classiﬁcation of biological samples by microarrays is a topic of much interest. A number of methods have been proposed and successfully applied to this problem. It has recently been shown that classiﬁcation by nearest centroids provides an accurate predictor that may outperform much more complicated methods. The ”Prediction Analysis of Microarrays” (PAM) approach is one such example, which the authors strongly motivate by its simplicity and intepretability. In this spirit, I seek to assess the performance of classiﬁers simpler than even PAM. Results: I surprisingly show that the modiﬁed t-statistics and shrun-ken centroids employed by PAM tend to increase misclassiﬁcation error when compared to their simpler counterparts. Based on these observations, I propose a classiﬁcation method called ”Classiﬁca-tion to Nearest Centroids” (ClaNC). ClaNC ranks genes by standard t-statistics, does not shrink centroids, and uses a class-speciﬁc gene-selection procedure. Because of these modiﬁcations, ClaNC is arguably simpler and easier to interpret than PAM, and it can be viewed as a traditional nearest centroid classiﬁer that uses specially selected genes. I demonstrate that ClaNC error rates tend to be signiﬁcantly less than those for PAM, for a given number of active genes. Availability: Point-and-click software is freely available at http://students.washington.edu/adabney/clanc",Erratum,pro91
pap1281,6c5eaf1b2fe00312f388188c2d3891c1c3e92de6,con11,European Conference on Modelling and Simulation,BIOINFORMATICS ORIGINAL PAPER,"Motivation: Comparative genomics heavily relies on alignments of large and often complex DNA sequences. From an engineering perspective, the problem here is to provide maximum sensitivity (to ﬁnd all there is to ﬁnd), speciﬁcity (to only ﬁnd real homology) and speed (to accommodate the billions of base pairs of vertebrate genomes). Results: Satsuma addresses all three issues through novel strategies: (i) cross-correlation, implemented via fast Fourier transform; (ii) a match scoring scheme that eliminates almost all false hits; and (iii) an asynchronous ‘battleship’-like search that allows for aligning two entire ﬁsh genomes (470 and 217Mb) in 120 CPU hours using 15 processors on a single machine. Availability: Satsuma is part of the Spines software package, implemented in C++ on Linux. The latest version of Spines can be freely downloaded under the LGPL license from http://www .broadinstitute.org",Erratum,pro11
pap1282,cb8491c54f0b1e63034343ac5c8325371089185e,con74,IEEE International Conference on Information Reuse and Integration,A review of the stability of feature selection techniques for bioinformatics data,"Feature selection is an important step in data mining and is used in various domains including genetics, medicine, and bioinformatics. Choosing the important features (genes) is essential for the discovery of new knowledge hidden within the genetic code as well as the identification of important biomarkers. Although feature selection methods can help sort through large numbers of genes based on their relevance to the problem at hand, the results generated tend to be unstable and thus cannot be reproduced in other experiments. Relatedly, research interest in the stability of feature ranking methods has grown recently and researchers have produced experimental designs for testing the stability of feature selection, creating new metrics for measuring stability and new techniques designed to improve the stability of the feature selection process. In this paper, we will introduce the role of stability in feature selection with DNA microarray data. We list various ways of improving feature ranking stability, and discuss feature selection techniques, specifically explaining ensemble feature ranking and presenting various ensemble feature ranking aggregation methods. Finally, we discuss experimental procedures such as dataset perturbation, fixed overlap partitioning, and cross validation procedures that help researchers analyze and measure the stability of feature ranking methods. Throughout this work, we investigate current research in the field and discuss possible avenues of continuing such research efforts.",Article,pro74
pap1283,17b07d8bb61a1462299053b66615f9eddd8f23d5,jou133,PLoS ONE,Comprehensive Decision Tree Models in Bioinformatics,"Purpose Classification is an important and widely used machine learning technique in bioinformatics. Researchers and other end-users of machine learning software often prefer to work with comprehensible models where knowledge extraction and explanation of reasoning behind the classification model are possible. Methods This paper presents an extension to an existing machine learning environment and a study on visual tuning of decision tree classifiers. The motivation for this research comes from the need to build effective and easily interpretable decision tree models by so called one-button data mining approach where no parameter tuning is needed. To avoid bias in classification, no classification performance measure is used during the tuning of the model that is constrained exclusively by the dimensions of the produced decision tree. Results The proposed visual tuning of decision trees was evaluated on 40 datasets containing classical machine learning problems and 31 datasets from the field of bioinformatics. Although we did not expected significant differences in classification performance, the results demonstrate a significant increase of accuracy in less complex visually tuned decision trees. In contrast to classical machine learning benchmarking datasets, we observe higher accuracy gains in bioinformatics datasets. Additionally, a user study was carried out to confirm the assumption that the tree tuning times are significantly lower for the proposed method in comparison to manual tuning of the decision tree. Conclusions The empirical results demonstrate that by building simple models constrained by predefined visual boundaries, one not only achieves good comprehensibility, but also very good classification performance that does not differ from usually more complex models built using default settings of the classical decision tree algorithm. In addition, our study demonstrates the suitability of visually tuned decision trees for datasets with binary class attributes and a high number of possibly redundant attributes that are very common in bioinformatics.",Conference paper,vol133
pap1284,3d4d4f7b574293c3d95bbbb69b8b7bcfa1b19d23,con3,Knowledge Discovery and Data Mining,Proteomics & Bioinformatics,.,Erratum,pro3
pap1285,df691ee9f5c5fd05e65c315d30ec8abe4ddb5daf,con51,Brazilian Symposium on Software Engineering,BIOINFORMATICS APPLICATIONS,Summary: BioShell is a suite of programs performing common tasks accompanying protein structure modeling. BioShell design is based on UNIXshellflexibilityandshouldbeusedasitsextension.UsingBioShell various molecular modeling procedures can be integrated in a single pipeline. Availability: BioShell package can be downloaded from its website http://biocomp.chem.uw.edu.pl/BioShell and these pages provide many examples and a detailed documentation for the newest version.,Erratum,pro51
pap1286,36cedf831ca23777472a7707ef7ff5bbb77b123c,con55,Workshop on Learning from Authoritative Security Experiment Results,Application Of Data Mining In Bioinformatics,This article highlights some of the basic concepts of bioinformatics and data mining. The major research areas of bioinformatics are highlighted. The application of data mining in the domain of bioinformatics is explained. It also highlights some of the current challenges and opportunities of data mining in bioinformatics.,Erratum,pro55
pap1287,0e6fa04227f98914408b77b9d52eab7dcc194c83,con70,International Conference on Graph Transformation,Rise and Demise of Bioinformatics? Promise and Progress,"The field of bioinformatics and computational biology has gone through a number of transformations during the past 15 years, establishing itself as a key component of new biology. This spectacular growth has been challenged by a number of disruptive changes in science and technology. Despite the apparent fatigue of the linguistic use of the term itself, bioinformatics has grown perhaps to a point beyond recognition. We explore both historical aspects and future trends and argue that as the field expands, key questions remain unanswered and acquire new meaning while at the same time the range of applications is widening to cover an ever increasing number of biological disciplines. These trends appear to be pointing to a redefinition of certain objectives, milestones, and possibly the field itself.",Erratum,pro70
pap1288,60d625c0f0c9ef72355e1f633e034562b3837e6c,jou109,Scientometrics,Detecting the knowledge structure of bioinformatics by mining full-text collections,,Article,vol109
pap1289,c42a04355ad3313e2a9d828d43e46fa348fd797f,con10,Americas Conference on Information Systems,Bioinformatics for personal genome interpretation,"An international consortium released the first draft sequence of the human genome 10 years ago. Although the analysis of this data has suggested the genetic underpinnings of many diseases, we have not yet been able to fully quantify the relationship between genotype and phenotype. Thus, a major current effort of the scientific community focuses on evaluating individual predispositions to specific phenotypic traits given their genetic backgrounds. Many resources aim to identify and annotate the specific genes responsible for the observed phenotypes. Some of these use intra-species genetic variability as a means for better understanding this relationship. In addition, several online resources are now dedicated to collecting single nucleotide variants and other types of variants, and annotating their functional effects and associations with phenotypic traits. This information has enabled researchers to develop bioinformatics tools to analyze the rapidly increasing amount of newly extracted variation data and to predict the effect of uncharacterized variants. In this work, we review the most important developments in the field--the databases and bioinformatics tools that will be of utmost importance in our concerted effort to interpret the human variome.",Erratum,pro10
pap1290,abd71797e0c682fabcfc57f38104a086f1770379,jou81,BMC Bioinformatics,Cancer bioinformatics: A new approach to systems clinical medicine,,Letter,vol81
pap1291,42b5e58dc252fb9009ff86fab8b77574c2a21705,jou212,Network Modeling Analysis in Health Informatics and Bioinformatics,Threshold-based feature selection techniques for high-dimensional bioinformatics data,,Article,vol212
pap1292,5b4c23484dea349a9f0f354e26a3f26c0ca6cc40,con23,International Conference on Open and Big Data,myExperiment: a repository and social network for the sharing of bioinformatics workflows,"myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.",Erratum,pro23
pap1293,e8db4c01d32cad0694852a94052c1b7d88842ef6,con50,International Workshop on Green and Sustainable Software,"Pseudo Amino Acid Composition and its Applications in Bioinformatics, Proteomics and System Biology","With the avalanche of protein sequences generated in the post-genomic age, it is highly desired to develop automated methods for efficiently identifying various attributes of uncharacterized proteins. This is one of the most im- portant tasks facing us today in bioinformatics, and the information thus obtained will have important impacts on the de- velopment of proteomics and system biology. To realize that, one of the keys is to find an effective model to represent the sample of a protein. The most straightforward model in this regard is its entire amino acid sequence; however, the entire sequence model would fail to work when the query protein did not have significant homology to proteins of known char- acteristics. Thus, various non-sequential models or discrete models were proposed. The simplest discrete model is the amino acid (AA) composition. Using it to represent a protein, however, all the sequence-order information would be com- pletely lost. To cope with such a dilemma, the concept of pseudo amino acid (PseAA) composition was introduced. Its es- sence is to keep using a discrete model to represent a protein yet without completely losing its sequence-order informa- tion. Therefore, in a broad sense, the PseAA composition of a protein is actually a set of discrete numbers that is de- rived from its amino acid sequence and that is different from the classical AA composition and able to harbour some sort of sequence order or pattern information. Ever since the first PseAA composition was formulated to predict protein sub- cellular localization and membrane protein types, it has stimulated many different modes of PseAA composition for studying various kinds of problems in proteins and proteins-related systems. In this review, we shall give a brief and sys- tematic introduction of various modes of PseAA composition and their applications. Meanwhile, the challenges for find- ing the optimal PseAA composition are also briefly discussed.",Erratum,pro50
pap1294,cf171d57f8232ba90a0696f8cb46144b39380d0b,con2,International Conference on Software Engineering,Bioinformatics - The Machine Learning Approach,,Erratum,pro2
pap1295,a65e32fe0d11ac2f9f932d2f2fa01c42f31af2f9,con86,The Web Conference,Bioinformatics for the Human Microbiome Project,"Microbes inhabit virtually all sites of the human body, yet we know very little about the role they play in our health. In recent years, there has been increasing interest in studying human-associated microbial communities, particularly since microbial dysbioses have now been implicated in a number of human diseases [1]–[3]. Dysbiosis, the disruption of the normal microbial community structure, however, is impossible to define without first establishing what “normal microbial community structure” means within the healthy human microbiome. Recent advances in sequencing technologies have made it feasible to perform large-scale studies of microbial communities, providing the tools necessary to begin to address this question [4], [5]. This led to the implementation of the Human Microbiome Project (HMP) in 2007, an initiative funded by the National Institutes of Health Roadmap for Biomedical Research and constructed as a large, genome-scale community research project [6]. Any such project must plan for data analysis, computational methods development, and the public availability of tools and data; here, we provide an overview of the corresponding bioinformatics organization, history, and results from the HMP (Figure 1).",Erratum,pro86
pap1296,ed3617a2b4f46ace154bbdaf08d4618381b57a45,jou228,Trends in Genetics,Bioinformatics challenges of new sequencing technology.,,Letter,vol228
pap1297,9c1c4d1d5ef82f4b934f732183869c7109d01aa6,jou229,Plant and Cell Physiology,Advances in Omics and Bioinformatics Tools for Systems Analyses of Plant Functions,"Omics and bioinformatics are essential to understanding the molecular systems that underlie various plant functions. Recent game-changing sequencing technologies have revitalized sequencing approaches in genomics and have produced opportunities for various emerging analytical applications. Driven by technological advances, several new omics layers such as the interactome, epigenome and hormonome have emerged. Furthermore, in several plant species, the development of omics resources has progressed to address particular biological properties of individual species. Integration of knowledge from omics-based research is an emerging issue as researchers seek to identify significance, gain biological insights and promote translational research. From these perspectives, we provide this review of the emerging aspects of plant systems research based on omics and bioinformatics analyses together with their associated resources and technological advances.",Letter,vol229
pap1298,27546dce4ee9a41dbac31a8c6a0b1f9b90f97a77,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Principal component analysis based methods in bioinformatics studies,"In analysis of bioinformatics data, a unique challenge arises from the high dimensionality of measurements. Without loss of generality, we use genomic study with gene expression measurements as a representative example but note that analysis techniques discussed in this article are also applicable to other types of bioinformatics studies. Principal component analysis (PCA) is a classic dimension reduction approach. It constructs linear combinations of gene expressions, called principal components (PCs). The PCs are orthogonal to each other, can effectively explain variation of gene expressions, and may have a much lower dimensionality. PCA is computationally simple and can be realized using many existing software packages. This article consists of the following parts. First, we review the standard PCA technique and their applications in bioinformatics data analysis. Second, we describe recent 'non-standard' applications of PCA, including accommodating interactions among genes, pathways and network modules and conducting PCA with estimating equations as opposed to gene expressions. Third, we introduce several recently proposed PCA-based techniques, including the supervised PCA, sparse PCA and functional PCA. The supervised PCA and sparse PCA have been shown to have better empirical performance than the standard PCA. The functional PCA can analyze time-course gene expression data. Last, we raise the awareness of several critical but unsolved problems related to PCA. The goal of this article is to make bioinformatics researchers aware of the PCA technique and more importantly its most recent development, so that this simple yet effective dimension reduction technique can be better employed in bioinformatics data analysis.",Erratum,pro49
pap1299,3a85ebfe3904e2afcf663eee32d0114727ee7cf4,con65,IEEE International Conference on Software Engineering and Formal Methods,Protein bioinformatics databases and resources.,,Erratum,pro65
pap1300,952908425d3cafb0144ee45ce82aee98d4a891f9,jou230,Plant Physiology,"A Bioinformatics Approach to the Identification, Classification, and Analysis of Hydroxyproline-Rich Glycoproteins[W][OA]","Hydroxyproline-rich glycoproteins (HRGPs) are a superfamily of plant cell wall proteins that function in diverse aspects of plant growth and development. This superfamily consists of three members: hyperglycosylated arabinogalactan proteins (AGPs), moderately glycosylated extensins (EXTs), and lightly glycosylated proline-rich proteins (PRPs). Hybrid and chimeric versions of HRGP molecules also exist. In order to “mine” genomic databases for HRGPs and to facilitate and guide research in the field, the BIO OHIO software program was developed that identifies and classifies AGPs, EXTs, PRPs, hybrid HRGPs, and chimeric HRGPs from proteins predicted from DNA sequence data. This bioinformatics program is based on searching for biased amino acid compositions and for particular protein motifs associated with known HRGPs. HRGPs identified by the program are subsequently analyzed to elucidate the following: (1) repeating amino acid sequences, (2) signal peptide and glycosylphosphatidylinositol lipid anchor addition sequences, (3) similar HRGPs via Basic Local Alignment Search Tool, (4) expression patterns of their genes, (5) other HRGPs, glycosyl transferase, prolyl 4-hydroxylase, and peroxidase genes coexpressed with their genes, and (6) gene structure and whether genetic mutants exist in their genes. The program was used to identify and classify 166 HRGPs from Arabidopsis (Arabidopsis thaliana) as follows: 85 AGPs (including classical AGPs, lysine-rich AGPs, arabinogalactan peptides, fasciclin-like AGPs, plastocyanin AGPs, and other chimeric AGPs), 59 EXTs (including SP5 EXTs, SP5/SP4 EXTs, SP4 EXTs, SP4/SP3 EXTs, a SP3 EXT, “short” EXTs, leucine-rich repeat-EXTs, proline-rich extensin-like receptor kinases, and other chimeric EXTs), 18 PRPs (including PRPs and chimeric PRPs), and AGP/EXT hybrid HRGPs.",Letter,vol230
pap1301,15a10bbde0cc53be4508ebd04d0230e7fe425d7a,jou231,Current Medicinal Chemistry,Structural bioinformatics and its impact to biomedical science.,"During the last two decades, the number of sequence-known proteins has increased rapidly. In contrast, the corresponding increment for structure-known proteins is much slower. The unbalanced situation has critically limited our ability to understand the molecular mechanism of proteins and conduct structure-based drug design by timely using the updated information of newly found sequences. Therefore, it is highly desired to develop an automated method for fast deriving the 3D (3-dimensional) structure of a protein from its sequence. Under such a circumstance, the structural bioinformatics was emerging naturally as the time required. In this review, three main strategies developed in structural bioinformatics, i.e., pure energetic approach, heuristic approach, and homology modeling approach, as well as their underlying principles, are briefly introduced. Meanwhile, a series of demonstrations are presented to show how the structural bioinformatics has been applied to timely derive the 3D structures of some functionally important proteins, helping to understand their action mechanisms and stimulating the course of drug discovery. Also, the limitation of these approaches and the future challenges of structural bioinformatics are briefly addressed.",Article,vol231
pap1302,5cd0b6d48ab997f351b39b614b02e512d0fb590a,jou81,BMC Bioinformatics,Clinical Bioinformatics: challenges and opportunities,,Article,vol81
pap1303,db5c742121d01d0fedd66dc33da0c1f5f38b2015,jou232,Proteomics,PhosphoSite: A bioinformatics resource dedicated to physiological protein phosphorylation,"PhosphoSite™ is a curated, web‐based bioinformatics resource dedicated to physiologic sites of protein phosphorylation in human and mouse. PhosphoSite is populated with information derived from published literature as well as high‐throughput discovery programs. PhosphoSite provides information about the phosphorylated residue and its surrounding sequence, orthologous sites in other species, location of the site within known domains and motifs, and relevant literature references. Links are also provided to a number of external resources for protein sequences, structure, post‐translational modifications and signaling pathways, as well as sources of phospho‐specific antibodies and probes. As the amount of information in the underlying knowledgebase expands, users will be able to systematically search for the kinases, phosphatases, ligands, treatments, and receptors that have been shown to regulate the phosphorylation status of the sites, and pathways in which the phosphorylation sites function. As it develops into a comprehensive resource of known in vivo phosphorylation sites, we expect that PhosphoSite will be a valuable tool for researchers seeking to understand the role of intracellular signaling pathways in a wide variety of biological processes.",Letter,vol232
pap1304,6c049c9258e31f5f196445ba8be04a735022dacf,jou9,Chemical Reviews,Bioinformatics and systems biology of the lipidome.,"Lipids play an important role in physiology and pathophysiology of living systems. Until a few decades ago, the number of lipid molecules that were chemically characterized was a few hundred at most and were catalogued in monographs and compendia.1 Since the advent of the era of the genome and the proteome, there has been increasing recognition that other macromolecules like lipids and polysaccharides in living systems display considerable structural diversity and systematic efforts are underway to identify, characterize and catalog these molecules. With mass spectrometric techniques coming of age, several thousand distinct molecular species have been identified from living species and the roles of several of these are beginning to be characterized.2 Unlike genes and proteins, whose defined alphabets provide the framework for ontologies and classification at the sequence level, lipids and polysaccharides have been characterized for the large part by popular names, with no foundations for systematic classification. 
 
The past two decades have witnessed two major advances in lipid biology. In the first, mass spectrometry has enabled the identification of thousands of lipid molecular species from cells and tissues and this has pointed to the important need for developing a systematic ontology that can rationally name and catalog the molecules. Second, the ability to investigate the functional roles of lipid molecules through systematic phenotypic studies has led to the identification of lipids as extremely important players in physiology and pathophysiology of living species.3 In combination with proteins and nucleic acids, lipids are integrally involved in biochemical networks that lead to phenotypes such as homeostasis, differentiation, and death of cells and tissues. Any approach to systems characterization of living systems, of necessity, has to include lipids along with other macromolecules and all complex cellular pathways involving lipid molecular species. Systems biology now extends in its scope to identify biosynthetic and metabolic lipid networks, cellular signaling networks that explicitly include lipid molecules and transcriptional and epigenetic networks where lipids play an integral role.4 
 
Several large scale projects to characterize lipids and their functional roles have been initiated as exemplified by the LIPID MAPS5 effort. The LIPID MAPS is an exemplar systems biology project that measures cell-wide lipid changes in an attempt to reconstruct biochemical pathways associated with lipid processing and signaling. The cell-wide measurements of components of these pathways include mass spectrometric measurements of lipid changes in response to stimulus in mammalian cells, changes in transcription profiles in response to stimulus and in select cases proteomic changes in response to stimulus. Figure 1 shows a schematic of the LIPID MAPS experiments related to different lipid categories/pathways and the subsequent processing of the experimental data generated. Network reconstruction efforts rely on organization, analysis and integration of these data and this requires a strong bioinformatics and systems biology effort. The former has to include development of a systematic and universal classification and nomenclature system, design and development of lipid and lipid-gene, lipid-protein databases with appropriate functional annotations, and efficient query and analysis systems that can be broadly useful to the biology research community. The latter has to include methods for analysis of large scale lipid measurements in cells, reconstruction of lipid metabolic and biosynthetic pathways, and quantitative models of lipid fluxes in cells under varied perturbations. In this review, we will provide a comprehensive summary of extant developments in lipid bioinformatics and systems biology and discuss the outlook for the future integration of lipidomics into cellular and organismic biology. The sections that follow are delineated into the informatics approaches specific to lipid biology followed by an overview and exemplar approach to analysis of large scale lipidomic data towards a systems description of mammalian cells. 
 
 
 
Figure 1 
 
Overview of the process of performing a quantitative lipid analysis of macrophage cell sample (in this example, a time-course experiment using bone marrow derived macrophages). Extraction methods, LC/GC purification methods, MS acquisition strategies ... 
 
 
 
 
2. Classification, Ontology, Nomenclature and Structure Representation of Lipid Molecules 
The first step towards classification of lipids is the establishment of an ontology that is extensible, flexible and scalable. One must be able to classify, name and represent these molecules in a logical manner which is amenable to data basing and computational manipulation. Lipids have been loosely defined as biological substances that are generally hydrophobic in nature and in many cases soluble in organic solvents.6 These chemical features are present in a broad range of molecules such as fatty acids, phospholipids, sterols, sphingolipids, terpenes and others. In view of the fact that lipids comprise an extremely heterogeneous collection of molecules from a structural and functional standpoint, it is not surprising that there are significant differences with regard to the scope and organization of current classification schemes. 
 
2.1. Classification, Ontology and Nomenclature 
In order to address the lack of a consistent classification and nomenclature methodology for lipids, LIPID MAPS consortium members have developed a comprehensive classification system for lipids.7 The consortium has taken a more chemistry-based approach and defines lipids as hydrophobic or amphipathic small molecules that may originate entirely or in part by carbanion based condensations of thioesters (such as fatty acids and polyketides) and/or by carbocation based condensations of isoprene units (such as prenols and sterols). Figure 2 shows the mechanisms of lipid biosynthesis.8 Based on this classification system, lipids have been divided into eight categories: Fatty acyls, Glycerolipids, Glycerophospholipids, Sphingolipids, Sterol lipids, Prenol lipids, Saccharolipids, and Polyketides. Each category is further divided into classes and subclasses. Additionally, following the existing rules and recommendations proposed by the International Union of Biochemistry and Applied Chemists and the International Union of Biochemistry and Molecular Biology (IUPAC-IUBMB) commission on Biochemical Nomenclature, a consistent nomenclature scheme has also been developed to provide systematic names for various classes and subclasses of lipids.7 
 
 
 
Figure 2 
 
Mechanisms of lipid biosynthesis. Biosynthesis of ketoacyl- and isoprene-containing lipids proceeds by carbanion and carbocation-mediated chain extension, respectively.8 
 
 
 
All lipids in the LIPID MAPS Structure Database (LMSD) are classified and annotated using this comprehensive classification and nomenclature system developed by the LIPID MAPS consortium.",Letter,vol9
pap1305,7e39604a4b65b27da14200b23e950d350da649f5,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,CloudBLAST: Combining MapReduce and Virtualization on Distributed Resources for Bioinformatics Applications,"This paper proposes and evaluates an approach to the parallelization, deployment and management of bioinformatics applications that integrates several emerging technologies for distributed computing. The proposed approach uses the MapReduce paradigm to parallelize tools and manage their execution, machine virtualization to encapsulate their execution environments and commonly used data sets into flexibly deployable virtual machines, and network virtualization to connect resources behind firewalls/NATs while preserving the necessary performance and the communication environment. An implementation of this approach is described and used to demonstrate and evaluate the proposed approach. The implementation integrates Hadoop, Virtual Workspaces, and ViNe as the MapReduce, virtual machine and virtual network technologies, respectively, to deploy the commonly used bioinformatics tool NCBI BLAST on a WAN-based test bed consisting of clusters at two distinct locations, the University of Florida and the University of Chicago. This WAN-based implementation, called CloudBLAST, was evaluated against both non-virtualized and LAN-based implementations in order to assess the overheads of machine and network virtualization, which were shown to be insignificant. To compare the proposed approach against an MPI-based solution, CloudBLAST performance was experimentally contrasted against the publicly available mpiBLAST on the same WAN-based test bed. Both versions demonstrated performance gains as the number of available processors increased, with CloudBLAST delivering speedups of 57 against 52.4 of MPI version, when 64 processors on 2 sites were used. The results encourage the use of the proposed approach for the execution of large-scale bioinformatics applications on emerging distributed environments that provide access to computing resources as a service.",Erratum,pro21
pap1306,887e97d6da0fedd885c1c05dcc899919ab6d189d,jou233,Methods of biochemical analysis,Bioinformatics - a practical guide to the analysis of genes and proteins,"Foreword (Lee Hood). Preface. Contributors. PART ONE: BIOLOGICAL DATABASES. 1. Sequence Databases (Rolf Apweiler). 2. Mapping Databases (Peter S. White and Tara C. Matise). 3. Information Retrieval from Biological Databases (Andreas D. Baxevanis). 4. Genomic Databases (Tyra G. Wolfsberg). PART TWO: ANALYSIS AT THE NUCLEOTIDE LEVEL. 5. Predictive Methods Using DNA Sequences (Enrique Blanco and Roderic Guigo). 6. Predictive Methods Using RNA Sequences (David Mathews and Michael Zuker). 7. Sequence Polymorphisms (James C. Mullikin and Stephen T. Sherry). PART THREE: ANALYSIS AT THE PROTEIN LEVEL. 8. Predictive Methods Using Protein Sequences (Yanay Ofran and Burkhard Rost). 9. Protein Structure Prediction and Analysis (David Wishart). 10. Intermolecular Interactions and Biological Pathways (Gary D. Bader and Anton J. Enright). PART FOUR: INFERRING RELATIONSHIPS. 11. Assessing Pairwise Sequence Similarity: BLAST and FASTA (Andreas D. Baxevanis). 12. Creation and Analysis of Protein Multiple Sequence Alignments (Geoffrey J. Barton). 13. Sequence Assembly and Finishing Methods (Nancy F. Hansen, Pamela Jacques Thomas and Gerard G. Bouffard). 14. Phylogenetic Analysis (Fiona S. L. Brinkman). 15. Computational Approaches in Comparative Genomics (Andreas D. Baxevanis). 16. Using DNA Microarrays to Assay Gene Expression (John Quackenbush). 17. Proteomics and Protein Identification (Mark R. Holmes, Kevin R. Ramkissoon and Morgan C. Giddings). PART FIVE: DEVELOPING TOOLS. 18. Using Perl to Facilitate Biological Analysis (Lincoln D. Stein). Appendices. Glossary. Index.",Conference paper,vol233
pap1307,2a4d5bb3e8c61d1bb86dfa093869564ac94ac613,jou234,Advances in Experimental Medicine and Biology,Decision tree and ensemble learning algorithms with their applications in bioinformatics.,,Conference paper,vol234
pap1308,36d44cb9c907defeecc3b72f2076393156bc7dad,con100,International Conference on Automatic Face and Gesture Recognition,Multiobjective Genetic Algorithms for Clustering - Applications in Data Mining and Bioinformatics,,Erratum,pro100
pap1309,a8a294704715ec82ac91f129734bf05ddf9bde14,con56,International Conference on Software Engineering and Knowledge Engineering,BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs,"MOTIVATION
Genomics has revolutionized biological research, but quality assessment of the resulting assembled sequences is complicated and remains mostly limited to technical measures like N50.


RESULTS
We propose a measure for quantitative assessment of genome assembly and annotation completeness based on evolutionarily informed expectations of gene content. We implemented the assessment procedure in open-source software, with sets of Benchmarking Universal Single-Copy Orthologs, named BUSCO.


AVAILABILITY AND IMPLEMENTATION
Software implemented in Python and datasets available for download from http://busco.ezlab.org.


CONTACT
evgeny.zdobnov@unige.ch


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro56
pap1310,da5e628776269bd89bbec28736dc420675dba8df,con18,International Conference on Exploring Services Science,Microarray bioinformatics.,,Erratum,pro18
pap1311,28884a708632a94128a9a8c83473ed803c419a3a,jou235,Journal of Clinical Bioinformatics,Clinical bioinformatics: a new emerging science,,Letter,vol235
pap1312,61508fb4e1313d3baae055e0b56938b9f49c6ae7,jou81,BMC Bioinformatics,dbOGAP - An Integrated Bioinformatics Resource for Protein O-GlcNAcylation,,Letter,vol81
pap1313,8488dd6a52010c8ff6b5236978cd66c257a6bfc5,jou236,Cell,Protein Diversity from Alternative Splicing A Challenge for Bioinformatics and Post-Genome Biology,,Conference paper,vol236
pap1314,cc90910b6e31fe44cddc1e341f21eec0aaa5db44,con88,European Conference on Computer Vision,Trimmomatic: a flexible trimmer for Illumina sequence data,"Motivation: Although many next-generation sequencing (NGS) read preprocessing tools already existed, we could not find any tool or combination of tools that met our requirements in terms of flexibility, correct handling of paired-end data and high performance. We have developed Trimmomatic as a more flexible and efficient preprocessing tool, which could correctly handle paired-end data. Results: The value of NGS read preprocessing is demonstrated for both reference-based and reference-free tasks. Trimmomatic is shown to produce output that is at least competitive with, and in many cases superior to, that produced by other tools, in all scenarios tested. Availability and implementation: Trimmomatic is licensed under GPL V3. It is cross-platform (Java 1.5+ required) and available at http://www.usadellab.org/cms/index.php?page=trimmomatic Contact: usadel@bio1.rwth-aachen.de Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro88
pap1315,024e616250c29cdccc1e0f274a71d397b4eb15c6,con40,Conference on Software Engineering Education and Training,BMC Bioinformatics BioMed Central Methodology article,,Erratum,pro40
pap1316,bb5306987d169d36d4b217b0b0f6e13e2d5cb097,con56,International Conference on Software Engineering and Knowledge Engineering,The use of classification trees for bioinformatics,"Classification trees are nonparametric statistical learning methods that incorporate feature selection and interactions, possess intuitive interpretability, are efficient, and have high prediction accuracy when used in ensembles. This paper provides a brief introduction to the classification tree‐based methods, a review of the recent developments, and a survey of the applications in bioinformatics and statistical genetics. © 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 55‐63 DOI: 10.1002/widm.14",Erratum,pro56
pap1317,95a47730774c3a7204046a651052117841335e9f,jou237,Nature Reviews Neurology,Multimodal monitoring and neurocritical care bioinformatics,,Conference paper,vol237
pap1318,4fdb2858b98199de280a1a694194d44a290b8d28,con76,IEEE International Conference on Tools with Artificial Intelligence,BioStar: An Online Question & Answer Resource for the Bioinformatics Community,"Although the era of big data has produced many bioinformatics tools and databases, using them effectively often requires specialized knowledge. Many groups lack bioinformatics expertise, and frequently find that software documentation is inadequate while local colleagues may be overburdened or unfamiliar with specific applications. Too often, such problems create data analysis bottlenecks that hinder the progress of biological research. In order to help address this deficiency, we present BioStar, a forum based on the Stack Exchange platform where experts and those seeking solutions to problems of computational biology exchange ideas. The main strengths of BioStar are its large and active group of knowledgeable users, rapid response times, clear organization of questions and responses that limit discussion to the topic at hand, and ranking of questions and answers that help identify their usefulness. These rankings, based on community votes, also contribute to a reputation score for each user, which serves to keep expert contributors engaged. The BioStar community has helped to answer over 2,300 questions from over 1,400 users (as of June 10, 2011), and has played a critical role in enabling and expediting many research projects. BioStar can be accessed at http://www.biostars.org/.",Erratum,pro76
pap1319,7915fd00117fcc2f380863b2a04ea6638c22951b,jou30,Metabolomics,Bioinformatics tools for cancer metabolomics,,Article,vol30
pap1320,ad220a3283c6c5f81482a9554319151a22d4233f,con14,Hawaii International Conference on System Sciences,"Omics technologies, data and bioinformatics principles.",,Erratum,pro14
pap1321,17a0de0cd9393b64c0e3ce97308cfc3cf13db3b3,jou81,BMC Bioinformatics,R/parallel – speeding up bioinformatics analysis with R,,Conference paper,vol81
pap1322,c67a8498369c0f0affb98f984344b9b027826d88,con61,International Conference on Predictive Models in Software Engineering,Using bioinformatics to predict the functional impact of SNVs,"MOTIVATION
The past decade has seen the introduction of fast and relatively inexpensive methods to detect genetic variation across the genome and exponential growth in the number of known single nucleotide variants (SNVs). There is increasing interest in bioinformatics approaches to identify variants that are functionally important from millions of candidate variants. Here, we describe the essential components of bioinformatics tools that predict functional SNVs.


RESULTS
Bioinformatics tools have great potential to identify functional SNVs, but the black box nature of many tools can be a pitfall for researchers. Understanding the underlying methods, assumptions and biases of these tools is essential to their intelligent application.",Erratum,pro61
pap1323,d8792446c467a83d3bb14dcc342926c72e1f7dfb,jou183,Nature Protocols,"The Phyre2 web portal for protein modeling, prediction and analysis",,Letter,vol183
pap1324,163ee23ad38c79f776e6fbaf526172893b504b37,con3,Knowledge Discovery and Data Mining,Translational bioinformatics: linking knowledge across biological and clinical realms,"Nearly a decade since the completion of the first draft of the human genome, the biomedical community is positioned to usher in a new era of scientific inquiry that links fundamental biological insights with clinical knowledge. Accordingly, holistic approaches are needed to develop and assess hypotheses that incorporate genotypic, phenotypic, and environmental knowledge. This perspective presents translational bioinformatics as a discipline that builds on the successes of bioinformatics and health informatics for the study of complex diseases. The early successes of translational bioinformatics are indicative of the potential to achieve the promise of the Human Genome Project for gaining deeper insights to the genetic underpinnings of disease and progress toward the development of a new generation of therapies.",Erratum,pro3
pap1325,4ad78c487670caeeeec8bcbc13ed841268c39884,jou238,Journal of Computing Science and Engineering,A Survey of Transfer and Multitask Learning in Bioinformatics,"Machine learning and data mining have found many applications in biological domains, where we look to build predictive models based on labeled training data. However, in practice, high quality labeled data is scarce, and to label new data incurs high costs. Transfer and multitask learning offer an attractive alternative, by allowing useful knowledge to be extracted and transferred from data in auxiliary domains helps counter the lack of data problem in the target domain. In this article, we survey recent advances in transfer and multitask learning for bioinformatics applications. In particular, we survey several key bioinformatics application areas, including sequence classification, gene expression data analysis, biological network reconstruction and biomedical applications. Category: Convergence computing",Article,vol238
pap1326,1b74c8798d6e6052bdeec1e9e51a46eafd41f06c,jou235,Journal of Clinical Bioinformatics,Role of clinical bioinformatics in the development of network-based Biomarkers,,Conference paper,vol235
pap1327,c700eee5f49366deed6d15f625083709cd323485,jou239,Molecular biology and evolution,"MEGA5: molecular evolutionary genetics analysis using maximum likelihood, evolutionary distance, and maximum parsimony methods.","Comparative analysis of molecular sequence data is essential for reconstructing the evolutionary histories of species and inferring the nature and extent of selective forces shaping the evolution of genes and species. Here, we announce the release of Molecular Evolutionary Genetics Analysis version 5 (MEGA5), which is a user-friendly software for mining online databases, building sequence alignments and phylogenetic trees, and using methods of evolutionary bioinformatics in basic biology, biomedicine, and evolution. The newest addition in MEGA5 is a collection of maximum likelihood (ML) analyses for inferring evolutionary trees, selecting best-fit substitution models (nucleotide or amino acid), inferring ancestral states and sequences (along with probabilities), and estimating evolutionary rates site-by-site. In computer simulation analyses, ML tree inference algorithms in MEGA5 compared favorably with other software packages in terms of computational efficiency and the accuracy of the estimates of phylogenetic trees, substitution parameters, and rate variation among sites. The MEGA user interface has now been enhanced to be activity driven to make it easier for the use of both beginners and experienced scientists. This version of MEGA is intended for the Windows platform, and it has been configured for effective use on Mac OS X and Linux desktops. It is available free of charge from http://www.megasoftware.net.",Letter,vol239
pap1328,481031811893580178a093339afee5e72bb59b8e,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Visual DSD: a design and analysis tool for DNA strand displacement systems,"Summary: The Visual DSD (DNA Strand Displacement) tool allows rapid prototyping and analysis of computational devices implemented using DNA strand displacement, in a convenient web-based graphical interface. It is an implementation of the DSD programming language and compiler described by Lakin et al. (2011) with additional features such as support for polymers of unbounded length. It also supports stochastic and deterministic simulation, construction of continuous-time Markov chains and various export formats which allow models to be analysed using third-party tools. Availability: Visual DSD is available as a web-based Silverlight application for most major browsers on Windows and Mac OS X at http://research.microsoft.com/dna. It can be installed locally for offline use. Command-line versions for Windows, Mac OS X and Linux are also available from the web page. Contact: aphillip@microsoft.com Supplementary Information:Supplementary data are available at Bioinformatics online.",Erratum,pro82
pap1329,2711117464ccbb23a310b9de727c9bcfec86ba2e,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",Geneious Basic: An integrated and extendable desktop software platform for the organization and analysis of sequence data,"Summary: The two main functions of bioinformatics are the organization and analysis of biological data using computational resources. Geneious Basic has been designed to be an easy-to-use and flexible desktop software application framework for the organization and analysis of biological data, with a focus on molecular sequences and related data types. It integrates numerous industry-standard discovery analysis tools, with interactive visualizations to generate publication-ready images. One key contribution to researchers in the life sciences is the Geneious public application programming interface (API) that affords the ability to leverage the existing framework of the Geneious Basic software platform for virtually unlimited extension and customization. The result is an increase in the speed and quality of development of computation tools for the life sciences, due to the functionality and graphical user interface available to the developer through the public API. Geneious Basic represents an ideal platform for the bioinformatics community to leverage existing components and to integrate their own specific requirements for the discovery, analysis and visualization of biological data. Availability and implementation: Binaries and public API freely available for download at http://www.geneious.com/basic, implemented in Java and supported on Linux, Apple OSX and MS Windows. The software is also available from the Bio-Linux package repository at http://nebc.nerc.ac.uk/news/geneiousonbl. Contact: peter@biomatters.com",Erratum,pro87
pap1330,f581ffaa6a409b36c249dd7453ccde16bb8cb89c,con51,Brazilian Symposium on Software Engineering,MultiQC: summarize analysis results for multiple tools and samples in a single report,"Motivation: Fast and accurate quality control is essential for studies involving next-generation sequencing data. Whilst numerous tools exist to quantify QC metrics, there is no common approach to flexibly integrate these across tools and large sample sets. Assessing analysis results across an entire project can be time consuming and error prone; batch effects and outlier samples can easily be missed in the early stages of analysis. Results: We present MultiQC, a tool to create a single report visualising output from multiple tools across many samples, enabling global trends and biases to be quickly identified. MultiQC can plot data from many common bioinformatics tools and is built to allow easy extension and customization. Availability and implementation: MultiQC is available with an GNU GPLv3 license on GitHub, the Python Package Index and Bioconda. Documentation and example reports are available at http://multiqc.info Contact: phil.ewels@scilifelab.se",Erratum,pro51
pap1331,114d9e30d388fa5b74797b864d092a0ee63e5b27,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Complex heatmaps reveal patterns and correlations in multidimensional genomic data,"UNLABELLED
Parallel heatmaps with carefully designed annotation graphics are powerful for efficient visualization of patterns and relationships among high dimensional genomic data. Here we present the ComplexHeatmap package that provides rich functionalities for customizing heatmaps, arranging multiple parallel heatmaps and including user-defined annotation graphics. We demonstrate the power of ComplexHeatmap to easily reveal patterns and correlations among multiple sources of information with four real-world datasets.


AVAILABILITY AND IMPLEMENTATION
The ComplexHeatmap package and documentation are freely available from the Bioconductor project: http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html


CONTACT
m.schlesner@dkfz.de


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro47
pap1332,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,jou240,International Journal of Systematic and Evolutionary Microbiology,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",Conference paper,vol240
pap1333,1c2486bc6e46abe5cfac23eac852f3f38900b882,jou22,Proceedings of the National Academy of Sciences of the United States of America,"DNA barcodes: Genes, genomics, and bioinformatics","It is not a coincidence that DNA barcoding has developed in concert with genomics-based investigations. DNA barcoding (a tool for rapid species identification based on DNA sequences) and genomics (which compares entire genome structure and expression) share an emphasis on large-scale genetic data acquisition that offers new answers to questions previously beyond the reach of traditional disciplines. DNA barcodes consist of a standardized short sequence of DNA (400–800 bp) that in principle should be easily generated and characterized for all species on the planet (1). A massive on-line digital library of barcodes will serve as a standard to which the DNA barcode sequence of an unidentified sample from the forest, garden, or market can be matched. Similar to genomics, which has accelerated the process of recognizing novel genes and comparing gene function, DNA barcoding will allow users to efficiently recognize known species and speed the discovery of species yet to be found in nature. DNA barcoding aims to use the information of one or a few gene regions to identify all species of life, whereas genomics, the inverse of barcoding, describes in one (e.g., humans) or a few selected species the function and interactions across all genes (Fig. 1). The work of Lahaye et al. (2) reported in a recent issue of PNAS brings the application of DNA barcoding one step closer to implementation in plants.",Conference paper,vol22
pap1334,99649284503c2abe6871ae45f3dbf394a3a86352,jou241,Science Signaling,Integrative Analysis of Complex Cancer Genomics and Clinical Profiles Using the cBioPortal,"The cBioPortal enables integration, visualization, and analysis of multidimensional cancer genomic and clinical data. The cBioPortal for Cancer Genomics (http://cbioportal.org) provides a Web resource for exploring, visualizing, and analyzing multidimensional cancer genomics data. The portal reduces molecular profiling data from cancer tissues and cell lines into readily understandable genetic, epigenetic, gene expression, and proteomic events. The query interface combined with customized data storage enables researchers to interactively explore genetic alterations across samples, genes, and pathways and, when available in the underlying data, to link these to clinical outcomes. The portal provides graphical summaries of gene-level data from multiple platforms, network visualization and analysis, survival analysis, patient-centric queries, and software programmatic access. The intuitive Web interface of the portal makes complex cancer genomics profiles accessible to researchers and clinicians without requiring bioinformatics expertise, thus facilitating biological discoveries. Here, we provide a practical guide to the analysis and visualization features of the cBioPortal for Cancer Genomics.",Conference paper,vol241
pap1335,51485938a6859c593a2b4d3144d8d408c89c06f8,jou229,Plant and Cell Physiology,Genomics and Bioinformatics Resources for Crop Improvement,"Recent remarkable innovations in platforms for omics-based research and application development provide crucial resources to promote research in model and applied plant species. A combinatorial approach using multiple omics platforms and integration of their outcomes is now an effective strategy for clarifying molecular systems integral to improving plant productivity. Furthermore, promotion of comparative genomics among model and applied plants allows us to grasp the biological properties of each species and to accelerate gene discovery and functional analyses of genes. Bioinformatics platforms and their associated databases are also essential for the effective design of approaches making the best use of genomic resources, including resource integration. We review recent advances in research platforms and resources in plant omics together with related databases and advances in technology.",Letter,vol229
pap1336,862e3ed2dedaf812d33ffcd24b21fe59de69c004,jou242,Toxicological Sciences,The evolution of bioinformatics in toxicology: advancing toxicogenomics.,"As one reflects back through the past 50 years of scientific research, a significant accomplishment was the advance into the genomic era. Basic research scientists have uncovered the genetic code and the foundation of the most fundamental building blocks for the molecular activity that supports biological structure and function. Accompanying these structural and functional discoveries is the advance of techniques and technologies to probe molecular events, in time, across environmental and chemical exposures, within individuals, and across species. The field of toxicology has kept pace with advances in molecular study, and the past 50 years recognizes significant growth and explosive understanding of the impact of the compounds and environment to basic cellular and molecular machinery. The advancement of molecular techniques applied in a whole-genomic capacity to the study of toxicant effects, toxicogenomics, is no doubt a significant milestone for toxicological research. Toxicogenomics has also provided an avenue for advancing a joining of multidisciplinary sciences including engineering and informatics in traditional toxicological research. This review will cover the evolution of the field of toxicogenomics in the context of informatics integration its current promise, and limitations.",Letter,vol242
pap1337,43a15ba37c0e1c88ebf28ff7f5cbe7e4ad20d6cf,con60,European Conference on Software Process Improvement,CD-HIT: accelerated for clustering the next-generation sequencing data,"Summary: CD-HIT is a widely used program for clustering biological sequences to reduce sequence redundancy and improve the performance of other sequence analyses. In response to the rapid increase in the amount of sequencing data produced by the next-generation sequencing technologies, we have developed a new CD-HIT program accelerated with a novel parallelization strategy and some other techniques to allow efficient clustering of such datasets. Our tests demonstrated very good speedup derived from the parallelization for up to ∼24 cores and a quasi-linear speedup for up to ∼8 cores. The enhanced CD-HIT is capable of handling very large datasets in much shorter time than previous versions. Availability: http://cd-hit.org. Contact: liwz@sdsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro60
pap1338,9dcc77221ddfdf919546f245ff34dd8aa0f03343,con67,IEEE International Software Metrics Symposium,BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm078 Sequence analysis,,Erratum,pro67
pap1339,a66b2486a1ae6580944eabb8a6fc9449caa205c0,jou81,BMC Bioinformatics,BLAST+: architecture and applications,,Article,vol81
pap1340,39a42d51b862cfe2f5cf290e50c3c9b4975db6d3,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,The EMBL-EBI search and sequence analysis tools APIs in 2019,"Abstract The EMBL-EBI provides free access to popular bioinformatics sequence analysis applications as well as to a full-featured text search engine with powerful cross-referencing and data retrieval capabilities. Access to these services is provided via user-friendly web interfaces and via established RESTful and SOAP Web Services APIs (https://www.ebi.ac.uk/seqdb/confluence/display/JDSAT/EMBL-EBI+Web+Services+APIs+-+Data+Retrieval). Both systems have been developed with the same core principles that allow them to integrate an ever-increasing volume of biological data, making them an integral part of many popular data resources provided at the EMBL-EBI. Here, we describe the latest improvements made to the frameworks which enhance the interconnectivity between public EMBL-EBI resources and ultimately enhance biological data discoverability, accessibility, interoperability and reusability.",Erratum,pro58
pap1341,a114acb9c90d29d9611674824b01a007b6b7a115,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications,"Summary: A combination of bisulfite treatment of DNA and high-throughput sequencing (BS-Seq) can capture a snapshot of a cell's epigenomic state by revealing its genome-wide cytosine methylation at single base resolution. Bismark is a flexible tool for the time-efficient analysis of BS-Seq data which performs both read mapping and methylation calling in a single convenient step. Its output discriminates between cytosines in CpG, CHG and CHH context and enables bench scientists to visualize and interpret their methylation data soon after the sequencing run is completed. Availability and implementation: Bismark is released under the GNU GPLv3+ licence. The source code is freely available from www.bioinformatics.bbsrc.ac.uk/projects/bismark/. Contact: felix.krueger@bbsrc.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro58
pap1342,d9433484c532ab2f14173afba786026fdee0af07,jou243,Nature Reviews Microbiology,Detecting genomic islands using bioinformatics approaches,,Letter,vol243
pap1343,6806fe4a47310f6961cdaeca17fca6aed513d33f,con94,Vision,BioRuby: bioinformatics software for the Ruby programming language,"Summary: The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. BioRuby has components for sequence analysis, pathway analysis, protein modelling and phylogenetic analysis; it supports many widely used data formats and provides easy access to databases, external programs and public web services, including BLAST, KEGG, GenBank, MEDLINE and GO. BioRuby comes with a tutorial, documentation and an interactive environment, which can be used in the shell, and in the web browser. Availability: BioRuby is free and open source software, made available under the Ruby license. BioRuby runs on all platforms that support Ruby, including Linux, Mac OS X and Windows. And, with JRuby, BioRuby runs on the Java Virtual Machine. The source code is available from http://www.bioruby.org/. Contact: katayama@bioruby.org",Erratum,pro94
pap1344,3040ac52b3154de53a831715f2ce7f4e7d3d723b,con5,Technical Symposium on Computer Science Education,BIOINFORMATICS APPLICATIONS NOTE Sequence analysis Manipulation of FASTQ data with Galaxy,"Summary: Here, we describe a tool suite that functions on all of the commonly known FASTQ format variants and provides a pipeline for manipulating next generation sequencing data taken from a sequencing machine all the way through the quality ﬁltering steps. Availability and Implementation: This open-source toolset was implemented in Python and has been integrated into the online data analysis platform Galaxy (public web access: http://usegalaxy.org; download: http://getgalaxy.org). Two short movies that highlight the functionality of tools described in this manuscript as well as results from testing components of this tool suite against a set of previously published ﬁles are available at http://usegalaxy.org/u/dan/p/fastq Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro5
pap1345,1fb4d85ebae0b7daf59456daed7b1143e2df007a,jou244,Journal of Pharmacology and Pharmacotherapeutics,Protein data bank,"Research Collaborators for structural Bioinformatics Protein Data Bank (RCBS PDB) began in 1970’s by group of the young crystallographers, including Edgar Meyer, Gerson Coheon and Helen M Berman.[1] The project was initiated in 1971 to list the structures of all the amino acids using neutron diffraction and has grown as a major international resource for structural biology containing more than 80,000 entries including proteins: 77,529; nucleic acids: 2420; proteins/ DNA complexes: 3795 and other: 24 (totally 83,768 entries) as of Tuesday Aug 14, 2012 at 5 PM DST. The website also proclaims that there have been 62987 structure hits and 11915 ligand hits on this date and that the PDB has 24583 citations.",Article,vol244
pap1346,0fff44b9387f5708a3634f31b001976b251f28f7,con75,Intelligent Systems in Molecular Biology,myGrid: personalised bioinformatics on the information grid,"MOTIVATION
The (my)Grid project aims to exploit Grid technology, with an emphasis on the Information Grid, and provide middleware layers that make it appropriate for the needs of bioinformatics. (my)Grid is building high level services for data and application integration such as resource discovery, workflow enactment and distributed query processing. Additional services are provided to support the scientific method and best practice found at the bench but often neglected at the workstation, notably provenance management, change notification and personalisation.


RESULTS
We give an overview of these services and their metadata. In particular, semantically rich metadata expressed using ontologies necessary to discover, select and compose services into dynamic workflows.",Article,pro75
pap1347,e64728f38ddeb4a134e92610f280a546c82bda91,jou88,bioRxiv,Roary: rapid large-scale prokaryote pan genome analysis,"Summary A typical prokaryote population sequencing study can now consist of hundreds or thousands of isolates. Interrogating these datasets can provide detailed insights into the genetic structure of of prokaryotic genomes. We introduce Roary, a tool that rapidly builds large-scale pan genomes, identifying the core and dispensable accessory genes. Roary makes construction of the pan genome of thousands of prokaryote samples possible on a standard desktop without compromising on the accuracy of results. Using a single CPU Roary can produce a pan genome consisting of 1000 isolates in 4.5 hours using 13 GB of RAM, with further speedups possible using multiple processors. Availability and implementation Roary is implemented in Perl and is freely available under an open source GPLv3 license from http://sanger-pathogens.github.io/Roary Contact roary@sanger.ac.uk Supplementary information Supplementary data are available at Bioinformatics online.",Letter,vol88
pap1348,1f64b33d2a2959784d6d71925b763529317e2bd4,jou81,BMC Bioinformatics,Visual gene developer: a fully programmable bioinformatics software for synthetic gene optimization,,Article,vol81
pap1349,56dff84c0afc7d03838027409a2df96487ea4677,con31,International Conference on Evaluation & Assessment in Software Engineering,Kernel Methods in Bioinformatics,,Erratum,pro31
pap1350,c5fc83256c2c7bc5081061932471957361546857,con67,IEEE International Software Metrics Symposium,BIOINFORMATICS ORIGINAL,"Motivation: Semantic tagging of organism mentions in full-text articles is an important part of literature mining and semantic enrichment solutions. Tagged organism mentions also play a pivotal role in disambiguating other entities in a text, such as proteins. A high-precision organism tagging system must be able to detect the numerous forms of organism mentions, including common names as well as the traditional taxonomic groups: genus, species and strains. In addition, such a system must resolve abbreviations and acronyms, assign the scientiﬁc name and if possible link the detected mention to the NCBI Taxonomy database for further semantic queries and literature navigation. Results: We present the OrganismTagger , a hybrid rule-based/machine learning system to extract organism mentions from the literature. It includes tools for automatically generating lexical and ontological resources from a copy of the NCBI Taxonomy database, thereby facilitating system updates by end users. Its novel ontology-based resources can also be reused in other semantic mining and linked data tasks. Each detected organism mention is normalized to a canonical name through the resolution of acronyms and abbreviations and subsequently grounded with an NCBI Taxonomy database ID. In particular, our system combines a novel machine-learning approach with rule-based and lexical methods for detecting strain mentions in documents. On our manually annotated OT corpus, the OrganismTagger achieves a precision of 95%, a recall of 94% and a grounding accuracy of 97.5%. On the manually annotated corpus of Linnaeus-100, the results show a precision of 99%, recall of 97% and grounding accuracy of 97.4%. Availability: The OrganismTagger, including supporting tools, resources, training data and manual annotations, as well as end user and developer documentation, is freely available under an open-source license at http://www.semanticsoftware.info/ organism-tagger.",Erratum,pro67
pap1351,e7d569fd76916cb44c54baf340eea8097fcde480,jou81,BMC Bioinformatics,Bioinformatics analysis of disordered proteins in prokaryotes,,Letter,vol81
pap1352,9f4c3994c2600f15f3dbda8ba4f15cd5ab610b99,con11,European Conference on Modelling and Simulation,Systems Biology: The Next Frontier for Bioinformatics,"Biochemical systems biology augments more traditional disciplines, such as genomics, biochemistry and molecular biology, by championing (i) mathematical and computational modeling; (ii) the application of traditional engineering practices in the analysis of biochemical systems; and in the past decade increasingly (iii) the use of near-comprehensive data sets derived from ‘omics platform technologies, in particular “downstream” technologies relative to genome sequencing, including transcriptomics, proteomics and metabolomics. The future progress in understanding biological principles will increasingly depend on the development of temporal and spatial analytical techniques that will provide high-resolution data for systems analyses. To date, particularly successful were strategies involving (a) quantitative measurements of cellular components at the mRNA, protein and metabolite levels, as well as in vivo metabolic reaction rates, (b) development of mathematical models that integrate biochemical knowledge with the information generated by high-throughput experiments, and (c) applications to microbial organisms. The inevitable role bioinformatics plays in modern systems biology puts mathematical and computational sciences as an equal partner to analytical and experimental biology. Furthermore, mathematical and computational models are expected to become increasingly prevalent representations of our knowledge about specific biochemical systems.",Erratum,pro11
pap1353,d2dd417b1820c8e4bd462a473af051b2322305e8,jou22,Proceedings of the National Academy of Sciences of the United States of America,Cloning of a human parvovirus by molecular screening of respiratory tract samples.,"The identification of new virus species is a key issue for the study of infectious disease but is technically very difficult. We developed a system for large-scale molecular virus screening of clinical samples based on host DNA depletion, random PCR amplification, large-scale sequencing, and bioinformatics. The technology was applied to pooled human respiratory tract samples. The first experiments detected seven human virus species without the use of any specific reagent. Among the detected viruses were one coronavirus and one parvovirus, both of which were at that time uncharacterized. The parvovirus, provisionally named human bocavirus, was in a retrospective clinical study detected in 17 additional patients and associated with lower respiratory tract infections in children. The molecular virus screening procedure provides a general culture-independent solution to the problem of detecting unknown virus species in single or pooled samples. We suggest that a systematic exploration of the viruses that infect humans, ""the human virome,"" can be initiated.",Conference paper,vol22
pap1354,239b44fd21c3f949f9fdf43bfedd4ab2a3d314e3,jou81,BMC Bioinformatics,pROC: an open-source package for R and S+ to analyze and compare ROC curves,,Article,vol81
pap1355,00b7e1a7228c49492d0506f2165c593a3e52e2b1,jou103,Science,"Structural Bioinformatics-Based Design of Selective, Irreversible Kinase Inhibitors","The active sites of 491 human protein kinase domains are highly conserved, which makes the design of selective inhibitors a formidable challenge. We used a structural bioinformatics approach to identify two selectivity filters, a threonine and a cysteine, at defined positions in the active site of p90 ribosomal protein S6 kinase (RSK). A fluoromethylketone inhibitor, designed to exploit both selectivity filters, potently and selectively inactivated RSK1 and RSK2 in mammalian cells. Kinases with only one selectivity filter were resistant to the inhibitor, yet they became sensitized after genetic introduction of the second selectivity filter. Thus, two amino acids that distinguish RSK from other protein kinases are sufficient to confer inhibitor sensitivity.",Conference paper,vol103
pap1356,0156a0851012c50292d26d8f0e7f5fdcf7b3b67c,con56,International Conference on Software Engineering and Knowledge Engineering,BMC Bioinformatics BioMed Central,,Erratum,pro56
pap1357,04b23f577c20d1a0e2a67aadda555f58e6d23d6e,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Support vector machines,"Support vector machines (SVMs) are a family of machine learning methods, originally introduced for the problem of classification and later generalized to various other situations. They are based on principles of statistical learning theory and convex optimization, and are currently used in various domains of application, including bioinformatics, text categorization, and computer vision. Copyright © 2009 John Wiley & Sons, Inc.",Erratum,pro98
pap1358,584fd593dc69720afad8b1b5663cd9aea4da9c45,con17,International Conference on Statistical and Scientific Database Management,Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences,"MOTIVATION
In 2001 and 2002, we published two papers (Bioinformatics, 17, 282-283, Bioinformatics, 18, 77-82) describing an ultrafast protein sequence clustering program called cd-hit. This program can efficiently cluster a huge protein database with millions of sequences. However, the applications of the underlying algorithm are not limited to only protein sequences clustering, here we present several new programs using the same algorithm including cd-hit-2d, cd-hit-est and cd-hit-est-2d. Cd-hit-2d compares two protein datasets and reports similar matches between them; cd-hit-est clusters a DNA/RNA sequence database and cd-hit-est-2d compares two nucleotide datasets. All these programs can handle huge datasets with millions of sequences and can be hundreds of times faster than methods based on the popular sequence comparison and database search tools, such as BLAST.",Erratum,pro17
pap1359,cb18c300bf209370736472b244e66911ac02a8f1,jou202,Journal of Biomedical Informatics,State of the nation in data integration for bioinformatics,,Letter,vol202
pap1360,6654afe7bdb669fa109891a23b24159315ad1677,con78,Neural Information Processing Systems,The SEED and the Rapid Annotation of microbial genomes using Subsystems Technology (RAST),"In 2004, the SEED (http://pubseed.theseed.org/) was created to provide consistent and accurate genome annotations across thousands of genomes and as a platform for discovering and developing de novo annotations. The SEED is a constantly updated integration of genomic data with a genome database, web front end, API and server scripts. It is used by many scientists for predicting gene functions and discovering new pathways. In addition to being a powerful database for bioinformatics research, the SEED also houses subsystems (collections of functionally related protein families) and their derived FIGfams (protein families), which represent the core of the RAST annotation engine (http://rast.nmpdr.org/). When a new genome is submitted to RAST, genes are called and their annotations are made by comparison to the FIGfam collection. If the genome is made public, it is then housed within the SEED and its proteins populate the FIGfam collection. This annotation cycle has proven to be a robust and scalable solution to the problem of annotating the exponentially increasing number of genomes. To date, >12 000 users worldwide have annotated >60 000 distinct genomes using RAST. Here we describe the interconnectedness of the SEED database and RAST, the RAST annotation pipeline and updates to both resources.",Erratum,pro78
pap1361,3ffc321d32b6210cb9e04a5ae907fe659feb4975,con25,IEEE International Parallel and Distributed Processing Symposium,Bioinformatics approaches for genomics and post genomics applications of next-generation sequencing,"Technical advances such as the development of molecular cloning, Sanger sequencing, PCR and oligonucleotide microarrays are key to our current capacity to sequence, annotate and study complete organismal genomes. Recent years have seen the development of a variety of so-called 'next-generation' sequencing platforms, with several others anticipated to become available shortly. The previously unimaginable scale and economy of these methods, coupled with their enthusiastic uptake by the scientific community and the potential for further improvements in accuracy and read length, suggest that these technologies are destined to make a huge and ongoing impact upon genomic and post-genomic biology. However, like the analysis of microarray data and the assembly and annotation of complete genome sequences from conventional sequencing data, the management and analysis of next-generation sequencing data requires (and indeed has already driven) the development of informatics tools able to assemble, map, and interpret huge quantities of relatively or extremely short nucleotide sequence data. Here we provide a broad overview of bioinformatics approaches that have been introduced for several genomics and functional genomics applications of next-generation sequencing.",Erratum,pro25
pap1362,3aca912f21d54b3931fa1fdfac0c199c557374a4,con60,European Conference on Software Process Improvement,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,Erratum,pro60
pap1363,4ca507758234dcf754a7adc458dd574b70a285d7,jou0,Nature Biotechnology,Bioinformatics prediction of HIV coreceptor usage,,Letter,vol0
pap1364,c0450f920c178d25e7707fa00432516fa6a96ce5,con57,International Workshop on Agent-Oriented Software Engineering,ClueGO: a Cytoscape plug-in to decipher functionally grouped gene ontology and pathway annotation networks,"Summary: We have developed ClueGO, an easy to use Cytoscape plug-in that strongly improves biological interpretation of large lists of genes. ClueGO integrates Gene Ontology (GO) terms as well as KEGG/BioCarta pathways and creates a functionally organized GO/pathway term network. It can analyze one or compare two lists of genes and comprehensively visualizes functionally grouped terms. A one-click update option allows ClueGO to automatically download the most recent GO/KEGG release at any time. ClueGO provides an intuitive representation of the analysis results and can be optionally used in conjunction with the GOlorize plug-in. Availability: http://www.ici.upmc.fr/cluego/cluegoDownload.shtml Contact: jerome.galon@crc.jussieu.fr Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro57
pap1365,a445a855650696b008cac2f8caa4e58249512ee2,con22,Grid Computing Environments,trimAl: a tool for automated alignment trimming in large-scale phylogenetic analyses,"Summary: Multiple sequence alignments are central to many areas of bioinformatics. It has been shown that the removal of poorly aligned regions from an alignment increases the quality of subsequent analyses. Such an alignment trimming phase is complicated in large-scale phylogenetic analyses that deal with thousands of alignments. Here, we present trimAl, a tool for automated alignment trimming, which is especially suited for large-scale phylogenetic analyses. trimAl can consider several parameters, alone or in multiple combinations, for selecting the most reliable positions in the alignment. These include the proportion of sequences with a gap, the level of amino acid similarity and, if several alignments for the same set of sequences are provided, the level of consistency across different alignments. Moreover, trimAl can automatically select the parameters to be used in each specific alignment so that the signal-to-noise ratio is optimized. Availability: trimAl has been written in C++, it is portable to all platforms. trimAl is freely available for download (http://trimal.cgenomics.org) and can be used online through the Phylemon web server (http://phylemon2.bioinfo.cipf.es/). Supplementary Material is available at http://trimal.cgenomics.org/publications. Contact: tgabaldon@crg.es",Erratum,pro22
pap1366,b450dedf0079b80c490e57fd9a20464351e5cbf3,jou245,IEEE Transactions on Visualization and Computer Graphics,An insight-based methodology for evaluating bioinformatics visualizations,"High-throughput experiments, such as gene expression microarrays in the life sciences, result in very large data sets. In response, a wide variety of visualization tools have been created to facilitate data analysis. A primary purpose of these tools is to provide biologically relevant insight into the data. Typically, visualizations are evaluated in controlled studies that measure user performance on predetermined tasks or using heuristics and expert reviews. To evaluate and rank bioinformatics visualizations based on real-world data analysis scenarios, we developed a more relevant evaluation method that focuses on data insight. This paper presents several characteristics of insight that enabled us to recognize and quantify it in open-ended user tests. Using these characteristics, we evaluated five microarray visualization tools on the amount and types of insight they provide and the time it takes to acquire it. The results of the study guide biologists in selecting a visualization tool based on the type of their microarray data, visualization designers on the key role of user interaction techniques, and evaluators on a new approach for evaluating the effectiveness of visualizations for providing insight. Though we used the method to analyze bioinformatics visualizations, it can be applied to other domains.",Conference paper,vol245
pap1367,0037fcb6b3df7038e7cffb64c48aebeb2f4b51da,con75,Intelligent Systems in Molecular Biology,TAMBIS: Transparent Access to Multiple Bioinformatics Information Sources,"The TAMBIS project aims to provide transparent access to disparate biological databases and analysis tools, enabling users to utilize a wide range of resources with the minimum of effort. A prototype system has been developed that includes a knowledge base of biological terminology (the biological Concept Model), a model of the underlying data sources (the Source Model) and a 'knowledge-driven' user interface. Biological concepts are captured in the knowledge base using a description logic called GRAIL. The Concept Model provides the user with the concepts necessary to construct a wide range of multiple-source queries, and the user interface provides a flexible means of constructing and manipulating those queries. The Source Model provides a description of the underlying sources and mappings between terms used in the sources and terms in the biological Concept Model. The Concept Model and Source Model provide a level of indirection that shields the user from source details, providing a high level of source transparency. Source independent, declarative queries formed from terms in the Concept Model are transformed into a set of source dependent, executable procedures. Query formulation, translation and execution is demonstrated using a working example.",Article,pro75
pap1368,8f4f844b9f7534169138dbc8239a3158d495bc9a,jou189,Protein Science,Toward understanding the origin and evolution of cellular organisms,"In this era of high‐throughput biology, bioinformatics has become a major discipline for making sense out of large‐scale datasets. Bioinformatics is usually considered as a practical field developing databases and software tools for supporting other fields, rather than a fundamental scientific discipline for uncovering principles of biology. The KEGG resource that we have been developing is a reference knowledge base for biological interpretation of genome sequences and other high‐throughput data. It is now one of the most utilized biological databases because of its practical values. For me personally, KEGG is a step toward understanding the origin and evolution of cellular organisms.",Conference paper,vol189
pap1369,bf6806dc1dfe057e907abfe786ee5037c4f7ea47,jou88,bioRxiv,Nextstrain: real-time tracking of pathogen evolution,"Summary Understanding the spread and evolution of pathogens is important for effective public health measures and surveillance. Nextstrain consists of a database of viral genomes, a bioinformatics pipeline for phylodynamics analysis, and an interactive visualisation platform. Together these present a real-time view into the evolution and spread of a range of viral pathogens of high public health importance. The visualization integrates sequence data with other data types such as geographic information, serology, or host species. Nextstrain compiles our current understanding into a single accessible location, publicly available for use by health professionals, epidemiologists, virologists and the public alike. Availability and implementation All code (predominantly JavaScript and Python) is freely available from github.com/nextstrain and the web-application is available at nextstrain.org.",Letter,vol88
pap1370,55f3edbfa20200a1566866147cce70c30f17eb00,con110,Very Large Data Bases Conference,PatMaN: rapid alignment of short sequences to large databases,"Summary: We present a tool suited for searching for many short nucleotide sequences in large databases, allowing for a predefined number of gaps and mismatches. The commandline-driven program implements a non-deterministic automata matching algorithm on a keyword tree of the search strings. Both queries with and without ambiguity codes can be searched. Search time is short for perfect matches, and retrieval time rises exponentially with the number of edits allowed. Availability: The C++ source code for PatMaN is distributed under the GNU General Public License and has been tested on the GNU/Linux operating system. It is available from http://bioinf.eva.mpg.de/patman. Contact: pruefer@eva.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro110
pap1371,f73944469b1ac676895ab30fbb85a8a96f234909,con38,International Symposium on Empirical Software Engineering and Measurement,BMC Bioinformatics,"Background: Simulation of DNA-microarray data serves at least three purposes: (i) optimizing the design of an intended DNA microarray experiment, (ii) comparing existing pre-processing and processing methods for best analysis of a given DNA microarray experiment, (iii) educating students, lab-workers and other researchers by making them aware of the many factors influencing DNA microarray experiments. Results: Our model has multiple layers of factors influencing the experiment. The relative influence of such factors can differ significantly between labs, experiments within labs, etc. Therefore, we have added a module to roughly estimate their parameters from a given data set. This guarantees that our simulated data mimics real data as closely as possible. Conclusion: We introduce a model for the simulation of dual-dye cDNA-microarray data closely resembling real data and coin the model and its software implementation "" SIMAGE "" which stands for simulation of microarray gene expression data. The software is freely accessible at: http://",Erratum,pro38
pap1372,1f4d6c1d0a7191c677049444e05dc282d46a34e1,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Bioinformatics for Whole-Genome Shotgun Sequencing of Microbial Communities,"The application of whole-genome shotgun sequencing to microbial communities represents a major development in metagenomics, the study of uncultured microbes via the tools of modern genomic analysis. In the past year, whole-genome shotgun sequencing projects of prokaryotic communities from an acid mine biofilm, the Sargasso Sea, Minnesota farm soil, three deep-sea whale falls, and deep-sea sediments have been reported, adding to previously published work on viral communities from marine and fecal samples. The interpretation of this new kind of data poses a wide variety of exciting and difficult bioinformatics problems. The aim of this review is to introduce the bioinformatics community to this emerging field by surveying existing techniques and promising new approaches for several of the most interesting of these computational problems.",Erratum,pro39
pap1373,770833aec0e6d8f6eaa642c948bb04b6e66fc036,jou246,Methods of Information in Medicine,What is Bioinformatics? A Proposed Definition and Overview of the Field,"Summary Background: The recent flood of data from genome sequences and functional genomics has given rise to new field, bioinformatics, which combines elements of biology and computer science. Objectives: Here we propose a definition for this new field and review some of the research that is being pursued, particularly in relation to transcriptional regulatory systems. Methods: Our definition is as follows: Bioinformatics is conceptualizing biology in terms of macromolecules (in the sense of physical-chemistry) and then applying “informatics” techniques (derived from disciplines such as applied maths, computer science, and statistics) to understand and organize the information associated with these molecules, on a large-scale. Results and Conclusions: Analyses in bioinformatics predominantly focus on three types of large datasets available in molecular biology: macromolecular structures, genome sequences, and the results of functional genomics experiments (eg expression data). Additional information includes the text of scientific papers and “relationship data” from metabolic pathways, taxonomy trees, and protein-protein interaction networks. Bioinformatics employs a wide range of computational techniques including sequence and structural alignment, database design and data mining, macromolecular geometry, phylogenetic tree construction, prediction of protein structure and function, gene finding, and expression data clustering. The emphasis is on approaches integrating a variety of computational methods and heterogeneous data sources. Finally, bioinformatics is a practical discipline. We survey some representative applications, such as finding homologues, designing drugs, and performing large-scale censuses. Additional information pertinent to the review is available over the web at http://bioinfo.mbb.yale.edu/what-is-it.",Conference paper,vol246
pap1374,ffce192970efca596ead43838605e06823af879f,con62,Australian Software Engineering Conference,"Briefings in Bioinformatics Advance Access published July 30, 2006 BRIEFINGS IN BIOINFORMATICS. page1of13",,Erratum,pro62
pap1375,8df59e797d3671b0dc75f2b6c1ca9af9533166cc,con34,International Conference on Agile Software Development,Multiple sequence alignment with the Clustal series of programs,"The Clustal series of programs are widely used in molecular biology for the multiple alignment of both nucleic acid and protein sequences and for preparing phylogenetic trees. The popularity of the programs depends on a number of factors, including not only the accuracy of the results, but also the robustness, portability and user-friendliness of the programs. New features include NEXUS and FASTA format output, printing range numbers and faster tree calculation. Although, Clustal was originally developed to run on a local computer, numerous Web servers have been set up, notably at the EBI (European Bioinformatics Institute) (http://www.ebi.ac.uk/clustalw/).",Erratum,pro34
pap1376,baedf3036e56edddf400f04e5a68b167014991ef,con76,IEEE International Conference on Tools with Artificial Intelligence,Kernel Methods for Pattern Analysis,"Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so.",Conference paper,pro76
pap1377,f09c52301704d1c89380f90b50b682dcc3f9d80f,con80,International Conference on Advanced Computer Science Applications and Technologies,ConsensusClusterPlus: a class discovery tool with confidence assessments and item tracking,"Summary: Unsupervised class discovery is a highly useful technique in cancer research, where intrinsic groups sharing biological characteristics may exist but are unknown. The consensus clustering (CC) method provides quantitative and visual stability evidence for estimating the number of unsupervised classes in a dataset. ConsensusClusterPlus implements the CC method in R and extends it with new functionality and visualizations including item tracking, item-consensus and cluster-consensus plots. These new features provide users with detailed information that enable more specific decisions in unsupervised class discovery. Availability: ConsensusClusterPlus is open source software, written in R, under GPL-2, and available through the Bioconductor project (http://www.bioconductor.org/). Contact: mwilkers@med.unc.edu Supplementary Information: Supplementary data are available at Bioinformatics online.",Erratum,pro80
pap1378,272a9f5981faba2d88332c4eaea009d5bef4c4e0,con107,Chinese Conference on Biometric Recognition,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btm066 Sequence analysis NucPred—Predicting nuclear localization of proteins,,Erratum,pro107
pap1379,35b7d24fbaec4f2754d66a5450f2349eedb3dcda,con63,International Colloquium on Theoretical Aspects of Computing,Metabolomics technology and bioinformatics,"Metabolomics is the global analysis of all or a large number of cellular metabolites. Like other functional genomics research, metabolomics generates large amounts of data. Handling, processing and analysis of this data is a clear challenge and requires specialized mathematical, statistical and bioinformatics tools. Metabolomics needs for bioinformatics span through data and information management, raw analytical data processing, metabolomics standards and ontology, statistical analysis and data mining, data integration and mathematical modelling of metabolic networks within a framework of systems biology. The major approaches in metabolomics, along with the modern analytical tools used for data generation, are reviewed in the context of these specific bioinformatics needs.",Erratum,pro63
pap1380,3223708ded2c4cc670b3c79d1b336541338477bb,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Recent developments in life sciences research: Role of bioinformatics,"Life sciences research and development has opened up new challenges and opportunities for bioinformatics. The contribution of bioinformatics advances made possible the mapping of the entire human genome and genomes of many other organisms in just over a decade. These discoveries, along with current efforts to determine gene and protein functions, have improved our ability to understand the root causes of human, animal and plant diseases and find new cures. Furthermore, many future Bioinformatic innovations will likely be spurred by the data and analysis demands of the life sciences. This review briefly describes the role of bioinformatics in biotechnology, drug discovery, biomarker discovery, biological databases, bioinformatic tools, bioinformatic tasks and its application in life sciences research.",Erratum,pro39
pap1381,15ba5c614e6c531c475c06c0f4a2630abdda4b11,jou247,Molecular Ecology Notes,bold: The Barcode of Life Data System (http://www.barcodinglife.org),"The Barcode of Life Data System (bold) is an informatics workbench aiding the acquisition, storage, analysis and publication of DNA barcode records. By assembling molecular, morphological and distributional data, it bridges a traditional bioinformatics chasm. bold is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances. This paper provides a brief introduction to the key elements of bold, discusses their functional capabilities, and concludes by examining computational resources and future prospects.",Letter,vol247
pap1382,bb45999d450855220728a55fdc606c8d6325eae3,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,BIOINFORMATICS APPLICATIONS NOTE,"Summary: Assemble is an intuitive graphical interface to analyze, manipulate and build complex 3D RNA architectures. It provides several advanced and unique features within the framework of a semi-automated modeling process that can be performed by homology and ab initio with or without electron density maps. Those include the interactive editing of a secondary structure and a searchable, embedded library of annotated tertiary structures. Assemble helps users with performing recurrent and otherwise tedious tasks in structural RNA research. Availability and Implementation: Assemble is released under an open-source license (MIT license) and is freely available at http://bioinformatics.org/assemble. It is implemented in the Java language and runs on MacOSX, Linux and Windows operating systems.",Erratum,pro21
pap1383,439ede62248e5f6202982afead02b33d3feffae7,jou106,Nucleic Acids Research,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",Letter,vol106
pap1384,1c405ce3655515312cba508f6e8bb8c904d23d25,con105,British Machine Vision Conference,ExPASy: the proteomics server for in-depth protein knowledge and analysis,"The ExPASy (the Expert Protein Analysis System) World Wide Web server (http://www.expasy.org), is provided as a service to the life science community by a multidisciplinary team at the Swiss Institute of Bioinformatics (SIB). It provides access to a variety of databases and analytical tools dedicated to proteins and proteomics. ExPASy databases include SWISS-PROT and TrEMBL, SWISS-2DPAGE, PROSITE, ENZYME and the SWISS-MODEL repository. Analysis tools are available for specific tasks relevant to proteomics, similarity searches, pattern and profile searches, post-translational modification prediction, topology prediction, primary, secondary and tertiary structure analysis and sequence alignment. These databases and tools are tightly interlinked: a special emphasis is placed on integration of database entries with related resources developed at the SIB and elsewhere, and the proteomics tools have been designed to read the annotations in SWISS-PROT in order to enhance their predictions. ExPASy started to operate in 1993, as the first WWW server in the field of life sciences. In addition to the main site in Switzerland, seven mirror sites in different continents currently serve the user community.",Erratum,pro105
pap1385,c369c9a40a36b013be3ef9c19a068f2d6578e4c3,con34,International Conference on Agile Software Development,Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible in ARB,"Title: Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible with ARB Authors: DeSantis, T.Z. 1 Hugenholtz, P. 2 Larsen, N. 3 Rojas, M. 4 Brodie, E.L. 1 Keller, K. 5 Huber, T. 6 Dalevi, D. 7 Hu, P. 1 Andersen, G.L. 1 Center for Environmental Biotechnology Lawrence Berkeley National Laboratory 1 Cyclotron Road, Mail Stop 70A-3317 Berkeley, CA 94720 USA Microbial Ecology Program DOE Joint Genome Institute 2800 Mitchell Drive Bldg 400-404 Walnut Creek, CA 94598 USA Danish Genome Institute Gustav Wieds vej 10 C DK-8000 Aarhus C Denmark Department of Bioinformatics Baylor University P.O. Box 97356, 1311 S. 5th St. Waco, TX 76798-7356 USA Department of Bioengineering University of California Berkeley, CA 94720 USA Departments of Biochemistry and Mathematics The University of Queensland Brisbane Qld 4072 Australia Department of Computer Science Chalmers University of Technology",Erratum,pro34
pap1386,08b43d84e6747e370ef307e2ada50675b414514a,jou248,IEEE Transactions on Neural Networks,Survey of clustering algorithms,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",Article,vol248
pap1387,ff8b25a72ca611a4138b48282277f6de78a02e05,con7,International Symposium on Intelligent Data Analysis,WIWS: a protein structure bioinformatics Web service collection,"The WHAT IF molecular-modelling and drug design program is widely distributed in the world of protein structure bioinformatics. Although originally designed as an interactive application, its highly modular design and inbuilt control language have recently enabled its deployment as a collection of programmatically accessible web services. We report here a collection of WHAT IF-based protein structure bioinformatics web services: these relate to structure quality, the use of symmetry in crystal structures, structure correction and optimization, adding hydrogens and optimizing hydrogen bonds and a series of geometric calculations. The freely accessible web services are based on the industry standard WS-I profile and the EMBRACE technical guidelines, and are available via both REST and SOAP paradigms. The web services run on a dedicated computational cluster; their function and availability is monitored daily.",Erratum,pro7
pap1388,5f04b1c953e9c2842039f9656b7e0c3364af4229,con53,Workshop on Web 2.0 for Software Engineering,Gene Expression Atlas at the European Bioinformatics Institute,"The Gene Expression Atlas (http://www.ebi.ac.uk/gxa) is an added-value database providing information about gene expression in different cell types, organism parts, developmental stages, disease states, sample treatments and other biological/experimental conditions. The content of this database derives from curation, re-annotation and statistical analysis of selected data from the ArrayExpress Archive of Functional Genomics Data. A simple interface allows the user to query for differential gene expression either (i) by gene names or attributes such as Gene Ontology terms, or (ii) by biological conditions, e.g. diseases, organism parts or cell types. The gene queries return the conditions where expression has been reported, while condition queries return which genes are reported to be expressed in these conditions. A combination of both query types is possible. The query results are ranked using various statistical measures and by how many independent studies in the database show the particular gene-condition association. Currently, the database contains information about more than 200 000 genes from nine species and almost 4500 biological conditions studied in over 30 000 assays from over 1000 independent studies.",Erratum,pro53
pap1389,984bb8680c30bbe8d2979486259efb373d108e62,con41,Asia-Pacific Software Engineering Conference,Phylogenetic diversity (PD) and biodiversity conservation: some bioinformatics challenges,"Biodiversity conservation addresses information challenges through estimations encapsulated in measures of diversity. A quantitative measure of phylogenetic diversity, “PD”, has been defined as the minimum total length of all the phylogenetic branches required to span a given set of taxa on the phylogenetic tree (Faith 1992a). While a recent paper incorrectly characterizes PD as not including information about deeper phylogenetic branches, PD applications over the past decade document the proper incorporation of shared deep branches when assessing the total PD of a set of taxa. Current PD applications to macroinvertebrate taxa in streams of New South Wales, Australia illustrate the practical importance of this definition. Phylogenetic lineages, often corresponding to new, “cryptic”, taxa, are restricted to a small number of stream localities. A recent case of human impact causing loss of taxa in one locality implies a higher PD value for another locality, because it now uniquely represents a deeper branch. This molecular-based phylogenetic pattern supports the use of DNA barcoding programs for biodiversity conservation planning. Here, PD assessments side-step the contentious use of barcoding-based “species” designations. Bioinformatics challenges include combining different phylogenetic evidence, optimization problems for conservation planning, and effective integration of phylogenetic information with environmental and socioeconomic data.",Erratum,pro41
pap1390,9235d511dea04aa563a577ab236506b8eb8242ff,con77,International Conference on Artificial Neural Networks,A Survey on Deep Transfer Learning,,Conference paper,pro77
pap1391,b6a4d26324daae0b9aa24e137455ead4c48896cd,con9,Big Data,ProtTest 3: fast selection of best-fit models of protein evolution,"UNLABELLED
We have implemented a high-performance computing (HPC) version of ProtTest that can be executed in parallel in multicore desktops and clusters. This version, called ProtTest 3, includes new features and extended capabilities.


AVAILABILITY
ProtTest 3 source code and binaries are freely available under GNU license for download from http://darwin.uvigo.es/software/prottest3, linked to a Mercurial repository at Bitbucket (https://bitbucket.org/).


CONTACT
dposada@uvigo.es


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro9
pap1392,780725e727d6cc7a8c32dd613540960457bf188a,jou81,BMC Bioinformatics,VennDiagram: a package for the generation of highly-customizable Venn and Euler diagrams in R,,Conference paper,vol81
pap1393,9e26c8be21f341102016fa5a67b06db362bf272a,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Statistical Applications in Genetics and Molecular Biology A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics,"Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are illsuited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity. Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.",Erratum,pro39
pap1394,58b16fce8ad015b75c92a934a8332fc50bd45465,con53,Workshop on Web 2.0 for Software Engineering,Search and sequence analysis tools services from EMBL-EBI in 2022,"Abstract The EMBL-EBI search and sequence analysis tools frameworks provide integrated access to EMBL-EBI’s data resources and core bioinformatics analytical tools. EBI Search (https://www.ebi.ac.uk/ebisearch) provides a full-text search engine across nearly 5 billion entries, while the Job Dispatcher tools framework (https://www.ebi.ac.uk/services) enables the scientific community to perform a diverse range of sequence analysis using popular bioinformatics applications. Both allow users to interact through user-friendly web applications, as well as via RESTful and SOAP-based APIs. Here, we describe recent improvements to these services and updates made to accommodate the increasing data requirements during the COVID-19 pandemic.",Erratum,pro53
pap1395,7dc089d3001586244d8d3700ad1ae25bc24deaa0,con108,International Conference on Information Integration and Web-based Applications & Services,AliView: a fast and lightweight alignment viewer and editor for large datasets,"Summary: AliView is an alignment viewer and editor designed to meet the requirements of next-generation sequencing era phylogenetic datasets. AliView handles alignments of unlimited size in the formats most commonly used, i.e. FASTA, Phylip, Nexus, Clustal and MSF. The intuitive graphical interface makes it easy to inspect, sort, delete, merge and realign sequences as part of the manual filtering process of large datasets. AliView also works as an easy-to-use alignment editor for small as well as large datasets. Availability and implementation: AliView is released as open-source software under the GNU General Public License, version 3.0 (GPLv3), and is available at GitHub (www.github.com/AliView). The program is cross-platform and extensively tested on Linux, Mac OS X and Windows systems. Downloads and help are available at http://ormbunkar.se/aliview Contact: anders.larsson@ebc.uu.se Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro108
pap1396,4ee58e4ef967461109dd35a4c3c0b2a35394c35b,jou7,Nature Methods,ChromHMM: automating chromatin-state discovery and characterization,,Article,vol7
pap1397,f64dd54444a724574deb7710888091350eebb2b9,con14,Hawaii International Conference on System Sciences,"BBMap: A Fast, Accurate, Splice-Aware Aligner","Alignment of reads is one of the primary computational tasks in bioinformatics. Of paramount importance to resequencing, alignment is also crucial to other areas - quality control, scaffolding, string-graph assembly, homology detection, assembly evaluation, error-correction, expression quantification, and even as a tool to evaluate other tools. An optimal aligner would greatly improve virtually any sequencing process, but optimal alignment is prohibitively expensive for gigabases of data. Here, we will present BBMap [1], a fast splice-aware aligner for short and long reads. We will demonstrate that BBMap has superior speed, sensitivity, and specificity to alternative high-throughput aligners bowtie2 [2], bwa [3], smalt, [4] GSNAP [5], and BLASR [6].",Erratum,pro14
pap1398,5d0e28c5a5cfe9820802ab6ec3c49f6927555af9,con56,International Conference on Software Engineering and Knowledge Engineering,GSDS 2.0: an upgraded gene feature visualization server,"Summary: Visualizing genes’ structure and annotated features helps biologists to investigate their function and evolution intuitively. The Gene Structure Display Server (GSDS) has been widely used by more than 60 000 users since its first publication in 2007. Here, we reported the upgraded GSDS 2.0 with a newly designed interface, supports for more types of annotation features and formats, as well as an integrated visual editor for editing the generated figure. Moreover, a user-specified phylogenetic tree can be added to facilitate further evolutionary analysis. The full source code is also available for downloading. Availability and implementation: Web server and source code are freely available at http://gsds.cbi.pku.edu.cn. Contact: gaog@mail.cbi.pku.edu.cn or gsds@mail.cbi.pku.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro56
pap1399,ec3c1f392bacdd47b47221082ecbd3aaf1bb33d3,jou249,Frontiers in Immunology,Toll-Like Receptor Signaling Pathways,"Toll-like receptors (TLRs) play crucial roles in the innate immune system by recognizing pathogen-associated molecular patterns derived from various microbes. TLRs signal through the recruitment of specific adaptor molecules, leading to activation of the transcription factors NF-κB and IRFs, which dictate the outcome of innate immune responses. During the past decade, the precise mechanisms underlying TLR signaling have been clarified by various approaches involving genetic, biochemical, structural, cell biological, and bioinformatics studies. TLR signaling appears to be divergent and to play important roles in many aspects of the innate immune responses to given pathogens. In this review, we describe recent progress in our understanding of TLR signaling regulation and its contributions to host defense.",Conference paper,vol249
pap1400,7d71be3ab1d6fb26c6bf72006ac7059dabde7863,jou250,Annual Review of Plant Biology,Lignin biosynthesis.,"The lignin biosynthetic pathway has been studied for more than a century but has undergone major revisions over the past decade. Significant progress has been made in cloning new genes by genetic and combined bioinformatics and biochemistry approaches. In vitro enzymatic assays and detailed analyses of mutants and transgenic plants altered in the expression of lignin biosynthesis genes have provided a solid basis for redrawing the monolignol biosynthetic pathway, and structural analyses have shown that plant cell walls can tolerate large variations in lignin content and structure. In some cases, the potential value for agriculture of transgenic plants with modified lignin structure has been demonstrated. This review presents a current picture of monolignol biosynthesis, polymerization, and lignin structure.",Article,vol250
pap1401,c5a3be735cfcf83b6337bbb62c43433f2d9f4edc,jou251,Journal of Antimicrobial Chemotherapy,ResFinder 4.0 for predictions of phenotypes from genotypes,"Abstract Objectives WGS-based antimicrobial susceptibility testing (AST) is as reliable as phenotypic AST for several antimicrobial/bacterial species combinations. However, routine use of WGS-based AST is hindered by the need for bioinformatics skills and knowledge of antimicrobial resistance (AMR) determinants to operate the vast majority of tools developed to date. By leveraging on ResFinder and PointFinder, two freely accessible tools that can also assist users without bioinformatics skills, we aimed at increasing their speed and providing an easily interpretable antibiogram as output. Methods The ResFinder code was re-written to process raw reads and use Kmer-based alignment. The existing ResFinder and PointFinder databases were revised and expanded. Additional databases were developed including a genotype-to-phenotype key associating each AMR determinant with a phenotype at the antimicrobial compound level, and species-specific panels for in silico antibiograms. ResFinder 4.0 was validated using Escherichia coli (n = 584), Salmonella spp. (n = 1081), Campylobacter jejuni (n = 239), Enterococcus faecium (n = 106), Enterococcus faecalis (n = 50) and Staphylococcus aureus (n = 163) exhibiting different AST profiles, and from different human and animal sources and geographical origins. Results Genotype–phenotype concordance was ≥95% for 46/51 and 25/32 of the antimicrobial/species combinations evaluated for Gram-negative and Gram-positive bacteria, respectively. When genotype–phenotype concordance was <95%, discrepancies were mainly linked to criteria for interpretation of phenotypic tests and suboptimal sequence quality, and not to ResFinder 4.0 performance. Conclusions WGS-based AST using ResFinder 4.0 provides in silico antibiograms as reliable as those obtained by phenotypic AST at least for the bacterial species/antimicrobial agents of major public health relevance considered.",Letter,vol251
pap1402,e3831b9cf7292a29a87ba4574a61b78d0e555994,con14,Hawaii International Conference on System Sciences,Penalized feature selection and classification in bioinformatics,"In bioinformatics studies, supervised classification with high-dimensional input variables is frequently encountered. Examples routinely arise in genomic, epigenetic and proteomic studies. Feature selection can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insights into the underlying causal relationships. In this article, we provide a review of several recently developed penalized feature selection and classification techniques--which belong to the family of embedded feature selection methods--for bioinformatics studies with high-dimensional input. Classification objective functions, penalty functions and computational algorithms are discussed. Our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high-dimensional bioinformatics data.",Erratum,pro14
pap1403,8247e3701d47a2efb88ff29b373445e7f338ba38,con102,Annual Haifa Experimental Systems Conference,BMC Bioinformatics Methodology article,,Erratum,pro102
pap1404,fedfcdd44bb13248ca99cb2fa4b82325d162b9c0,jou252,Human Genetics,"The Human Gene Mutation Database: building a comprehensive mutation repository for clinical and molecular genetics, diagnostic testing and personalized genomic medicine",,Article,vol252
pap1405,bdf18697608d67b7a05193732dcf4ac22cb959e3,jou158,Lecture Notes in Computer Science,Algorithms in Bioinformatics,,Letter,vol158
pap1406,e0fa4cccc48d3deda3414c7f62eda73b10722c4d,con97,ACM SIGMOD Conference,Machine learning: an indispensable tool in bioinformatics.,,Erratum,pro97
pap1407,125d0980a4ef8e0be4be3b413d0fb92043e7b26c,con105,British Machine Vision Conference,Over-optimism in bioinformatics: an illustration,"MOTIVATION
In statistical bioinformatics research, different optimization mechanisms potentially lead to 'over-optimism' in published papers. So far, however, a systematic critical study concerning the various sources underlying this over-optimism is lacking.


RESULTS
We present an empirical study on over-optimism using high-dimensional classification as example. Specifically, we consider a 'promising' new classification algorithm, namely linear discriminant analysis incorporating prior knowledge on gene functional groups through an appropriate shrinkage of the within-group covariance matrix. While this approach yields poor results in terms of error rate, we quantitatively demonstrate that it can artificially seem superior to existing approaches if we 'fish for significance'. The investigated sources of over-optimism include the optimization of datasets, of settings, of competing methods and, most importantly, of the method's characteristics. We conclude that, if the improvement of a quantitative criterion such as the error rate is the main contribution of a paper, the superiority of new algorithms should always be demonstrated on independent validation data.


AVAILABILITY
The R codes and relevant data can be downloaded from http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/overoptimism/, such that the study is completely reproducible.",Erratum,pro105
pap1408,b2bd25d9dc805cdb1fd7f7b1349751d433da80c2,con88,European Conference on Computer Vision,Probabilistic Modeling in Bioinformatics and Medical Informatics,,Erratum,pro88
pap1409,a093ba08980422d9d2d1f7d49aac350b8b7e1676,con30,PS,ShinyGO: a graphical gene-set enrichment tool for animals and plants,"MOTIVATION
Gene lists are routinely produced from various genome-wide studies. Enrichment analysis can link these gene lists with underlying molecular pathways and functional categories such as gene ontology (GO) and other databases.


RESULTS
To complement existing tools, we developed ShinyGO based on a large annotation database derived from Ensembl and STRING-db for 59 plant, 256 animal, 115 archaeal, and 1678 bacterial species. ShinyGO's novel features include graphical visualization of enrichment results and gene characteristics, and application program interface (API) access to KEGG and STRING for the retrieval of pathway diagrams and protein-protein interaction networks. ShinyGO is an intuitive, graphical web application that can help researchers gain actionable insights from gene lists.


AVAILABILITY
http://ge-lab.org/go/.


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro30
pap1410,0c8dacf66498f7e73d73ceb508dd1d2af81b680e,con44,International Conference Knowledge Engineering and Knowledge Management,Pathview: an R/Bioconductor package for pathway-based data integration and visualization,"Summary: Pathview is a novel tool set for pathway-based data integration and visualization. It maps and renders user data on relevant pathway graphs. Users only need to supply their data and specify the target pathway. Pathview automatically downloads the pathway graph data, parses the data file, maps and integrates user data onto the pathway and renders pathway graphs with the mapped data. Although built as a stand-alone program, Pathview may seamlessly integrate with pathway and functional analysis tools for large-scale and fully automated analysis pipelines. Availability: The package is freely available under the GPLv3 license through Bioconductor and R-Forge. It is available at http://bioconductor.org/packages/release/bioc/html/pathview.html and at http://Pathview.r-forge.r-project.org/. Contact: luo_weijun@yahoo.com Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro44
pap1411,0ba69357a59a556759fc09f09ce268b6b7b4f5c2,con28,International Conference Geographic Information Science,Broader incorporation of bioinformatics in education: opportunities and challenges,"The major opportunities for broader incorporation of bioinformatics in education can be placed into three general categories: general applicability of bioinformatics in life science and related curricula; inherent fit of bioinformatics for promoting student learning in most biology programs; and the general experience and associated comfort students have with computers and technology. Conversely, the major challenges for broader incorporation of bioinformatics in education can be placed into three general categories: required infrastructure and logistics; instructor knowledge of bioinformatics and continuing education; and the breadth of bioinformatics, and the diversity of students and educational objectives. Broader incorporation of bioinformatics at all education levels requires overcoming the challenges to using transformative computer-requiring learning activities, assisting faculty in collecting assessment data on mastery of student learning outcomes, as well as creating more faculty development opportunities that span diverse skill levels, with an emphasis placed on providing resource materials that are kept up-to-date as the field and tools change.",Erratum,pro28
pap1412,93315068530a331e50edba8618234c2da4e04761,con68,Experimental Software Engineering Network,Introduction to bioinformatics,"Bioinformatics is a multifaceted discipline combining many scientific fields including computational biology, statistics, mathematics, molecular biology, and genetics. Bioinformatics enables biomedical investigators to exploit existing and emerging computational technologies to seamlessly store, mine, retrieve, and analyze data from genomics and proteomics technologies. This is achieved by creating unified data models, standardizing data interfaces, developing structured vocabularies, generating new data visualization methods, and capturing detailed metadata that describes various aspects of the experimental design and analysis methods. Already there are a number of related undertakings that are dividing the field into more specialized groups. Clinical Bioinformatics and Biomedical Informatics are emerging as transitional fields to promote the utilization of genomics and proteomics data combined with medical history and demographic data towards personalized medicine, molecular diagnostics, pharmacogenomics and predicting outcomes of therapeutic interventions. The field of bioinformatics will continue to evolve through the incorporation of diverse technologies and methodologies that draw experts from disparate fields to create the latest computational and informational tools specifically design for the biomedical research enterprise.",Erratum,pro68
pap1413,1baee4ae5e5eaf75e322b53afa3cbdea89dcc2d0,con78,Neural Information Processing Systems,"Efficient and Robust Feature Selection via Joint ℓ2, 1-Norms Minimization","Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint l2,1-norm minimization on both loss function and regularization. The l2,1-norm based loss function is robust to outliers in data points and the l2,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method.",Letter,pro78
pap1414,b4445a19a9d7e404cf512833bc185811c88b8538,con8,Frontiers in Education Conference,phangorn: phylogenetic analysis in R,"Summary: phangorn is a package for phylogenetic reconstruction and analysis in the R language. Previously it was only possible to estimate phylogenetic trees with distance methods in R. phangorn, now offers the possibility of reconstructing phylogenies with distance based methods, maximum parsimony or maximum likelihood (ML) and performing Hadamard conjugation. Extending the general ML framework, this package provides the possibility of estimating mixture and partition models. Furthermore, phangorn offers several functions for comparing trees, phylogenetic models or splits, simulating character data and performing congruence analyses. Availability: phangorn can be obtained through the CRAN homepage http://cran.r-project.org/web/packages/phangorn/index.html. phangorn is licensed under GPL 2. Contact: klaus.kschliep@snv.jussieu.fr Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro8
pap1415,4ccd70db97b3670920b390bd28853307f296f5e0,jou81,BMC Bioinformatics,BIGSdb: Scalable analysis of bacterial genome variation at the population level,,Article,vol81
pap1416,f6ad89da462694bc17fb9e32e8b263e9ed7231ca,jou63,PLoS Biology,Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum,"Community Page Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum Jayna L. Ditty 1 , Christopher A. Kvaal 2 , Brad Goodner 3 , Sharyn K. Freyermuth 4 , Cheryl Bailey 5 , Robert A. Britton 6 , Stuart G. Gordon 7 , Sabine Heinhorst 8 , Kelynne Reed 9 , Zhaohui Xu 10 , Erin R. Sanders-Lorenz 11 , Seth Axen 12 , Edwin Kim 12 , Mitrick Johns 13 , Kathleen Scott 14 , Cheryl A. Kerfeld 12,15 * 1 Department of Biology, University of St. Thomas, St. Paul, Minnesota, United States of America, 2 Department of Biological Sciences, St. Cloud State University, St. Cloud, Minnesota, United States of America, 3 Department of Biology, Hiram College, Hiram, Ohio, United States of America, 4 Biochemistry Department, University of Missouri- Columbia, Columbia, Missouri, United States of America, 5 Department of Biochemistry, University of Nebraska-Lincoln, Lincoln, Nebraska, United States of America, 6 Department of Microbiology and Molecular Genetics, Michigan State University, East Lansing, Michigan, United States of America, 7 Department of Biology, Presbyterian College, Clinton, South Carolina, United States of America, 8 Department of Chemistry and Biochemistry, The University of Southern Mississippi, Hattiesburg, Mississippi, United States of America, 9 Biology Department, Austin College, Sherman, Texas, United States of America, 10 Department of Biological Sciences, Bowling Green State University, Bowling Green, Ohio, United States of America, 11 Department of Microbiology, Immunology and Molecular Genetics, University of California – Los Angeles, Los Angeles, California, United States of America, 12 Department of Energy-Joint Genome Institute, Walnut Creek, California, United States of America, 13 Department of Biological Sciences, Northern Illinois University, DeKalb, Illinois, United States of America, 14 Department of Integrative Biology, University of South Florida, Tampa, Florida, United States of America, 15 Department of Plant and Microbial Biology, University of California Berkley, Berkeley, California, United States of America Introduction Undergraduate life sciences education needs an overhaul, as clearly described in the National Research Council of the National Academies’ publication BIO 2010: Transforming Undergraduate Education for Future Research Biologists. Among BIO 2010’s top recommendations is the need to involve students in working with real data and tools that reflect the nature of life sciences research in the 21st century [1]. Education research studies support the importance of utilizing primary literature, designing and implementing experiments, and analyzing results in the context of a bona fide scientific question [1–12] in cultivating the analytical skills necessary to become a scientist. Incorporating these basic scientific methodologies in under- graduate education leads to increased undergraduate and post-graduate reten- tion in the sciences [13–16]. Toward this end, many undergraduate teaching orga- nizations offer training and suggestions for faculty to update and improve their teaching approaches to help students learn as scientists, through design and discovery (e.g., Council of Undergraduate Research [www.cur.org] and Project Kaleidoscope [ www.pkal.org]). With the advent of genome sequencing and bioinformatics, many scientists now formulate biological questions and inter- pret research results in the context of genomic information. Just as the use of bioinformatic tools and databases changed the way scientists investigate problems, it must change how scientists teach to create new opportunities for students to gain experiences reflecting the influence of genomics, proteomics, and bioinformatics on modern life sciences research [17–41]. Educators have responded by incorpo- rating bioinformatics into diverse life science curricula [42–44]. While these published exercises in, and guidelines for, bioinformatics curricula are helpful and inspirational, faculty new to the area of bioinformatics inevitably need training in the theoretical underpinnings of the algo- rithms [45]. Moreover, effectively inte- grating bioinformatics into courses or independent research projects requires infrastructure for organizing and assessing student work. Here, we present a new platform for faculty to keep current with the rapidly changing field of bioinfor- matics, the Integrated Microbial Genomes Annotation Collaboration Toolkit (IMG- ACT) (Figure 1). It was developed by instructors from both research-intensive and predominately undergraduate institu- tions in collaboration with the Department of Energy-Joint Genome Institute (DOE- JGI) as a means to innovate and update undergraduate education and faculty de- velopment. The IMG-ACT program pro- vides a cadre of tools, including access to a clearinghouse of genome sequences, bioin- formatics databases, data storage, instruc- tor course management, and student notebooks for organizing the results of their bioinformatic investigations. In the process, IMG-ACT makes it feasible to provide undergraduate research opportu- nities to a greater number and diversity of students, in contrast to the traditional mentor-to-student apprenticeship model for undergraduate research, which can be too expensive and time-consuming to provide for every undergraduate. The IMG-ACT serves as the hub for the network of faculty and students that use the system for microbial genome analysis. Open access of the IMG-ACT infrastructure to participating schools en- sures that all types of higher education institutions can utilize it. With the infra- structure in place, faculty can focus their efforts on the pedagogy of bioinformatics, involvement of students in research, and use of this tool for their own research agenda. What the original faculty mem- bers of the IMG-ACT development team present here is an overview of how the IMG-ACT program has affected our Citation: Ditty JL, Kvaal CA, Goodner B, Freyermuth SK, Bailey C, et al. (2010) Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum. PLoS Biol 8(8): e1000448. doi:10.1371/journal.pbio.1000448 Published August 10, 2010 Copyright: s 2010 Ditty et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: No specific funding was received for this work. The Community Page is a forum for organizations and societies to highlight their efforts to enhance the dissemination and value of scientific knowledge. Competing Interests: The authors have declared that no competing interests exist. Abbreviations: IMG-ACT; Integrated Microbial Genomes Annotation Collaboration Toolkit * E-mail: ckerfeld@lbl.gov PLoS Biology | www.plosbiology.org August 2010 | Volume 8 | Issue 8 | e1000448",Conference paper,vol63
pap1417,f06b2502434bf2b4c8536552a11b99bd6b646785,con71,Annual Conference on Innovation and Technology in Computer Science Education,Bioinformatics and molecular modeling in glycobiology,,Erratum,pro71
pap1418,bd9c08720ea74750006df5d0cb48f4847105120f,con70,International Conference on Graph Transformation,"Bioinformatics training: a review of challenges, actions and support requirements","As bioinformatics becomes increasingly central to research in the molecular life sciences, the need to train non-bioinformaticians to make the most of bioinformatics resources is growing. Here, we review the key challenges and pitfalls to providing effective training for users of bioinformatics services, and discuss successful training strategies shared by a diverse set of bioinformatics trainers. We also identify steps that trainers in bioinformatics could take together to advance the state of the art in current training practices. The ideas presented in this article derive from the first Trainer Networking Session held under the auspices of the EU-funded SLING Integrating Activity, which took place in November 2009.",Erratum,pro70
pap1419,bb38c180777326ed103ad0aa31545c8754da959f,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Machine Learning Approaches to Bioinformatics,"This book covers a wide range of subjects in applying machine learning approaches for bioinformatics projects. The book succeeds on two key unique features. First, it introduces the most widely used machine learning approaches in bioinformatics and discusses, with evaluations from real case studies, how they are used in individual bioinformatics projects. Second, it introduces state-of-the-art bioinformatics research methods. The theoretical parts and the practical parts are well integrated for readers to follow the existing procedures in individual research. Unlike most of the bioinformatics books on the market, the content coverage is not limited to just one subject. A broad spectrum of relevant topics in bioinformatics including systematic data mining and computational systems biology researches are brought together in this book, thereby offering an efficient and convenient platform for teaching purposes. An essential reference for both final year undergraduates and graduate students in universities, as well as a comprehensive handbook for new researchers, this book will also serve as a practical guide for software development in relevant bioinformatics projects.",Erratum,pro58
pap1420,0f9a9310d8ff5ab620f83dc8066b105d2f8a3df0,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,ThunderSTORM: a comprehensive ImageJ plug-in for PALM and STORM data analysis and super-resolution imaging,"Summary: ThunderSTORM is an open-source, interactive and modular plug-in for ImageJ designed for automated processing, analysis and visualization of data acquired by single-molecule localization microscopy methods such as photo-activated localization microscopy and stochastic optical reconstruction microscopy. ThunderSTORM offers an extensive collection of processing and post-processing methods so that users can easily adapt the process of analysis to their data. ThunderSTORM also offers a set of tools for creation of simulated data and quantitative performance evaluation of localization algorithms using Monte Carlo simulations. Availability and implementation: ThunderSTORM and the online documentation are both freely accessible at https://code.google.com/p/thunder-storm/ Contact: guy.hagen@lf1.cuni.cz Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro13
pap1421,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing","Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",Erratum,pro87
pap1422,0e6aefc52cc98104fdd802cd4fa9878d9f5fbca5,con40,Conference on Software Engineering Education and Training,Cytoscape 2.8: new features for data integration and network visualization,"Summary: Cytoscape is a popular bioinformatics package for biological network visualization and data integration. Version 2.8 introduces two powerful new features—Custom Node Graphics and Attribute Equations—which can be used jointly to greatly enhance Cytoscape's data integration and visualization capabilities. Custom Node Graphics allow an image to be projected onto a node, including images generated dynamically or at remote locations. Attribute Equations provide Cytoscape with spreadsheet-like functionality in which the value of an attribute is computed dynamically as a function of other attributes and network properties. Availability and implementation: Cytoscape is a desktop Java application released under the Library Gnu Public License (LGPL). Binary install bundles and source code for Cytoscape 2.8 are available for download from http://cytoscape.org. Contact: msmoot@ucsd.edu",Erratum,pro40
pap1423,ea398413f2827444c455325dfe1c3846cef0c543,jou88,bioRxiv,GenomeScope: Fast reference-free genome profiling from short reads,"Summary GenomeScope is an open-source web tool to rapidly estimate the overall characteristics of a genome, including genome size, heterozygosity rate, and repeat content from unprocessed short reads. These features are essential for studying genome evolution, and help to choose parameters for downstream analysis. We demonstrate its accuracy on 324 simulated and 16 real datasets with a wide range in genome sizes, heterozygosity levels, and error rates. Availability and Implementation http://genomescope.org, https://github.com/schatzlab/genomescope.git Contact mschatz@jhu.edu. Supplementary information Supplementary data are available at Bioinformatics online.",Conference paper,vol88
pap1424,40048a932aba606e415f91d0c1955dc7c54088d7,con25,IEEE International Parallel and Distributed Processing Symposium,PROVEAN web server: a tool to predict the functional effect of amino acid substitutions and indels,"UNLABELLED
We present a web server to predict the functional effect of single or multiple amino acid substitutions, insertions and deletions using the prediction tool PROVEAN. The server provides rapid analysis of protein variants from any organisms, and also supports high-throughput analysis for human and mouse variants at both the genomic and protein levels.


AVAILABILITY AND IMPLEMENTATION
The web server is freely available and open to all users with no login requirements at http://provean.jcvi.org.


CONTACT
achan@jcvi.org


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro25
pap1425,4b3b74aec92b46812ea87011058326794f59db10,con65,IEEE International Conference on Software Engineering and Formal Methods,Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data,"Motivation: Detection of random errors and systematic biases is a crucial step of a robust pipeline for processing high-throughput sequencing (HTS) data. Bioinformatics software tools capable of performing this task are available, either for general analysis of HTS data or targeted to a specific sequencing technology. However, most of the existing QC instruments only allow processing of one sample at a time. Results: Qualimap 2 represents a next step in the QC analysis of HTS data. Along with comprehensive single-sample analysis of alignment data, it includes new modes that allow simultaneous processing and comparison of multiple samples. As with the first version, the new features are available via both graphical and command line interface. Additionally, it includes a large number of improvements proposed by the user community. Availability and implementation: The implementation of the software along with documentation is freely available at http://www.qualimap.org. Contact: meyer@mpiib-berlin.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro65
pap1426,1e4c6ca6b2d9476362d43ac6aa7d0ea072f2d243,jou88,bioRxiv,Bandage: interactive visualization of de novo genome assemblies,"Summary While de novo assembly graphs contain assembled contigs (nodes), the connections between those contigs (edges) are difficult for users to access. Bandage (a Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a tool for visualising assembly graphs with connections. Users can zoom in to specific areas of the graph and interact with it by moving nodes, adding labels, changing colours and extracting sequences. BLAST searches can be performed within the Bandage GUI and the hits are displayed as highlights in the graph. By displaying connections between contigs, Bandage presents new possibilities for analysing de novo assemblies that are not possible through investigation of contigs alone. Availability and implementation Source code and binaries are freely available at https://github.com/rrwick/Bandage. Bandage is implemented in C++ and supported on Linux, OS X and Windows. Contact rrwick@gmail.com Supplementary information A full feature list and screenshots are available at Bioinformatics online and http://rrwick.github.io/Bandage.",Letter,vol88
pap1427,35f304e3b47ae4008108916476801df89bcceccf,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Nonnegative Matrix and Tensor Factorizations - Applications to Exploratory Multi-way Data Analysis and Blind Source Separation,"This book provides a broad survey of models and efficient algorithms for Nonnegative Matrix Factorization (NMF). This includes NMFs various extensions and modifications, especially Nonnegative Tensor Factorizations (NTF) and Nonnegative Tucker Decompositions (NTD). NMF/NTF and their extensions are increasingly used as tools in signal and image processing, and data analysis, having garnered interest due to their capability to provide new insights and relevant information about the complex latent relationships in experimental data sets. It is suggested that NMF can provide meaningful components with physical interpretations; for example, in bioinformatics, NMF and its extensions have been successfully applied to gene expression, sequence analysis, the functional characterization of genes, clustering and text mining. As such, the authors focus on the algorithms that are most useful in practice, looking at the fastest, most robust, and suitable for large-scale models. Key features: Acts as a single source reference guide to NMF, collating information that is widely dispersed in current literature, including the authors own recently developed techniques in the subject area. Uses generalized cost functions such as Bregman, Alpha and Beta divergences, to present practical implementations of several types of robust algorithms, in particular Multiplicative, Alternating Least Squares, Projected Gradient and Quasi Newton algorithms. Provides a comparative analysis of the different methods in order to identify approximation error and complexity. Includes pseudo codes and optimized MATLAB source codes for almost all algorithms presented in the book. The increasing interest in nonnegative matrix and tensor factorizations, as well as decompositions and sparse representation of data, will ensure that this book is essential reading for engineers, scientists, researchers, industry practitioners and graduate students across signal and image processing; neuroscience; data mining and data analysis; computer science; bioinformatics; speech processing; biomedical engineering; and multimedia.",Erratum,pro82
pap1428,73dcc27f06eeff886be506b5d12354dd8dae2c04,jou147,Scientific Reports,RASTtk: A modular and extensible implementation of the RAST algorithm for building custom annotation pipelines and annotating batches of genomes,,Article,vol147
pap1429,61d090d329434e8edfc8c08b34c61a0c7b378fb8,con37,International Symposium on Search Based Software Engineering,Scalable web services for the PSIPRED Protein Analysis Workbench,"Here, we present the new UCL Bioinformatics Group’s PSIPRED Protein Analysis Workbench. The Workbench unites all of our previously available analysis methods into a single web-based framework. The new web portal provides a greatly streamlined user interface with a number of new features to allow users to better explore their results. We offer a number of additional services to enable computationally scalable execution of our prediction methods; these include SOAP and XML-RPC web server access and new HADOOP packages. All software and services are available via the UCL Bioinformatics Group website at http://bioinf.cs.ucl.ac.uk/.",Erratum,pro37
pap1430,d03820bb0ee8a4b936cc5fd13053588d659ff946,jou88,bioRxiv,NanoPack: visualizing and processing long-read sequencing data,"Summary: Here we describe NanoPack, a set of tools developed for visualization and processing of long read sequencing data from Oxford Nanopore Technologies and Pacific Biosciences. Availability and Implementation: The NanoPack tools are written in Python3 and released under the GNU GPL3.0 Licence. The source code can be found at https://github.com/wdecoster/nanopack, together with links to separate scripts and their documentation. The scripts are compatible with Linux, Mac OS and the MS Windows 10 subsystem for linux and are available as a graphical user interface, a web service at http://nanoplot.bioinf.be and command line tools. Contact: wouter.decoster@molgen.vib-ua.be Supplementary information: Supplementary tables and figures are available at Bioinformatics online.",Article,vol88
pap1431,5aea95e1ae78a66474051a330ded374e199b658c,con0,International Conference on Machine Learning,Representation Learning on Graphs with Jumping Knowledge Networks,"Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ""neighboring"" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.",Article,pro0
pap1432,017438e9f62834694a4ea85f87f48bfcbd0490d7,con78,Neural Information Processing Systems,Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization,"Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized ""robust principal component analysis"" problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the corrupted entries E are unknown and the errors can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns by solving a simple convex program, for which we give a fast and provably convergent algorithm. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.",Conference paper,pro78
pap1433,aee9709704f84defcf97d622b490eb662593d026,con16,International Conference on Data Science and Advanced Analytics,Modeling and Simulation of Genetic Regulatory Systems: A Literature Review,"In order to understand the functioning of organisms on the molecular level, we need to know which genes are expressed, when and where in the organism, and to which extent. The regulation of gene expression is achieved through genetic regulatory systems structured by networks of interactions between DNA, RNA, proteins, and small molecules. As most genetic regulatory networks of interest involve many components connected through interlocking positive and negative feedback loops, an intuitive understanding of their dynamics is hard to obtain. As a consequence, formal methods and computer tools for the modeling and simulation of genetic regulatory networks will be indispensable. This paper reviews formalisms that have been employed in mathematical biology and bioinformatics to describe genetic regulatory systems, in particular directed graphs, Bayesian networks, Boolean networks and their generalizations, ordinary and partial differential equations, qualitative differential equations, stochastic equations, an...",Erratum,pro16
pap1434,b18d3c40f373bff28d5cfe3d8d2a674c7aed6841,jou22,Proceedings of the National Academy of Sciences of the United States of America,Bioinformatics identification of MurJ (MviN) as the peptidoglycan lipid II flippase in Escherichia coli,"Peptidoglycan is a cell-wall glycopeptide polymer that protects bacteria from osmotic lysis. Whereas in Gram-positive bacteria it also serves as scaffold for many virulence factors, in Gram-negative bacteria, peptidoglycan is an anchor for the outer membrane. For years, we have known the enzymes required for the biosynthesis of peptidoglycan; what was missing was the flippase that translocates the lipid-anchored precursors across the cytoplasmic membrane before their polymerization into mature peptidoglycan. Using a reductionist bioinformatics approach, I have identified the essential inner-membrane protein MviN (renamed MurJ) as a likely candidate for the peptidoglycan flippase in Escherichia coli. Here, I present genetic and biochemical data that confirm the requirement of MurJ for peptidoglycan biosynthesis and that are in agreement with a role of MurJ as a flippase. Because of its essential nature, MurJ could serve as a target in the continuing search for antimicrobial compounds.",Conference paper,vol22
pap1435,81b74385488ab8704ce7e04a332de4971d00a499,jou253,International Journal of Plant Genomics,Blast2GO: A Comprehensive Suite for Functional Analysis in Plant Genomics,"Functional annotation of novel sequence data is a primary requirement for the utilization of functional genomics approaches in plant research. In this paper, we describe the Blast2GO suite as a comprehensive bioinformatics tool for functional annotation of sequences and data mining on the resulting annotations, primarily based on the gene ontology (GO) vocabulary. Blast2GO optimizes function transfer from homologous sequences through an elaborate algorithm that considers similarity, the extension of the homology, the database of choice, the GO hierarchy, and the quality of the original annotations. The tool includes numerous functions for the visualization, management, and statistical analysis of annotation results, including gene set enrichment analysis. The application supports InterPro, enzyme codes, KEGG pathways, GO direct acyclic graphs (DAGs), and GOSlim. Blast2GO is a suitable tool for plant genomics research because of its versatility, easy installation, and friendly use.",Conference paper,vol253
pap1436,80ff8a1603192dfeb7665c28ebd9bd682cf4907b,con71,Annual Conference on Innovation and Technology in Computer Science Education,SNAP: a web-based tool for identification and annotation of proxy SNPs using HapMap,"SUMMARY
The interpretation of genome-wide association results is confounded by linkage disequilibrium between nearby alleles. We have developed a flexible bioinformatics query tool for single-nucleotide polymorphisms (SNPs) to identify and to annotate nearby SNPs in linkage disequilibrium (proxies) based on HapMap. By offering functionality to generate graphical plots for these data, the SNAP server will facilitate interpretation and comparison of genome-wide association study results, and the design of fine-mapping experiments (by delineating genomic regions harboring associated variants and their proxies).


AVAILABILITY
SNAP server is available at http://www.broad.mit.edu/mpg/snap/.",Erratum,pro71
pap1437,c84a4abc96f5ecf2783cba9aeef579510b78af3f,con103,IEEE International Conference on Multimedia and Expo,"Error filtering, pair assembly and error correction for next-generation sequencing reads","MOTIVATION
Next-generation sequencing produces vast amounts of data with errors that are difficult to distinguish from true biological variation when coverage is low.


RESULTS
We demonstrate large reductions in error frequencies, especially for high-error-rate reads, by three independent means: (i) filtering reads according to their expected number of errors, (ii) assembling overlapping read pairs and (iii) for amplicon reads, by exploiting unique sequence abundances to perform error correction. We also show that most published paired read assemblers calculate incorrect posterior quality scores.


AVAILABILITY AND IMPLEMENTATION
These methods are implemented in the USEARCH package. Binaries are freely available at http://drive5.com/usearch.


CONTACT
robert@drive5.com


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",Erratum,pro103
pap1438,8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8,con35,IEEE Working Conference on Mining Software Repositories,Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification,"Summary: State‐of‐the‐art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time‐consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user‐designed image features or classifiers. Availability and Implementation: TWS is distributed as open‐source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable_Weka_Segmentation. Contact: ignacio.arganda@ehu.eus Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro35
pap1439,e8dba5bf84c7c3cdae3648a76d0af6ff1e393009,con75,Intelligent Systems in Molecular Biology,De novo identification of repeat families in large genomes,"MOTIVATION
De novo repeat family identification is a challenging algorithmic problem of great practical importance. As the number of genome sequencing projects increases, there is a pressing need to identify the repeat families present in large, newly sequenced genomes. We develop a new method for de novo identification of repeat families via extension of consensus seeds; our method enables a rigorous definition of repeat boundaries, a key issue in repeat analysis.


RESULTS
Our RepeatScout algorithm is more sensitive and is orders of magnitude faster than RECON, the dominant tool for de novo repeat family identification in newly sequenced genomes. Using RepeatScout, we estimate that approximately 2% of the human genome and 4% of mouse and rat genomes consist of previously unannotated repetitive sequence.


AVAILABILITY
Source code is available for download at http://www-cse.ucsd.edu/groups/bioinformatics/software.html",Conference paper,pro75
pap1440,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,con96,Interspeech,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",Erratum,pro96
pap1441,7f16fc679c2c66bbe9d04beaa8af14be834ad178,jou254,F1000Research,GFF Utilities: GffRead and GffCompare,"Summary: GTF (Gene Transfer Format) and GFF (General Feature Format) are popular file formats used by bioinformatics programs to represent and exchange information about various genomic features, such as gene and transcript locations and structure. GffRead and GffCompare are open source programs that provide extensive and efficient solutions to manipulate files in a GTF or GFF format. While GffRead can convert, sort, filter, transform, or cluster genomic features, GffCompare can be used to compare and merge different gene annotations. Availability and implementation: GFF utilities are implemented in C++ for Linux and OS X and released as open source under an MIT license ( https://github.com/gpertea/gffread, https://github.com/gpertea/gffcompare).",Letter,vol254
pap1442,6383011826c14af5d63700b4a3dd6e33391c55d0,con106,International Conference on Mobile Data Management,Bioinformatics - the machine learning approach (2. ed.),,Erratum,pro106
pap1443,1221de833f4909b7bc27790fa0cd080d756d8982,jou106,Nucleic Acids Research,"The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants","FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.",Letter,vol106
pap1444,3142f9c54220f78c8500c6149df1667332ea9d6c,jou103,Science,An Extensive Class of Small RNAs in Caenorhabditis elegans,"The lin-4 and let-7 antisense RNAs are temporal regulators that control the timing of developmental events inCaenorhabditis elegans by inhibiting translation of target mRNAs. let-7 RNA is conserved among bilaterian animals, suggesting that this class of small RNAs [microRNAs (miRNAs)] is evolutionarily ancient. Using bioinformatics and cDNA cloning, we found 15 new miRNA genes in C. elegans. Several of these genes express small transcripts that vary in abundance during C. elegans larval development, and three of them have apparent homologs in mammals and/or insects. Small noncoding RNAs of the miRNA class appear to be numerous and diverse.",Conference paper,vol103
pap1445,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,con8,Frontiers in Education Conference,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",Erratum,pro8
pap1446,cc9c8a81a5e2be0c8abdb39c905f5faaf621dd81,jou5,Genome Biology,Toward almost closed genomes with GapFiller,,Letter,vol5
pap1447,c6480aecd32cff4ebbd9d12e0c351249cd76a760,con24,International Conference on Data Technologies and Applications,GEOquery: a bridge between the Gene Expression Omnibus (GEO) and BioConductor,"UNLABELLED
Microarray technology has become a standard molecular biology tool. Experimental data have been generated on a huge number of organisms, tissue types, treatment conditions and disease states. The Gene Expression Omnibus (Barrett et al., 2005), developed by the National Center for Bioinformatics (NCBI) at the National Institutes of Health is a repository of nearly 140,000 gene expression experiments. The BioConductor project (Gentleman et al., 2004) is an open-source and open-development software project built in the R statistical programming environment (R Development core Team, 2005) for the analysis and comprehension of genomic data. The tools contained in the BioConductor project represent many state-of-the-art methods for the analysis of microarray and genomics data. We have developed a software tool that allows access to the wealth of information within GEO directly from BioConductor, eliminating many the formatting and parsing problems that have made such analyses labor-intensive in the past. The software, called GEOquery, effectively establishes a bridge between GEO and BioConductor. Easy access to GEO data from BioConductor will likely lead to new analyses of GEO data using novel and rigorous statistical and bioinformatic tools. Facilitating analyses and meta-analyses of microarray data will increase the efficiency with which biologically important conclusions can be drawn from published genomic data.


AVAILABILITY
GEOquery is available as part of the BioConductor project.",Erratum,pro24
pap1448,9d514eec71b0aed1bfe44764b3564ba41fd7dc19,jou255,Pharmacogenomics (London),Pharmacogenomics and bioinformatics: PharmGKB.,"The NIH initiated the PharmGKB in April 2000. The primary mission was to create a repository of primary data, tools to track associations between genes and drugs, and to catalog the location and frequency of genetic variations known to impact drug response. Over the past 10 years, new technologies have shifted research from candidate gene pharmacogenetics to phenotype-based pharmacogenomics with a consequent explosion of data. PharmGKB has refocused on curating knowledge rather than housing primary genotype and phenotype data, and now, captures more complex relationships between genes, variants, drugs, diseases and pathways. Going forward, the challenges are to provide the tools and knowledge to plan and interpret genome-wide pharmacogenomics studies, predict gene-drug relationships based on shared mechanisms and support data-sharing consortia investigating clinical applications of pharmacogenomics.",Conference paper,vol255
pap1449,d84ce894cc3b9114a6de910ce2e1eefe940eb63f,con90,Computer Vision and Pattern Recognition,The Roots of Bioinformatics,"Every new scientific discipline or methodology reaches a point in its maturation where it is fruitful for it to turn its gaze inward, as well as backward. Such introspection helps to clarify the essential structure of a field of study, facilitating communication, pedagogy, standardization, and the like, while retrospection aids this process by accounting for its beginnings and underpinnings. 
 
In this spirit, PLoS Computational Biology is launching a new series of themed articles tracing the roots of bioinformatics. Essays from prominent workers in the field will relate how selected scientific, technological, economic, and even cultural threads came to influence the development of the field we know today. These are not intended to be review articles, nor personal reminiscences, but rather narratives from individual perspectives about the origins and foundations of bioinformatics, and are expected to provide both historical and technical insights. Ideally, these articles will offer an archival record of the field's development, as well as a human face on an important segment of science, for the benefit of current and future workers. 
 
Upcoming articles, already commissioned, will cover the roots of bioinformatics in structural biology, in evolutionary biology, and in artificial intelligence, with more in the works. These topics are obviously very broad, and so are likely to be subdivided or otherwise revisited in future installments by authors with varying perspectives. Topics and authors will be chosen at the discretion of the editors along lines broadly corresponding to the usual content of this journal. 
 
The author, having been asked to serve as Series Editor by the Editor-in-Chief, will endeavor to maintain a uniform flow of articles solicited from luminaries in the field. As a starting point to the series, I offer below a few vignettes and reflections on some longer-term influences that have shaped the discipline. I first consider the unique status of bioinformatics vis-a-vis science and technology, and then explore historical trends in biology and related fields that anticipated and prepared the way for bioinformatics. Examining the context of key moments when computers were first taken up by early adopters reveals how deep the roots of bioinformatics go.",Erratum,pro90
pap1450,36c76c76e77237bd5da1d1dc4289641002dba7f5,jou81,BMC Bioinformatics,An innovative approach for testing bioinformatics programs using metamorphic testing,,Article,vol81
pap1451,e8da033f03c679e87b95a9e8cc1cfdd8c6ccb526,con95,IEEE International Conference on Computer Vision,Foundations of multidimensional and metric data structures,"Multidimensional data is data that exists and changes in more than one dimension, by time, or spatially, or both, sometimes dynamically. Think here of tracking hurricane data in order to project the storm's path, for just one example. As spatial and other multidimensional data structures become increasingly important for the applications in game programming, data mining, bioinformatics, and many other areas--including astronomy, geographic information systems, physics, etc., the need for a comprehensive book on the subject is paramount. This book is truly a life's work by the author who is clearly the best person for the job.",Erratum,pro95
pap1452,33780e4aba639a97f9fb7f7e773853f74dd494b7,jou2,Genome Research,The Bioperl toolkit: Perl modules for the life sciences.,"The Bioperl project is an international open-source collaboration of biologists, bioinformaticians, and computer scientists that has evolved over the past 7 yr into the most comprehensive library of Perl modules available for managing and manipulating life-science information. Bioperl provides an easy-to-use, stable, and consistent programming interface for bioinformatics application programmers. The Bioperl modules have been successfully and repeatedly used to reduce otherwise complex tasks to only a few lines of code. The Bioperl object model has been proven to be flexible enough to support enterprise-level applications such as EnsEMBL, while maintaining an easy learning curve for novice Perl programmers. Bioperl is capable of executing analyses and processing results from programs such as BLAST, ClustalW, or the EMBOSS suite. Interoperation with modules written in Python and Java is supported through the evolving BioCORBA bridge. Bioperl provides access to data stores such as GenBank and SwissProt via a flexible series of sequence input/output modules, and to the emerging common sequence data storage format of the Open Bioinformatics Database Access project. This study describes the overall architecture of the toolkit, the problem domains that it addresses, and gives specific examples of how the toolkit can be used to solve common life-sciences problems. We conclude with a discussion of how the open-source nature of the project has contributed to the development effort.",Article,vol2
pap1453,2501576dc02c6ca1504b045885eb1cec8d4d3f16,jou187,Nature reviews genetics,Repetitive DNA and next-generation sequencing: computational challenges and solutions,,Letter,vol187
pap1454,6e6f0fb09cc0604e1dca938c1950f1a3e1550c4a,con45,International Conference on Global Software Engineering,"Anatomy Ontologies for Bioinformatics, Principles and Practice",,Erratum,pro45
pap1455,f167c2df8a0d85e05029f9a25fc52e6cd4c5d19b,jou256,IEEE Transactions on Parallel and Distributed Systems,Cloud Technologies for Bioinformatics Applications,"Executing large number of independent jobs or jobs comprising of large number of tasks that perform minimal intertask communication is a common requirement in many domains. Various technologies ranging from classic job schedulers to the latest cloud technologies such as MapReduce can be used to execute these ""many-tasks” in parallel. In this paper, we present our experience in applying two cloud technologies Apache Hadoop and Microsoft DryadLINQ to two bioinformatics applications with the above characteristics. The applications are a pairwise Alu sequence alignment application and an Expressed Sequence Tag (EST) sequence assembly program. First, we compare the performance of these cloud technologies using the above applications and also compare them with traditional MPI implementation in one application. Next, we analyze the effect of inhomogeneous data on the scheduling mechanisms of the cloud technologies. Finally, we present a comparison of performance of the cloud technologies under virtual and nonvirtual hardware platforms.",Conference paper,vol256
pap1456,e9cfe56b8eb7a84dab4237dced7e15a56d5eaebc,jou257,Electrophoresis,Automated comparative protein structure modeling with SWISS‐MODEL and Swiss‐PdbViewer: A historical perspective,"SWISS‐MODEL pioneered the field of automated modeling as the first protein modeling service on the Internet. In combination with the visualization tool Swiss‐PdbViewer, the Internet‐based Workspace and the SWISS‐MODEL Repository, it provides a fully integrated sequence to structure analysis and modeling platform. This computational environment is made freely available to the scientific community with the aim to hide the computational complexity of structural bioinformatics and encourage bench scientists to make use of the ever‐increasing structural information available. Indeed, over the last decade, the availability of structural information has significantly increased for many organisms as a direct consequence of the complementary nature of comparative protein modeling and experimental structure determination. This has a very positive and enabling impact on many different applications in biomedical research as described in this paper.",Conference paper,vol257
pap1457,66340a5d9237efb858f895d6f8af77762d25bbad,jou205,Current Topics in Medicinal Chemistry,Medicinal chemistry and bioinformatics--current trends in drugs discovery with networks topological indices.,"The numerical encoding of chemical structure with Topological Indices (TIs) is currently growing in importance in Medicinal Chemistry and Bioinformatics. This approach allows the rapid collection, annotation, retrieval, comparison and mining of chemical structures within large databases. TIs can subsequently be used to seek quantitative structure-activity relationships (QSAR), which are models connecting chemical structure with biological activity. In the early 1990's, there was an explosion in the introduction and definition of new TIs. The Handbook of Molecular Descriptors by Todeschini and Consonni lists more than 1500 of these indices. At the end of the last century, researchers produced a large number of TIs with essentially the same advantages and/or disadvantages. Consequently, many researchers abandoned the definition of TIs for a time. In our opinion, one of the problems associated with TIs is that researchers aimed their efforts only at the codification of chemical connectivity for small-sized drugs. As a consequence, recently it seems that we have arrived at ""Fukuyama's End of History in TIs definition"". In the work described here, we review and comment on the ""quo vadis"" and challenges in the definition of TIs as we enter the new century. Emphasis is placed on new chiral TIs (CTIs), flexible TIs for unifying QSAR models with multiple targets, topographic indices (TPGIs), TIs for DNA and protein sequences, TIs for 2D RNA structures, TPGIs and drug-protein or drug-RNA quantitative structure-binding relationship (QSBR) studies, TIs to encode protein surface information and TIs for protein interaction networks (PINs).",Conference paper,vol205
pap1458,ae09bdaf2611dc8b8d4793a5f01bcc239d5fbcae,con100,International Conference on Automatic Face and Gesture Recognition,BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btm112 Databases and ontologies OBO-Edit—an ontology editor for biologists,,Erratum,pro100
pap1459,7cc9496750d82e5d839159e7489e63351b404ea6,jou243,Nature Reviews Microbiology,Fungal secondary metabolism — from biochemistry to genomics,,Conference paper,vol243
pap1460,cd0c8bc7a22efced33bdb84e5a9abbd30bbd863b,jou258,IEEE/ACM Transactions on Computational Biology & Bioinformatics,Multiobjective Optimization in Bioinformatics and Computational Biology,"This paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. A survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. An original contribution of the review is the identification of five distinct ""contexts,"" giving rise to multiple objectives: These are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique",Article,vol258
pap1461,a3fef5ea63459c3af2c6f58e12fcb9cedb6611af,con38,International Symposium on Empirical Software Engineering and Measurement,Bioinformatics Applications Note Structural Bioinformatics 2struc: the Secondary Structure Server,"The defined secondary structure of proteins method is often considered the gold standard for assignment of secondary structure from three-dimensional coordinates. However, there are alternative methods. '2Struc: The Secondary Structure Server' has been created as a single point of access for eight different secondary structure assignment methods. It has been designed to enable comparisons between methods for analyzing the secondary structure content for a single protein. It also includes a second functionality, 'Compare-the-Protein' to enable comparisons of the secondary structure features from any one method to be made within a collection of nuclear magnetic resonance models, or between the crystal structures of two different proteins.",Erratum,pro38
pap1462,71b25c9080e3070dd07e0958998e2ad26bf6f384,con66,International Conference on Software Reuse,Over-optimism in bioinformatics research,"The problem of ”false research findings” in medical research has focused much attention in the last few years (Ioannidis, 2005). One of the main problems, termed as ”fishing for significance” in the present letter, is that researchers often (consciously or subconsciously) report results that are in fact the product of an intensive optimization, i.e. of multiple comparisons. Such results are typically unlikely to be reproduced in an independent study and have a high probability to be false (Ioannidis, 2005). The ”fishing for significance” problem is enhanced by the so-called ”publication bias”: positive results have a much higher chance to get published than negative results, as already acknowledged fifty years ago (Sterling, 1959). In a word, many false positive results are produced through multiple comparisons, and false positives have higher chance to get published than true negatives. Moreover, the difficulty to publish negative results obviously encourages authors to find something positive in their study by performing numerous analyses until one of them yields positive results by chance, i.e. to fish for significance. Although this issue is by far less acknowledged and publicly admitted than in the medical context, the same types of problems occur in biostatistics and bioinformatics research.",Erratum,pro66
pap1463,e04796bee4f83e99250dce77c88d09ed1b8ecf82,jou259,Genes,Bioinformatics for Next Generation Sequencing Data,"The emergence of next-generation sequencing (NGS) platforms imposes increasing demands on statistical methods and bioinformatic tools for the analysis and the management of the huge amounts of data generated by these technologies. Even at the early stages of their commercial availability, a large number of softwares already exist for analyzing NGS data. These tools can be fit into many general categories including alignment of sequence reads to a reference, base-calling and/or polymorphism detection, de novo assembly from paired or unpaired reads, structural variant detection and genome browsing. This manuscript aims to guide readers in the choice of the available computational tools that can be used to face the several steps of the data analysis workflow.",Article,vol259
pap1464,378b5e4f4305fa723a32eb489abba4a917589397,jou260,Genome Medicine,Translational bioinformatics in the cloud: an affordable alternative,,Letter,vol260
pap1465,3e24ff6f5f8eba407effc09a3e23ee583d4ee0e7,con53,Workshop on Web 2.0 for Software Engineering,"Oncomine 3.0: genes, pathways, and networks in a collection of 18,000 cancer gene expression profiles.","DNA microarrays have been widely applied to cancer transcriptome analysis; however, the majority of such data are not easily accessible or comparable. Furthermore, several important analytic approaches have been applied to microarray analysis; however, their application is often limited. To overcome these limitations, we have developed Oncomine, a bioinformatics initiative aimed at collecting, standardizing, analyzing, and delivering cancer transcriptome data to the biomedical research community. Our analysis has identified the genes, pathways, and networks deregulated across 18,000 cancer gene expression microarrays, spanning the majority of cancer types and subtypes. Here, we provide an update on the initiative, describe the database and analysis modules, and highlight several notable observations. Results from this comprehensive analysis are available at http://www.oncomine.org.",Erratum,pro53
pap1466,17a2c1b4016ac226fa6e74d93247c2b67dec05de,jou81,BMC Bioinformatics,Proteinortho: Detection of (Co-)orthologs in large-scale analysis,,Article,vol81
pap1467,9079c4dfd4be155efc670ce1333cdcc3e5c6480f,con26,Decision Support Systems,The Austronesian Basic Vocabulary Database: From Bioinformatics to Lexomics,"Phylogenetic methods have revolutionised evolutionary biology and have recently been applied to studies of linguistic and cultural evolution. However, the basic comparative data on the languages of the world required for these analyses is often widely dispersed in hard to obtain sources. Here we outline how our Austronesian Basic Vocabulary Database (ABVD) helps remedy this situation by collating wordlists from over 500 languages into one web-accessible database. We describe the technology underlying the ABVD and discuss the benefits that an evolutionary bioinformatic approach can provide. These include facilitating computational comparative linguistic research, answering questions about human prehistory, enabling syntheses with genetic data, and safe-guarding fragile linguistic information.",Erratum,pro26
pap1468,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,con38,International Symposium on Empirical Software Engineering and Measurement,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",Erratum,pro38
pap1469,7efd35eb509190bf1d006d4a28b1bf89b812241a,con5,Technical Symposium on Computer Science Education,Activities at the Universal Protein Resource (UniProt),"The mission of the Universal Protein Resource (UniProt) (http://www.uniprot.org) is to provide the scientific community with a comprehensive, high-quality and freely accessible resource of protein sequences and functional annotation. It integrates, interprets and standardizes data from literature and numerous resources to achieve the most comprehensive catalog possible of protein information. The central activities are the biocuration of the UniProt Knowledgebase and the dissemination of these data through our Web site and web services. UniProt is produced by the UniProt Consortium, which consists of groups from the European Bioinformatics Institute (EBI), the SIB Swiss Institute of Bioinformatics (SIB) and the Protein Information Resource (PIR). UniProt is updated and distributed every 4 weeks and can be accessed online for searches or downloads.",Erratum,pro5
pap1470,69e3442fb05c33b537471007345a79397e849e68,jou22,Proceedings of the National Academy of Sciences of the United States of America,Bioinformatics construction of the human cell surfaceome,"Cell surface proteins are excellent targets for diagnostic and therapeutic interventions. By using bioinformatics tools, we generated a catalog of 3,702 transmembrane proteins located at the surface of human cells (human cell surfaceome). We explored the genetic diversity of the human cell surfaceome at different levels, including the distribution of polymorphisms, conservation among eukaryotic species, and patterns of gene expression. By integrating expression information from a variety of sources, we were able to identify surfaceome genes with a restricted expression in normal tissues and/or differential expression in tumors, important characteristics for putative tumor targets. A high-throughput and efficient quantitative real-time PCR approach was used to validate 593 surfaceome genes selected on the basis of their expression pattern in normal and tumor samples. A number of candidates were identified as potential diagnostic and therapeutic targets for colorectal tumors and glioblastoma. Several candidate genes were also identified as coding for cell surface cancer/testis antigens. The human cell surfaceome will serve as a reference for further studies aimed at characterizing tumor targets at the surface of human cells.",Conference paper,vol22
pap1471,6f85730e4efa0f6472b16f0f3c330ef0dae150d0,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Bioinformatics for DNA Sequence Analysis,,Erratum,pro54
pap1472,c1950d028d11df7c71f707d65202c0c3f48373e7,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Better bioinformatics through usability analysis,"MOTIVATION
Improving the usability of bioinformatics resources enables researchers to find, interact with, share, compare and manipulate important information more effectively and efficiently. It thus enables researchers to gain improved insights into biological processes with the potential, ultimately, of yielding new scientific results. Usability 'barriers' can pose significant obstacles to a satisfactory user experience and force researchers to spend unnecessary time and effort to complete their tasks. The number of online biological databases available is growing and there is an expanding community of diverse users. In this context there is an increasing need to ensure the highest standards of usability.


RESULTS
Using 'state-of-the-art' usability evaluation methods, we have identified and characterized a sample of usability issues potentially relevant to web bioinformatics resources, in general. These specifically concern the design of the navigation and search mechanisms available to the user. The usability issues we have discovered in our substantial case studies are undermining the ability of users to find the information they need in their daily research activities. In addition to characterizing these issues, specific recommendations for improvements are proposed leveraging proven practices from web and usability engineering. The methods and approach we exemplify can be readily adopted by the developers of bioinformatics resources.",Erratum,pro73
pap1473,4c921e841bf85f2d27eaf7c25d2d7d1b8b81cd25,con100,International Conference on Automatic Face and Gesture Recognition,A Quick Guide for Developing Effective Bioinformatics Programming Skills,"Bioinformatics programming skills are becoming a necessity across many facets of biology and medicine, owed in part to the continuing explosion of biological data aggregation and the complexity and scale of questions now being addressed through modern bioinformatics. Although many are now receiving formal training in bioinformatics through various university degree and certificate programs, this training is often focused strongly on bioinformatics methodology, leaving many important and practical aspects of bioinformatics to self-education and experience. The following set of guidelines distill several key principals of effective bioinformatics programming, which the authors learned through insights gained across many years of combined experience developing popular bioinformatics software applications and database systems in both academic and commercial settings [1]–[6]. Successful adoption of these principals will serve both beginner and experienced bioinformaticians alike in career development and pursuit of professional and scientific goals.",Erratum,pro100
pap1474,ea2d20d78f8b482c9e92e33a9d93ae788ece37d5,con107,Chinese Conference on Biometric Recognition,Bioinformatics : applications in life and environmental sciences,,Erratum,pro107
pap1475,9a7579bc492d834ad7c1a0b6fad35489d19b5e7e,con66,International Conference on Software Reuse,BIOINFORMATICS APPLICATIONS NOTE,"Summary: Recent parallel pyrosequencing methods and the increasing number of ﬁnished genomes encourage the sequencing and investigation of closely related strains. Although the sequencing itself becomes easier and cheaper with each machine generation, the ﬁnishing of the genomes remains difﬁcult. Instead of the desired whole genomic sequence, a set of contigs is the result of the assembly. In this applications note, we present the tool r2cat (related reference contig arrangement tool) that helps in the task of comparative assembly and also provides an interactive visualization for synteny inspection.",Erratum,pro66
pap1476,e41a4e92bf68aaa3170e22e7ae1d57fc495a93fc,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Biodoop: Bioinformatics on Hadoop,"Bioinformatics applications currently require both processing of huge amounts of data and heavy computation. Fulfilling these requirements calls for simple ways to implement parallel computing. MapReduce is a general-purpose parallelization model that seems particularly well-suited to this task and for which an open source implementation (Hadoop) is available. Here we report on its application to three relevant algorithms: BLAST, GSEA and GRAMMAR. The first is characterized by relatively low-weight computation on large data sets, while the second requires heavy processing of relatively small data sets. The third one can be considered as containing a mixture of these two computational flavors. Our results are encouraging and indicate that the framework could have a wide range of bioinformatics applications while maintaining good computational efficiency, scalability and ease of maintenance.",Erratum,pro82
pap1477,bb1461f8e65914f6755a924398c760bb1aec5f44,con103,IEEE International Conference on Multimedia and Expo,Ontology-based Knowledge Representation for Bioinformatics,"Much of biology works by applying prior knowledge ('what is known') to an unknown entity, rather than the application of a set of axioms that will elicit knowledge. In addition, the complex biological data stored in bioinformatics databases often require the addition of knowledge to specify and constrain the values held in that database. One way of capturing knowledge within bioinformatics applications and databases is the use of ontologies. An ontology is the concrete form of a conceptualisation of a community's knowledge of a domain. This paper aims to introduce the reader to the use of ontologies within bioinformatics. A description of the type of knowledge held in an ontology will be given.The paper will be illustrated throughout with examples taken from bioinformatics and molecular biology, and a survey of current biological ontologies will be presented. From this it will be seen that the use to which the ontology is put largely determines the content of the ontology. Finally, the paper will describe the process of building an ontology, introducing the reader to the techniques and methods currently in use and the open research questions in ontology development.",Erratum,pro103
pap1478,50f174ad5d1de758ef371a178209a95c36e24f37,jou81,BMC Bioinformatics,XMPP for cloud computing in bioinformatics supporting discovery and invocation of asynchronous web services,,Letter,vol81
pap1479,d3a4e5a31fc9383089f8bd39010bf86a3bf62eb0,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Bioinformatics analysis of microarray data.,,Erratum,pro73
pap1480,020ef4bc1b4a234015207ada1ea5d4e09636d493,con26,Decision Support Systems,BMC Bioinformatics BioMed Central Methodology article,,Erratum,pro26
pap1481,5070e30df0923b5661c80365273d21a12c517b20,jou81,BMC Bioinformatics,BIRI: a new approach for automatically discovering and indexing available public bioinformatics resources from the literature,,Article,vol81
pap1482,57c4f921e6ac7bf27a51fbbcb5ee1d160cf9bb98,jou261,Biopolymers,Protein secondary structure analyses from circular dichroism spectroscopy: methods and reference databases.,"Circular dichroism (CD) spectroscopy has been a valuable method for the analysis of protein secondary structures for many years. With the advent of synchrotron radiation circular dichroism (SRCD) and improvements in instrumentation for conventional CD, lower wavelength data are obtainable and the information content of the spectra increased. In addition, new computation and bioinformatics methods have been developed and new reference databases have been created, which greatly improve and facilitate the analyses of CD spectra. This article discusses recent developments in the analysis of protein secondary structures, including features of the DICHROWEB analysis webserver.",Conference paper,vol261
pap1483,fbc913faf39b1e369dfcdcfefb354d846a46573c,con50,International Workshop on Green and Sustainable Software,"Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond","From the Publisher: 
In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. 
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",Erratum,pro50
pap1484,55c51bcb8c0c26985386018ef6210539404ac6d3,jou262,Nature Genetics,"Multiancestry genome-wide association study of 520,000 subjects identifies 32 loci associated with stroke and stroke subtypes",,Conference paper,vol262
pap1485,bf19714930d10b4263b8d0b6966702a4785f5fd0,jou81,BMC Bioinformatics,A flexible R package for nonnegative matrix factorization,,Letter,vol81
pap1486,4b537a6953d4726ddd6ced4fe1ef4ca96296b597,jou103,Science,The Obesity-Associated FTO Gene Encodes a 2-Oxoglutarate-Dependent Nucleic Acid Demethylase,"Variants in the FTO (fat mass and obesity associated) gene are associated with increased body mass index in humans. Here, we show by bioinformatics analysis that FTO shares sequence motifs with Fe(II)- and 2-oxoglutarate–dependent oxygenases. We find that recombinant murine Fto catalyzes the Fe(II)- and 2OG-dependent demethylation of 3-methylthymine in single-stranded DNA, with concomitant production of succinate, formaldehyde, and carbon dioxide. Consistent with a potential role in nucleic acid demethylation, Fto localizes to the nucleus in transfected cells. Studies of wild-type mice indicate that Fto messenger RNA (mRNA) is most abundant in the brain, particularly in hypothalamic nuclei governing energy balance, and that Fto mRNA levels in the arcuate nucleus are regulated by feeding and fasting. Studies can now be directed toward determining the physiologically relevant FTO substrate and how nucleic acid methylation status is linked to increased fat mass.",Conference paper,vol103
pap1487,6426ee8528fb9996b60351a02a646e70b31d5938,con92,Human Language Technology - The Baltic Perspectiv,Soft Computing Methodologies in Bioinformatics,"Bioinformatics is a promising and innovative research field in 21st century. Despite of a high number of techniques specifically dedicated to bioinformatics problems as well as many successful applications, we are in the beginning of a process to massively integrate the aspects and experiences in the different core subjects such as biology, medicine, computer science, engineering, chemistry, physics, and mathematics. Recently the use of soft computing tools for solving bioinformatics problems have been gaining the attention of researchers because of their ability to handle imprecision, uncertainty in large and complex search spaces. The paper will focus on soft computing paradigm in bioinformatics with particular emphasis on integrative research.",Erratum,pro92
pap1488,eb9c924383aca0d137c736a41271d569a12cf518,con83,Networks,An Introduction to Conditional Random Fields,"Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.",Erratum,pro83
pap1489,6000d13f57b91b29061af94375bc25ccc960a3dd,jou263,Gut,"Epidemiological, clinical and virological characteristics of 74 cases of coronavirus-infected disease 2019 (COVID-19) with gastrointestinal symptoms","Objective The SARS-CoV-2-infected disease (COVID-19) outbreak is a major threat to human beings. Previous studies mainly focused on Wuhan and typical symptoms. We analysed 74 confirmed COVID-19 cases with GI symptoms in the Zhejiang province to determine epidemiological, clinical and virological characteristics. Design COVID-19 hospital patients were admitted in the Zhejiang province from 17 January 2020 to 8 February 2020. Epidemiological, demographic, clinical, laboratory, management and outcome data of patients with GI symptoms were analysed using multivariate analysis for risk of severe/critical type. Bioinformatics were used to analyse features of SARS-CoV-2 from Zhejiang province. Results Among enrolled 651 patients, 74 (11.4%) presented with at least one GI symptom (nausea, vomiting or diarrhoea), average age of 46.14 years, 4-day incubation period and 10.8% had pre-existing liver disease. Of patients with COVID-19 with GI symptoms, 17 (22.97%) and 23 (31.08%) had severe/critical types and family clustering, respectively, significantly higher than those without GI symptoms, 47 (8.14%) and 118 (20.45%). Of patients with COVID-19 with GI symptoms, 29 (39.19%), 23 (31.08%), 8 (10.81%) and 16 (21.62%) had significantly higher rates of fever >38.5°C, fatigue, shortness of breath and headache, respectively. Low-dose glucocorticoids and antibiotics were administered to 14.86% and 41.89% of patients, respectively. Sputum production and increased lactate dehydrogenase/glucose levels were risk factors for severe/critical type. Bioinformatics showed sequence mutation of SARS-CoV-2 with m6A methylation and changed binding capacity with ACE2. Conclusion We report COVID-19 cases with GI symptoms with novel features outside Wuhan. Attention to patients with COVID-19 with non-classic symptoms should increase to protect health providers.",Letter,vol263
pap1490,ad3f933d1fdb30d447d0cf543c3d882b341cc0e6,con110,Very Large Data Bases Conference,PRGdb: a bioinformatics platform for plant resistance gene analysis,"PRGdb is a web accessible open-source (http://www.prgdb.org) database that represents the first bioinformatic resource providing a comprehensive overview of resistance genes (R-genes) in plants. PRGdb holds more than 16 000 known and putative R-genes belonging to 192 plant species challenged by 115 different pathogens and linked with useful biological information. The complete database includes a set of 73 manually curated reference R-genes, 6308 putative R-genes collected from NCBI and 10463 computationally predicted putative R-genes. Thanks to a user-friendly interface, data can be examined using different query tools. A home-made prediction pipeline called Disease Resistance Analysis and Gene Orthology (DRAGO), based on reference R-gene sequence data, was developed to search for plant resistance genes in public datasets such as Unigene and Genbank. New putative R-gene classes containing unknown domain combinations were discovered and characterized. The development of the PRG platform represents an important starting point to conduct various experimental tasks. The inferred cross-link between genomic and phenotypic information allows access to a large body of information to find answers to several biological questions. The database structure also permits easy integration with other data types and opens up prospects for future implementations.",Erratum,pro110
pap1491,78d054b11a2f91623dffecc1f5a2d372ada0451f,con2,International Conference on Software Engineering,MorphoLibJ: integrated library and plugins for mathematical morphology with ImageJ,"MOTIVATION
Mathematical morphology (MM) provides many powerful operators for processing 2D and 3D images. However, most MM plugins currently implemented for the popular ImageJ/Fiji platform are limited to the processing of 2D images.


RESULTS
The MorphoLibJ library proposes a large collection of generic tools based on MM to process binary and grey-level 2D and 3D images, integrated into user-friendly plugins. We illustrate how MorphoLibJ can facilitate the exploitation of 3D images of plant tissues.


AVAILABILITY AND IMPLEMENTATION
MorphoLibJ is freely available at http://imagej.net/MorphoLibJ CONTACT: david.legland@nantes.inra.frSupplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro2
pap1492,b4e5ed0cb88ce0aa6eb453f1af6fb9f30f98c012,jou264,BMC Systems Biology,Bioinformatics strategies for lipidomics analysis: characterization of obesity related hepatic steatosis,,Letter,vol264
pap1493,31c5beeafccb48877530db023610e8604b1136ea,jou5,Genome Biology,GoMiner: a resource for biological interpretation of genomic and proteomic data,,Letter,vol5
pap1494,05aa2c49c418d2dc099cc945b6f51a845a6a316a,con78,Neural Information Processing Systems,BioJava: an open-source framework for bioinformatics,"Summary: BioJava is a mature open-source project that provides a framework for processing of biological data. BioJava contains powerful analysis and statistical routines, tools for parsing common file formats and packages for manipulating sequences and 3D structures. It enables rapid bioinformatics application development in the Java programming language. Availability: BioJava is an open-source project distributed under the Lesser GPL (LGPL). BioJava can be downloaded from the BioJava website (http://www.biojava.org). BioJava requires Java 1.5 or higher. Contact: andreas.prlic@gmail.com. All queries should be directed to the BioJava mailing lists. Details are available at http://biojava.org/wiki/BioJava:MailingLists.",Erratum,pro78
pap1495,97cd8334bff9b6325a3513d1557109b5e54299d8,con102,Annual Haifa Experimental Systems Conference,Web Services at the European Bioinformatics Institute,"We present a new version of the European Bioinformatics Institute Web Services, a complete suite of SOAP-based web tools for structural and functional analysis, with new and improved applications. New functionality has been added to most of the services already available, and an improved version of the underlying framework has allowed us to include more applications. Information on the EBI Web Services, tutorials and clients can be found at http://www.ebi.ac.uk/Tools/webservices.",Erratum,pro102
pap1496,3822821ec99fc8788a640820eb2a1027ed41e3c3,jou81,BMC Bioinformatics,Ontology-driven indexing of public datasets for translational bioinformatics,,Letter,vol81
pap1497,f4cc83f0ca55c503d74a36e7c0e276738ad07142,con1,International Conference on Human Factors in Computing Systems,From Protein Structure to Function with Bioinformatics,,Erratum,pro1
pap1498,cd49acefc8d51e324aa562e5337e1c2aff067053,con23,International Conference on Open and Big Data,An Overview of Multi-task Learning,"As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.",Erratum,pro23
pap1499,1c369241ef2f76d98d3f720189a91b4c9b4c9de8,con72,Bioinformatics and Computational Biology,Association Analysis Techniques for Bioinformatics Problems,,Letter,pro72
pap1500,1ee0abcb8f0afd74d602255d529d7c2a036a8f02,con97,ACM SIGMOD Conference,Graph Theory,,Erratum,pro97
pap1501,69906b74ee369cd25522ec432ecbc601a77c9d87,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Spectral graph theory,"Spectral graph theory is a vast and expanding area of combinatorics. We start these notes by introducing and motivating classical matrices associated with a graph, and then show how to derive combinatorial properties of a graph from the eigenvalues of these matrices. We then examine more modern results such as polynomial interlacing and high dimensional expanders",Erratum,pro42
pap1502,3aca80d2a6ec2014342c4abe6611d498c789f7fa,jou265,Symmetry,Graph Theory,Abstract,Article,vol265
pap1503,b07c157e7d40e06a4f2d486b16d5180d8b24acb9,con34,International Conference on Agile Software Development,Algebraic Graph Theory,,Erratum,pro34
pap1504,0a8bcccd8acc7ec899b57d4ba76db4ee21295092,con64,British Computer Society Conference on Human-Computer Interaction,Extremal Graph Theory,,Erratum,pro64
pap1505,6bc77c4dc6075ee81c05f0f5f43e44b2a34a5876,con84,Workshop on Interdisciplinary Software Engineering Research,Graph Theory with Applications,"When I first entered the world of Mathematics, I became aware of a strange and little-regarded sect of ""Graph Theorists"", inhabiting a shadowy borderland known to the rest of the community as the ""slums of Topology"". What changes there have been in a few short years! That shadowy borderland has become a thriving metropolis. International conferences on Graph Theory occur with almost embarrassing frequency. Journals on Graph Theory abound: I once counted the Editorial Offices of three of them in one of the mathematical departments of one of the Universities of one of the smaller cities of Canada. Any connection with Topology is likely to be firmly repudiated as soon as noted. I became aware of the burgeoning of Graph Theory when I studied the 1940 paper of Brooks, Smith, Stone and Tutte in the Duke Mathematical Journal, ostensibly on squared rectangles. They wrote of trees and Kirchhoffs Laws, of 3-connection and planarity, of duality and symmetry, of determinantal identities and coprime integers, ~ all in the Quest of the Perfect Square. I invariably recommend that paper to my students. ""Go to it"", I say, ""you will",Erratum,pro84
pap1506,415224a9aff759f6972189df8b7761dfd6a81154,jou266,Mathematical Gazette,Introduction to graph theory,"In graph theory, the term graph refers to a set of vertices and a set of edges. A vertex can be used to represent any object. Graphs may contain undirected or directed edges. An undirected edge is a set of two vertices. A directed edge is an ordered pair of two vertices where the edge goes from the first vertex to the second vertex. Graphs that contain directed edges are called directed graphs or digraphs.",Letter,vol266
pap1507,f59dbb2e58a39e298f04690d7a71972cf2e480ac,jou267,Oberwolfach Reports,Graph Theory,,Conference paper,vol267
pap1508,75d82765de0900fed1a7a073415d8f7c625f79e8,con104,Biometrics and Identity Management,Spectral Graph Theory,"Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.",Erratum,pro104
pap1509,c2f21a6b917286c7e904e0f168b53bbaa2bda4ba,jou268,Frontiers in Neuroscience,Application of Graph Theory for Identifying Connectivity Patterns in Human Brain Networks: A Systematic Review,"Background: Analysis of the human connectome using functional magnetic resonance imaging (fMRI) started in the mid-1990s and attracted increasing attention in attempts to discover the neural underpinnings of human cognition and neurological disorders. In general, brain connectivity patterns from fMRI data are classified as statistical dependencies (functional connectivity) or causal interactions (effective connectivity) among various neural units. Computational methods, especially graph theory-based methods, have recently played a significant role in understanding brain connectivity architecture. Objectives: Thanks to the emergence of graph theoretical analysis, the main purpose of the current paper is to systematically review how brain properties can emerge through the interactions of distinct neuronal units in various cognitive and neurological applications using fMRI. Moreover, this article provides an overview of the existing functional and effective connectivity methods used to construct the brain network, along with their advantages and pitfalls. Methods: In this systematic review, the databases Science Direct, Scopus, arXiv, Google Scholar, IEEE Xplore, PsycINFO, PubMed, and SpringerLink are employed for exploring the evolution of computational methods in human brain connectivity from 1990 to the present, focusing on graph theory. The Cochrane Collaboration's tool was used to assess the risk of bias in individual studies. Results: Our results show that graph theory and its implications in cognitive neuroscience have attracted the attention of researchers since 2009 (as the Human Connectome Project launched), because of their prominent capability in characterizing the behavior of complex brain systems. Although graph theoretical approach can be generally applied to either functional or effective connectivity patterns during rest or task performance, to date, most articles have focused on the resting-state functional connectivity. Conclusions: This review provides an insight into how to utilize graph theoretical measures to make neurobiological inferences regarding the mechanisms underlying human cognition and behavior as well as different brain disorders.",Letter,vol268
pap1510,bea139a35ffe458d7aec576b5e651cd8ac80e4d2,con106,International Conference on Mobile Data Management,Introduction to Graph Theory,"1. Fundamental Concepts. What Is a Graph? Paths, Cycles, and Trails. Vertex Degrees and Counting. Directed Graphs. 2. Trees and Distance. Basic Properties. Spanning Trees and Enumeration. Optimization and Trees. 3. Matchings and Factors. Matchings and Covers. Algorithms and Applications. Matchings in General Graphs. 4. Connectivity and Paths. Cuts and Connectivity. k-connected Graphs. Network Flow Problems. 5. Coloring of Graphs. Vertex Colorings and Upper Bounds. Structure of k-chromatic Graphs. Enumerative Aspects. 6. Planar Graphs. Embeddings and Euler's Formula. Characterization of Planar Graphs. Parameters of Planarity. 7. Edges and Cycles. Line Graphs and Edge-Coloring. Hamiltonian Cycles. Planarity, Coloring, and Cycles. 8. Additional Topics (Optional). Perfect Graphs. Matroids. Ramsey Theory. More Extremal Problems. Random Graphs. Eigenvalues of Graphs. Appendix A: Mathematical Background. Appendix B: Optimization and Complexity. Appendix C: Hints for Selected Exercises. Appendix D: Glossary of Terms. Appendix E: Supplemental Reading. Appendix F: References. Indices.",Erratum,pro106
pap1511,18f7d9eb515e3f93fafc887cf0080ea2dfe1f9fa,jou269,Quality & Quantity: International Journal of Methodology,"Dean Lusher, Johan Koskinen, Garry Robins. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. New York: Cambridge University Press, 2012. 360 pp, $36.99 (pbk), ISBN: 9780521141383",,Article,vol269
pap1512,2e75dd6fb569ee917b6571d89afadd980af499c0,jou270,Frontiers in Bioengineering and Biotechnology,A Guide to Conquer the Biological Network Era Using Graph Theory,"Networks are one of the most common ways to represent biological systems as complex sets of binary interactions or relations between different bioentities. In this article, we discuss the basic graph theory concepts and the various graph types, as well as the available data structures for storing and reading graphs. In addition, we describe several network properties and we highlight some of the widely used network topological features. We briefly mention the network patterns, motifs and models, and we further comment on the types of biological and biomedical networks along with their corresponding computer- and human-readable file formats. Finally, we discuss a variety of algorithms and metrics for network analyses regarding graph drawing, clustering, visualization, link prediction, perturbation, and network alignment as well as the current state-of-the-art tools. We expect this review to reach a very broad spectrum of readers varying from experts to beginners while encouraging them to enhance the field further.",Article,vol270
pap1513,92b7248f2b6eb568d0c69a5e89bf33cfe225cd92,jou271,Biotechnology and Bioprocessing,Revisiting the paper on “Applications of graph theory to enzyme kinetics and protein folding kinetics: steady and non-steady state systems”,About 30 years ago a very important paper on “Applications of graph theory to enzyme kinetics and protein folding kinetics: steady and non-steady state systems”,Conference paper,vol271
pap1514,455416dd3b2b34786cda6ce86740a6357a787ff3,con4,Conference on Innovative Data Systems Research,Graph Theory: A Comprehensive Survey about Graph Theory Applications in Computer Science and Social Networks,"Graph theory (GT) concepts are potentially applicable in the field of computer science (CS) for many purposes. The unique applications of GT in the CS field such as clustering of web documents, cryptography, and analyzing an algorithm’s execution, among others, are promising applications. Furthermore, GT concepts can be employed to electronic circuit simplifications and analysis. Recently, graphs have been extensively used in social networks (SNs) for many purposes related to modelling and analysis of the SN structures, SN operation modelling, SN user analysis, and many other related aspects. Considering the widespread applications of GT in SNs, this article comprehensively summarizes GT use in the SNs. The goal of this survey paper is twofold. First, we briefly discuss the potential applications of GT in the CS field along with practical examples. Second, we explain the GT uses in the SNs with sufficient concepts and examples to demonstrate the significance of graphs in SN modeling and analysis.",Erratum,pro4
pap1515,6acabf73828a3e137a0e26673726834fef99d04a,jou272,IEEE Access,A Graph Theory-Based Modeling of Functional Brain Connectivity Based on EEG: A Systematic Review in the Context of Neuroergonomics,"Graph theory analysis, a mathematical approach, has been applied in brain connectivity studies to explore the organization of network patterns. The computation of graph theory metrics enables the characterization of the stationary behavior of electroencephalogram (EEG) signals that cannot be explained by simple linear methods. The main purpose of this study was to systematically review the graph theory applications for mapping the functional connectivity of the EEG data in neuroergonomics. Moreover, this article proposes a pipeline for constructing an unweighted functional brain network from EEG data using both source and sensor methods. Out of 57 articles, our results show that graph theory metrics used to characterize EEG data have attracted increasing attention since 2006, with the highest frequency of publications in 2018. Most studies have focused on cognitive tasks in comparison with motor tasks. The mean phase coherence method, based on the “phase-locking value,” was the most frequently used functional estimation technique in the reviewed studies. Furthermore, the unweighted functional brain network has received substantially more attention in the literature than the weighted network. The global clustering coefficient and characteristic path length were the most prevalent metrics for differentiating between global integration and local segregation, and the small-worldness property emerged as a compelling metric for the characterization of information processing. This review provides insight into the use of graph theory metrics to model functional brain connectivity in the context of neuroergonomics research.",Letter,vol272
pap1516,73dd81990333e603469fbab07958fbb7e47d302f,jou273,Dialogues in Clinical Neuroscience,Graph theory methods: applications in brain networks,"Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.",Article,vol273
pap1517,bc4aa2998973727ea84d91141a0175bbf1ff4923,con11,European Conference on Modelling and Simulation,Graph Theory,,Erratum,pro11
pap1518,56a13467a3cfb7a9ec00b7f3ed5e953324225233,con65,IEEE International Conference on Software Engineering and Formal Methods,Graph Theory with Applications,,Erratum,pro65
pap1519,321fede71792a304d44d8143399309c5d88c73a1,con0,International Conference on Machine Learning,Graph Theory and Its Applications,,Erratum,pro0
pap1520,d96d7c7dd979ad51b04fe06e32904bece29696fd,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication","Algebraic Graph Theory: Morphisms, Monoids and Matrices","This is a highly self-contained book about algebraic graph theory which iswritten with a view to keep the lively and unconventional atmosphere of a spoken text to communicate the enthusiasm the author feels about this subject. The focus is on homomorphisms and endomorphisms, matrices and eigenvalues. Graph models are extremely useful for almost all applications and applicators as they play an important role as structuring tools. They allow to model net structures -like roads, computers, telephones -instances of abstract data structures -likelists, stacks, trees -and functional or object oriented programming.",Erratum,pro52
pap1521,ee4fd9cd27836870dd18eb2d81efac596a758fb1,jou274,Journal of manufacturing science and engineering,Thermal Modeling in Metal Additive Manufacturing Using Graph Theory,"The goal of this work is to predict the effect of part geometry and process parameters on the instantaneous spatiotemporal distribution of temperature, also called the thermal field or temperature history, in metal parts as they are being built layer-by-layer using additive manufacturing (AM) processes. In pursuit of this goal, the objective of this work is to develop and verify a graph theory-based approach for predicting the temperature distribution in metal AM parts. This objective is consequential to overcome the current poor process consistency and part quality in AM. One of the main reasons for poor part quality in metal AM processes is ascribed to the nature of temperature distribution in the part. For instance, steep thermal gradients created in the part during printing leads to defects, such as warping and thermal stress-induced cracking. Existing nonproprietary approaches to predict the temperature distribution in AM parts predominantly use mesh-based finite element analyses that are computationally tortuous—the simulation of a few layers typically requires several hours, if not days. Hence, to alleviate these challenges in metal AM processes, there is a need for efficient computational models to predict the temperature distribution, and thereby guide part design and selection of process parameters instead of expensive empirical testing. Compared with finite element analyses techniques, the proposed mesh-free graph theory-based approach facilitates prediction of the temperature distribution within a few minutes on a desktop computer. To explore these assertions, we conducted the following two studies: (1) comparing the heat diffusion trends predicted using the graph theory approach with finite element analysis, and analytical heat transfer calculations based on Green’s functions for an elementary cuboid geometry which is subjected to an impulse heat input in a certain part of its volume and (2) simulating the laser powder bed fusion metal AM of three-part geometries with (a) Goldak’s moving heat source finite element method, (b) the proposed graph theory approach, and (c) further comparing the thermal trends predicted from the last two approaches with a commercial solution. From the first study, we report that the thermal trends approximated by the graph theory approach are found to be accurate within 5% of the Green’s functions-based analytical solution (in terms of the symmetric mean absolute percentage error). Results from the second study show that the thermal trends predicted for the AM parts using graph theory approach agree with finite element analyses, and the computational time for predicting the temperature distribution was significantly reduced with graph theory. For instance, for one of the AM part geometries studied, the temperature trends were predicted in less than 18 min within 10% error using the graph theory approach compared with over 180 min with finite element analyses. Although this paper is restricted to theoretical development and verification of the graph theory approach, our forthcoming research will focus on experimental validation through in-process thermal measurements.",Article,vol274
pap1522,85f51e3d3173a30e4bcfa3640354f940303d7023,jou88,bioRxiv,Graph theory approaches to functional network organization in brain disorders: A critique for a brave new small-world,"Over the past two decades, resting-state functional connectivity (RSFC) methods have provided new insights into the network organization of the human brain. Studies of brain disorders such as Alzheimer’s disease or depression have adapted tools from graph theory to characterize differences between healthy and patient populations. Here, we conducted a review of clinical network neuroscience, summarizing methodological details from 106 RSFC studies. Although this approach is prevalent and promising, our review identified four challenges. First, the composition of networks varied remarkably in terms of region parcellation and edge definition, which are fundamental to graph analyses. Second, many studies equated the number of connections across graphs, but this is conceptually problematic in clinical populations and may induce spurious group differences. Third, few graph metrics were reported in common, precluding meta-analyses. Fourth, some studies tested hypotheses at one level of the graph without a clear neurobiological rationale or considering how findings at one level (e.g., global topology) are contextualized by another (e.g., modular structure). Based on these themes, we conducted network simulations to demonstrate the impact of specific methodological decisions on case-control comparisons. Finally, we offer suggestions for promoting convergence across clinical studies in order to facilitate progress in this important field.",Conference paper,vol88
pap1523,72744fb5963b168b4590e5eaa148c8d0e5eafe38,jou182,Proceedings of the IEEE,"Electrical Networks and Algebraic Graph Theory: Models, Properties, and Applications","Algebraic graph theory is a cornerstone in the study of electrical networks ranging from miniature integrated circuits to continental-scale power systems. Conversely, many fundamental results of algebraic graph theory were laid out by early electrical circuit analysts. In this paper, we survey some fundamental and historic as well as recent results on how algebraic graph theory informs electrical network analysis, dynamics, and design. In particular, we review the algebraic and spectral properties of graph adjacency, Laplacian, incidence, and resistance matrices and how they relate to the analysis, network reduction, and dynamics of certain classes of electrical networks. We study these relations for models of increasing complexity ranging from static resistive direct current (dc) circuits, over dynamic resistor..inductor..capacitor (RLC) circuits, to nonlinear alternating current (ac) power flow. We conclude this paper by presenting a set of fundamental open questions at the intersection of algebraic graph theory and electrical networks.",Letter,vol182
pap1524,78793b5f9394379aab1c84c782bdbef819667d56,con50,International Workshop on Green and Sustainable Software,Graph Theory,"Mathematics acts an important and essential need in different fields. One of the significant roles in mathematics is played by graph theory that is used in structural models and innovative methods, models in various disciplines for better strategic decisions. In mathematics, graph theory is the study through graphs by which the structural relationship studied with a pair wise relationship between different objects. The different types of network theory or models or model of the network are called graphs. These graphs do not form a part of analytical geometry, but they are called graph theory, which is points connected by lines. The various concepts of graph theory have varied applications in diverse fields. The chapter will deal with graph theory and its application in various financial market decisions. The topological properties of the network of stocks will provide a deeper understanding and a good conclusion to the market structure and connectivity. The chapter is very useful for academicians, market researchers, financial analysts, and economists.",Erratum,pro50
pap1525,16b8b3a7840e9b61756db16097e2a8b1d18b8bf6,con103,IEEE International Conference on Multimedia and Expo,Graph Theory,"Mathematics acts an important and essential need in different fields. One of the significant roles in mathematics is played by graph theory that is used in structural models and innovative methods, models in various disciplines for better strategic decisions. In mathematics, graph theory is the study through graphs by which the structural relationship studied with a pair wise relationship between different objects. The different types of network theory or models or model of the network are called graphs. These graphs do not form a part of analytical geometry, but they are called graph theory, which is points connected by lines. The various concepts of graph theory have varied applications in diverse fields. The chapter will deal with graph theory and its application in various financial market decisions. The topological properties of the network of stocks will provide a deeper understanding and a good conclusion to the market structure and connectivity. The chapter is very useful for academicians, market researchers, financial analysts, and economists.",Erratum,pro103
pap1526,6acfec1c7361e88dd96ce60676cfed4c82a6e40a,jou267,Oberwolfach Reports,Graph Theory,"Book file PDF easily for everyone and every device. You can download and read online Graph Theory file PDF Book only if you are registered here. And also you can download or read online all Book PDF file that related with Graph Theory book. Happy reading Graph Theory Bookeveryone. Download file Free Book PDF Graph Theory at Complete PDF Library. This Book have some digital formats such us :paperbook, ebook, kindle, epub, fb2 and another formats. Here is The Complete PDF Book Library. It's free to register here to get Book file PDF Graph Theory.",Conference paper,vol267
pap1527,756bd8b609f9754fee6ae9e9056f5face4386726,jou88,bioRxiv,BRAPH: A graph theory software for the analysis of brain connectivity,"The brain is a large-scale complex network whose workings rely on the interaction between its various regions. In the past few years, the organization of the human brain network has been studied extensively using concepts from graph theory, where the brain is represented as a set of nodes connected by edges. This representation of the brain as a connectome can be used to assess important measures that reflect its topological architecture. We have developed a freeware MatLab-based software (BRAPH – BRain Analysis using graPH theory) for connectivity analysis of brain networks derived from structural magnetic resonance imaging (MRI), functional MRI (fMRI), positron emission tomography (PET) and electroencephalogram (EEG) data. BRAPH allows building connectivity matrices, calculating global and local network measures, performing non-parametric permutations for group comparisons, assessing the modules in the network, and comparing the results to random networks. By contrast to other toolboxes, it allows performing longitudinal comparisons of the same patients across different points in time. Furthermore, even though a user-friendly interface is provided, the architecture of the program is modular (object-oriented) so that it can be easily expanded and customized. To demonstrate the abilities of BRAPH, we performed structural and functional graph theory analyses in two separate studies. In the first study, using MRI data, we assessed the differences in global and nodal network topology in healthy controls, patients with amnestic mild cognitive impairment, and patients with Alzheimer’s disease. In the second study, using resting-state fMRI data, we compared healthy controls and Parkinson’s patients with mild cognitive impairment.",Letter,vol88
pap1528,aec03b62900277709b10793a168058c5d05fd34f,jou275,Bulletin of the Australian Mathematical Society,A GENERAL POSITION PROBLEM IN GRAPH THEORY,"The paper introduces a graph theory variation of the general position problem: given a graph $G$ , determine a largest set $S$ of vertices of $G$ such that no three vertices of $S$ lie on a common geodesic. Such a set is a max-gp-set of $G$ and its size is the gp-number $\text{gp}(G)$ of $G$ . Upper bounds on $\text{gp}(G)$ in terms of different isometric covers are given and used to determine the gp-number of several classes of graphs. Connections between general position sets and packings are investigated and used to give lower bounds on the gp-number. It is also proved that the general position problem is NP-complete.",Letter,vol275
pap1529,968ea337e15004a2ed3e1442d7f632a1214e2268,jou276,Engineering computations,Novel reliable routing method for engineering of internet of vehicles based on graph theory,"
Purpose
The communication link in the engineering of Internet of Vehicle (IOV) is more frequent than the communication link in the Mobile ad hoc Network (MANET). Therefore, the highly dynamic network routing reliability problem is a research hotspot to be solved.


Design/methodology/approach
The graph theory is used to model the MANET communication diagram on the highway and propose a new reliable routing method for internet of vehicles based on graph theory.


Findings
The expanded graph theory can help capture the evolution characteristics of the network topology and predetermine the reliable route to promote quality of service (QoS) in the routing process. The program can find the most reliable route from source to the destination from the MANET graph theory.


Originality/value
The good performance of the proposed method is verified and compared with the related algorithms of the literature.
",Conference paper,vol276
pap1530,6097094b745b265de8dad6eaf86f7d832e4e7695,con79,IEEE Annual Symposium on Foundations of Computer Science,Spectral Graph Theory and its Applications,"Spectral graph theory is the study of the eigenvalues and eigenvectors of matrices associated with graphs. In this tutorial, we will try to provide some intuition as to why these eigenvectors and eigenvalues have combinatorial significance, and will sitn'ey some of their applications.",Conference paper,pro79
pap1531,e1274867d404fd1dbf73a654c217ea5c0c32852c,jou277,The Neuroscientist,Graph Theory and Brain Connectivity in Alzheimer’s Disease,This article presents a review of recent advances in neuroscience research in the specific area of brain connectivity as a potential biomarker of Alzheimer’s disease with a focus on the application of graph theory. The review will begin with a brief overview of connectivity and graph theory. Then resent advances in connectivity as a biomarker for Alzheimer’s disease will be presented and analyzed.,Letter,vol277
pap1532,ecaa9d581d26cb23a759ca1310b0a5d8ce27b7b2,con3,Knowledge Discovery and Data Mining,Topics in graph theory,"A graph is a system G = (V, E) consisting of a set V of vertices and a set E (disjoint from V ) of edges, together with an incidence function End : E → M2(V ), where M2(V ) is set of all 2-element sub-multisets of V . We usually write V = V (G), E = E(G), and End = EndG. For each edge e ∈ E with End(e) = {u, v}, we called u, v the end-vertices of e, and say that the edge e is incident with the vertices u, v, or the vertices u, v are incident with the edge e, or the vertices u, v are adjacent by the edge e. Sometimes it is more convenient to just write the incidence relation as e = uv. If u = v, the edge e is called a loop; if u 6= v, the edge is called a link. Two edges are said to be parallel if their end vertices are the same. Parallel edges are also referred to multiple edges. A simple graph is a graph without loops and multiple edges. When we emphasize that a graph may have loops and multiple edges, we refer the graph as a multigraph. A graph is said to be (i) finite if it has finite number of vertices and edges; (ii) null if it has no vertices, and consequently has no edges; (iii) trivial if it has only one vertex with possible loops; (iv) empty if its has no edges; and (v) nontrivial if it is not trivial. A complete graph is a simple graph that every pair of vertices are adjacent. A complete graph with n vertices is denoted by Kn. A graph G is said to be bipartite if its vertex set V (G) can be partitioned into two disjoint nonempty parts X,Y such that every edge has one end-vertex in X and the other in Y ; such a partition {X,Y } is called a bipartition of G, and such a bipartite graph is denoted by G[X,Y ]. A bipartite graph G[X,Y ] is called a complete bipartite graph if each vertex in X is joined to every vertex in Y ; we abbreviate G[X,Y ] to Km,n if |X| = m and |Y | = n. Let G be a graph. Two vertices of G are called neighbors each other if they are adjacent. For each vertex v ∈ V (G), the set of neighbors of v in G is denoted by Nv(G), the number of edges incident with v (loops counted twice) is called the degree of v in G, denoted deg (v) or deg G(v). A vertex of degree 0 is called an isolated vertex; a vertex of degree 1 is called a leaf. A graph is said to be regular if its every vertex has the same degree. A graph is said to be k-regular if its every vertex has degree k. We always have",Erratum,pro3
pap1533,f66b6c35b1820dfb39dfbda11fa4040913c1bb85,con4,Conference on Innovative Data Systems Research,Graph Theory,,Erratum,pro4
pap1534,b9d26450520aa98e6ccb1b2e11a48fec29f273b9,con7,International Symposium on Intelligent Data Analysis,Cytoscape.js: a graph theory library for visualisation and analysis,"Summary: Cytoscape.js is an open-source JavaScript-based graph library. Its most common use case is as a visualization software component, so it can be used to render interactive graphs in a web browser. It also can be used in a headless manner, useful for graph operations on a server, such as Node.js. Availability and implementation: Cytoscape.js is implemented in JavaScript. Documentation, downloads and source code are available at http://js.cytoscape.org. Contact: gary.bader@utoronto.ca",Erratum,pro7
pap1535,fc9051185b4879606ed006d00055940aee4da5e1,con10,Americas Conference on Information Systems,Modern Graph Theory,,Erratum,pro10
pap1536,0e63d3bea6a258017c43bb6c91f923839d260c8f,con12,The Compass,Graph Theory,,Erratum,pro12
pap1537,12748e904f5025ca1757ce49d72cad3878e1be8f,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Graph Theory-Based Pinning Synchronization of Stochastic Complex Dynamical Networks,"This paper is concerned with the adaptive pinning synchronization problem of stochastic complex dynamical networks (CDNs). Based on algebraic graph theory and Lyapunov theory, pinning controller design conditions are derived, and the rigorous convergence analysis of synchronization errors in the probability sense is also conducted. Compared with the existing results, the topology structures of stochastic CDN are allowed to be unknown due to the use of graph theory. In particular, it is shown that the selection of nodes for pinning depends on the unknown lower bounds of coupling strengths. Finally, an example on a Chua’s circuit network is given to validate the effectiveness of the theoretical results.",Erratum,pro42
pap1538,2f1214615605b298733c008ffbd8a79051992473,con69,Formal Concept Analysis,Fuzzy Graph Theory,,Erratum,pro69
pap1539,f8dab78b4ddd64a83bbc62cb153161a033dc06d1,jou278,Saudi Journal of Biological Sciences,Study of biological networks using graph theory,,Conference paper,vol278
pap1540,74eb4d6abf1d0236be338c1bd5ee59a498b961b1,con35,IEEE Working Conference on Mining Software Repositories,Band connectivity for topological quantum chemistry: Band structures as a graph theory problem,"The conventional theory of solids is well suited to describing band structures locally near isolated points in momentum space, but struggles to capture the full, global picture necessary for understanding topological phenomena. In part of a recent paper [B. Bradlyn et al., Nature 547, 298 (2017)], we have introduced the way to overcome this difficulty by formulating the problem of sewing together many disconnected local ""k-dot-p"" band structures across the Brillouin zone in terms of graph theory. In the current manuscript we give the details of our full theoretical construction. We show that crystal symmetries strongly constrain the allowed connectivities of energy bands, and we employ graph-theoretic techniques such as graph connectivity to enumerate all the solutions to these constraints. The tools of graph theory allow us to identify disconnected groups of bands in these solutions, and so identify topologically distinct insulating phases.",Erratum,pro35
pap1541,be0e61c17d574fd5a29716d0cf57759b75470e67,con60,European Conference on Software Process Improvement,Introduction to Chemical Graph Theory,,Erratum,pro60
pap1542,7d08932d11d5cf03ac2725250e83ec68f5a0f878,con68,Experimental Software Engineering Network,Graph Theory 1736 1936,"Thank you very much for downloading graph theory 1736 1936. Maybe you have knowledge that, people have search hundreds times for their favorite readings like this graph theory 1736 1936, but end up in malicious downloads. Rather than enjoying a good book with a cup of coffee in the afternoon, instead they cope with some malicious bugs inside their laptop. graph theory 1736 1936 is available in our digital library an online access to it is set as public so you can get it instantly. Our books collection spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the graph theory 1736 1936 is universally compatible with any devices to read.",Erratum,pro68
pap1543,abb780f85a1a27919e461efdcc2a77cf1ab46c8b,con72,Bioinformatics and Computational Biology,Applying Graph Theory in Ecological Research,"Graph theory can be applied to ecological questions in many ways, and more insights can be gained by expanding the range of graph theoretical concepts applied to a specific system. But how do you know which methods might be used? And what do you do with the graph once it has been obtained? This book provides a broad introduction to the application of graph theory in different ecological systems, providing practical guidance for researchers in ecology and related fields. Readers are guided through the creation of an appropriate graph for the system being studied, including the application of spatial, spatio-temporal, and more abstract structural process graphs. Simple figures accompany the explanations to add clarity, and a broad range of ecological phenomena from many ecological systems are covered. This is the ideal book for graduate students and researchers looking to apply graph theoretical methods in their work.",Erratum,pro72
pap1544,73be8ad32db73d4f4f50c5dd96d71bdfb02ea9bb,con104,Biometrics and Identity Management,Algorithmic graph theory and perfect graphs,,Erratum,pro104
pap1545,ea84995e21bf19d8deecc1c88d0b47be83ddb267,con68,Experimental Software Engineering Network,Topics in Chromatic Graph Theory: Colouring random graphs,"How many colours are typically necessary to colour a graph? We survey a number of perspectives on this natural question, which is central to random graph theory and to probabilistic and extremal combinatorics. It has stimulated a vibrant area of research, with a rich history extending back through more than half a century. Erdős and Rényi [36] asked a form of this question in a celebrated early paper on random graphs in 1960. Let Gn,m be a graph chosen uniformly at random from the set of graphs with vertex-set [n] = {1, 2, . . . , n} and m edges. In this probabilistic model, we cannot rule out the possibility that Gn,m is, for example, the disjoint union of one large clique and some isolated vertices, or perhaps one Turán graph (a balanced complete multipartite graph) and some isolated vertices. The resulting range is large: in the former situation the chromatic number could be about √ 2m, while in the latter it could be 2 if m ≤ n2/4. These outcomes are unlikely, however, and we are interested in the most probable ones. To state the question properly, we say that an event An (which here always describes a property of a random graph on the vertex-set [n]) holds asymptotically almost surely (a.a.s.) if the probability that An holds satisfies P(An)→ 1 as n→∞. Erdős and Rényi asked the following question. Suppose that m ∼ cn for some c ≥ 12 . Is there a positive integer function f = fc(n) for which χ(Gn,m) = f a.a.s., and if so how large is it?",Erratum,pro68
pap1546,36e7356a12554d8af87094a9ca245b49b4f4a5c4,con8,Frontiers in Education Conference,Basic Graph Theory,,Erratum,pro8
pap1547,8e8152d46c8ff1070805096c214df7f389c57b80,con34,International Conference on Agile Software Development,Wavelets on Graphs via Spectral Graph Theory,,Erratum,pro34
pap1548,583c47a08841b662c661973951af40045cec3088,jou279,Journal of Neuroscience Research,"Network science and the human brain: Using graph theory to understand the brain and one of its hubs, the amygdala, in health and disease","Over the past 15 years, the emerging field of network science has revealed the key features of brain networks, which include small‐world topology, the presence of highly connected hubs, and hierarchical modularity. The value of network studies of the brain is underscored by the range of network alterations that have been identified in neurological and psychiatric disorders, including epilepsy, depression, Alzheimer's disease, schizophrenia, and many others. Here we briefly summarize the concepts of graph theory that are used to quantify network properties and describe common experimental approaches for analysis of brain networks of structural and functional connectivity. These range from tract tracing to functional magnetic resonance imaging, diffusion tensor imaging, electroencephalography, and magnetoencephalography. We then summarize the major findings from the application of graph theory to nervous systems ranging from Caenorhabditis elegans to more complex primate brains, including man. Focusing, then, on studies involving the amygdala, a brain region that has attracted intense interest as a center for emotional processing, fear, and motivation, we discuss the features of the amygdala in brain networks for fear conditioning and emotional perception. Finally, to highlight the utility of graph theory for studying dysfunction of the amygdala in mental illness, we review data with regard to changes in the hub properties of the amygdala in brain networks of patients with depression. We suggest that network studies of the human brain may serve to focus attention on regions and connections that act as principal drivers and controllers of brain function in health and disease.†Published 2016",Article,vol279
pap1549,e3edfc60099cf92b32c1927c3306219b73e9d3d6,jou280,Information Sciences,Quantitative Graph Theory: A new branch of graph theory and network science,,Letter,vol280
pap1550,f6c5a2a6cccae1db5f0a753946a2e89d7d22aa5d,con53,Workshop on Web 2.0 for Software Engineering,"Handbook of graph theory, combinatorial optimization, and algorithms","Basic Concepts and Algorithms Basic Concepts in Graph Theory and Algorithms Subramanian Arumugam and Krishnaiyan ""KT"" Thulasiraman Basic Graph Algorithms Krishnaiyan ""KT"" Thulasiraman Depth-First Search and Applications Krishnaiyan ""KT"" Thulasiraman Flows in Networks Maximum Flow Problem F. Zeynep Sargut, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Minimum Cost Flow Problem Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Multi-Commodity Flows Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Algebraic Graph Theory Graphs and Vector Spaces Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Incidence, Cut, and Circuit Matrices of a Graph Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Matrix and Signal Flow Graphs Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Spectrum and the Laplacian Spectrum of a Graph R. Balakrishnan Resistance Networks, Random Walks, and Network Theorems Krishnaiyan ""KT"" Thulasiraman and Mamta Yadav Structural Graph Theory Connectivity Subramanian Arumugam and Karam Ebadi Connectivity Algorithms Krishnaiyan ""KT"" Thulasiraman Graph Connectivity Augmentation Andras Frank and Tibor Jordan Matchings Michael D. Plummer Matching Algorithms Krishnaiyan ""KT"" Thulasiraman Stable Marriage Problem Shuichi Miyazaki Domination in Graphs Subramanian Arumugam and M. Sundarakannan Graph Colorings Subramanian Arumugam and K. Raja Chandrasekar Planar Graphs Planarity and Duality Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Edge Addition Planarity Testing Algorithm John M. Boyer Planarity Testing Based on PC-Trees Wen-Lian Hsu Graph Drawing Md. Saidur Rahman and Takao Nishizeki Interconnection Networks Introduction to Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Cayley Graphs S. Lakshmivarahan, Lavanya Sivakumar, and S.K. Dhall Graph Embedding and Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Special Graphs Program Graphs Krishnaiyan ""KT"" Thulasiraman Perfect Graphs Chinh T. Hoang and R. Sritharan Tree-Structured Graphs Andreas Brandstadt and Feodor F. Dragan Partitioning Graph and Hypergraph Partitioning Sachin B. Patkar and H. Narayanan Matroids Matroids H. Narayanan and Sachin B. Patkar Hybrid Analysis and Combinatorial Optimization H. Narayanan Probabilistic Methods, Random Graph Models, and Randomized Algorithms Probabilistic Arguments in Combinatorics C.R. Subramanian Random Models and Analyses for Chemical Graphs Daniel Pascua, Tina M. Kouri, and Dinesh P. Mehta Randomized Graph Algorithms: Techniques and Analysis Surender Baswana and Sandeep Sen Coping with NP-Completeness General Techniques for Combinatorial Approximation Sartaj Sahni epsilon-Approximation Schemes for the Constrained Shortest Path Problem Krishnaiyan ""KT"" Thulasiraman Constrained Shortest Path Problem: Lagrangian Relaxation-Based Algorithmic Approaches Ying Xiao and Krishnaiyan ""KT"" Thulasiraman Algorithms for Finding Disjoint Paths with QoS Constraints Alex Sprintson and Ariel Orda Set-Cover Approximation Neal E. Young Approximation Schemes for Fractional Multicommodity Flow Problems George Karakostas Approximation Algorithms for Connectivity Problems Ramakrishna Thurimella Rectilinear Steiner Minimum Trees Tao Huang and Evangeline F.Y. Young Fixed-Parameter Algorithms and Complexity Venkatesh Raman and Saket Saurabh",Erratum,pro53
pap1551,37c0809e246e593d524fa8a918eaee661124c21f,con50,International Workshop on Green and Sustainable Software,An Introduction to Bipolar Single Valued Neutrosophic Graph Theory,"In this paper, we first define the concept of bipolar single neutrosophic graphs as the generalization of bipolar fuzzy graphs, N-graphs, intuitionistic fuzzy graph, single valued neutrosophic graphs and bipolar intuitionistic fuzzy graphs.",Erratum,pro50
pap1552,d66c93134b8345d76c6d4d25cab52abb160fee67,con15,Pacific Symposium on Biocomputing,A Brief Introduction to Spectral Graph Theory,"Spectral graph theory starts by associating matrices to graphs, notably, the adjacency matrix and the laplacian matrix. The general theme is then, firstly, to compute or estimate the eigenvalues of such matrices, and secondly, to relate the eigenvalues to structural properties of graphs. As it turns out, the spectral perspective is a powerful tool. Some of its loveliest applications concern facts that are, in principle, purely graph-theoretic or combinatorial. To give just one example, spectral ideas are a key ingredient in the proof of the so-called Friendship Theorem: if, in a group of people, any two persons have exactly one common friend, then there is a person who is everybody’s friend. This text is an introduction to spectral graph theory, but it could also be seen as an invitation to algebraic graph theory. On the one hand, there is, of course, the linear algebra that underlies the spectral ideas in graph theory. On the other hand, most of our examples are graphs of algebraic origin. The two recurring sources are Cayley graphs of groups, and graphs built out of finite fields. In the study of such graphs, some further algebraic ingredients (e.g., characters) naturally come up. The table of contents gives, as it should, a good glimpse of where is this text going. Very broadly, the first half is devoted to graphs, finite fields, and how they come together. This part is meant as an appealing and meaningful motivation. It provides a context that frames and fuels much of the second, spectral, half. Most sections have one or two exercises. Their position within the text is a hint. The exercises are optional, in the sense that virtually nothing in the main body depends on them. But the exercises are often of the non-trivial variety, and they should enhance the text in an interesting way. The hope is that the reader will enjoy them. We assume a basic familiarity with linear algebra, finite fields, and groups, but not necessarily with graph theory. This, again, betrays our algebraic perspective. This text is based on a course I taught in Göttingen, in the Fall of 2015. I would like to thank Jerome Baum for his help with some of the drawings. The present version is preliminary, and comments are welcome (email: bogdan.nica@gmail.com).",Erratum,pro15
pap1553,24987e91a046a71de3cf1017cf6eb907a8f7a444,jou281,Frontiers in Psychiatry,Support Vector Machine Classification of Major Depressive Disorder Using Diffusion-Weighted Neuroimaging and Graph Theory,"Recently, there has been considerable interest in understanding brain networks in major depressive disorder (MDD). Neural pathways can be tracked in the living brain using diffusion-weighted imaging (DWI); graph theory can then be used to study properties of the resulting fiber networks. To date, global abnormalities have not been reported in tractography-based graph metrics in MDD, so we used a machine learning approach based on “support vector machines” to differentiate depressed from healthy individuals based on multiple brain network properties. We also assessed how important specific graph metrics were for this differentiation. Finally, we conducted a local graph analysis to identify abnormal connectivity at specific nodes of the network. We were able to classify depression using whole-brain graph metrics. Small-worldness was the most useful graph metric for classification. The right pars orbitalis, right inferior parietal cortex, and left rostral anterior cingulate all showed abnormal network connectivity in MDD. This is the first use of structural global graph metrics to classify depressed individuals. These findings highlight the importance of future research to understand network properties in depression across imaging modalities, improve classification results, and relate network alterations to psychiatric symptoms, medication, and comorbidities.",Article,vol281
pap1554,cfc104f686b2190f64db03c8933d8f2a7fff5a5d,con100,International Conference on Automatic Face and Gesture Recognition,Molecular Orbital Calculations Using Chemical Graph Theory,"molecular orbital calculations using chemical graph theory is available in our digital library an online access to it is set as public so you can get it instantly. Our book servers hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the molecular orbital calculations using chemical graph theory is universally compatible with any devices to read.",Erratum,pro100
pap1555,e9b90c75c5c99353a43cbf5e083334a59e50cca8,con77,International Conference on Artificial Neural Networks,Rational exponents in extremal graph theory,"Given a family of graphs $\mathcal{H}$, the extremal number $\textrm{ex}(n, \mathcal{H})$ is the largest $m$ for which there exists a graph with $n$ vertices and $m$ edges containing no graph from the family $\mathcal{H}$ as a subgraph. We show that for every rational number $r$ between $1$ and $2$, there is a family of graphs $\mathcal{H}_r$ such that $\textrm{ex}(n, \mathcal{H}_r) = \Theta(n^r)$. This solves a longstanding problem in the area of extremal graph theory.",Erratum,pro77
pap1556,788eab78c4c6e998fc6812b7d7af7ce6505af2e4,con68,Experimental Software Engineering Network,Graph Theory And Sparse Matrix Computation,"Thank you for downloading graph theory and sparse matrix computation. As you may know, people have look numerous times for their chosen readings like this graph theory and sparse matrix computation, but end up in infectious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their laptop. graph theory and sparse matrix computation is available in our book collection an online access to it is set as public so you can get it instantly. Our books collection hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the graph theory and sparse matrix computation is universally compatible with any devices to read.",Erratum,pro68
pap1557,d2ae0082986d965469ece8c0ebcf4885c84b9ccc,jou282,Epilepsia,Presurgery resting‐state local graph‐theory measures predict neurocognitive outcomes after brain surgery in temporal lobe epilepsy,This study determined the ability of resting‐state functional connectivity (rsFC) graph‐theory measures to predict neurocognitive status postsurgery in patients with temporal lobe epilepsy (TLE) who underwent anterior temporal lobectomy (ATL).,Letter,vol282
pap1558,bba08b50d33448a3b65b4baf7c0df3c1eca8b10d,jou283,Proceedings of the Royal Society A,Evolutionary graph theory revisited: when is an evolutionary process equivalent to the Moran process?,"Evolution in finite populations is often modelled using the classical Moran process. Over the last 10 years, this methodology has been extended to structured populations using evolutionary graph theory. An important question in any such population is whether a rare mutant has a higher or lower chance of fixating (the fixation probability) than the Moran probability, i.e. that from the original Moran model, which represents an unstructured population. As evolutionary graph theory has developed, different ways of considering the interactions between individuals through a graph and an associated matrix of weights have been considered, as have a number of important dynamics. In this paper, we revisit the original paper on evolutionary graph theory in light of these extensions to consider these developments in an integrated way. In particular, we find general criteria for when an evolutionary graph with general weights satisfies the Moran probability for the set of six common evolutionary dynamics.",Conference paper,vol283
pap1559,e3acbca01b107b43b04f73499fbce2eeadc5970a,con68,Experimental Software Engineering Network,Neutrosophic Graphs: A New Dimension to Graph Theory,"In this book authors for the first time have made a through study of neutrosophic graphs. This study reveals that these neutrosophic graphs give a new dimension to graph theory. The important feature of this book is it contains over 200 neutrosophic graphs to provide better understanding of this concepts. Further these graphs happen to behave in a unique way inmost cases, for even the edge colouring problem is different from the classical one. Several directions and dimensions in graph theory are obtained from this study.",Erratum,pro68
pap1560,b255694c69768b03bf854a72f3b0520befb0c509,con22,Grid Computing Environments,The Fascinating World of Graph Theory,"The fascinating world of graph theory goes back several centuries and revolves around the study of graphsmathematical structures showing relations between objects. With applications in biology, computer science, transportation science, and other areas, graph theory encompasses some of the most beautiful formulas in mathematicsand some of its most famous problems. For example, what is the shortest route for a traveling salesman seeking to visit a number of cities in one trip? What is the least number of colors needed to fill in any map so that neighboring regions are always colored differently? Requiring readers to have a math background only up to high school algebra, this book explores the questions and puzzles that have been studied, and often solved, through graph theory. In doing so, the book looks at graph theorys development and the vibrant individuals responsible for the fields growth. Introducing graph theorys fundamental concepts, the authors explore a diverse plethora of classic problems such as the Lights Out Puzzle, the Minimum Spanning Tree Problem, the Knigsberg Bridge Problem, the Chinese Postman Problem, a Knights Tour, and the Road Coloring Problem. They present every type of graph imaginable, such as bipartite graphs, Eulerian graphs, the Petersen graph, and trees. Each chapter contains math exercises and problems for readers to savor. An eye-opening journey into the world of graphs, this book offers exciting problem-solving possibilities for mathematics and beyond.",Erratum,pro22
pap1561,5b2d23a567b117ffe32dad3907a473e328127ff2,con16,International Conference on Data Science and Advanced Analytics,Modern Graph Theory,,Erratum,pro16
pap1562,9893595f6950ca1340055da05fd2fae2b7b4dfd3,con102,Annual Haifa Experimental Systems Conference,Three conjectures in extremal spectral graph theory,,Erratum,pro102
pap1563,b19fd39320634d3421191e0614b32f809d8b0866,con88,European Conference on Computer Vision,A Survey on some Applications of Graph Theory in Cryptography,"Abstract Graph theory is rapidly moving into the main stream of research because of its applications in diverse fields such as biochemistry (genomics), coding theory, communication networks and their security etc. In particular researchers are exploring the concepts of graph theory that can be used in different areas of Cryptography. In this paper a review of the works carried out in the field of Cryptography which use the concepts of Graph Theory, is given. Some of the Cryptographic Algorithms based on general graph theory concepts, Extremal Graph Theory and Expander Graphs are analyzed.",Erratum,pro88
pap1564,d53aa6487575762c1a14addf273ea271fef24d29,con75,Intelligent Systems in Molecular Biology,Algorithmic graph theory and perfect graphs,,Erratum,pro75
pap1565,088e404fb84c0dc1a5c77724705e04b7872b1e01,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Graph theory,,Erratum,pro21
pap1566,4fe29b8330dc11095cdda0f2913b781e0a2262de,con81,International Conference on Learning Representations,n-Nucleotide circular codes in graph theory,"The circular code theory proposes that genes are constituted of two trinucleotide codes: the classical genetic code with 61 trinucleotides for coding the 20 amino acids (except the three stop codons {TAA,TAG,TGA}) and a circular code based on 20 trinucleotides for retrieving, maintaining and synchronizing the reading frame. It relies on two main results: the identification of a maximal C3 self-complementary trinucleotide circular code X in genes of bacteria, eukaryotes, plasmids and viruses (Michel 2015 J. Theor. Biol. 380, 156–177. (doi:10.1016/j.jtbi.2015.04.009); Arquès & Michel 1996 J. Theor. Biol. 182, 45–58. (doi:10.1006/jtbi.1996.0142)) and the finding of X circular code motifs in tRNAs and rRNAs, in particular in the ribosome decoding centre (Michel 2012 Comput. Biol. Chem. 37, 24–37. (doi:10.1016/j.compbiolchem.2011.10.002); El Soufi & Michel 2014 Comput. Biol. Chem. 52, 9–17. (doi:10.1016/j.compbiolchem.2014.08.001)). The univerally conserved nucleotides A1492 and A1493 and the conserved nucleotide G530 are included in X circular code motifs. Recently, dinucleotide circular codes were also investigated (Michel & Pirillo 2013 ISRN Biomath. 2013, 538631. (doi:10.1155/2013/538631); Fimmel et al. 2015 J. Theor. Biol. 386, 159–165. (doi:10.1016/j.jtbi.2015.08.034)). As the genetic motifs of different lengths are ubiquitous in genes and genomes, we introduce a new approach based on graph theory to study in full generality n-nucleotide circular codes X, i.e. of length 2 (dinucleotide), 3 (trinucleotide), 4 (tetranucleotide), etc. Indeed, we prove that an n-nucleotide code X is circular if and only if the corresponding graph is acyclic. Moreover, the maximal length of a path in corresponds to the window of nucleotides in a sequence for detecting the correct reading frame. Finally, the graph theory of tournaments is applied to the study of dinucleotide circular codes. It has full equivalence between the combinatorics theory (Michel & Pirillo 2013 ISRN Biomath. 2013, 538631. (doi:10.1155/2013/538631)) and the group theory (Fimmel et al. 2015 J. Theor. Biol. 386, 159–165. (doi:10.1016/j.jtbi.2015.08.034)) of dinucleotide circular codes while its mathematical approach is simpler.",Erratum,pro81
pap1567,9455977ce159fdc496cda7128ec35e03615baf43,con20,ACM Conference on Economics and Computation,Graph theory in the geosciences,,Erratum,pro20
pap1568,23d4cba08cc13f3e9b9442762dc8ad745c865b65,con72,Bioinformatics and Computational Biology,An Introduction To The Theory Of Graph Spectra,"an introduction to the theory of graph spectra is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the an introduction to the theory of graph spectra is universally compatible with any devices to read.",Erratum,pro72
pap1569,9699f9186a38107ff5dfaae9d44dd1796cbe7ac7,con99,North American Chapter of the Association for Computational Linguistics,Water Network Sectorization Based on Graph Theory and Energy Performance Indices,"AbstractThis paper proposes a new methodology for the optimal design of water network sectorization, which is an essential technique for improving the management and security of multiple-source water supply systems. In particular, the network sectorization problem under consideration concerns the definition of isolated district meter areas, each of which is supplied by its own source (or sources) and is completely disconnected from the rest of the water system through boundary valves or permanent pipe sectioning. The proposed methodology uses graph theory principles and a heuristic procedure based on minimizing the amount of dissipated power in the water network. The procedure has been tested on two existing water distribution networks (WDNs) (in Parete, Italy and San Luis Rio Colorado, Mexico) using different performance indices. The simulation results, which confirmed the effectiveness of the proposed methodology, surpass empirical trial-and-error approaches and offer water utilities a tool for the desi...",Erratum,pro99
pap1570,59cdf849049627e4c30f3bd866e3a7e03e893251,jou284,Nature Reviews Neuroscience,Complex brain networks: graph theoretical analysis of structural and functional systems,,Conference paper,vol284
pap1571,89ce4cf020e64bb4c7820b550ed9895525d31872,jou285,Neurobiology of Aging,Functional connectivity and graph theory in preclinical Alzheimer's disease,,Article,vol285
pap1572,d4334742a490ff8ef9830daa8b3cde35abd5fc84,con16,International Conference on Data Science and Advanced Analytics,Encryption Algorithm Using Graph Theory,"In the recent years, with the increase of using Internet and other new telecommunication technologies, cryptography has become a key area to research and improve in order to transfer data securely between two or more entities, especially when the data transferred classified as a critical or important data .Even there are many encryption algorithms exist, the need of new non-standard encryption algorithms raise to prevent any traditional opportunity to sniff dat a. The proposed algorithm represents a new encryption algorithm to encrypt and decrypt data securely with the benefits of graph theory properties, the new symmetric encryption algorithm use the concepts of cycle graph, complete graph and minimum spanning tree to generate a complexcipher text using a shared key.",Erratum,pro16
pap1573,1f69420a7a8f7b78a7edc74036c89586f09ef517,con77,International Conference on Artificial Neural Networks,Graph theory-recent developments of its application in geomorphology,,Erratum,pro77
pap1574,61290e15e45007ce55554d65ef364508390663e1,con80,International Conference on Advanced Computer Science Applications and Technologies,Graph Theory and Cyber Security,"One of the most important fields in discrete mathematics is graph theory. Graph theory is discrete structures, consisting of vertices and edges that connect these vertices. Problems in almost every conceivable discipline can be solved using graph models. The field graph theory started its journey from the problem of Konigsberg Bridges in 1735. This paper is a guide for the applied mathematician who would like to know more about network security, cryptography and cyber security based of graph theory. The paper gives a brief overview of the subject and the applications of graph theory in computer security, and provides pointers to key research and recent survey papers in the area.",Article,pro80
pap1575,b131ac160af2c3ef91aff47f6578067183ca4c4b,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies","Descriptive Complexity, Canonisation, and Definable Graph Structure Theory","Descriptive complexity theory establishes a connection between the computational complexity of algorithmic problems (the computational resources required to solve the problems) and their descriptive complexity (the language resources required to describe the problems). This ground-breaking book approaches descriptive complexity from the angle of modern structural graph theory, specifically graph minor theory. It develops a ‘definable structure theory’ concerned with the logical definability of graph-theoretic concepts such as tree decompositions and embeddings. The first part starts with an introduction to the background, from logic, complexity, and graph theory, and develops the theory up to first applications in descriptive complexity theory and graph isomorphism testing. It may serve as the basis for a graduate-level course. The second part is more advanced and mainly devoted to the proof of a single, previously unpublished theorem: properties of graphs with excluded minors are decidable in polynomial time if, and only if, they are definable in fixed-point logic with counting.",Erratum,pro49
pap1576,3278d34e0eb3d58d36cc2300a4396664a6c950e5,con92,Human Language Technology - The Baltic Perspectiv,Chemical Graph Theory,This chapter on chemical graph theory forms part of the natural science and processes section of the handbook,Erratum,pro92
pap1577,19c0d004bd0e42a6449d8b7717cbda4431a67e65,con78,Neural Information Processing Systems,Principal Neighbourhood Aggregation for Graph Nets,"Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.",Letter,pro78
pap1578,e4715a13f6364b1c81e64f247651c3d9e80b6808,con78,Neural Information Processing Systems,Link Prediction Based on Graph Neural Networks,"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",Letter,pro78
pap1579,3bf89f38b238dd2d4ca4870e6b1fb28dbd136c84,con106,International Conference on Mobile Data Management,"Handbook of Graph Theory, Second Edition","In the ten years since the publication of the best-selling first edition, more than 1,000 graph theory papers have been published each year. Reflecting these advances, Handbook of Graph Theory, Second Edition provides comprehensive coverage of the main topics in pure and applied graph theory. This second editionover 400 pages longer than its predecessorincorporates 14 new sections. Each chapter includes lists of essential definitions and facts, accompanied by examples, tables, remarks, and, in some cases, conjectures and open problems. A bibliography at the end of each chapter provides an extensive guide to the research literature and pointers to monographs. In addition, a glossary is included in each chapter as well as at the end of each section. This edition also contains notes regarding terminology and notation. With 34 new contributors, this handbook is the most comprehensive single-source guide to graph theory. It emphasizes quick accessibility to topics for non-experts and enables easy cross-referencing among chapters.",Erratum,pro106
pap1580,9318f9c688debeee0d671faae0f2cc384a673499,jou286,Biomedical Optics Express,Automatic cone photoreceptor segmentation using graph theory and dynamic programming,"Geometrical analysis of the photoreceptor mosaic can reveal subclinical ocular pathologies. In this paper, we describe a fully automatic algorithm to identify and segment photoreceptors in adaptive optics ophthalmoscope images of the photoreceptor mosaic. This method is an extension of our previously described closed contour segmentation framework based on graph theory and dynamic programming (GTDP). We validated the performance of the proposed algorithm by comparing it to the state-of-the-art technique on a large data set consisting of over 200,000 cones and posted the results online. We found that the GTDP method achieved a higher detection rate, decreasing the cone miss rate by over a factor of five.",Article,vol286
pap1581,a23407b19100acba66fcfc2803d251a3c829e9e3,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Applying Graph theory to the Internet of Things,"In the Internet of Things (IoT), we all are ``things''. Graph theory, a branch of discrete mathematics, has been proven to be useful and powerful in understanding complex networks in history. By means of graph theory, we define new concepts and terminology, and explore the definition of IoT, and then show that IoT is the union of a topological network, a data-functional network and a domi-functional network.",Erratum,pro13
pap1582,4d89d0da3bb1c8bc2ef36c7641abd41149def20b,con63,International Colloquium on Theoretical Aspects of Computing,Graph theory and molecular orbitals. Total φ-electron energy of alternant hydrocarbons,,Erratum,pro63
pap1583,bfacd964ba59bcc6b87b251fa3b30df90736c315,con110,Very Large Data Bases Conference,Fuzzy Graph Theory: A Survey,"A fuzzy graph (f-graph) is a pair G : ( σ, �) where σ is a fuzzy subset of a set S andis a fuzzy relation on σ. A fuzzy graph H : ( τ, υ) is called a partial fuzzy subgraph of G : ( σ, �) if τ (u) ≤ σ(u) for every u and υ (u, v) ≤ �(u, v) for every u and v . In particular we call a partial fuzzy subgraph H : ( τ, υ) a fuzzy subgraph of G : ( σ, � ) if τ (u) = σ(u) for every u in τ * and υ (u, v) = �(u, v) for every arc (u, v) in υ*. A connected f-graph G : ( σ, �) is a fuzzy tree(f-tree) if it has a fuzzy spannin g subgraph F : (σ, υ), which is a tree, where for all arcs (x, y) not i n F there exists a path from x to y in F whose strength is more than �(x, y). A path P of length n is a sequence of disti nct nodes u0, u 1, ..., u n such that �(u i1 , u i) > 0, i = 1, 2, ..., n and the degree of membershi p of a weakest arc is defined as its strength. If u 0 = u n and n ≥ 3, then P is called a cycle and a cycle P is called a fuzzy cycle(f-cycle) if it cont ains more than one weakest arc . The strength of connectedness between two nodes x and y is defined as the maximum of the strengths of all paths between x and y and is denot ed by CONN G(x, y). An x y path P is called a strongest x y",Erratum,pro110
pap1584,6b5d3dbfe9ed31517b657ea9072064ec1a024b4b,con41,Asia-Pacific Software Engineering Conference,Introduction to Graph Theory and Algebraic Graph Theory,,Erratum,pro41
pap1585,fbe0aeb6d1aaca06c84c985ce63b7c061569dd99,jou133,PLoS ONE,Comparing Brain Networks of Different Size and Connectivity Density Using Graph Theory,"Graph theory is a valuable framework to study the organization of functional and anatomical connections in the brain. Its use for comparing network topologies, however, is not without difficulties. Graph measures may be influenced by the number of nodes (N) and the average degree (k) of the network. The explicit form of that influence depends on the type of network topology, which is usually unknown for experimental data. Direct comparisons of graph measures between empirical networks with different N and/or k can therefore yield spurious results. We list benefits and pitfalls of various approaches that intend to overcome these difficulties. We discuss the initial graph definition of unweighted graphs via fixed thresholds, average degrees or edge densities, and the use of weighted graphs. For instance, choosing a threshold to fix N and k does eliminate size and density effects but may lead to modifications of the network by enforcing (ignoring) non-significant (significant) connections. Opposed to fixing N and k, graph measures are often normalized via random surrogates but, in fact, this may even increase the sensitivity to differences in N and k for the commonly used clustering coefficient and small-world index. To avoid such a bias we tried to estimate the N,k-dependence for empirical networks, which can serve to correct for size effects, if successful. We also add a number of methods used in social sciences that build on statistics of local network structures including exponential random graph models and motif counting. We show that none of the here-investigated methods allows for a reliable and fully unbiased comparison, but some perform better than others.",Letter,vol133
pap1586,3ba0fab49dc15f0d39c1ffa8f8f767b506e2918c,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Graph Theory with Algorithms and its Applications,,Erratum,pro39
pap1587,d089f5e5d0548df6066ff281f3918ed67ae742a0,con11,European Conference on Modelling and Simulation,Recent developments in graph Ramsey theory,"Given a graph $H$, the Ramsey number $r(H)$ is the smallest natural number $N$ such that any two-colouring of the edges of $K_N$ contains a monochromatic copy of $H$. The existence of these numbers has been known since 1930 but their quantitative behaviour is still not well understood. Even so, there has been a great deal of recent progress on the study of Ramsey numbers and their variants, spurred on by the many advances across extremal combinatorics. In this survey, we will describe some of this progress.",Erratum,pro11
pap1588,d133cb102ad0f81e3fd17a7db090b28afc124c4a,jou100,Physical Review Letters,Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.,"The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.",Article,vol100
pap1589,4ce9c20642dce5eb7930966053a1e3da4ef617f2,con81,International Conference on Learning Representations,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erdős -- Renyi graph. We show that when the Erdős -- Renyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at this https URL.",Conference paper,pro81
pap1590,ab9accbaf61438bf518de0af0946f4bf12b7e158,jou287,Research Synthesis Methods,"Network meta‐analysis, electrical networks and graph theory","Network meta‐analysis is an active field of research in clinical biostatistics. It aims to combine information from all randomized comparisons among a set of treatments for a given medical condition. We show how graph‐theoretical methods can be applied to network meta‐analysis. A meta‐analytic graph consists of vertices (treatments) and edges (randomized comparisons). We illustrate the correspondence between meta‐analytic networks and electrical networks, where variance corresponds to resistance, treatment effects to voltage, and weighted treatment effects to current flows. Based thereon, we then show that graph‐theoretical methods that have been routinely applied to electrical networks also work well in network meta‐analysis. In more detail, the resulting consistent treatment effects induced in the edges can be estimated via the Moore–Penrose pseudoinverse of the Laplacian matrix. Moreover, the variances of the treatment effects are estimated in analogy to electrical effective resistances. It is shown that this method, being computationally simple, leads to the usual fixed effect model estimate when applied to pairwise meta‐analysis and is consistent with published results when applied to network meta‐analysis examples from the literature. Moreover, problems of heterogeneity and inconsistency, random effects modeling and including multi‐armed trials are addressed. Copyright © 2012 John Wiley & Sons, Ltd.",Letter,vol287
pap1591,29af614088e9a2b387c2dc81539cf8420483c28a,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",A First Course in Graph Theory,"Because of its inherent simplicity, graph theory has a wide range of applications in engineering, and in physical sciences. It has of course uses in social sciences, in linguistics and in numerous other areas. In fact, a graph can be used to represent almost any physical situation involving discrete objects and the relationship among them. Now with the solutions to engineering and other problems becoming so complex leading to larger graphs, it is virtually difficult to analyze without the use of computers. This book is recommended in IIT Kharagpur, West Bengal for B.Tech Computer Science, NIT Arunachal Pradesh, NIT Nagaland, NIT Agartala, NIT Silchar, Gauhati University, Dibrugarh University, North Eastern Regional Institute of Management, Assam Engineering College, West Bengal Univerity of Technology (WBUT) for B.Tech, M.Tech Computer Science, University of Burdwan, West Bengal for B.Tech. Computer Science, Jadavpur University, West Bengal for M.Sc. Computer Science, Kalyani College of Engineering, West Bengal for B.Tech. Computer Science. Key Features: This book provides a rigorous yet informal treatment of graph theory with an emphasis on computational aspects of graph theory and graph-theoretic algorithms. Numerous applications to actual engineering problems are incorpo-rated with software design and optimization topics.",Erratum,pro73
pap1592,b54b6e737d5ba0f1b2c128a90570652b9b2fc3f5,con77,International Conference on Artificial Neural Networks,Extremal Graph Theory,"The basic statement of extremal graph theory is Mantel’s theorem, proved in 1907, which states that any graph on n vertices with no triangle contains at most n2/4 edges. This is clearly best possible, as one may partition the set of n vertices into two sets of size bn/2c and dn/2e and form the complete bipartite graph between them. This graph has no triangles and bn2/4c edges. As a warm-up, we will give a number of different proofs of this simple and fundamental theorem.",Erratum,pro77
pap1593,995f4c79933bf2219c26a634207449f255ae7b6e,con48,ACM Symposium on Applied Computing,Boundary-connectivity via graph theory,"We generalize theorems of Kesten and Deuschel-Pisztora about the connectedness of the exterior boundary of a connected subset of Zd, where “connectedness” and “boundary” are understood with respect to various graphs on the vertices of Zd. These theorems are widely used in statistical physics and related areas of probability. We provide simple and elementary proofs of their results. It turns out that the proper way of viewing these questions is graph theory instead of topology. Denote by Z the usual nearest-neighbor lattice on Z, i.e., two points of Z are adjacent if they differ only in one coordinate, by 1. Let Zd∗ be the graph on the same vertex set and edges between every two distinct points that differ in every coordinate by at most 1. We say that a set of vertices in Z is *-connected if it is connected in the graph Zd∗. In [DP] Deuschel and Pisztora prove that the part of the outer vertex boundary of a finite connected subgraph C in Zd∗ that is visible from infinity (the exterior boundary) is *-connected. Earlier, Kesten [K] showed that the set of points in the *-boundary of a connected subgraph C ⊂ Zd∗ that are Z-visible from infinity is connected in Z. Similar results were proved about the case when C is in an n× n box of Z [DP], or Zd∗ [H]. See the second paragraphs of Theorem 3 and Theorem 4 for the precise statements. We generalize these results about Z and Zd∗ to a very general family of pairs of graphs; see Lemma 2, Theorem 3 and Theorem 4. Our method also gives an elementary and short alternative to the original proofs for the cubic grid case. This approach seems to be efficient to treat possible other questions about the connectedness of boundaries. Although [K] mentions that some use of algebraic topology seems to be unavoidable, the greater generality (and simplicity) of our proof is a result of using purely graph-theoretic arguments. Also, it makes slight modifications of the results (such as considering boundaries in some subset of Z instead of boundaries in Z) straightforward, while previously one had to go through the original proofs and make significant modifications. In two dimensions, the use of some duality argument makes connectedness of boundaries more straightforward to prove. The lack of duality (that is, the correspondance that a cycle in one graph is a separating set in its dual) in higher Received by the editors March 25, 2010 and, in revised form, February 21, 2011; July 1, 2011; and July 5, 2011. 2010 Mathematics Subject Classification. Primary 05C10, 05C63; Secondary 20F65, 60K35. c ©2012 American Mathematical Society Reverts to public domain 28 years from publication",Erratum,pro48
pap1594,5656c718164f6ab8950f13d78ccf5d3b9942df7f,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Test-Retest Reliability of Graph Theory Measures of Structural Brain Connectivity,,Article,pro82
pap1595,a51a630f1088baff85282993ebf14fb910ccf126,jou288,BioData Mining,Using graph theory to analyze biological networks,,Conference paper,vol288
pap1596,6fa7236e8ff0d958b77164f72b4ce93df5c61caa,con80,International Conference on Advanced Computer Science Applications and Technologies,Graph Algorithms,,Erratum,pro80
pap1597,f2ab3c4e6fae8e666cc68ba1b5672dfddea71123,con3,Knowledge Discovery and Data Mining,Local Higher-Order Graph Clustering,"Local graph clustering methods aim to find a cluster of nodes by exploring a small region of the graph. These methods are attractive because they enable targeted clustering around a given seed node and are faster than traditional global graph clustering methods because their runtime does not depend on the size of the input graph. However, current local graph partitioning methods are not designed to account for the higher-order structures crucial to the network, nor can they effectively handle directed networks. Here we introduce a new class of local graph clustering methods that address these issues by incorporating higher-order network information captured by small subgraphs, also called network motifs. We develop the Motif-based Approximate Personalized PageRank (MAPPR) algorithm that finds clusters containing a seed node with minimal \emph{motif conductance}, a generalization of the conductance metric for network motifs. We generalize existing theory to prove the fast running time (independent of the size of the graph) and obtain theoretical guarantees on the cluster quality (in terms of motif conductance). We also develop a theory of node neighborhoods for finding sets that have small motif conductance, and apply these results to the case of finding good seed nodes to use as input to the MAPPR algorithm. Experimental validation on community detection tasks in both synthetic and real-world networks, shows that our new framework MAPPR outperforms the current edge-based personalized PageRank methodology.",Article,pro3
pap1598,cfa6e9e2331fca45f5c948a3477164fd14c956c9,con29,ACM-SIAM Symposium on Discrete Algorithms,"Graph Theory, 4th Edition",,Erratum,pro29
pap1599,287650fad5a478581f0040ad064e6856c7c13af2,con46,Software Product Lines Conference,Topological Graph Theory,Introduction Voltage Graphs and Covering Spaces Surfaces and Graph Imbeddings Imbedded Voltage Graphs and Current Graphs Map Colorings The Genus of A Group References.,Erratum,pro46
pap1600,8df5d1e909de14932e42b347adf35070f60dc9ba,jou289,Transactions of the American Mathematical Society,"An $L^p$ theory of sparse graph convergence I: Limits, sparse random graph models, and power law distributions","We introduce and develop a theory of limits for sequences of sparse graphs based on $L^p$ graphons, which generalizes both the existing $L^\infty$ theory of dense graph limits and its extension by Bollob\'as and Riordan to sparse graphs without dense spots. In doing so, we replace the no dense spots hypothesis with weaker assumptions, which allow us to analyze graphs with power law degree distributions. This gives the first broadly applicable limit theory for sparse graphs with unbounded average degrees. In this paper, we lay the foundations of the $L^p$ theory of graphons, characterize convergence, and develop corresponding random graph models, while we prove the equivalence of several alternative metrics in a companion paper.",Article,vol289
pap1601,fc472f5017c6200e79b7540be07bc6b4798b4ef8,con41,Asia-Pacific Software Engineering Conference,A review of evolutionary graph theory with applications to game theory,,Erratum,pro41
pap1602,1f400c9732c13eae95be34a181c2b8042a774a39,con94,Vision,Handbook of graph theory,"Introduction to Graphs Fundamentals of Graph Theory, Jonathan L. Gross and Jay Yellen Families of Graphs and Digraphs, Lowell W. Beineke History of Graph Theory, Robin J. Wilson Graph Representation Computer Representation of Graphs, Alfred V. Aho Graph Isomorphism, Brendan D. McKay The Reconstruction Problem, Josef Lauri Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Structural Graph Theory, Maria Chudnovsky Directed Graphs Basic Digraph Models and Properties, Jay Yellen Directed Acyclic Graphs, Stephen B. Maurer Tournaments, K.B. Reid Connectivity and Traversability Connectivity Properties and Structure, Camino Balbuena, Josep Fabrega, and Miguel Angel Fiol Eulerian Graphs, Herbert Fleischner Chinese Postman Problems, R. Gary Parker and Richard B. Borie DeBruijn Graphs and Sequences, A.K. Dewdney Hamiltonian Graphs, Ronald J. Gould Traveling Salesman Problems, Gregory Gutin Further Topics in Connectivity, Josep Fabrega and Miguel Angel Fiol Colorings and Related Topics Graph Coloring, Zsolt Tuza Further Topics in Graph Coloring, Zsolt Tuza Independence and Cliques, Gregory Gutin Factors and Factorization, Michael Plummer Applications to Timetabling, Edmund Burke, Dominique de Werra, and Jeffrey Kingston Graceful Labelings, Joseph A. Gallian Algebraic Graph Theory Automorphisms, Mark E. Watkins Cayley Graphs, Brian Alspach Enumeration, Paul K. Stockmeyer Graphs and Vector Spaces, Krishnaiyan ""KT"" Thulasiraman Spectral Graph Theory, Michael Doob Matroidal Methods in Graph Theory, James Oxley Topological Graph Theory Graphs on Surfaces, Tomaz Pisanski and Primoz Potocnik Minimum Genus and Maximum Genus, Jianer Chen Genus Distributions, Jonathan L. Gross Voltage Graphs, Jonathan L. Gross The Genus of a Group, Thomas W. Tucker Maps, Roman Nedela and Martin Skoviera Representativity, Dan Archdeacon Triangulations, Seiya Negami Graphs and Finite Geometries, Arthur T. White Crossing Numbers, R. Bruce Richter and Gelasio Salazar Analytic Graph Theory Extremal Graph Theory, Bela Bollobas and Vladimir Nikiforov Random Graphs, Nicholas Wormald Ramsey Graph Theory, Ralph J. Faudree The Probabilistic Method, Alan Frieze and Po-Shen Loh Graph Limits, Bojan Mohar Graphical Measurement Distance in Graphs, Gary Chartrand and Ping Zhang Domination in Graphs, Teresa W. Haynes and Michael A. Henning Tolerance Graphs, Martin Charles Golumbic Bandwidth, Robert C. Brigham Pursuit-Evasion Problems, Richard B. Borie, Sven Koenig, and Craig A. Tovey Graphs in Computer Science Searching, Harold N. Gabow Dynamic Graph Algorithms, Camil Demetrescu, Irene Finocchi, and Giuseppe F. Italiano Drawings of Graphs, Emilio Di Giacomo, Giuseppe Liotta, and Roberto Tamassia Algorithms on Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Fuzzy Graphs, John N. Mordeson and D.S. Malik Expander Graphs, Mike Krebs and Anthony Shaheen Visibility Graphs, Alice M. Dean and Joan P. Hutchinson Networks and Flows Maximum Flows, Clifford Stein Minimum Cost Flows, Lisa Fleischer Matchings and Assignments, Jay Sethuraman and Douglas R. Shier Communication Networks Complex Networks, Anthony Bonato and Fan Chung Broadcasting and Gossiping, Hovhannes A. Harutyunyan, Arthur L. Liestman, Joseph G. Peters, and Dana Richards Communication Network Design Models, Prakash Mirchandani and David Simchi-Levi Network Science for Graph Theorists, David C. Arney and Steven B. Horton Natural Science and Processes Chemical Graph Theory, Ernesto Estrada and Danail Bonchev Ties between Graph Theory and Biology, Jacek Blazewicz, Marta Kasprzak, and Nikos Vlassis Index A Glossary appears at the end of each chapter.",Erratum,pro94
pap1603,3265e606639c89e64e5cac5e32dd3e9081b387be,con110,Very Large Data Bases Conference,Spectral Graph Theory,16.,Erratum,pro110
pap1604,74b6089b46e3e12f74da7d5d981d787aa7fb8459,con3,Knowledge Discovery and Data Mining,Active semi-supervised learning using sampling theory for graph signals,"We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available. The data points are represented by the vertices of an undirected graph with the similarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function defined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for active learning based on sampling set selection which aims at maximizing the frequency of the signals that can be reconstructed from their samples on the set. Experiments show the effectiveness of our method.",Letter,pro3
pap1605,36e3cf186ebc510800fb6e217cc8edae93fbf4f9,con72,Bioinformatics and Computational Biology,"An $L^{p}$ theory of sparse graph convergence II: LD convergence, quotients and right convergence","We extend the LpLp theory of sparse graph limits, which was introduced in a companion paper, by analyzing different notions of convergence. Under suitable restrictions on node weights, we prove the equivalence of metric convergence, quotient convergence, microcanonical ground state energy convergence, microcanonical free energy convergence and large deviation convergence. Our theorems extend the broad applicability of dense graph convergence to all sparse graphs with unbounded average degree, while the proofs require new techniques based on uniform upper regularity. Examples to which our theory applies include stochastic block models, power law graphs and sparse versions of WW-random graphs.",Erratum,pro72
pap1606,7dbdb4209626fd92d2436a058663206216036e68,con105,British Machine Vision Conference,Elements of Information Theory,"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.",Erratum,pro105
pap1607,69381b5efd97e7c55f51c2730caccab3d632d4d2,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,"A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions",Article,vol290
pap1608,7f5ea861a57e14796f033fd0f5580dbc34ff88f2,con0,International Conference on Machine Learning,Relational Pooling for Graph Representations,"This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.",Article,pro0
pap1609,f271c09c88e5b136d52d42d82f16e2e3f3a19642,con100,International Conference on Automatic Face and Gesture Recognition,Graph theory,,Erratum,pro100
pap1610,39afec82e60d44f003cbd15e85d892b6fc5016e9,con78,Neural Information Processing Systems,"Algorithms, Graph Theory, and Linear Equations in Laplacian Matrices","The Laplacian matrices of graphs are fundamental. In addition to facilitating the application of linear algebra to graph theory, they arise in many practical problems. In this talk we survey recent progress on the design of provably fast algorithms for solving linear equations in the Laplacian matrices of graphs. These algorithms motivate and rely upon fascinating primitives in graph theory, including low-stretch spanning trees, graph sparsifiers, ultra-sparsifiers, and local graph clustering. These are all connected by a definition of what it means for one graph to approximate another. While this definition is dictated by Numerical Linear Algebra, it proves useful and natural from a graph theoretic perspective. Mathematics Subject Classification (2010). Primary 68Q25; Secondary 65F08.",Erratum,pro78
pap1611,f2a34394994089ee965dd9fa7a4bbc2446432770,con8,Frontiers in Education Conference,Applications of Graph Theory and Network Science to Transit Network Design,"Abstract While the network nature of public transportation systems is well known, the study of their design from a topological/geometric perspective remains relatively limited. From the work of Euler in the 1750s to the discovery of scale-free networks in the late 1990s, the goal of this paper is to review the topical literature that applied concepts of graph theory and network science. After briefly introducing the origins of graph theory, we review early indicators developed to study transport networks, which notably includes the works of Garrison and Marble, and Kansky. Afterwards, we examine network indicators and characteristics developed to study transit systems specifically, in particular by reviewing the works of Vuchic and Musso. Subsequently, we introduce the concepts of small-worlds and scale-free networks from the emerging field network science, and review early applications to transit networks. Finally, we identify three challenges that will need to be addressed in the future. As transit systems are likely to grow in the world, the study of their network feature could be of substantial help to planners so as to better design the transit systems of tomorrow, but much work lies ahead.",Erratum,pro8
pap1612,54be361ad20a895831dd19f70712c885c94a6289,jou291,IEEE Transactions on Sustainable Energy,Spatio-Temporal Graph Deep Neural Network for Short-Term Wind Speed Forecasting,"Wind speed forecasting is still a challenge due to the stochastic and highly varying characteristics of wind. In this paper, a graph deep learning model is proposed to learn the powerful spatio-temporal features from the wind speed and wind direction data in neighboring wind farms. The underlying wind farms are modeled by an undirected graph, where each node corresponds to a wind site. For each node, temporal features are extracted using a long short-term memory Network. A scalable graph convolutional deep learning architecture (GCDLA), motivated by the localized first-order approximation of spectral graph convolutions, leverages the extracted temporal features to forecast the wind-speed time series of the whole graph nodes. The proposed GCDLA captures spatial wind features as well as deep temporal features of the wind data at each wind site. To further improve the prediction accuracy and capture robust latent representations, the rough set theory is incorporated with the proposed graph deep network by introducing upper and lower bound parameter approximations in the model. Simulation results show the advantages of capturing deep spatial and temporal interval features in the proposed framework compared to the state-of-the-art deep learning models as well as shallow architectures in the recent literature.",Letter,vol291
pap1613,2ced07a295025f46ae51efa0a61679815c0866c5,con31,International Conference on Evaluation & Assessment in Software Engineering,Some new results in extremal graph theory,"In recent years several classical results in extremal graph theory have been improved in a uniform way and their proofs have been simplified and streamlined. These results include a new Erd\H{o}s-Stone-Bollob\'as theorem, several stability theorems, several saturation results and bounds for the number of graphs with large forbidden subgraphs. Another recent trend is the expansion of spectral extremal graph theory, in which extremal properties of graphs are studied by means of eigenvalues of various matrices. One particular achievement in this area is the casting of the central results above in spectral terms, often with additional enhancement. In addition, new, specific spectral results were found that have no conventional analogs. All of the above material is scattered throughout various journals, and since it may be of some interest, the purpose of this survey is to present the best of these results in a uniform, structured setting, together with some discussions of the underpinning ideas.",Erratum,pro31
pap1614,6b7fe2878d6c42079562f8adb57635dd2f2a2772,con15,Pacific Symposium on Biocomputing,Review of graph theory: a problem oriented approach by Daniel Marcus,"Graph Theory: A Problem Oriented Approach is a textbook cum workbook with the goal to assist a proactive and problem-oriented approach to learning graph theory. This goal is clear from the offset as many key concepts, examples and even proof arguments are clarified or highlighted by the help of carefully chosen problem questions. A reader unfamiliar with graph theory is not kept in the dark as problems are preceded by some introductory text which nudges the reader along. The book consists of over three hundred strategically placed problems interspersed by necessary text. There are also further problems to think about at the end of each chapter. The book is written in a similar style as a book on combinatorics by the same author [2] .",Erratum,pro15
pap1615,8bd4a8eb93ede5c55c09bed842be671c8cb3413f,jou286,Biomedical Optics Express,Robust automatic segmentation of corneal layer boundaries in SDOCT images using graph theory and dynamic programming,"Segmentation of anatomical structures in corneal images is crucial for the diagnosis and study of anterior segment diseases. However, manual segmentation is a time-consuming and subjective process. This paper presents an automatic approach for segmenting corneal layer boundaries in Spectral Domain Optical Coherence Tomography images using graph theory and dynamic programming. Our approach is robust to the low-SNR and different artifact types that can appear in clinical corneal images. We show that our method segments three corneal layer boundaries in normal adult eyes more accurately compared to an expert grader than a second grader—even in the presence of significant imaging outliers.",Article,vol286
pap1616,1a6b1bac33da9cdff5fc311e9b9e4bd065bd7a9f,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Systemic Risk in Energy Derivative Markets: A Graph-Theory Analysis,"Abstract This article uses graph theory to provide novel evidence regarding market integration, a favorable condition for systemic risk to appear in. Relying on daily futures returns covering a 12-year period, we examine cross- and intermarket linkages, both within the commodity complex and between commodities and other financial assets. In such a high dimensional analysis, graph theory enables us to understand the dynamic behavior of our price system. We show that energy markets—as a whole—stand at the heart of this system. We also establish that crude oil is itself at the center of the energy complex. Further, we provide evidence that commodity markets have become more integrated over time. Keywords: Systemic risk, Energy, Derivative markets, High dimensional analysis, Graph theory, Minimum spanning trees",Erratum,pro73
pap1617,a44b5f0314a12aca034d52cf047445cdf7c884bf,con76,IEEE International Conference on Tools with Artificial Intelligence,"Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications","Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications focuses on discrete mathematics and combinatorial algorithms interacting with real world problems in computer science, operations research, applied mathematics and engineering.The book containseleven chapters written by experts in their respective fields, and covers a wide spectrum of high-interest problems across these discipline domains. Among the contributing authors are Richard Karp of UC Berkeley and Robert Tarjan of Princeton; both are at the pinnacle of research scholarship in Graph Theory and Combinatorics. The chapters from the contributing authors focus on ""real world"" applications, all of which will be of considerable interest across the areas of Operations Research, Computer Science, Applied Mathematics, and Engineering. These problems include Internet congestion control, high-speed communication networks, multi-object auctions, resource allocation, software testing, data structures, etc. In sum, this is a book focused on major, contemporary problems, written by the top research scholars in the field, using cutting-edge mathematical and computational techniques.",Erratum,pro76
pap1618,cc2c33aea50ef4e2aec9413f83ccd7d35dace3c6,con66,International Conference on Software Reuse,Applications of Graph Theory in Computer Science,"Graphs are among the most ubiquitous models of both natural and human-made structures. They can be used to model many types of relations and process dynamics in computer science, physical, biological and social systems. Many problems of practical interest can be represented by graphs. In general graphs theory has a wide range of applications in diverse fields. This paper explores different elements involved in graph theory including graph representations using computer systems and graph-theoretic data structures such as list structure and matrix structure. The emphasis of this paper is on graph applications in computer science. To demonstrate the importance of graph theory in computer science, this article addresses most common applications for graph theory in computer science. These applications are presented especially to project the idea of graph theory and to demonstrate its importance in computer science.",Erratum,pro66
pap1619,30cb5c2906115873a4b9fcb0fc099a10f30465cc,con65,IEEE International Conference on Software Engineering and Formal Methods,Applications of Graph Theory,"Graph theory is becoming increasingly significant as it is applied to other areas of mathematics, science and technology. It is being actively used in fields as varied as biochemistry (genomics), electrical engineering (communication networks and coding theory), computer science (algorithms and computation) and operations research (scheduling). The powerful combinatorial methods found in graph theory have also been used to prove fundamental results in other areas of pure mathematics. This paper, besides giving a general outlook of these facts, includes new graph theoretical proofs of Fermat’s Little Theorem and the Nielson-Schreier Theorem. New applications to DNA sequencing (the SNP assembly problem) and computer network security (worm propagation) using minimum vertex covers in graphs are discussed. We also show how to apply edge coloring and matching in graphs for scheduling (the timetabling problem) and vertex coloring in graphs for map coloring and the assignment of frequencies in GSM mobile phone networks. Finally, we revisit the classical problem of finding re-entrant knight’s tours on a chessboard using Hamiltonian circuits in graphs.",Erratum,pro65
pap1620,f4d8cfaab1a9bf229ca8b7de997a445446754a3a,con83,Networks,Graph Theory with Applications to Engineering and Computer Science,Key,Letter,pro83
pap1621,e39ba4c989874285e9473b4532dc275c12ed78c0,con29,ACM-SIAM Symposium on Discrete Algorithms,Computing the shortest path: A search meets graph theory,"We propose shortest path algorithms that use A* search in combination with a new graph-theoretic lower-bounding technique based on landmarks and the triangle inequality. Our algorithms compute optimal shortest paths and work on any directed graph. We give experimental results showing that the most efficient of our new algorithms outperforms previous algorithms, in particular A* search with Euclidean bounds, by a wide margin on road networks and on some synthetic problem families.",Letter,pro29
pap1622,db9967871249ffd77adc0c0cc16154ccc37d09ad,con46,Software Product Lines Conference,A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory,,Erratum,pro46
pap1623,f96f659ada20f35e9fce66306c8f8f6a471e592d,con102,Annual Haifa Experimental Systems Conference,Graph theoretical analysis of magnetoencephalographic functional connectivity in Alzheimer's disease.,"In this study we examined changes in the large-scale structure of resting-state brain networks in patients with Alzheimer's disease compared with non-demented controls, using concepts from graph theory. Magneto-encephalograms (MEG) were recorded in 18 Alzheimer's disease patients and 18 non-demented control subjects in a no-task, eyes-closed condition. For the main frequency bands, synchronization between all pairs of MEG channels was assessed using a phase lag index (PLI, a synchronization measure insensitive to volume conduction). PLI-weighted connectivity networks were calculated, and characterized by a mean clustering coefficient and path length. Alzheimer's disease patients showed a decrease of mean PLI in the lower alpha and beta band. In the lower alpha band, the clustering coefficient and path length were both decreased in Alzheimer's disease patients. Network changes in the lower alpha band were better explained by a 'Targeted Attack' model than by a 'Random Failure' model. Thus, Alzheimer's disease patients display a loss of resting-state functional connectivity in lower alpha and beta bands even when a measure insensitive to volume conduction effects is used. Moreover, the large-scale structure of lower alpha band functional networks in Alzheimer's disease is more random. The modelling results suggest that highly connected neural network 'hubs' may be especially at risk in Alzheimer's disease.",Erratum,pro102
pap1624,3375dc753685773f050392d1b5dfd9a4e3d20b96,con64,British Computer Society Conference on Human-Computer Interaction,Chemical Graph Theory,"INTRODUCTION. ELEMENTS OF GRAPH THEORY. The Definition of a Graph. Isomorphic Graphs and Graph Automorphism. Walks, Trails, Paths, Distances and Valencies in Graphs. Subgraphs. Regular Graphs. Trees. Planar Graphs. The Story of the Koenigsberg Bridge Problem and Eulerian Graphs. Hamiltonian Graphs. Line Graphs. Vertex Coloring of a Graph. CHEMICAL GRAPHS. The Concept of a Chemical Graph. Molecular Topology. Huckel Graphs. Polyhexes and Benzenoid Graphs. Weighted Graphs. GRAPH-THEORETICAL MATRICES. The Adjacency Matrix. The Distance Matrix. THE CHARACTERISTIC POLYNOMIAL OF A GRAPH. The Definition of the Characteristic Polynomial. The Method of Sachs for Computing the Characteristic Polynomial. The Characteristic Polynomials of Some Classes of Simple Graphs. The Le Verrier-Faddeev-Frame Method for Computing the Characteristic Polynomial. TOPOLOGICAL ASPECTS OF HUECKEL THEORY. Elements of Huckel Theory. Isomorphism of Huckel Theory and Graph Spectral Theory. The Huckel Spectrum. Charge Densities and Bond Orders in Conjugated Systems. The Two-Color Problem in Huckel Theory. Eigenvalues of Linear Polyenes. Eigenvalues of Annulenes. Eigenvalues of Moebius Annulenes. A Classification Scheme for Monocyclic Systems. Total p-Electron Energy. TOPOLOGICAL RESONANCE ENERGY. Huckel Resonance Energy. Dewar Resonance Energy. The Concept of Topological Resonance Energy. Computation of the Acyclic Polynomial. Applications of the TRE Model. ENUMERATION OF KEKULE VALENCE STRUCTURES. The Role of Kekule Valence Structures in Chemistry. The Identification of Kekule Systems. Methods for the Enumeration of Kekule Structures. The Concept of Parity of Kekule Structures. THE CONJUGATED-CIRCUIT MODEL. The Concept of Conjugated Circuits. The p-Resonance Energy Expression. Selection of the Parameters. Computational Procedure. Applications of the Conjugated-Circuit Model. Parity of Conjugated Circuits. TOPOLOGICAL INDICES. Definitions of Topological Indices. The Three-Dimensional Wiener Number. ISOMER ENUMERATION. The Cayley Generation Functions. The Henze-Blair Approach. The Polya Enumeration Method. The Enumeration Method Based on the N-Tuple Code.",Erratum,pro64
pap1625,c50328abeea3d9dea2146af86d1ca90996de067d,con88,European Conference on Computer Vision,Assessing the vulnerability of supply chains using graph theory,,Erratum,pro88
pap1626,3f2e5b88c4b631c931f6db805f4ae3c0b531a8e8,con62,Australian Software Engineering Conference,APPLICATIONS OF GRAPH THEORY IN COMPUTER SCIENCE AN OVERVIEW,"The field of mathematics plays vital role in various fields. One of the important areas in mathematics is graph theory which is used in structural models. This structural arrangements of various objects or technologies lead to new inventions and modifications in the existing environment for enhancement in those fields. The field graph theory started its journey from the problem of Koinsberg bridge in 1735. This paper gives an overview of the applications of graph theory in heterogeneous fields to some extent but mainly focuses on the computer science applications that uses graph theoretical concepts. Various papers based on graph theory have been studied related to scheduling concepts, computer science applications and an overview has been presented here.",Erratum,pro62
pap1627,e24f2851d5cf4282239b4fd6a74a5d3bff7f8897,jou292,Studies in Computational Intelligence,Applied Graph Theory in Computer Vision and Pattern Recognition,,Article,vol292
pap1628,6362675dc7f0a41061ceb6c10852e84e6141e509,con1,International Conference on Human Factors in Computing Systems,Graph Theory in the Information Age,"I n the past decade, graph theory has gone through a remarkable shift and a profound transformation. The change is in large part due to the humongous amount of information that we are confronted with. A main way to sort through massive data sets is to build and examine the network formed by interrelations. For example, Google’s successful Web search algorithms are based on the WWW graph, which contains all Web pages as vertices and hyperlinks as edges. There are all sorts of information networks, such as biological networks built from biological databases and social networks formed by email, phone calls, instant messaging, etc., as well as various types of physical networks. Of particular interest to mathematicians is the collaboration graph, which is based on the data from Mathematical Reviews. In the collaboration graph, every mathematician is a vertex, and two mathematicians who wrote a joint paper are connected by an edge. Figure 1 illustrates a portion of the collaboration graph consisting of about 5,000 vertices, representing mathematicians with Erdős number 2 (i.e., mathematicians who wrote a paper with a coauthor of Paul Erdős). Graph theory has two hundred years of history studying the basic mathematical structures called graphs. A graph G consists of a collection V of vertices and a collection E of edges that connect pairs of vertices. In the past, graph theory has",Erratum,pro1
pap1629,3bc2b3028064f5b580ee8d5dcc17b6bb346015fb,con83,Networks,Graph Theory and Complex Networks: An Introduction,"Chapter Description 01: Introduction History, background 02: Foundations Basic terminology and properties of graphs 03: Extensions Directed & weighted graphs, colorings 04: Network traversal Walking through graphs (cf. traveling) 05: Trees Graphs without cycles; routing algorithms 06: Network analysis Basic metrics for analyzing large graphs 07: Random networks Introduction modeling real-world networks 08: Computer networks The Internet & WWW seen as a huge graph 09: Social networks Communities seen as graphs",Erratum,pro83
pap1630,250748b4494cec56abd55ae049bdd38f4d42e5c8,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,An Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation,"A novel graph theoretic approach for data clustering is presented and its application to the image segmentation problem is demonstrated. The data to be clustered are represented by an undirected adjacency graph G with arc capacities assigned to reflect the similarity between the linked vertices. Clustering is achieved by removing arcs of G to form mutually exclusive subgraphs such that the largest inter-subgraph maximum flow is minimized. For graphs of moderate size ( approximately 2000 vertices), the optimal solution is obtained through partitioning a flow and cut equivalent tree of G, which can be efficiently constructed using the Gomory-Hu algorithm (1961). However for larger graphs this approach is impractical. New theorems for subgraph condensation are derived and are then used to develop a fast algorithm which hierarchically constructs and partitions a partially equivalent tree of much reduced size. This algorithm results in an optimal solution equivalent to that obtained by partitioning the complete equivalent tree and is able to handle very large graphs with several hundred thousand vertices. The new clustering algorithm is applied to the image segmentation problem. The segmentation is achieved by effectively searching for closed contours of edge elements (equivalent to minimum cuts in G), which consist mostly of strong edges, while rejecting contours containing isolated strong edges. This method is able to accurately locate region boundaries and at the same time guarantees the formation of closed edge contours. >",Conference paper,vol290
pap1631,66eaaeb2ddcc73a27247cfce92f7e6691fe988c8,con94,Vision,Introduction to Graph Theory,,Erratum,pro94
pap1632,ef2704b8e44692c7f6430451434af64df913d575,con63,International Colloquium on Theoretical Aspects of Computing,Introduction to Graph and Hypergraph Theory,Preface Basic Definitions and Concepts Trees and Bipartite Graphs Chordal Graphs Planar Graphs Graph Coloring Traversals and Flows Basic Hypergraph Concepts Hypertrees and Chordal Hypergraphs Some Other Remarkable Hypergraph Classes Hypergraph Coloring Modeling with Hypergraphs Appendix Index.,Erratum,pro63
pap1633,75264a58faee7b29b72ff329951d7fd353649da8,con89,Conference on Uncertainty in Artificial Intelligence,On an extremal problem in graph theory,"G ( n;l ) will denote a graph of n vertices and l edges. Let f 0 ( n, k ) be the smallest integer such that there is a G ( n;f 0 (n, k )) in which for every set of k vertices there is a vertex joined to each of these. Thus for example f o = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f o = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k , and then f 0 ( n, k ) = f(n, k) .",Erratum,pro89
pap1634,b696862bbb0f73c6f39fce6a46f9f02842295040,con77,International Conference on Artificial Neural Networks,Signed Graph Attention Networks,,Article,pro77
pap1635,2d832d0d1baaaba224a21ca926a10c0529da3628,jou293,BMC Neuroscience,Functional neural network analysis in frontotemporal dementia and Alzheimer's disease using EEG and graph theory,,Letter,vol293
pap1636,a2b76b4529bc7da1696b3b5883ba5b9ab9564d20,con30,PS,Algebraic Graph Theory: COLOURING PROBLEMS,1. Introduction to algebraic graph theory Part I. Linear Algebra in Graphic Thoery: 2. The spectrum of a graph 3. Regular graphs and line graphs 4. Cycles and cuts 5. Spanning trees and associated structures 6. The tree-number 7. Determinant expansions 8. Vertex-partitions and the spectrum Part II. Colouring Problems: 9. The chromatic polynomial 10. Subgraph expansions 11. The multiplicative expansion 12. The induced subgraph expansion 13. The Tutte polynomial 14. Chromatic polynomials and spanning trees Part III. Symmetry and Regularity: 15. Automorphisms of graphs 16. Vertex-transitive graphs 17. Symmetric graphs 18. Symmetric graphs of degree three 19. The covering graph construction 20. Distance-transitive graphs 21. Feasibility of intersection arrays 22. Imprimitivity 23. Minimal regular graphs with given girth References Index.,Erratum,pro30
pap1637,70b0f299d5838aecf6386fb53429407487cae1a8,jou17,IEEE Transactions on Signal Processing,Efficient Sampling Set Selection for Bandlimited Graph Signals Using Graph Spectral Proxies,"We study the problem of selecting the best sampling set for bandlimited reconstruction of signals on graphs. A frequency domain representation for graph signals can be defined using the eigenvectors and eigenvalues of variation operators that take into account the underlying graph connectivity. Smoothly varying signals defined on the nodes are of particular interest in various applications, and tend to be approximately bandlimited in the frequency basis. Sampling theory for graph signals deals with the problem of choosing the best subset of nodes for reconstructing a bandlimited signal from its samples. Most approaches to this problem require a computation of the frequency basis (i.e., the eigenvectors of the variation operator), followed by a search procedure using the basis elements. This can be impractical, in terms of storage and time complexity, for real datasets involving very large graphs. We circumvent this issue in our formulation by introducing quantities called graph spectral proxies, defined using the powers of the variation operator, in order to approximate the spectral content of graph signals. This allows us to formulate a direct sampling set selection approach that does not require the computation and storage of the basis elements. We show that our approach also provides stable reconstruction when the samples are noisy or when the original signal is only approximately bandlimited. Furthermore, the proposed approach is valid for any choice of the variation operator, thereby covering a wide range of graphs and applications. We demonstrate its effectiveness through various numerical experiments.",Letter,vol17
pap1638,54155120a8801ed3afcc3bd3d4c6eb0fb4d028c7,jou102,IEEE Transactions on Geoscience and Remote Sensing,Urban-Area and Building Detection Using SIFT Keypoints and Graph Theory,"Very high resolution satellite images provide valuable information to researchers. Among these, urban-area boundaries and building locations play crucial roles. For a human expert, manually extracting this valuable information is tedious. One possible solution to extract this information is using automated techniques. Unfortunately, the solution is not straightforward if standard image processing and pattern recognition techniques are used. Therefore, to detect the urban area and buildings in satellite images, we propose the use of scale invariant feature transform (SIFT) and graph theoretical tools. SIFT keypoints are powerful in detecting objects under various imaging conditions. However, SIFT is not sufficient for detecting urban areas and buildings alone. Therefore, we formalize the problem in terms of graph theory. In forming the graph, we represent each keypoint as a vertex of the graph. The unary and binary relationships between these vertices (such as spatial distance and intensity values) lead to the edges of the graph. Based on this formalism, we extract the urban area using a novel multiple subgraph matching method. Then, we extract separate buildings in the urban area using a novel graph cut method. We form a diverse and representative test set using panchromatic 1-m-resolution Ikonos imagery. By extensive testings, we report very promising results on automatically detecting urban areas and buildings.",Conference paper,vol102
pap1639,f81a943a22abc31009f5bafc8c6c966189623593,jou182,Proceedings of the IEEE,Graph theory with applications to engineering and computer science,,Conference paper,vol182
pap1640,2b10bdea7bfe8061a47185d7525c998b8e446832,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Graph Theory. An Algorithmic Approach,,Erratum,pro85
pap1641,ca50b8cb4e34aeca0df18b060911ba28dd79ef90,con77,International Conference on Artificial Neural Networks,Graph theory and molecular orbitals,,Erratum,pro77
pap1642,54cf44503876c4a0c4875aa6f0dff837f0e17f39,con30,PS,Network Analysis of World Subway Systems Using Updated Graph Theory,"This paper demonstrates that network topologies play a key role in attracting people to use public transit; ridership is not solely determined by cultural characteristics (North American versus European versus Asian) or city design (transit oriented versus automobile oriented). The analysis considers 19 subway systems worldwide: those in Toronto, Ontario, Canada; Montreal, Quebec, Canada; Chicago, Illinois; New York City; Washington, D.C.; San Francisco, California; Mexico City, Mexico; London; Paris; Lyon, France; Madrid, Spain; Berlin; Athens, Greece; Stockholm, Sweden; Moscow; Tokyo; Osaka, Japan; Seoul, South Korea; and Singapore. The relationship between ridership and network design was studied by using updated graph theory concepts. Ridership was computed as the annual number of boardings per capita. Network design was measured according to three major indicators. The first is a measure of transit coverage and is based on the total number of stations and land area. The second relates to the maximum number of transfers necessary to go from one station to another and is called directness. The third attempts to get an overall view of transfer possibilities to travel in the network to appreciate a sense of mobility; it is termed connectivity. Multiple-regression analysis showed a strong relationship between these three indicators and ridership, achieving a goodness of fit (adjusted R2 value) of .725. The importance of network design is significant and should be considered in future public transportation projects.",Erratum,pro30
pap1643,cb03d03377bba14607b0a5603bc90aa4c0650b78,jou294,Genetic Epidemiology,Discovering genetic ancestry using spectral graph theory,"As one approach to uncovering the genetic underpinnings of complex disease, individuals are measured at a large number of genetic variants (usually SNPs) across the genome and these SNP genotypes are assessed for association with disease status. We propose a new statistical method called Spectral‐GEM for the analysis of genome‐wide association studies; the goal of Spectral‐GEM is to quantify the ancestry of the sample from such genotypic data. Ignoring structure due to differential ancestry can lead to an excess of spurious findings and reduce power. Ancestry is commonly estimated using the eigenvectors derived from principal component analysis (PCA). To develop an alternative to PCA we draw on connections between multidimensional scaling and spectral graph theory. Our approach, based on a spectral embedding derived from the normalized Laplacian of a graph, can produce more meaningful delineation of ancestry than by using PCA. Often the results from Spectral‐GEM are straightforward to interpret and therefore useful in association analysis. We illustrate the new algorithm with an analysis of the POPRES data [Nelson et al., 2008]. Genet. Epidemiol. 34:51–59, 2010. © 2009 Wiley‐Liss, Inc.",Conference paper,vol294
pap1644,e0ba6ba6c753958b1111b0fda344d9b2fe43ab5d,jou295,Journal of Biological Chemistry,Insights into the Organization of Biochemical Regulatory Networks Using Graph Theory Analyses*,"Graph theory has been a valuable mathematical modeling tool to gain insights into the topological organization of biochemical networks. There are two types of insights that may be obtained by graph theory analyses. The first provides an overview of the global organization of biochemical networks; the second uses prior knowledge to place results from multivariate experiments, such as microarray data sets, in the context of known pathways and networks to infer regulation. Using graph analyses, biochemical networks are found to be scale-free and small-world, indicating that these networks contain hubs, which are proteins that interact with many other molecules. These hubs may interact with many different types of proteins at the same time and location or at different times and locations, resulting in diverse biological responses. Groups of components in networks are organized in recurring patterns termed network motifs such as feedback and feed-forward loops. Graph analysis revealed that negative feedback loops are less common and are present mostly in proximity to the membrane, whereas positive feedback loops are highly nested in an architecture that promotes dynamical stability. Cell signaling networks have multiple pathways from some input receptors and few from others. Such topology is reminiscent of a classification system. Signaling networks display a bow-tie structure indicative of funneling information from extracellular signals and then dispatching information from a few specific central intracellular signaling nexuses. These insights show that graph theory is a valuable tool for gaining an understanding of global regulatory features of biochemical networks.",Letter,vol295
pap1645,abdf8623c1e5d8d2fd234a255a1ca6c32c8d6be7,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Algorithmic Graph Theory,"Preface 1. Introducing graphs and algorithmic complexity 2. Spanning-trees, branchings and connectivity 3. Planar graphs 4. Networks and flows 5. Matchings 6. Eulerian and Hamiltonian tours 7. Colouring graphs 8. Graph problems and intractability Appendix Author Index Subject Index.",Erratum,pro54
pap1646,4a95403d8fcadaccbf6d46c2a03c0ecf70cb3f9c,con76,IEEE International Conference on Tools with Artificial Intelligence,Computational Discrete Mathematics: Combinatorics and Graph Theory with Mathematica ®,"With examples of all 450 functions in action plus tutorial text on the mathematics, this book is the definitive guide to Experimenting with Combinatorica, a widely used software package for teaching and research in discrete mathematics. Three interesting classes of exercises are provided--theorem/proof, programming exercises, and experimental explorations--ensuring great flexibility in teaching and learning the material. The Combinatorica user community ranges from students to engineers, researchers in mathematics, computer science, physics, economics, and the humanities. Recipient of the EDUCOM Higher Education Software Award, Combinatorica is included with every copy of the popular computer algebra system Mathematica.",Erratum,pro76
pap1647,6211974939c9e83dd332278af5645228a32ccf57,con44,International Conference Knowledge Engineering and Knowledge Management,Discrete Mathematics and Graph Theory,"This comprehensive and self-contained text provides a thorough understanding of the concepts and applications of discrete mathematics and graph theory. It is written in such a manner that beginners can develop an interest in the subject. Besides providing the essentials, it also provides problem-solving techniques and develops the skill of how to think logically. Organized into two parts. The first part on discrete mathematics covers a wide range of topics such as predicate logic, recurrences, generating function, combinatorics, partially-ordered sets, lattices, Boolean algebra, finite state machines, finite fields, elementary number theory and discrete probability. The second part on graph theory covers planarity, colouring and partitioning, directed and algebraic graphs.",Erratum,pro44
pap1648,3fdef60d2c7738739bd7b74f5a3f7c076826ed30,con28,International Conference Geographic Information Science,Spectral Graph Theory and Network Dependability,"The paper introduces methods of graph theory for ranking substations of an electric power grid. In particular, spectral graph theory is used and several ranking algorithms are described. The procedure is illustrated on a practical numerical example",Erratum,pro28
pap1649,bdf8787c9aa949043f866116354e8c8fbc4d5f41,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Introduction to Graph Theory,Preface Basic Definitions and Concepts Trees and Bipartite Graphs Chordal Graphs Planar Graphs Graph Coloring Graph Traversals and Flows Appendix Index.,Erratum,pro49
pap1650,08b24601e1a1db623f6cebf740c557b23809b509,con77,International Conference on Artificial Neural Networks,Application of the graph theory and matrix methods to contractor ranking,,Erratum,pro77
pap1651,a02c8182213c3501962bc3214cf236beb85b1684,jou129,Conservation Biology,A Graph‐Theory Framework for Evaluating Landscape Connectivity and Conservation Planning,"Abstract:  Connectivity of habitat patches is thought to be important for movement of genes, individuals, populations, and species over multiple temporal and spatial scales. We used graph theory to characterize multiple aspects of landscape connectivity in a habitat network in the North Carolina Piedmont (U.S.A).. We compared this landscape with simulated networks with known topology, resistance to disturbance, and rate of movement. We introduced graph measures such as compartmentalization and clustering, which can be used to identify locations on the landscape that may be especially resilient to human development or areas that may be most suitable for conservation. Our analyses indicated that for songbirds the Piedmont habitat network was well connected. Furthermore, the habitat network had commonalities with planar networks, which exhibit slow movement, and scale‐free networks, which are resistant to random disturbances. These results suggest that connectivity in the habitat network was high enough to prevent the negative consequences of isolation but not so high as to allow rapid spread of disease. Our graph‐theory framework provided insight into regional and emergent global network properties in an intuitive and visual way and allowed us to make inferences about rates and paths of species movements and vulnerability to disturbance. This approach can be applied easily to assessing habitat connectivity in any fragmented or patchy landscape.",Letter,vol129
pap1652,c29287c9f51cb5adc2db7d62439d1a54a295debb,con65,IEEE International Conference on Software Engineering and Formal Methods,Chromatic Graph Theory,"Beginning with the origin of the four color problem in 1852, the field of graph colorings has developed into one of the most popular areas of graph theory. Introducing graph theory with a coloring theme, Chromatic Graph Theory explores connections between major topics in graph theory and graph colorings as well as emerging topics. This self-contained book first presents various fundamentals of graph theory that lie outside of graph colorings, including basic terminology and results, trees and connectivity, Eulerian and Hamiltonian graphs, matchings and factorizations, and graph embeddings. The remainder of the text deals exclusively with graph colorings. It covers vertex colorings and bounds for the chromatic number, vertex colorings of graphs embedded on surfaces, and a variety of restricted vertex colorings. The authors also describe edge colorings, monochromatic and rainbow edge colorings, complete vertex colorings, several distinguishing vertex and edge colorings, and many distance-related vertex colorings. With historical, applied, and algorithmic discussions, this text offers a solid introduction to one of the most popular areas of graph theory.",Erratum,pro65
pap1653,d869bcd9c4c2eb29fb4d01906841d1d79d5c9cb6,con24,International Conference on Data Technologies and Applications,AN EXTREMAL PROBLEM IN GRAPH THEORY,"G(?z; I) will denote a graph of n vertices and 1 edges. Let fO(lz, K) be the smallest integer such that there is a G (n; f,, (n, k)) in which for every set of K vertices there is a vertex joined to each of these. Thus for example fO(3, 2) = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f,(4, 2) = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k, and",Erratum,pro24
pap1654,7fc5859635b8779519698c33168256fd99ac3741,con90,Computer Vision and Pattern Recognition,"Algorithmic Graph Theory and Perfect Graphs (Annals of Discrete Mathematics, Vol 57)",,Erratum,pro90
pap1655,36d3fc0411bec5aadfba8087a33022f99658ccf4,con66,International Conference on Software Reuse,Graph Theory and Interconnection Networks,"The advancement of large scale integrated circuit technology has enabled the construction of complex interconnection networks. Graph theory provides a fundamental tool for designing and analyzing such networks. Graph Theory and Interconnection Networks provides a thorough understanding of these interrelated topics. After a brief introduction to graph terminology, this book presents well-known interconnection networks as examples of graphs, followed by in-depth coverage of Hamiltonian graphs. Different types of problems illustrate the wide range of available methods for solving such problems. The text also explores recent progress on the diagnosability of graphs under various models.",Erratum,pro66
pap1656,f96289e50a9486e8d423e9d207e89777d8eed5ef,jou296,"Journal of Speech, Language and Hearing Research",What can graph theory tell us about word learning and lexical retrieval?,"PURPOSE
Graph theory and the new science of networks provide a mathematically rigorous approach to examine the development and organization of complex systems. These tools were applied to the mental lexicon to examine the organization of words in the lexicon and to explore how that structure might influence the acquisition and retrieval of phonological word-forms.


METHOD
Pajek, a program for large network analysis and visualization (V. Batagelj & A. Mvrar, 1998), was used to examine several characteristics of a network derived from a computerized database of the adult lexicon. Nodes in the network represented words, and a link connected two nodes if the words were phonological neighbors.


RESULTS
The average path length and clustering coefficient suggest that the phonological network exhibits small-world characteristics. The degree distribution was fit better by an exponential rather than a power-law function. Finally, the network exhibited assortative mixing by degree. Some of these structural characteristics were also found in graphs that were formed by 2 simple stochastic processes suggesting that similar processes might influence the development of the lexicon.


CONCLUSIONS
The graph theoretic perspective may provide novel insights about the mental lexicon and lead to future studies that help us better understand language development and processing.",Letter,vol296
pap1657,a83bfe8999ba5f6c4baceb03e7a212a9d3b18e00,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Topics in Graph Theory: Graphs and Their Cartesian Product,"From specialists in the field, learn about interesting connections and recent developments in the field of graph theory by looking in particular at Cartesian products arguably the most important of the four standard graph products. Many new results in this area appear for the first time in print in this book. Written in an accessible way, this book can be used for personal study in advanced applications of graph theory or for an advanced graph theory course.",Erratum,pro49
pap1658,17c4b0fe4eaf5d96053952a0ffb5ac0b1fbfbe3d,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,A BIRD'S EYE VIEW OF THE CUT METHOD AND A SURVEY OF ITS APPLICATIONS IN CHEMICAL GRAPH THEORY,"A general description of the cut method is presented and an overview of its applications in chemical graph theory is given. Applications include the Wiener index, the Szeged index, the hyper-Wiener index, the PI index, the weighted Wiener index, Wiener-type indices, and classes of chemical graphs such as trees, benzenoid graphs and phenylenes. A computation of the Wiener index of an arbitrary connected graph using its canonical metric representation is described. Algorithmic issues are also briefly mentioned as well as are the recently introduced CI index and related polynomials.",Erratum,pro58
pap1659,fa1f95d61ab2ac44c9bfdf9b53a80d6617545b95,jou297,Current protein and peptide science,Proteins as networks: usefulness of graph theory in protein science.,"The network paradigm is based on the derivation of emerging properties of studied systems by their representation as oriented graphs: any system is traced back to a set of nodes (its constituent elements) linked by edges (arcs) correspondent to the relations existing between the nodes. This allows for a straightforward quantitative formalization of systems by means of the computation of mathematical descriptors of such graphs (graph theory). The network paradigm is particularly useful when it is clear which elements of the modelled system must play the role of nodes and arcs respectively, and when topological constraints have a major role with respect to kinetic ones. In this review we demonstrate how nodes and arcs of protein topology are characterized at different levels of definition: 1. Recurrence matrix of hydrophobicity patterns along the sequence 2. Contact matrix of alpha carbons of 3D structures 3. Correlation matrix of motions of different portion of the molecule in molecular dynamics. These three conditions represent different but potentially correlated reticular systems that can be profitably analysed by means of network analysis tools.",Letter,vol297
pap1660,20980d595d9dbdd3f3ef47e5b072b65799f0525a,con7,International Symposium on Intelligent Data Analysis,From Graph Theory to Models of Economic Networks. A Tutorial.,,Erratum,pro7
pap1661,e71dc6f83f18f654824c00f66361d346aa896049,jou298,Evolutionary Applications,Applications of graph theory to landscape genetics,"We investigated the relationships among landscape quality, gene flow, and population genetic structure of fishers (Martes pennanti) in ON, Canada. We used graph theory as an analytical framework considering each landscape as a network node. The 34 nodes were connected by 93 edges. Network structure was characterized by a higher level of clustering than expected by chance, a short mean path length connecting all pairs of nodes, and a resiliency to the loss of highly connected nodes. This suggests that alleles can be efficiently spread through the system and that extirpations and conservative harvest are not likely to affect their spread. Two measures of node centrality were negatively related to both the proportion of immigrants in a node and node snow depth. This suggests that central nodes are producers of emigrants, contain high‐quality habitat (i.e., deep snow can make locomotion energetically costly) and that fishers were migrating from high to low quality habitat. A method of community detection on networks delineated five genetic clusters of nodes suggesting cryptic population structure. Our analyses showed that network models can provide system‐level insight into the process of gene flow with implications for understanding how landscape alterations might affect population fitness and evolutionary potential.",Conference paper,vol298
pap1662,7233d8f8d9a0aaaa98b395b998b6e82f2f44ca28,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,GRAPH THEORY AND COMPLEX NETWORKS,"In the past ten years,the fast development of complex network theory has provided a good support for the study of complexity and complex systems,since they describe clearly the important characteristics of complex systems,and show bright prospects in theory and applications.This paper presents mainly the application of graph theory to complex networks,especially to the synchronization problem of complex networks First,its application to the estimations of smallest nonzero,largest eigenvalues and synchronizability index of certain graphs are commented,followed by the effects of subgraph and graph eigenvector in the estimation of synchronizability index.Furthermore,the complexity between the relationships of synchronizability and network structural parameters are discussed via two simple graphs,and the effects of complementary graph, edge-addition and graph operation on the synchronization of complex networks are elaborated.Finally,some possible development directions in complex networks are predicted from the viewpoint of graph and control theory.",Erratum,pro82
pap1663,4f1e6ec98650287ead42b1f231831234385c5e6b,con69,Formal Concept Analysis,Applying Graph Theory to Interaction Design,,Erratum,pro69
pap1664,616e3dce90c8c261dc51c2f8db6306e0c634f518,con60,European Conference on Software Process Improvement,ON A PROBLEM OF GRAPH THEORY,"Let G "" be a non-directed graph having n vertices, without parallel edges and slings. Let the vertices of Gn be denoted by F 1 ,. . ., Pn. Let v(P j) denote the valency of the point P i and put (0. 1) V(G,) = max v(Pj). 1ninn Let E(G.) denote the number of edges of Gn. Let H d (n, k) denote the set of all graphs Gn for which V (G n) = k and the diameter D (Gn) of which is-d, In the present paper we shall investigate the quantity (0 .2) Thus we want to determine the minimal number N such that there exists a graph having n vertices, N edges and diameter-d and the maximum of the valencies of the vertices of the graph is equal to k. To help the understanding of the problem let us consider the following interpretation. Let be given in a country n airports ; suppose we want to plan a network of direct flights between these airports so that the maximal number of airports to which a given airport can be connected by a direct flight should be equal to k (i .e. the maximum of the capacities of the airports is prescribed), further it should be possible to fly from every airport to any other by changing the plane at most d-1 times ; what is the minimal number of flights by which such a plan can be realized? For instance, if n = 7, k = 3, d= 2 we have F2 (7, 3) = 9 and the extremal graph is shown by Fig. 1. The problem of determining Fd (n, k) has been proposed and discussed recently by two of the authors (see [1]). In § 1 we give a short summary of the results of the paper [1], while in § 2 and 3 we give some new results which go beyond those of [1]. Incidentally we solve a long-standing problem about the maximal number of edges of a graph not containing a cycle of length 4. In § 4 we mention some unsolved problems. Let us mention that our problem can be formulated also in terms of 0-1 matrices as follows : Let M=(a il) be a symmetrical n by n zero-one matrix such 2",Erratum,pro60
pap1665,d52a75da5a3785753437bd0e74935692764cb157,con72,Bioinformatics and Computational Biology,Recent Results in the Theory of Graph Spectra,,Erratum,pro72
pap1666,48ebab32e739e314cfcc6a8de9dd145b934aa5e6,jou299,"Proteins: Structure, Function, and Bioinformatics",Protein flexibility predictions using graph theory,"Techniques from graph theory are applied to analyze the bond networks in proteins and identify the flexible and rigid regions. The bond network consists of distance constraints defined by the covalent and hydrogen bonds and salt bridges in the protein, identified by geometric and energetic criteria. We use an algorithm that counts the degrees of freedom within this constraint network and that identifies all the rigid and flexible substructures in the protein, including overconstrained regions (with more crosslinking bonds than are needed to rigidify the region) and underconstrained or flexible regions, in which dihedral bond rotations can occur. The number of extra constraints or remaining degrees of bond‐rotational freedom within a substructure quantifies its relative rigidity/flexibility and provides a flexibility index for each bond in the structure. This novel computational procedure, first used in the analysis of glassy materials, is approximately a million times faster than molecular dynamics simulations and captures the essential conformational flexibility of the protein main and side‐chains from analysis of a single, static three‐dimensional structure. This approach is demonstrated by comparison with experimental measures of flexibility for three proteins in which hinge and loop motion are essential for biological function: HIV protease, adenylate kinase, and dihydrofolate reductase. Proteins 2001;44:150–165. © 2001 Wiley‐Liss, Inc.",Article,vol299
pap1667,603c893660f04899f6b220795348f236d6896cf3,jou300,Ecological Applications,Graph theory as a proxy for spatially explicit population models in conservation planning.,"Spatially explicit population models (SEPMs) are often considered the best way to predict and manage species distributions in spatially heterogeneous landscapes. However, they are computationally intensive and require extensive knowledge of species' biology and behavior, limiting their application in many cases. An alternative to SEPMs is graph theory, which has minimal data requirements and efficient algorithms. Although only recently introduced to landscape ecology, graph theory is well suited to ecological applications concerned with connectivity or movement. This paper compares the performance of graph theory to a SEPM in selecting important habitat patches for Wood Thrush (Hylocichla mustelina) conservation. We use both models to identify habitat patches that act as population sources and persistent patches and also use graph theory to identify patches that act as stepping stones for dispersal. Correlations of patch rankings were very high between the two models. In addition, graph theory offers the ability to identify patches that are very important to habitat connectivity and thus long-term population persistence across the landscape. We show that graph theory makes very similar predictions in most cases and in other cases offers insight not available from the SEPM, and we conclude that graph theory is a suitable and possibly preferable alternative to SEPMs for species conservation in heterogeneous landscapes.",Letter,vol300
pap1668,6caa5eddffaed74dcc9bc7f83b0787d4d0918e0e,con78,Neural Information Processing Systems,Identifying Critical Locations in a Spatial Network with Graph Theory,Effective management of infrastructural networks in the case of a crisis requires a prior analysis of the vulnerability of spatial networks and identification of critical locations where an interdiction would cause damage and disruption. This article presents a mathematical method for modelling the vulnerability risk of network elements which can be used for identification of critical locations in a spatial network. The method combines dual graph modelling with connectivity analysis and topological measures and has been tested on the street network of the Helsinki Metropolitan Area in Finland. Based on the results of this test the vulnerability risk of the network elements was experimentally defined. Further developments are currently under consideration for eventually developing a risk model not only for one but for a group of co‐located spatial networks.,Erratum,pro78
pap1669,fd6432fa2b032dd5f246d26460bf3353c43c257a,jou17,IEEE Transactions on Signal Processing,Discrete Signal Processing on Graphs: Sampling Theory,"We propose a sampling theory for signals that are supported on either directed or undirected graphs. The theory follows the same paradigm as classical sampling theory. We show that perfect recovery is possible for graph signals bandlimited under the graph Fourier transform. The sampled signal coefficients form a new graph signal, whose corresponding graph structure preserves the first-order difference of the original graph signal. For general graphs, an optimal sampling operator based on experimentally designed sampling is proposed to guarantee perfect recovery and robustness to noise; for graphs whose graph Fourier transforms are frames with maximal robustness to erasures as well as for Erdös-Rényi graphs, random sampling leads to perfect recovery with high probability. We further establish the connection to the sampling theory of finite discrete-time signal processing and previous work on signal recovery on graphs. To handle full-band graph signals, we propose a graph filter bank based on sampling theory on graphs. Finally, we apply the proposed sampling theory to semi-supervised classification of online blogs and digit images, where we achieve similar or better performance with fewer labeled samples compared to previous work.",Letter,vol17
pap1670,bf187d5d6bf7b18d8ab19b01da796665d622c60c,jou9,Chemical Reviews,Some new trends in chemical graph theory.,"Unidad de Investigación de Diseño de Farmacos y Conectividad Molecular, Departamento de Quı́mica Fisica, Facultad de Farmacı́a, Universitat de València, 46100 Burjassot, València, Spain, Instituto de Tecnologia Quimica, CSIC-Universidad Politecnica de Valencia, Av. de los Naranjos s/n, 46022 València, Spain, and Dipartimento di Chimica, Università della Calabria, via P. Bucci 14/C, 87036 Rende (CS), Italy",Article,vol9
pap1671,88552bf7baa57b884e3e4aa0838f20da921b8cbe,con88,European Conference on Computer Vision,Graph theory and its applications,"In this paper we will discuss how problems like Page ranking and finding the shortest paths can be solved by using Graph Theory. At its core, graph theory is the study of graphs as mathematical structures. In our paper, we will first cover Graph Theory as a broad topic. Then we will move on to Linear Algebra. Linear Algebra is the study of matrices. We will apply the skills discussed in these two sections to Dijkstra Algorithms which cover how to find the shortest paths in graphs. Finally, we will present PageRank where we will demonstrate how to rank pages based on their importance.",Erratum,pro88
pap1672,04ff6d8d8708aef4ebc45ebee132fcc6f055bbd4,con59,Annual Workshop of the Psychology of Programming Interest Group,Modern temporal network theory: a colloquium,,Erratum,pro59
pap1673,5076d4a29581a986ec08e5c48cac0d6f3b2a1f3c,con74,IEEE International Conference on Information Reuse and Integration,Landscape connectivity: A conservation application of graph theory,"We use focal-species analysis to apply a graph-theoretic approach to landscape connectivity in the Coastal Plain of North Carolina. In doing so we demonstrate the utility of a mathematical graph as an ecological construct with respect to habitat connectivity. Graph theory is a well established mainstay of information technology and is concerned with highly efficient network flow. It employs fast algorithms and compact data structures that are easily adapted to landscape-level focal species analysis. American mink (Mustela vison) and prothonotary warblers (Protonotaria citrea) share the same habitat but have different dispersal capabilities, and therefore provide interesting comparisons on connections in the landscape. We built graphs using GIS coverages to define habitat patches and determined the functional distance between the patches with least-cost path modeling. Using graph operations concerned with edge and node removal we found that the landscape is fundamentally connected for mink and fundamentally unconnected for prothonotary warblers. The advantage of a graph-theoretic approach over other modeling techniques is that it is a heuristic framework which can be applied with very little data and improved from the initial results. We demonstrate the use of graph theory in a metapopulation context, and suggest that graph theory as applied to conservation biology can provide leverage on applications concerned with landscape connectivity.",Erratum,pro74
pap1674,125bb9e2f465f3fec6646ef286edc6d41074ca50,con84,Workshop on Interdisciplinary Software Engineering Research,On Graph Theory and Its Application,"Graph theory has around 300 years of history,but many problems haven't been solved.With the development of computer science,graph theory becomes hot point again.In this paper,the application of graph theory is discussed.",Erratum,pro84
pap1675,f253627bfa7485c0e6e5e8f53a682d7149f0f790,jou301,NeuroImage,Characterizing brain anatomical connections using diffusion weighted MRI and graph theory,,Letter,vol301
pap1676,47199ebc578a0c80ecdafe496bd05b44140d6cdc,jou302,Electronic Journal of Combinatorics,Extremal Graph Theory for Metric Dimension and Diameter,"A set of vertices $S$ resolves a connected graph $G$ if every vertex is uniquely determined by its vector of distances to the vertices in $S$. The metric dimension of $G$ is the minimum cardinality of a resolving set of $G$. Let ${\cal G}_{\beta,D}$ be the set of graphs with metric dimension $\beta$ and diameter $D$. It is well-known that the minimum order of a graph in ${\cal G}_{\beta,D}$ is exactly $\beta+D$. The first contribution of this paper is to characterise the graphs in ${\cal G}_{\beta,D}$ with order $\beta+D$ for all values of $\beta$ and $D$. Such a characterisation was previously only known for $D\leq2$ or $\beta\leq1$. The second contribution is to determine the maximum order of a graph in ${\cal G}_{\beta,D}$ for all values of $D$ and $\beta$. Only a weak upper bound was previously known.",Article,vol302
pap1677,04eb71ccc65edf1c2b2f3b43bdb89a8e90fe8a8b,con32,International Conference on Software Technology: Methods and Tools,PEARLS in GRAPH THEORY,,Erratum,pro32
pap1678,de6f9eb6762046bde56edf788be65f1858e33396,jou133,PLoS ONE,Fast Approximate Quadratic Programming for Graph Matching,"Quadratic assignment problems arise in a wide variety of domains, spanning operations research, graph theory, computer vision, and neuroscience, to name a few. The graph matching problem is a special case of the quadratic assignment problem, and graph matching is increasingly important as graph-valued data is becoming more prominent. With the aim of efficiently and accurately matching the large graphs common in big data, we present our graph matching algorithm, the Fast Approximate Quadratic assignment algorithm. We empirically demonstrate that our algorithm is faster and achieves a lower objective value on over 80% of the QAPLIB benchmark library, compared with the previous state-of-the-art. Applying our algorithm to our motivating example, matching C. elegans connectomes (brain-graphs), we find that it efficiently achieves performance.",Article,vol133
pap1679,52e2d4299190d1391ca92794e9e708b5c4f6463c,con34,International Conference on Agile Software Development,Graph Theory,,Erratum,pro34
pap1680,ced040290fa21c51c43a5be0eb2bbd5f2e59d54b,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Implementing discrete mathematics - combinatorics and graph theory with Mathematica,"Permutations and Combinations Permutations Permutation Groups Inversions and Inversion Vectors Special Classes of Permutations Combinations Exercises and Research Problems * Partitions, Compositions, and Young Tableaux Partitions Compositions Young Tableaux Exercises and Research Problems * Representing Graphs Data Structures for Graphs Elementary Graph Operations Graph Embeddings Storage Formats Exercises and Research Problems * Generating Graphs Regular Structures Trees Random Graphs Relations and Functional Graphs Exercises and Research Problems * Properties of Graphs Connectivity Graph Isomorphism Cycles in Graphs Partial Orders Graph Coloring Cliques, Vertex Covers, and Independent Sets Exercises and Research Problems * Algorithmic Graph Theory Shortest Paths Minimum Spanning Trees Network Flow Matching Planar Graphs Exercises and Research Problems",Erratum,pro13
pap1681,278fde8c3c7d0ee96f0e562eba1c9cbb15428532,con110,Very Large Data Bases Conference,Graph theory and molecular orbitals. XII. Acyclic polyenes,"A graph‐theoretical study of acyclic polyenes is carried out with an emphasis on the influence of branching on several molecular properties. A definition of branching is given and several branching indices are analyzed. The case of polyenes without a Kekule structure is discussed briefly. The main conclusions are: (a) thermodynamic stability of conjugated polyenes decreases with branching, but (b) reactivity, in general, increases with branching.",Erratum,pro110
pap1682,e764da56e8b4ba6bc21a80ec70b34a8a7b69de59,jou303,IET Systems Biology,Graph theory and networks in Biology.,"A survey of the use of graph theoretical techniques in Biology is presented. In particular, recent work on identifying and modelling the structure of bio-molecular networks is discussed, as well as the application of centrality measures to interaction networks and research on the hierarchical structure of such networks and network motifs. Work on the link between structural network properties and dynamics is also described, with emphasis on synchronisation and disease propagation.",Article,vol303
pap1683,eee1a2253d1d90cdb69b5666c59cfeb25498c957,con101,International Conference on Biometrics,Szemeredi''s Regularity Lemma and its applications in graph theory,"Szemer\''edi''s Regularity Lemma is an important tool in discrete mathematics. It says that, in some sense, all graphs can be approximated by random-looking graphs. Therefore the lemma helps in proving theorems for arbitrary graphs whenever the corresponding result is easy for random graphs. Recently quite a few new results were obtained by using the Regularity Lemma, and also some new variants and generalizations appeared. In this survey we describe some typical applications and some generalizations.",Erratum,pro101
pap1684,37c1011ac59b0b4685b3dc8a9dda4d8f6e68f6c2,con66,International Conference on Software Reuse,A textbook of graph theory,Basic Results.- Directed Graphs.- Connectivity.- Trees.- Independent Sets and Matchings.- Eulerian and Hamiltonian Graphs.- Graph Colourings.- Planarity.- Triangulated Graphs.- Applications.,Erratum,pro66
pap1685,4eb5e8f4ff335fcc7115ab382db847fd3dfb0a14,con109,International Society for Music Information Retrieval Conference,Introduction to Graph Theory,"Introduction * Definitions and examples* Paths and cycles* Trees* Planarity* Colouring graphs* Matching, marriage and Menger's theorem* Matroids Appendix 1: Algorithms Appendix 2: Table of numbers List of symbols Bibliography Solutions to selected exercises Index",Erratum,pro109
pap1686,f6433be2ca69c760d51e9e68f7b2859e50422985,con108,International Conference on Information Integration and Web-based Applications & Services,A Walk Through Combinatorics: An Introduction to Enumeration and Graph Theory,"Basic Methods: Seven Is More Than Six. The Pigeon-Hole Principle One Step at a Time. The Method of Mathematical Induction Enumerative Combinatorics: There Are a Lot of Them. Elementary Counting Problems No Matter How You Slice It. The Binomial Theorem and Related Identities Divide and Conquer. Partitions Not So Vicious Cycles. Cycles in Permutations You Shall Not Overcount. The Sieve A Function is Worth Many Numbers. Generating Functions Graph Theory: Dots and Lines. The Origins of Graph Theory Staying Connected. Trees Finding a Good Match. Coloring and Matching Do Not Cross. Planar Graphs Horizons: Does It Clique? Ramsey Theory So Hard to Avoid. Subsequence Conditions on Permutations Who Knows What It Looks Like, but It Exists. The Probabilistic Method At Least Some Order. Partial Orders and Lattices The Sooner The Better. Combinatorial Algorithms Does Many Mean More Than One? Computational Complexity.",Erratum,pro108
pap1687,d141ad437d2b932d647cf16b81b8531645a4cd6c,con37,International Symposium on Search Based Software Engineering,Applied and algorithmic graph theory,"Designed as the bridge to cross the widening gap between mathematics and computer science, and planned as the mathematical base for computer science students, this maths text is written for upper-level college students who have had previous coursework involving proofs and proof techniques. The close tie between the theoretical and algorithmic aspects of graph theory, and graphs that lend themselves naturally as models in computer science, results in a need for efficient algorithims to solve any large scale problems. Each algorithm in the text includes explanatory statements that clarify individual steps, a worst-case complexity analysis, and algorithmic correctness proofs. As a result, the student will develop an understanding of the concept of an efficient algorithm.",Erratum,pro37
pap1688,b7bf5e535ea38a781268b4580265a96cfc7da7c2,con95,IEEE International Conference on Computer Vision,A LIMIT THEOREM IN GRAPH THEORY,"In this paper G(n ; I) will denote a graph of n vertices and l edges, K„ will denote the complete graph of p vertices G (p ; (PA and K,(p i , . . ., p,) will denote the rchromatic graph with p i vertices of the i-th colour, in which every two vertices of different colour are adjacent . 7r(G) will denote the number of vertices of G and v(G) denotes the number of edges of G . G(n :1) denotes the complementary graph of G(n : l) i . e. G(n ; 1) is the G (ii : (211) -/) which has the samevertices as G(n ; 1)",Erratum,pro95
pap1689,afb562a7910b7c7be22fc0638d2d4951a5ff7b36,con48,ACM Symposium on Applied Computing,Topics in algebraic graph theory,Foreword Peter J. Cameron Introduction 1. Eigenvalues of graphs Michael Doob 2. Graphs and matrices Richard A. Brualdi and Bryan L. Shader 3. Spectral graph theory Dragos Cvetkovic and Peter Rowlinson 4. Graph Laplacians Bojan Mohar 5. Automorphism groups Peter J. Cameron 6. Cayley graphs Brian Alspach 7. Finite symmetric graphs Cheryle E. Praeger 8. Strongly regular graphs Peter J. Cameron 9. Distance-transitive graphs Arjeh M. Cohen 10. Computing with graphs and groups Leonard H. Soicher.,Erratum,pro48
pap1690,f6ab7318774c16680681d0b764cefe51a31bd49c,con54,Conference of the Centre for Advanced Studies on Collaborative Research,A Walk Through Combinatorics: An Introduction to Enumeration and Graph Theory,"Basic Methods: Seven Is More Than Six. The Pigeon-Hole Principle One Step at a Time. The Method of Mathematical Induction Enumerative Combinatorics: There Are a Lot of Them. Elementary Counting Problems No Matter How You Slice It. The Binomial Theorem and Related Identities Divide and Conquer. Partitions Not So Vicious Cycles. Cycles in Permutations You Shall Not Overcount. The Sieve A Function is Worth Many Numbers. Generating Functions Graph Theory: Dots and Lines. The Origins of Graph Theory Staying Connected. Trees Finding a Good Match. Coloring and Matching Do Not Cross. Planar Graphs Horizons: Does It Clique? Ramsey Theory So Hard to Avoid. Subsequence Conditions on Permutations Who Knows What It Looks Like, but It Exists. The Probabilistic Method At Least Some Order. Partial Orders and Lattices The Sooner The Better. Combinatorial Algorithms Does Many Mean More Than One? Computational Complexity.",Erratum,pro54
pap1691,0b8a18c78e7493fe145382d98fc46f6b427a093f,con99,North American Chapter of the Association for Computational Linguistics,"Graph Theory: Modeling, Applications, and Algorithms","Preface 1 Introduction to Graph Theory 2 Basic Concepts in Graph Theory 3 TreesandForests 4 Spanning Trees 5 Fundamental Properties of Graphs and Digraphs 6 Connectivity and Flow 7 Planar Graphs 8 Graph Coloring 9 Coloring Enumerations and Chordal Graphs 10 Independence,Dominance, and Matchings 11 Cover Parameters and MatchingPolynomials 12 GraphCounting 13 Graph Algorithms APPENDICES A Greek Alphabet B Notation C Top Ten Online References Index ix",Erratum,pro99
pap1692,5dde7192c840ae6f348008bd0fa0a652306da145,con61,International Conference on Predictive Models in Software Engineering,GRAPH THEORY AND PROBABILITY,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(lz) are (3)",Erratum,pro61
pap1693,c8a9622e6cde873c680d1888e957837553672e89,jou219,Journal of chemical information and computer sciences,Applications of graph theory in chemistry,"Graph theoretical (GT) applications in chemistry underwent a dramatic revival lately. Constitutional (molecular) graphs have points (vertices) representing atoms and lines (edges) symbolizing malent bonds. This review deals with definition. enumeration. and systematic coding or nomenclature of constitutional or steric isomers, valence isomers (especially of annulenes). and condensed polycyclic aromatic hydrocarbons. A few key applications of graph theory in theoretical chemistry are pointed out. The complete set of all poasible monocyclic aromatic and heteroaromatic compounds may be explored by a mmbination of Pauli's principle, P6lya's theorem. and electronegativities. Topological indica and some of their applications are reviewed. Reaction graphs and synthon graphs differ from constitutional graphs i n their meaning of vertices and edges and find other kinds of chemical applications. This paper ends with a review of the use of GT applications for chemical nomenclature (nodal nomenclature and related areas), coding. and information processing/storage/retrieval",Conference paper,vol219
pap1694,9a5a7a1b13f51af9369ef4ddeb75bb8b5ff87e70,jou266,Mathematical Gazette,Graph Theory: An Introductory Course,"From the reviews: ""Bla Bollob's introductory course on graph theory deserves to be considered as a watershed in the development of this theory as a serious academic subject...The book has chapters on electrical networks, flows, connectivity and matchings, extremal problems, colouring, Ramsey theory, random graphs, and graphs and groups. Each chapter starts at a measured and gentle pace. Classical results are proved and new insight is provided, with the examples at the end of each chapter fully supplementing the text...Even so this allows an introduction not only to some of the deeper results but, more vitally, provides outlines of, and firm insights into, their proofs. Thus in an elementary text book, we gain an overall understanding of well-known standard results, and yet at the same time constant hints of, and guidelines into, the higher levels of the subject. It is this aspect of the book which should guarantee it a permanent place in the literature.""",Conference paper,vol266
pap1695,67fbb295f46f4d630fd76f6bee13f0b985216bce,con61,International Conference on Predictive Models in Software Engineering,PROTEIN STRUCTURE: INSIGHTS FROM GRAPH THEORY,"The sequence and structure of a large body of proteins are becoming increasingly available. It is desirable to explore mathematical tools for ecient extraction of information from such sources. The principles of graph theory, which was earlier applied in elds such as electrical engineering and computer networks are now being adopted to investigate protein structure, folding, stability, function and dynamics. This review deals with a brief account of relevant graphs and graph theoretic concepts. The concepts of protein graph construction are discussed. The manner in which graphs are analyzed and parameters relevant to protein structure are extracted, are explained. The structural and biological information derived from protein structures using these methods is presented.",Erratum,pro61
pap1696,bfb523f12a2d3b7b3cc4fa11effd4de0a7ff998f,con57,International Workshop on Agent-Oriented Software Engineering,Graph Theory Methods for the Analysis of Neural Connectivity Patterns,,Erratum,pro57
pap1697,84ba27f5ed15a9751aaac63d0d5068034351da0a,con97,ACM SIGMOD Conference,SPECTRAL GRAPH THEORY AND THE INVERSE EIGENVALUE PROBLEM OF A GRAPH,"Spectral Graph Theory is the study of the spectra of certain matrices defined from a given graph, including the adjacency matrix, the Laplacian matrix and other related matrices. Graph spectra have been studied extensively for more than fifty years. In the last fifteen years, interest has developed in the study of generalized Laplacian matrices of a graph, that is, real symmetric matrices with negative off-diagonal entries in the positions described by the edges of the graph (and zero in every other off-diagonal position). The set of all real symmetric matrices having nonzero off-diagonal entries exactly where the graph G has edges is denoted by S(G). Given a graph G, the problem of characterizing the possible spectra of B, such that B ∈S (G), has been referred to as the Inverse Eigenvalue Problem of a Graph .I n the last fifteen years a number of papers on this problem have appeared, primarily concerning trees. The adjacency matrix and Laplacian matrix of G and their normalized forms are all in S(G). Recent work on generalized Laplacians and Colin de Verdiere matrices is bringing the two areas closer together. This paper surveys results in Spectral Graph Theory and the Inverse Eigenvalue Problem of a Graph, examines the connections between these problems, and presents some new results on construction of a matrix of minimum rank for a given graph having a special form such as a 0,1-matrix or a generalized Laplacian.",Erratum,pro97
pap1698,7d0d69eec235daf374e5a72a0831346e27fd1cae,con84,Workshop on Interdisciplinary Software Engineering Research,Application of graph theory to OO software engineering,"Graph Theory, which studies the properties of graphs, has been widely accepted as a core subject in the knowledge of computer scientists. So is Object-Oriented (OO) software engineering, which deals with the analysis, design and implementation of systems employing classes as modules. The latter field can greatly benefit from the application of Graph Theory, since the main mode of representation, namely the class diagram, is essentially a directed graph. The study of graph properties can be valuable in many ways for understanding the characteristics of the underlying software systems. Representative examples for the usefulness of graph theory on OO systems based on recent research results are presented in this paper.",Conference paper,pro84
pap1699,0a99f890d1190443b0c8c02062363780d64dd907,con89,Conference on Uncertainty in Artificial Intelligence,Complex Networks: from Graph Theory to Biology,,Erratum,pro89
pap1700,a0894388f6dfc841c68d2458a390d924a9c0db0f,con108,International Conference on Information Integration and Web-based Applications & Services,A graph theory interpretation of nodal regions,,Erratum,pro108
pap1701,c091d3b62283cecb205f5e4af63e677166bf281f,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Dynamic Modelling of Mechatronic Multibody Systems With Symbolic Computing and Linear Graph Theory,"The application of linear graph theory to the modelling of flexible multibody systems is described. When combined with symbolic computing methods, linear graph theory leads to efficient dynamic models that facilitate real-time simulation of systems of rigid bodies and flexible beams. The natural extension of linear graphs to the modelling of mechatronic multibody systems is presented, along with a recently-developed theory for building complex system models from models of individual subsystems.",Erratum,pro54
pap1702,92a46efd598cd478bccddacd8703a254bfc885b0,con3,Knowledge Discovery and Data Mining,Model structure analysis through graph theory: partition heuristics and feedback structure decomposition,"The argument of this article is that it is possible to focus on the structural complexity of system dynamics models to design a partition strategy that maximizes the test points between the model and the real world, and a calibration sequence that permits an incremental development of model confidence. It further argues that graph theory could be used as a basis for making sense of the structural complexity of system dynamics models, and that this structure could be used as a basis for more formal analysis of dynamic complexity. After reviewing the graph representation of system structure, the article presents the rationale and algorithms for model partitions based on data availability and structural characteristics. Special attention is given to the decomposition of cycle partitions that contain all the model’s feedback loops, and a unique and granular representation of feedback complexity is derived. The article concludes by identifying future research avenues in this arena. Copyright © 2004 John Wiley & Sons, Ltd.",Erratum,pro3
pap1703,142ea3e53037cb8d72849380dab8dcd1a6bb910c,con31,International Conference on Evaluation & Assessment in Software Engineering,Some applications of graph theory to clustering,,Erratum,pro31
pap1704,686b3b9e3cb8d757c73a5a1813890c214a8a809e,con34,International Conference on Agile Software Development,Some Basic Definitions in Graph Theory,"A systematic list of definitions of some basic concepts in graph theory of application to physics is presented. An index, some illustrative theorems, and a brief bibliography are included.",Erratum,pro34
pap1705,3f558ea2e9a0aab2e247b3a67e3e6e343398056a,jou304,Journal of Graph Theory,Reflections on graph theory,"At the occasion of the 250th anniversary of graph theory, we recall some of the basic results and unsolved problems, some of the attractive and surprising methods and results, and some possible future directions in graph theory.",Conference paper,vol304
pap1706,da4ab623477d041c1944a66808cf38740e913a3f,con99,North American Chapter of the Association for Computational Linguistics,Combinatorics and graph theory,,Erratum,pro99
pap1707,8ea2e1fc1fe8367627d868980a9f6bc4ca88d57a,con22,Grid Computing Environments,On the use of linear graph theory in multibody system dynamics,,Erratum,pro22
pap1708,d326d75238945047fcaa7401eeb87cb1b400ecd0,con86,The Web Conference,Power transfer allocation for open access using graph theory-fundamentals and applications in systems without loopflow,"In this paper, graph theory is used to calculate the contributions of individual generators and loads to line flows and the real power transfer between individual generators and loads that are significant to transmission open access. Related lemmas are proved which present necessary conditions required by the method. Based on AC load flow solution a novel method is suggested which can decide downstream and upstream power flow tracing paths very fast and can calculate the contribution factors of generations and loads to the line flows efficiently. The power transfer between generators and loads can also be determined. The suggested method is suitable for both active and reactive power tracings of real power systems.",Erratum,pro86
pap1709,f2e614adc8a6139cc1f5c725365bd132df4ed0a7,con31,International Conference on Evaluation & Assessment in Software Engineering,SOME RECENT RESULTS ON EXTREMAL PROBLEMS IN GRAPH THEORY (Results),"Three years ago I gave a talk on extremal problems in graph theory at Smolenice [2]. I will refer to this paper as I. I will only discuss results which have been found since the publication of I. ~(9) will denote the number of vertices, V(g) the number of edges of 9. s(a ; I) will denote a graph of n vertices and I edges. The vertices of 3 will be denoted by letters x, y, . . . 9(x,, . . . . xk) will denote the subgraph of 9 spanned by the vertices x1, . . . . xA. u(x), the valence of X, denotes the number of edges incident to x. x(S) will denote the",Erratum,pro31
pap1710,1b62915056c5b768db22b47684eea15bce0aa450,con54,Conference of the Centre for Advanced Studies on Collaborative Research,"Topology optimization of trusses using genetic algorithm, force method and graph theory","In this article size/topology optimization of trusses is performed using a genetic algorithm (GA), the force method and some concepts of graph theory. One of the main difficulties with optimization with a GA is that the parameters involved are not completely known and the number of operations needed is often quite high. Application of some concepts of the force method, together with theory of graphs, make the generation of a suitable initial population well‐matched with critical paths for the transformation of internal forces feasible. In the process of optimization generated topologically unstable trusses are identified without any matrix manipulation and highly penalized. Identifying a suitable range for the cross‐section of each member for the ground structure in the list of profiles, the length of the substrings representing the cross‐sectional design variables are reduced. Using a contraction algorithm, the length of the strings is further reduced and a GA is performed in a smaller domain of design space. The above process is accompanied by efficient methods for selection, and by using a suitable penalty function in order to reduce the number of numerical operations and to increase the speed of the optimization toward a global optimum. The efficiency of the present method is illustrated using some examples, and compared to those of previous studies. Copyright © 2003 John Wiley & Sons, Ltd.",Erratum,pro54
pap1711,8a18f6b55267c844060652ed38ffe49bfef9be88,con15,Pacific Symposium on Biocomputing,The Foundations of Topological Graph Theory,,Erratum,pro15
pap1712,ef83866e2aa9e807bb4d3f07456fd9ec33b79f8b,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Graph Theory 1736-1936,1. Oaths 2. Circuits 3. Trees 4. Chemical graphs 5. Euler's polyhedral formula 6. The four-colour problem - early history 7. Colouring maps on surfaces 8. Ideas from algebra and topology 9. The four-colour problem - to 1936 10. The factorization of graphs Appendix 1: Graph theory since 1936 Appendix 2: Bibliographical notes Appendix 3: Bibliography: 1736-1936,Erratum,pro52
pap1713,c637b74814bd5049323bb6bbf077875a3282e02b,con76,IEEE International Conference on Tools with Artificial Intelligence,Graph Theory and Social Networks: A Technical Comment on Connectedness and Connectivity,"Concepts taken from graph theory and other branches of topology have been used by many sociologists and social psychologists, in particular Kurt Lewin and J. L. Moreno. Similar ideas have been used to construct statistical models of nervous systems, and these have been applied by J. S. Coleman and others to the spread of information and other social phenomena. The study of social networks by anthropologists has been based, knowingly or unknowingly, on the basic notions of graph theory, as has the identification and analysis of social cliques. There is little consensus among mathematicians about terminology, and social scientists have drawn fortuitously on various mathematical vocabularies as well as inventing their own technical terms. Applied to social networks, the words `connectedness' and `connectivity' may refer to properties of the distance between persons, the number of paths between them, whether there is a path at all, or the proportion of possible paths actually in existence. These different usages are contrasted by restating them all in the terminology set out in Structural models (1965) by Harary, Norman and Cartwright.",Erratum,pro76
pap1714,d0eb57dc62688054d62f60117394288b5b9ef11f,con66,International Conference on Software Reuse,Application of graph theory to the synchronization in an array of coupled nonlinear oscillators,"In this letter, we show how algebraic graph theory can be used to derive sufficient conditions for an array of resistively coupled nonlinear oscillators to synchronize. These conditions are derived from the connectivity graph, which describes how the oscillators are connected. In particular, we show how such a sufficient condition is dependent on the algebraic connectivity of the connectivity graph. Intuition tells us that if the oscillators are more ""closely connected"" to each other, then they are more likely to synchronize. We discuss how to quantify connectedness in graph-theoretical terms and its relation to algebraic connectivity and show that our results are in accordance with this intuition. We also give an upper bound on the coupling conductance required for synchronization for arbitrary graphs, which is in the order of n/sup 2/, where n is the number of oscillators. >",Erratum,pro66
pap1715,73799d9f998feb583c9e2a0ea42f419952a41f04,con101,International Conference on Biometrics,On the ground states of the frustration model of a spin glass by a matching method of graph theory,"The ground states of a quenched random Ising spin system with variable concentration of mixed nearest-neighbour exchange couplings +or-J on a square lattice (frustration model) are studied by a new method of graph theory. The search for ground states is mapped into the problem of perfect matching of minimum weight in the graph of frustrated plaquettes, a problem which can be solved by the algorithm of Edmonds. A pedestrian presentation of this elaborated algorithm is given with a discussion of the condition of validity.",Erratum,pro101
pap1716,eb44563de6a8fcc1ba53a913d83dc0a5938ce627,con80,International Conference on Advanced Computer Science Applications and Technologies,Chemical Graph Theory: Introduction and Fundamentals,The Origins of Chemical Graph Theory Elements of Graph Theory for Chemists Nomenclature of Chemical Compounds Polynomials in Graph Theory Enumeration of Isomers Graph Theory and Molecular Orbitals,Erratum,pro80
pap1717,c6fe0d808c37848ce661400a6cba5adee07304ac,con89,Conference on Uncertainty in Artificial Intelligence,Dynamics of Flexible Multibody Systems Using Virtual Work and Linear Graph Theory,,Erratum,pro89
pap1718,689d1028008a1e14f437c4f80693afbcebe81084,jou219,Journal of chemical information and computer sciences,Neural Network-Graph Theory Approach to the Prediction of the Physical Properties of Organic Compounds,A new computational scheme is developed to predict physical properties of organic compounds on the basis of their molecular structure. The method uses graph theory to encode the structural information which is the numerical input for a neutral network. Calculated results for a series of saturated hydrocarbons demonstrate average accuracies of 1--2% with maximum deviations of 12--14%.,Article,vol219
pap1719,7d6094c924f5e0d8a10bfa78ff43b97d286e2b2b,jou305,Landscape Ecology,Landscape graphs: Ecological modeling with graph theory to detect configurations common to diverse landscapes,,Article,vol305
pap1720,ffeab4ba1bc96bf7a5c10d9c7b4b33b8855fde2b,con56,International Conference on Software Engineering and Knowledge Engineering,Graph theory and sparse matrix computation,,Erratum,pro56
pap1721,7f7227a16c26b6f90693a2ec8ae2270e272eb548,con43,IEEE International Conference on Software Maintenance and Evolution,Metric graph theory and geometry: a survey,"The article surveys structural characterizations of several graph classes defined by distance properties, which have in part a general algebraic flavor and can be interpreted as subdirect decomposition. The graphs we feature in the first place are the median graphs and their various kinds of generalizations, e.g., weakly modular graphs, or fiber-complemented graphs, or l1-graphs. Several kinds of l1-graphs admit natural geometric realizations as polyhedral complexes. Particular instances of these graphs also occur in other geometric contexts, for example, as dual polar graphs, basis graphs of (even ∆-)matroids, tope graphs, lopsided sets, or plane graphs with vertex degrees and face sizes bounded from below. Several other classes of graphs, e.g., Helly graphs (as injective objects), or bridged graphs (generalizing chordal graphs), or tree-like graphs such as distance-hereditary graphs occur in the investigation of graphs satisfying some basic properties of the distance function, such as the Helly property for balls, or the convexity of balls or of the neighborhoods of convex sets, etc. Operators between graphs or complexes relate some of the graph classes reported in this survey.",Erratum,pro43
pap1722,0ddc015d3ff0a4247eca90a97cae64970275a19b,jou306,Discrete Mathematics,A method in graph theory,,Article,vol306
pap1723,342a3f86045d99b1d83aec36b3c68611d945c227,con105,British Machine Vision Conference,Graph Connections: Relationships between Graph Theory and Other Areas of Mathematics,"The purpose of this book is to inform mathematicians about the applicability of graph theory to other areas of mathematics, from number theory, to linear algebra, knots, neural networks, and finance. This is achieved through a series of expository chapters, each devoted to a different field and written by an expert in that field. The book, however, is more than a collection of essays. Each chapter has been carefully edited to ensure a common level of exposition, with terminology and notation standarised as far as possible.",Erratum,pro105
pap1724,207327c3cb2ce8ef6498d433d58488aece6fbf18,con108,International Conference on Information Integration and Web-based Applications & Services,A GRAPH THEORY APPROACH TO DEMOGRAPHIC LOOP ANALYSIS,"A demographic analysis of the life-cycle graph can be used to quantify the separate contributions of different life-history types to the population growth rate. Loop analysis has been proposed (van Groenendael et al. 1994) as the appropriate method for partitioning the elasticity matrix to determine these contributions. However, in the analysis of complex demographic models it is difficult to derive the loops by simple inspection of the life-cycle graph. I show how graph theory can be used to describe a general and systematic procedure for deriving the loops from the structure of the life-cycle graph. I demonstrate that the concept of nullity (from graph theory) can be applied in this context to correctly determine the number of loops for any graph. Using examples from Campanula americana, Dipsacus sylvestris, and Caretta caretta, I illustrate the relationship of the loops to biologically relevant life-history contrasts. This relationship is crucial for the application of loop analysis to life-history evolution for the purpose of partitioning the separate effects on the population growth rate among different life-history components.",Erratum,pro108
pap1725,13ec4520f33b8724d6dc2b66da1302ffeb4231b5,con86,The Web Conference,Graph Theory Techniques in Model-Based Testing,"Models are a method of representing software behavior. Graph theory is an area of mathematics that can help us use this model information to test applications in many different ways. This paper describes several graph theory techniques, where they came from, and how they can be used to improve software testing.",Erratum,pro86
pap1726,a3c7bcf8e2090de45281cf160e5893766af27100,jou304,Journal of Graph Theory,Examples and Counterexamples in Graph Theory,"It sounds good when knowing the examples and counterexamples in graph theory in this website. This is one of the books that many people looking for. In the past, many people ask about this book as their favourite book to read and collect. And now, we present hat you need quickly. It seems to be so happy to offer you this famous book. It will not become a unity of the way for you to get amazing benefits at all. But, it will serve something that will let you get the best time and moment to spend for reading the book.",Article,vol304
pap1727,42eaf1aea33b9da962f32bf5e1a81fc4c60f0e40,con45,International Conference on Global Software Engineering,A SEMINAR ON GRAPH THEORY,"Abstract : The opening six chapters present a coherent body of graph theoretic concepts. The remaining eight chapters report lectures presented by various seminar participants. Topics presented include: Complete Bipartite Graphs, Extremal Problems in Graph Theory, Applications of Probabilistic Methods to Graph Theory, The Minimal Regular Graph Containing a Given Graph, Various Proofs of Cayley's Formula for Counting Trees, On Well-Quasi-Ordering Trees, Universal Graphs and Graphs and Composite Games. (Author)",Erratum,pro45
pap1728,6f18387ee1b3533126d5d1e0dccc1fa1c81c8e17,con56,International Conference on Software Engineering and Knowledge Engineering,Lattice Constant Systems and Graph Theory,"The two principal systems of lattice constants that have arisen in the study of cooperative phenomena and related problems on crystal lattices are the strong (low‐temperature) and the weak (high‐temperature) systems. The two systems are defined in terms of the concepts of graph theory, and a general theorem relevant to cluster expansions is stated. The interrelation of the two systems is studied and exploited to derive configurational data for the face‐centered cubic lattice. All star graphs with up to seven points (vertices) or nine lines (edges) that are embeddable on the face‐centered cubic are described. A general classification of stars with cyclomatic index 3 is given.",Erratum,pro56
pap1729,1098c6bad469b388be83566bfe0445b47ba4256f,jou304,Journal of Graph Theory,Open problems of Paul Erdös in graph theory,"The main treasure that Paul Erdős has left us is his collection of problems, most of which are still open today. These problems are seeds that Paul sowed and watered by giving numerous talks at meetings big and small, near and far. In the past, his problems have spawned many areas in graph theory and beyond (e.g., in number theory, probability, geometry, algorithms and complexity theory). Solutions or partial solutions to Erdős problems usually lead to further questions, often in new directions. These problems provide inspiration and serve as a common focus for all graph theorists. Through the problems, the legacy of Paul Erdős continues (particularly if solving one of these problems results in creating three new problems, for example.) There is a huge literature of almost 1500 papers written by Erdős and his (more than 460) collaborators. Paul wrote many problem papers, some of which appeared in various (really hard-to-find) proceedings. Here is an attempt to collect and organize these problems in the area of graph theory. The list here is by no means complete or exhaustive. Our goal is to state the problems, locate the sources, and provide the references related to these problems. We will include the earliest and latest known references without covering the entire history of the problems because of space limitations (The most up-to-date list of Erdős' papers can be found in [65]; an electronic file is maintained by Jerry Grossman at grossman@oakland.edu.) There are many survey papers on the impact of Paul's work, e.g., see those in the books: A Tribute to Paul Erdős [84], Combinatorics, Paul Erdős is Eighty, Volumes 1 and 2 [83], and The Mathematics of Paul Erdős, Volumes I and II [81]. To honestly follow the unique style of Paul Erdős, we will mention the fact that Erdős often offered monetary awards for solutions to a number of his favorite problems. In November 1996, a committee of Erdős' friends decided no more such awards will be given in Erdős' name. However, the author, with the help of Ron Graham, will honor future claims on the problems in this paper, provided the requirements previously set by Paul are satisfied (e.g., proofs have been verified and published in recognized journals). Throughout this paper, the constants c, c′, c1, c2, · · · and extremal functions f(n), f(n, k), f(n, k, r, t), g(n), · · · are used extensively, although within the context of each problem, the",Letter,vol304
pap1730,b879e3124a79513ac0cf86979fc1a33fbbeb4875,con6,Annual Conference on Genetic and Evolutionary Computation,Hybrid Graph Theory and Network Analysis,"This book combines traditional graph theory with the matroid view of graphs in order to throw light on the mathematical approach to network analysis. The authors examine in detail two dual structures associated with a graph, namely circuits and cutsets. These are strongly dependent on one another and together constitute a third, hybrid, vertex-independent structure called a graphoid, whose study is here termed hybrid graph theory. This approach has particular relevance for network analysis. The first account of the subject in book form, the text includes many new results as well as the synthesizing and reworking of much research done over the past thirty years (historically, the study of hybrid aspects of graphs owes much to the foundational work of Japanese researchers). This work will be regarded as the definitive account of the subject, suitable for all working in theoretical network analysis: mathematicians, computer scientists or electrical engineers.",Erratum,pro6
pap1731,b13a8bd57def8a98f4e59fced4cce361875aa719,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,On constructing a block layout by graph theory,"This paper examines the problem of developing block layouts using graph theory. It is shown that there are several limitations associated with such layouts, particularly when facility relationships are represented quantitatively by a from-to chart. A modification to conventional construction-type layout procedures is presented which allows a graph theoretic block layout to be developed, regardless of the type of facility relationships used, and without performing all the steps required in the graph theoretic approach. This new method helps to avoid the limitations of the approach and alleviate its computational burden.",Erratum,pro39
pap1732,682c67d11c3bc884ca6d551b546c8b1c6414c9b3,con14,Hawaii International Conference on System Sciences,On domination related concepts in Graph Theory,,Erratum,pro14
pap1733,2880478c793e500c42f4a0cf06d5036f0061c0bf,con51,Brazilian Symposium on Software Engineering,Conference on Graph Theory and Topology in Chemistry.,"Abstract : Prof. R. B. King and Dr. D. H. Rouvray organized an International Conference on Graph Theory and Topology in Chemistry which was held at the University of Georgia, Athens, Georgia, during the period March 15-20, 1987. A volume containing the papers presented at this conference is being published by Elsevier Scientific Publishing Company, Amsterdam, and will appear around the end of 1987. The following items are attached: (1) The program of the conference. (2) The short abstracts of the paper presented at the conference. (3) The contents of the conference volume. (4) The preface of the conference volume. Knots, Marcromolecules and Chemical Dynamics Topological Stereochemistry: Knot Theory of Molecular Graphs Extrinsic Topological Chirality Indices of Molecular Graphs A Topological Approach to the Stereochemistry of Nonrigid Molecules Chirality of Non-Standardly Embedded Mobius Ladders .",Erratum,pro51
pap1734,82c93321706b13def2d091a6a30f9d5efb627b38,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,Pattern vectors from algebraic graph theory,"Graph structures have proven computationally cumbersome for pattern analysis. The reason for this is that, before graphs can be converted to pattern vectors, correspondences must be established between the nodes of structures which are potentially of different size. To overcome this problem, in this paper, we turn to the spectral decomposition of the Laplacian matrix. We show how the elements of the spectral matrix for the Laplacian can be used to construct symmetric polynomials that are permutation invariants. The coefficients of these polynomials can be used as graph features which can be encoded in a vectorial manner. We extend this representation to graphs in which there are unary attributes on the nodes and binary attributes on the edges by using the spectral decomposition of a Hermitian property matrix that can be viewed as a complex analogue of the Laplacian. To embed the graphs in a pattern space, we explore whether the vectors of invariants can be embedded in a low-dimensional space using a number of alternative strategies, including principal components analysis (PCA), multidimensional scaling (MDS), and locality preserving projection (LPP). Experimentally, we demonstrate that the embeddings result in well-defined graph clusters. Our experiments with the spectral representation involve both synthetic and real-world data. The experiments with synthetic data demonstrate that the distances between spectral feature vectors can be used to discriminate between graphs on the basis of their structure. The real-world experiments show that the method can be used to locate clusters of graphs.",Conference paper,vol290
pap1735,2777556053aed8a2e97055dd82f2ac026319b32c,con104,Biometrics and Identity Management,Applications of combinatorics and graph theory to the biological and social sciences,,Erratum,pro104
pap1736,19ccc67b2442c08baefdb30a772463134ec21d37,con10,Americas Conference on Information Systems,Computational Graph Theory,,Erratum,pro10
pap1737,188c8889b26451ef6e0262e939373a946965ef62,con37,International Symposium on Search Based Software Engineering,Application of Graph Theory: Relationship of Eccentric Connectivity Index and Wiener's Index with Anti-inflammatory Activity,"Abstract Graph theory was successfully applied in developing a relationship between chemical structure and biological activity. The relationship of two graph invariants—the eccentric connectivity index and the Wiener's index was investigated with regard to anti-inflammatory activity, for a data set consisting of 76 pyrazole carboxylic acid hydrazide analogues. The values of the eccentric connectivity index and the Wiener's index of each analogue in the data set were computed and active ranges were identified. Subsequently, each analogue was assigned a biological activity that was compared with the anti-inflammatory activity reported as percent reduction in paw swelling. Prediction with an accuracy of ∼90% was obtained using the eccentric connectivity index as compared to 84% in the case of Wiener's index.",Erratum,pro37
pap1738,2c494605fe60583c0b8f20facc463ec49cb06a0c,con79,IEEE Annual Symposium on Foundations of Computer Science,Potts model and graph theory,,Erratum,pro79
pap1739,9aa530783226dc392f570207e90f32cee8b20b96,con29,ACM-SIAM Symposium on Discrete Algorithms,AN APPLICATION OF GRAPH THEORY TO ALGEBRA,"[Al, * * * , Ak ] 54-0. The original proof of the theorem [1] was elementary but very complicated. In attempting to simplify this proof, I found a more transparent proof based on the use of graph theory.3 One advantage of this approach is that complicated algebraic definitions can be replaced by much simpler geometric definitions merely by drawing a picture of the appropriate graph. Before stating the graph theoretic theorem which implies Theorem 1, I will give some elementary definitions and lemmas from graph theory.",Erratum,pro29
pap1740,056c50de715e9a3c2aff98d6f90caf877e9acf71,con61,International Conference on Predictive Models in Software Engineering,Some recent results in topological graph theory,,Erratum,pro61
pap1741,e47f58edd1af4ee6b236d771b1e863920382c45d,con25,IEEE International Parallel and Distributed Processing Symposium,Fractional Graph Theory: A Rational Approach to the Theory of Graphs,General Theory: Hypergraphs. Fractional Matching. Fractional Coloring. Fractional Edge Coloring. Fractional Arboricity and Matroid Methods. Fractional Isomorphism. Fractional Odds and Ends. Appendix. Bibliography. Indexes.,Erratum,pro25
pap1742,24f1ff4e693c9b362a565047b12659189a3eadbf,con44,International Conference Knowledge Engineering and Knowledge Management,Neural networks and graph theory,"The relationships between artificial neural networks and graph theory are considered in detail. The applications of artificial neural networks to many difficult problems of graph theory, especially NP-complete problems, and the applications of graph theory to artificial neural networks are discussed. For example graph theory is used to study the pattern classification problem on the discrete type feedforward neural networks, and the stability analysis of feedback artificial neural networks etc.",Erratum,pro44
pap1743,8850c6b96200bc8b92935d16a7685e621214d51c,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Facilities Planning with Graph Theory,Basic concepts of Graph Theory are discussed which are relevant to solving problems of locating economic activities within a service or manufacturing facility. The location problem is formulated in terms of Graph Theory knowledge and a solution procedure proposed. An example is provided and finally boundary conditions are elaborated.,Erratum,pro21
pap1744,9be30e1d9f4301afbe46a975c67e8a1481e3d56f,con20,ACM Conference on Economics and Computation,Problems in combinatorics and graph theory,"Three hundred and sixty-nine problems with fully worked solutions for courses in computer science, combinatorics, and graph theory, designed to provide graded practice to students with as little as a high school algebra background. Originally used to prepare Rumanian candidates for participation in the International Mathematical Olympiads, this book includes both simple problems and complex ones, arranged according to subject. It provides various levels of problems, some of which had been previously available only in research journals. All details of the proofs are given in the solutions.",Erratum,pro20
pap1745,cd570ac39865e4f297491c8321df914eeaa2ac7d,con1,International Conference on Human Factors in Computing Systems,On some solved and unsolved problems of chemical graph theory,"The development of several novel graph theoretical concepts and their applications in different branches of chemistry are reviewed. After a few introductory remarks we follow with an outline of selected important graph theoretical invariants, introducing some new results and indicating some open problems. We continue with discussing the problem of graph characterization and construction of graphs of chemical interest, with a particular emphasis on large systems. Finally we consider various problems and difficulties associated with special subgraphs, including subgraphs representing Kekule valence structures. The paper ends with a brief review of structure-property and structure-activity correlations, the topic which is one of prime motivations for application of graph theory to chemistry.",Erratum,pro1
pap1746,7d0e37b3f932ac8c0dcec37c43e71bbbb0ce2811,con34,International Conference on Agile Software Development,Graph theory and molecular orbitals. VII. The role of resonance structures,"The relations between the simplest variants of MO and VB theory are discussed. It is shown that there is a unique principle causing all the cases of congruity between these two theories‐Kekule structures being related to the permutations contained in the molecular graph [Eqs. (6) and (7)]. The class of benzenoid hydrocarbons where both theories are substantially equivalent is rigorously defined using graph theory. A number of topological regularities for these hydrocarbons are proved. Thus, the Dewar‐Longuet‐Higgins equation, the proof of the Ruedenberg's and Pauling's bond orders, the relation between the VB and MO spin and charge density, and Heilbronner's formula are obtained. The limits of validity for all these results are strictly determined.",Erratum,pro34
pap1747,bbbff2de136561c6cb4c7890d9f93b357a99e244,jou307,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,Graph Theory and Probability,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) — 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(n) are (3) It is not even known that g(n)1/n tends to a limit. The lower bound in (1) has been obtained by combinatorial and probabilistic arguments without an explicit construction.",Conference paper,vol307
pap1748,25c6565750d7d46c63d9bc073e6826b4d5898498,con77,International Conference on Artificial Neural Networks,Graph Theory and Q-Analysis,"Structures of graph theory are compared with those of Q-analysis and there are many similarities. The graph and simplicial complex defined by a relation are equivalent in terms of the information they represent, so that the choice between graph theory and Q-analysis depends on which gives the most natural and complete description of a system. The higher dimensional graphs are shown to be simplicial families or complexes. Although network theory is very successful in those physical science applications for which it was developed, it is argued that Q-analysis gives a better description of human network systems as patterns of traffic on a backcloth of simplicial complexes. The q-nearness graph represents the q-nearness of pairs of simplices for a given q-value. It is concluded that known results from graph theory could be applied to the q-nearness graph to assist in the investigation of q-connectivity, to introduce the notion of connection defined by graph cuts, and to assist in computation. The application of the q-nearness graph to q-transmission and shomotopy is investigated.",Erratum,pro77
pap1749,7b4a49f52fe60c0882469ad8afcb9c279d0e3314,con1,International Conference on Human Factors in Computing Systems,Algorithmic graph theory,"Introduction to graph theory algorithmic techniques shortest paths trees and acyclic diagraphs depth first search connectivity and routing graph colouring covers, domination, independent sets, matchings and factors, parallel algorithms computational complexity.",Erratum,pro1
pap1750,fd3388199757c081f45ec4c02b25cdab84076935,con24,International Conference on Data Technologies and Applications,Graph Theory for Rule-Based Modeling of Biochemical Networks,,Erratum,pro24
pap1751,d007e0675a45264aedeacbbe12c731ea78bba070,con101,International Conference on Biometrics,Problems and Results in Graph Theory and Combinatorial Analysis,"I published several papers with similar titles. One of my latest ones [13] (also see [16] and the yearly meetings at Boca Raton or Baton Rouge) contains, in the introduction, many references to my previous papers . I discuss here as much as possible new problems, and present proofs in only one case. I use the same notation as used in my previous papers . G(' )(n ;1) denotes an r-graph (uniform hypergraph all of whose edges have size r) of n vertices and I edges . If r = 2 and there is no danger of confusion . I omit the upper index r = 2 . K ( r ) (n) denotes the complete hypergraph G ( ' ) (n ; (;)) . K(a, b) denotes the complete bipartite graph (r = 2) of a white and b black vertices . K (r )(t) denotes the hypergraph of It vertices x (i ' ) , I < i < t, 1 < j < 1, and whose (I)tr edges are {x,1t ) , . . . , x~ )} where all the i's and all the j's are distinct . e(G(in)) is the number of edges of G(m) (graph of m vertices), the girth is the length of a smallest circuit of the graph .",Erratum,pro101
pap1752,8691bd56aad7be73d5a6342ee434bea2d4774e00,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,"ON SOME PROBLEMS IN GRAPH THEORY , COMBINATORIAL ANALYSIS AND COMBINATORIAL NUMBER THEORY","1. G(n) is a graph of n vertices and G(n ; e) is a graph of n vertices and e edges. Is it true that if every induced subgraph of a G(10n) of 5n vertices has more than 2n 2 edges then our G(10n) contains a triangle? It is easy to show that if true this result is best possible . To see this let A i , i =1, 2, . . . , 5, be sets of 2n vertices, put A, = A 6 and join every vertex of A, to every vertex of A; + , . This G(10n ; 20n 2) has of course no triangle and every induced subgraph of 5n vertices contains at least 2n2 edges . Equality is of course possible : choose A,, A, and half the vertices of A, Simonovits pointed out to me that a graph of completely different structure also shows that the conjecture, if true, is best possible . Consider the Petersen graph, which is a G(10 ; 15) . Replace each vertex by a set of n vertices and replace every edge of the Petersen graph by the n 2 edges of a K(n, n) . This gives a G(10n ; 15n 2) and it is easy to see that every induced subgraph of 5n vertices has at least 2n2 edges . The fact that two graphs of different structure are extremal perhaps indicates that the conjecture is either false or difficult to prove . I certainly hope that the latter is the case . It is perhaps tempting to conjecture that my graph has the following extremal property . If a G(10n) has no triangle and every induced subgraph of 5n vertices has at least 2n2 edges, then our graph can have at most 20n2 edges. Perhaps the graph of Simonovits has the smallest number of edges among all extremal graphs; perhaps in fact these two graphs are the only extremal graphs . Many generalizations are possible ; the triangle could be replaced by other graphs . Is it true that every G((4h+2)n), every induced subgraph",Erratum,pro58
pap1753,0f6d06e5e321698682c31ee07e25d76d03be6e52,con40,Conference on Software Engineering Education and Training,A material selection model using graph theory and matrix approach,,Erratum,pro40
pap1754,bccb739463ffd0189a5c6b2f5e6415a9f50fb313,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Selected Topics in Graph Theory 2,,Erratum,pro42
pap1755,199f55f80973ff42c0df77495cc639ff6e211fac,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Topics in Intersection Graph Theory,"Preface 1. Intersection Graphs. Basic Concepts Intersection Classes Parsimonious Set Representations Clique Graphs Line Graphs Hypergraphs 2. Chordal Graphs. Chordal Graphs as Intersection Graphs Other Characterizations Tree Hypergraphs Some Applications of Chordal Graphs Split Graphs 3. Interval Graphs. Definitions and Characterizations Interval Hypergraphs Proper Interval Graphs Some Applications of Interval Graphs 4. Competition Graphs. Neighborhood Graphs Competition Graphs Interval Competition Graphs Upper Bound Graphs 5. Threshold Graphs. Definitions and Characterizations Threshold Graphs as Intersection Graphs Difference Graphs and Ferrers Digraphs Some Applications of Threshold Graphs 6. Other Kinds of Intersection. p-Intersection Graphs Intersection Multigraphs and Pseudographs Tolerance Intersection Graphs 7. Guide to Related Topics. Assorted Geometric Intersection Graphs Bipartite Intersection Graphs, Intersection Digraphs, and Catch (Di)Graphs Chordal Bipartite and Weakly Chordal Graphs Circle Graphs and Permutation Graphs Clique Graphs of Chordal Graphs and Clique-Helly Graphs Containment, Comparability, Cocomparability, and Asteroidal Triple-Free Graphs Infinite Intersection Graphs Miscellaneous Topics P4-Free Chordal Graphs and Cographs Powers of Intersection Graphs Sphere-of-Influence Graphs Strongly Chordal Graphs Bibliography Index.",Erratum,pro82
pap1756,21fac6c5fcfdfb51fa60a7f361179d25661c0a46,con35,IEEE Working Conference on Mining Software Repositories,Graph theory for image analysis: an approach based on the shortest spanning tree,"The paper describes methods of image segmentation and edge detection based on graph-theoretic representations of images. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The simplest of these uses the shortest spanning tree (SST), a notion that forms the basis of the other improved methods. These further methods make use of global pictorial information, removing many of the problems of the SST segmentation in its simple form and of other pixel linking algorithms. An important feature in all of the proposed methods is that regions may be described in a hierarchical way.",Erratum,pro35
pap1757,e4f2a719a622ba3b0ed49a82cde9bf07d6ce67f6,con48,ACM Symposium on Applied Computing,Graph Theory and Its Applications,"INTRODUCTION TO GRAPH MODELS Graphs and Digraphs Common Families of Graphs Graph Modeling Applications Walks and Distance Paths, Cycles, and Trees Vertex and Edge Attributes: More Applications STRUCTURE AND REPRESENTATION Graph Isomorphism Revised! Automorphisms and Symmetry Moved and revised! Subgraphs Some Graph Operations Tests for Non-Isomorphism Matrix Representation More Graph Operations TREES Reorganized and revised! Characterizations and Properties of Trees Rooted Trees, Ordered Trees, and Binary Trees Binary-Tree Traversals Binary-Search Trees Huffman Trees and Optimal Prefix Codes Priority Trees Counting Labeled Trees: Prufer Encoding Counting Binary Trees: Catalan Recursion SPANNING TREES Reorganized and revised! Tree-Growing Depth-First and Breadth-First Search Minimum Spanning Trees and Shortest Paths Applications of Depth-First Search Cycles, Edge Cuts, and Spanning Trees Graphs and Vector Spaces Matroids and the Greedy Algorithm CONNECTIVITY Revised! Vertex- and Edge-Connectivity Constructing Reliable Networks Max-Min Duality and Menger's Theorems Block Decompositions OPTIMAL GRAPH TRAVERSALS Eulerian Trails and Tours DeBruijn Sequences and Postman Problems Hamiltonian Paths and Cycles Gray Codes and Traveling Salesman Problems PLANARITY AND KURATOWSKI'S THEOREM Reorganized and revised! Planar Drawings and Some Basic Surfaces Subdivision and Homeomorphism Extending Planar Drawings Kuratowski's Theorem Algebraic Tests for Planarity Planarity Algorithm Crossing Numbers and Thickness DRAWING GRAPHS AND MAPS Reorganized and revised! The Topology of Low Dimensions Higher-Order Surfaces Mathematical Model for Drawing Graphs Regular Maps on a Sphere Imbeddings on Higher-Order Surfaces Geometric Drawings of Graphs New! GRAPH COLORINGS Vertex-Colorings Map-Colorings Edge-Colorings Factorization New! MEASUREMENT AND MAPPINGS New Chapter! Distance in Graphs New! Domination in Graphs New! Bandwidth New! Intersection Graphs New! Linear Graph Mappings Moved and revised! Modeling Network Emulation Moved and revised! ANALYTIC GRAPH THEORY New Chapter! Ramsey Graph Theory New! Extremal Graph Theory New! Random Graphs New! SPECIAL DIGRAPH MODELS Reorganized and revised! Directed Paths and Mutual Reachability Digraphs as Models for Relations Tournaments Project Scheduling and Critical Paths Finding the Strong Components of a Digraph NETWORK FLOWS AND APPLICATIONS Flows and Cuts in Networks Solving the Maximum-Flow Problem Flows and Connectivity Matchings, Transversals, and Vertex Covers GRAPHICAL ENUMERATION Reorganized and revised! Automorphisms of Simple Graphs Graph Colorings and Symmetry Burnside's Lemma Cycle-Index Polynomial of a Permutation Group More Counting, Including Simple Graphs Polya-Burnside Enumeration ALGEBRAIC SPECIFICATION OF GRAPHS Cyclic Voltages Cayley Graphs and Regular Voltages Permutation Voltages Symmetric Graphs and Parallel Architectures Interconnection-Network Performance NON-PLANAR LAYOUTS Reorganized and revised! Representing Imbeddings by Rotations Genus Distribution of a Graph Voltage-Graph Specification of Graph Layouts Non KVL Imbedded Voltage Graphs Heawood Map-Coloring Problem APPENDIX Logic Fundamentals Relations and Functions Some Basic Combinatorics Algebraic Structures Algorithmic Complexity Supplementary Reading BIBLIOGRAPHY General Reading References SOLUTIONS AND HINTS New! INDEXES Index of Applications Index of Algorithms Index of Notations General Index",Erratum,pro48
pap1758,18261bcc9557f697e795d3aa3a9bb74da54fe58c,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,"Introduction to Chordal Graphs and Clique Trees, in Graph Theory and Sparse Matrix Computation","Kjjrull U., Triangulation of graph-algorithms giving small total state space. 19 in the number of minimal separators. This possible amendment will resolve a theoretical problem raised by KBMK93, KBMK94] and further addressed by PS95] but will hardly aaect the running time of our algorithm. 18 The entry Frag in Table 3 measures the number of fragments produced for the dynamic programming phase. The entries R and R k measure, respectively, the number of minimal separators and the number of minimal separators of size less than k where k = 7. Note again that R k is smaller than R; Many minimal separators are generated in the rst phase of QuickTree but are not needed for the dynamic programming phase. A second reason for the high values of T 1 is that the algorithm runs over almost all pairs of vertices and for each pair fa; bg produces all minimal a; b-separators. However, after a few pairs, the algorithm usually nds most of the minimal sepa-rators and the remaining run time is just used to verify that indeed all minimal separators have been generated. Table 4 shows the number of Good-Pairs|pairs that generated at least one new minimal separator. All-Pairs denote the number of pairs we used which guarantee that all minimal separators have been generated. For Table 4, 30% of the edges were dropped. Table 4 suggests that if there is no needed guarantee of optimal triangulation, then Phase 1 can be run on a fraction of the possible pairs of vertices and then the dynamic programming phase can be applied. For example, on 3 graphs with 75 nodes and treewidth 7, we selected the top 20% of pairs that had a maximal mutual distance and got close to optimal triangulations (in two instances we got the optimal treewidth 7 and once we got 9 instead of 7). The average running time was reduced from 675 to 373 seconds. 6 Discussion In many applications the treewidth of a triangulation is just an approximation to the real optimization problem. For example, for the updating problem in Bayesian networks, one needs to nd a triangulation that minimizes the sum P i 2 w(Ci) where w is a positive additive weight function on the vertices of G and C i are the cliques of the triangulation Kj90, BG96]. The triangulation algorithm presented herein can be modiied to accommodate such variants. Currently, the algorithm …",Erratum,pro13
pap1759,175a476feb8cf13d7f2c739c347de3262f817063,con1,International Conference on Human Factors in Computing Systems,A ring in graph theory,"We call a point set in a complex K a 0-cell if it contains just one point of K, and a 1-cell if it is an open arc. A set L of 0-cells and 1-cells of K is called a linear graph on K if (i) no two members of L intersect, (ii) the union of all the members of L is K, (iii) each end-point of a 1-cell of L is a 0-cell of L and (iv) the number of 0-cells and 1-cells of L is finite and not 0.",Erratum,pro1
pap1760,0a431a6f6816931287bd3b14b48884aa63913b41,con41,Asia-Pacific Software Engineering Conference,Lectures on Spectral Graph Theory,Contents Chapter 1. Eigenvalues and the Laplacian of a graph 1 1.1. Introduction 1 1.2. The Laplacian and eigenvalues 2 1.3. Basic facts about the spectrum of a graph 6,Erratum,pro41
pap1761,a372f7ee74933e324b63dc34747ec28201048cb3,con34,International Conference on Agile Software Development,Graph theory with applications to algorithms and computer science,"Partial table of contents: Finite Figures Consisting Of Regular Polygons (J. Akiyama, et al.). Eigenvalues, Geometric Expanders and Sorting in Rounds (N. Alon). Long Path Enumeration Algorithms for Timing Verification on Large Digital Systems (T. Asano and S. Sato). On Upsets in Bipartite Tournaments (K. Bagga). Some Results on Binary Matrices Obtained via Bipartite Tournaments (K. Bagga and L. Beineke). Partitioning the Nodes of a Graph (E. Barnes). A Graph Theoretical Characterization of Minimal Deadlocks in Petri Nets (J. Bermond and G. Memmi). On Graceful Directed Graphs that Are Computational Models of Some Algebraic Systems (G. Bloom and D. Hsu). The Cut Frequency Vector (F. Boesch). Diameter Vulnerability in Networks (J. Bond and C. Peyrat). Generalized Colorings of Outerplanar and Planar Graphs (I. Broere and C. Mynhardt). The Ramsey Number for the Pair Complete Bipartite Graph-Graph of Limited Degree (S. Burr, et al.). Embedding Graphs in Books: A Layout Problem with Applications to VLSI Design (F. Chung). Hamilton Cycles and Quotients of Bipartite Graphs (I. Dejter). Problems and Results on Chromatic Numbers in Finite and Infinite Graphs (P. Erdos). Supraconvergence and Functions that Sum to Zero on Cycles (V. Faber and A. White, Jr.). Edge-Disjoint Hamiltonian Cycles (R. Faudree, et al.). Studies Related to the Ramsey Number r(K d5 u - e) (R. Faudree, et al). The Structural Complexity of Flowgraphs (N. Fenton). n-Domination in Graphs (J. Fink and M. Jacobson).",Erratum,pro34
pap1762,2c4f5a30c195ef810415b3272a2bfc4af845c96c,con59,Annual Workshop of the Psychology of Programming Interest Group,On some extremal problems in graph theory,"In this paper we are concerned with various graph invariants (girth, diameter, expansion constants, eigenvalues of the Laplacian, tree number) and their analogs for weighted graphs -- weighing the graph changes a combinatorial problem to one in analysis. We study both weighted and unweighted graphs which are extremal for these invariants. In the unweighted case we concentrate on finding extrema among all (usually) regular graphs with the same number of vertices; we also study the relationships between such graphs.",Erratum,pro59
pap1763,0e83b4f43f32727d7aa5cefbd12db8a7ed3f1346,con88,European Conference on Computer Vision,Some Topics in Graph Theory,1. Basic terminology 2. Edge-colourings of graphs 3. Symmetries in graphs 4. Packing of graphs 5. Computational complexity of graph properties.,Erratum,pro88
pap1764,ed98016990bc4bd91c910070563d7c5ad5f5a38c,con37,International Symposium on Search Based Software Engineering,A Friendly Introduction to Graph Theory,1. Introductory Concepts. 2. Introduction to Graphs and their Uses. 3. Trees and Bipartite Graphs. 4. Distance and Connectivity. 5. Eularian and Hamiltonian Graphs. 6. Graph Coloring. 7. Matrices. 8. Graph Algorithms. 9. Planar Graphs. 10. Digraphs and Networks. 11. Special Topics. Answers/Solutions to Selected Exercises. Index.,Erratum,pro37
pap1765,7ab055436895b106e5a82e910693539b9877d884,con41,Asia-Pacific Software Engineering Conference,Compactness results in extremal graph theory,,Erratum,pro41
pap1766,bc1b7919b0a78f792edcf2cb3abbd082518b45c2,con34,International Conference on Agile Software Development,Use of Graph Theory to Support Map Generalization,"In the generalization of a concept, we seek to preserve the essential characteristics and behavior of objects. In map generalization, the appropriate selection and application of procedures (such as merging, exaggeration, and selection) require information at the geometric, attribute, and topological levels. This article highlights the potential of graph theoretic representations in providing the topological information necessary for the efficient and effective application of specific generalization procedures. Besides ease of algebraic manipulation, the principal benefit of a graph theoretic approach is the ability to detect and thus preserve topological characteristics of map objects such as isolation, adjacency, and connectivity. While it is true that topologically based systems have been developed for consistency checking and error detection during editing, this article emphasizes the benefits from a map-generalization perspective. Examples are given with respect to specific generalization procedures ...",Erratum,pro34
pap1767,2808f916c72757f891a101c39a9942fb27f22b78,jou304,Journal of Graph Theory,Extremal problems in graph theory,The aim of this note is to give an account of some recent results and state a number of conjectures concerning extremal properties of graphs.,Letter,vol304
pap1768,d22b5e502389a840a47fd5c6a4fae829e1d8bb8c,con53,Workshop on Web 2.0 for Software Engineering,Computational Discrete Mathematics: Algorithmic Graph Theory,,Erratum,pro53
pap1769,a0c1483e0e34fb9bc1baf929ce892f111e329fa6,jou22,Proceedings of the National Academy of Sciences of the United States of America,From time series to complex networks: The visibility graph,"In this work we present a simple and fast computational method, the visibility algorithm, that converts a time series into a graph. The constructed graph inherits several properties of the series in its structure. Thereby, periodic series convert into regular graphs, and random series do so into random graphs. Moreover, fractal series convert into scale-free networks, enhancing the fact that power law degree distributions are related to fractality, something highly discussed recently. Some remarkable examples and analytical tools are outlined to test the method's reliability. Many different measures, recently developed in the complex network theory, could by means of this new approach characterize time series from a new point of view.",Article,vol22
pap1770,9aa27be1bbeb898e6daf8c106ccafae6e09856f9,jou307,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,Graph Theory and Probability. II,"Define f(k, l) as the least integer so t h a t every graph having f(k, l) vertices contains either a complete graph of order k or a set of l independent vertices (a complete graph of order k is a graph of k vertices every two of which are connected by an edge, a set of I vertices is called independent if no two are connected by an edge). Throughout this paper c1, c2, … will denote positive absolute constants. It is known (1, 2) that (1) and in a previous paper (3) I stated that I can prove that for every ∈ > 0 and l > l(∈), f (3, l) > l2-∈ . In the present paper I am going to prove that (2)",Letter,vol307
pap1771,b2cc957d2c913899deea22ec37df9963c7af2b46,con4,Conference on Innovative Data Systems Research,"Graph Theory: Flows, Matrices","STRUCTURE OF THE GRAPH MODEL The abstract graph Geometrical realization of graphs Components Leaves Blocks The strongly connected components of directed graphs Problems OPTIMAL FLOWS Two basic problems Maximal set of independent paths The optimal assignment problem The Hungarian method Max flow-min cut Dynamic flow The mobilization problem The synthesis of flow problems Optical planning The role of the critical path Minimal cost transportation Minimal cost flows Problems GRAPHS AND MATRICES The adjacency matrix The incidence matrix The circuit matrix Interrelations between the matrices of graphs The spectrum of graphs, the complexity Linear electrical networks Further matrices associated with graphs Problems and solutions",Erratum,pro4
pap1772,7a05bef155bc7a29581cac1c9b2ee9e8ca3dca42,con51,Brazilian Symposium on Software Engineering,Fundamentals of Graph Theory,,Erratum,pro51
pap1773,f3f26c7fd8bca7262b23210bf5318037a27f939d,con45,International Conference on Global Software Engineering,Parallel computations in graph theory,"In parallel computation two approaches are common; namely unbounded parallelism and bounded parallelism. In this paper both approaches will be considered. The problem of unbounded parallelism is studied in section II and some lower and upper bounds on different connectivity problems for directed and undirected graphs are presented. In section III we mention bounded parallelism and three different k-parallel graph search techniques, namely k-depth search, breadth depth search, and breadth-first search. Each algorithm is analyzed with respect to the optimal serial algorithm. It is shown that for sufficiently dense graphs the parallel breadth first search technique is very close to the optimal bound. Techniques for searching sparse graphs are also discussed.",Erratum,pro45
pap1774,a38faf02b193d8d121fc9946d5e7c53b13bd9d57,jou308,Acta Crystallographica Section B Structural Science,Graph-set analysis of hydrogen-bond patterns in organic crystals.,"A method is presented based on graph theory for categorizing hydrogen-bond motifs in such a way that complex hydrogen-bond patterns can be disentangled, or decoded, systematically and consistently. This method is based on viewing hydrogen-bond patterns topologically as if they were intertwined nets with molecules as the nodes and hydrogen bonds as the lines. Surprisingly, very few parameters are needed to define the hydrogen-bond motifs comprising these networks. The methods for making these assignments, and examples of their chemical utility are given.",Conference paper,vol308
pap1775,d15c7f4c9eff3f5f1583a4ef3ac1c4fd88caf972,con59,Annual Workshop of the Psychology of Programming Interest Group,A Beginner's Guide to Graph Theory,"Graphs.- Walks, Paths and Cycles.- Connectivity.- Trees.- Linear Spaces Associated with Graphs.- Factorizations.- Graph Colorings.- Planarity.- Labeling.- Ramsey Theory.- Digraphs.- Critical Paths.- Flows in Networks.- Computational Considerations.- Communications Networks and Small-Worlds.",Erratum,pro59
pap1776,5549a317454b8314f585a6c13db39664c76686d2,con7,International Symposium on Intelligent Data Analysis,Graph theory and its engineering applications,Basic theory foundations of electrical network theory directed-graph solutions of linear algebraic equations topological analysis of linear systems trees and their generation the realizability of directed graphs with prescribed degrees state equations of networks.,Erratum,pro7
pap1777,84b5314ed88bb9fad764b85833fe8038f7f0f57f,con78,Neural Information Processing Systems,Graph Theory and the Evolution of Autocatalytic Networks,"We give a self-contained introduction to the theory of directed graphs, leading up to the relationship between the Perron-Frobenius eigenvectors of a graph and its autocatalytic sets. Then we discuss a particular dynamical system on a fixed but arbitrary graph, that describes the population dynamics of species whose interactions are determined by the graph. The attractors of this dynamical system are described as a function of graph topology. Finally we consider a dynamical system in which the graph of interactions of the species coevolves with the populations of the species. We show that this system exhibits complex dynamics including self-organization of the network by autocatalytic sets, growth of complexity and structure, and collapse of the network followed by recoveries. We argue that a graph theoretic classification of perturbations of the network is helpful in predicting the future impact of a perturbation over short and medium time scales.",Erratum,pro78
pap1778,f8048aa570e607428f8cfbfbcdc2bfa15218df75,con79,IEEE Annual Symposium on Foundations of Computer Science,Matrices in Combinatorics and Graph Theory,,Erratum,pro79
pap1779,349cb135f4f4216c58d000f12bfef8db6390c602,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Molecular Orbital Calculations Using Chemical Graph Theory,,Erratum,pro85
pap1780,8d2e0a40e065555c75891132def53556d7c958b9,jou309,Mathematical Systems Theory,On finite 0-simple semigroups and graph theory,,Letter,vol309
pap1781,5c0e88953a818d257805ae5f1fd8ba5a16661ada,con20,ACM Conference on Economics and Computation,Chemical applications of graph theory,,Erratum,pro20
pap1782,ad950d0d65329910e6251168315db07e1cf7a35f,jou310,SIAM Journal on Discrete Mathematics,On-Line Coloring and Recursive Graph Theory,"An on-line vertex coloring algorithm receives the vertices ofa graph in some externally determined order, and, whenever a new vertex is presented, the algorithm also learns to which of the previously presented vertices the new vertex is adjacent. As each vertex is received, the algorithm must make an irrevocable choice of a color to assign the new vertex, and it makes this choice without knowledge of future vertices. A class of graphs $r$ is said to be on-line $\chi$-bounded if there exists an on-line algorithm $A$ and a function $f$ such that $A$ uses at most $f(\omega(G))$ colors to properly color any graph $G$ in \Gamma. If $H$ is a graph, let Forb($H$) denote the class of graphs that do not induce $H$. The goal of this paper is to establish that Forb($T$) is on-line $\chi$-bounded for every radius-2 tree $T$. As a corollary, the authors answer a question of Schmerl's; the authors show that every recursive cocomparability graph can be recursively colored with a number of colors that depends only on its clique number.",Article,vol310
pap1783,90fc44ddfc0a4e43e317699e16c5123429b58cf6,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Algorithmic graph theory and perfect graphs,,Erratum,pro39
pap1784,f736de70b906f5aff9fc89a098adf5045cfa8360,con32,International Conference on Software Technology: Methods and Tools,Graph Theory,"These notes have not been checked by Prof. A.G. Thomason and should not be regarded as ocial notes for the course. In particular, the responsibility for any errors is mine please email Sebastian Pancratz (sfp25) with any comments or corrections.",Erratum,pro32
pap1785,33d54858878c14923035ffcbc018b7128170a1fb,con50,International Workshop on Green and Sustainable Software,A First Look at Graph Theory,"This book is intended to be an introductory text for mathematics and computer science students at the second and third year levels in universities. It gives an introduction to the subject with sufficient theory for students at those levels, with emphasis on algorithms and applications.",Erratum,pro50
pap1786,2a7c8231105a4cb615b2b9b57a59658eadba3751,con83,Networks,Applied Graph Theory,,Erratum,pro83
pap1787,0b9a20fd991a2dc33e6e27b66941b206cb8c638b,con1,International Conference on Human Factors in Computing Systems,On a Hopf Algebra in Graph Theory,"We introduce and start the study of a bialgebra of graphs, which we call the 4-bialgebra, and of the dual bialgebra of 4-invariants. The 4-bialgebra is similar to the ring of graphs introduced by W. T. Tutte in 1946, but its structure is more complicated. The roots of the definition are in low dimensional topology, namely, in the recent theory of Vassiliev knot invariants. In particular, 4-invariants of graphs determine Vassiliev invariants of knots. The relation between the two notions is discussed.",Erratum,pro1
pap1788,3767ec228395d3f50aced3cc823d7f5f67c52973,con97,ACM SIGMOD Conference,Geometric Graph Theory,"Note: Professor Pach's number: [172]; 2nd edition Reference DCG-CHAPTER-2008-027 Record created on 2008-11-18, modified on 2017-05-12",Erratum,pro97
pap1789,9883b126a94221c7a1e607438e629b5c465604d9,con107,Chinese Conference on Biometric Recognition,Extremal Graph Theory,,Erratum,pro107
pap1790,034f6a2897bacfe8b86f123f4dfe5aa29af8ac1b,jou106,Nucleic Acids Research,Exploring the repertoire of RNA secondary motifs using graph theory; implications for RNA design.,"Understanding the structural repertoire of RNA is crucial for RNA genomics research. Yet current methods for finding novel RNAs are limited to small or known RNA families. To expand known RNA structural motifs, we develop a two-dimensional graphical representation approach for describing and estimating the size of RNA's secondary structural repertoire, including naturally occurring and other possible RNA motifs. We employ tree graphs to describe RNA tree motifs and more general (dual) graphs to describe both RNA tree and pseudoknot motifs. Our estimates of RNA's structural space are vastly smaller than the nucleotide sequence space, suggesting a new avenue for finding novel RNAs. Specifically our survey shows that known RNA trees and pseudoknots represent only a small subset of all possible motifs, implying that some of the 'missing' motifs may represent novel RNAs. To help pinpoint RNA-like motifs, we show that the motifs of existing functional RNAs are clustered in a narrow range of topological characteristics. We also illustrate the applications of our approach to the design of novel RNAs and automated comparison of RNA structures; we report several occurrences of RNA motifs within larger RNAs. Thus, our graph theory approach to RNA structures has implications for RNA genomics, structure analysis and design.",Conference paper,vol106
pap1791,df6e19d90ae8be22e3c4f81d9193af9456a21253,con37,International Symposium on Search Based Software Engineering,Applications of Combinatorics and Graph Theory to Spectroscopy and Quantum Chemistry,,Erratum,pro37
pap1792,4f270d2b736a4ef269bdfe0e7a1478027e370085,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Some Applications of Graph Theory to the Structural Analysis of Mechanisms,,Erratum,pro39
pap1793,ab695b94256553dc06276fc44455acb559611282,jou311,Adaptive Behavior,"Affordances. Motivations, and the World Graph Theory","O'Keefe and Nadel (1978) distinguish two paradigms for navigation, the ""locale system"" for map-based navigation and the ""taxon (behavioral orientation) system"" for route navigation. This article models the taxon system, the map-based system, and their interaction, and argues that the map-based system involves the interaction of hippocampus and other systems. We relate taxes to the notion of an affordance. Just as a rat may have basic taxes for approaching food or avoiding a bright light, so does it have a wider repertoire of affordances for possible actions associated with immediate sensing of its environment. We propose that affordances are extracted by the rat posterior parietal cortex, which guides action selection by the premotor cortex and is influenced also by hypothalamic drive information. The taxon-affordances model (TAM) for taxon-based determination of movement direction is based on models of frog detour behavior, with expectations of future reward implemented using reinforcement learning. The specification of the direction of movement is refined by current affordances and motivational information to yield an appropriate course of action. The world graph (WG) theory expands the idea of a map by developing the hypothesis that cognitive and motivational states interact. This article describes an implementation of this theory, the WG model. The integrated TAM-WG model then allows us to explain data on the behavior of rats with and without fornix lesions, which disconnect the hippocampus from other neural systems.",Conference paper,vol311
pap1794,8dff88f4f78ebaebe62d6a8adc6aad586fb5e9de,con3,Knowledge Discovery and Data Mining,Introductory Graph Theory,,Erratum,pro3
pap1795,188b3b2f4afda1303b6fe4dc61daa30c33ee497d,jou312,American Scientist,Graph Theory in Practice: Part II,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",Article,vol312
pap1796,b2aa0851bf11cb4c21ccc9af6eec4d84dbcdd49a,con88,European Conference on Computer Vision,An Application of Graph Theory to Additive Number Theory,,Erratum,pro88
pap1797,4c584c886296f6c37f0ae71df57344bd184e5a5b,con71,Annual Conference on Innovation and Technology in Computer Science Education,Basic graph theory: paths and circuits,,Erratum,pro71
pap1798,e8821c26f6cdd1a7d1f1ebb59dff63585de530f7,con76,IEEE International Conference on Tools with Artificial Intelligence,Graph theory,,Erratum,pro76
pap1799,851eb4b78f6deb8aba9d39529e462c5319940f51,con57,International Workshop on Agent-Oriented Software Engineering,"Spectral Graph Theory, Regional Conference Series in Math.",,Erratum,pro57
pap1800,188b3b2f4afda1303b6fe4dc61daa30c33ee497d,jou312,American Scientist,Graph Theory in Practice: Part II,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",Article,vol312
pap1801,df6e19d90ae8be22e3c4f81d9193af9456a21253,con66,International Conference on Software Reuse,Applications of Combinatorics and Graph Theory to Spectroscopy and Quantum Chemistry,,Erratum,pro66
pap1802,58d1f5507a3f0275639a6cd968bd282b8773240a,con11,European Conference on Modelling and Simulation,Topological Organic Chemistry. Part 1. Graph Theory and Topological Indices of Alkanes.,,Erratum,pro11
pap1803,e8821c26f6cdd1a7d1f1ebb59dff63585de530f7,con48,ACM Symposium on Applied Computing,Graph theory,,Erratum,pro48
pap1804,4c584c886296f6c37f0ae71df57344bd184e5a5b,con43,IEEE International Conference on Software Maintenance and Evolution,Basic graph theory: paths and circuits,,Erratum,pro43
pap1805,5e6a8fd4713137fe76ab15f27c6e6e5da686cc74,con96,Interspeech,Chemical Applications of Graph Theory,,Erratum,pro96
pap1806,b2aa0851bf11cb4c21ccc9af6eec4d84dbcdd49a,con76,IEEE International Conference on Tools with Artificial Intelligence,An Application of Graph Theory to Additive Number Theory,,Erratum,pro76
pap1807,74e3c1ddf732f0970882ef3712465b3b62318d11,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Graph theory and Feynman integrals,,Erratum,pro49
pap1808,dc9515a0f39739fd0e009a90d0dc48afb6e3ac07,con104,Biometrics and Identity Management,Introductory Graph Theory,,Erratum,pro104
pap1809,dbda5f7876960771410e3b2ec96e16ca1fc85529,con35,IEEE Working Conference on Mining Software Repositories,Algebraic Graph Theory: Regular graphs and line graphs,,Erratum,pro35
pap1810,28c1795001dfd52abf46d5b88c064811f974add2,con46,Software Product Lines Conference,Graph theory and combinatorics,"WORKING PAPERS q C. Borgs, J. Chayes, N. Immorlica, A. Kalai, V. S. Mirrokni and C. Papadimitriou, The Myth of the Folk Theorem, submitted to STOC. q U. Feige, N. Immorlica, V.S.Mirrokni and H. Nazerzadeh, Functional Approximations: A new approach for analyzing Heuristics, submitted to STOC. q B. Awerbuch, Y. Azar, and A. Epstein, V. S. Mirrokni, A. Skopalik, Fast Convergence to Nearly Optimal Solutions in Potential Games, submitted to STOC. q J. Hartline, V. S. Mirrokni, and M. Sundararajan, Marketing Strategies over Social Networks, submitted to WWW. q U. Feige, N. Immorlica, V.S. Mirrokni and H. Nazerzadeh, Combinatorial Allocation Mechanisms with Penalties for Banner Advertisement, submitted to WWW. q R. Andersen, C. Borgs, J. Chayes, U. Feige, A. Flaxman, A. Kalai, V. S. Mirrokni, M. Tennenholtz, Trust-based Recommendation Systems: An axiomatic Approach, submitted to WWW. q V. S. Mirrokni, M. Schapira, J. Vondrak, Tight Information-Theoretic Lower Bounds for Maximizing Social-Welfare in Combinatorial Auctions, submitted to IPCO. q M. Goemans, N. Harvey, R. Kleinberg, V. S. Mirrokni, On Learning submodular functions, to be submitted to ICALP. q H. Ackermann, P. Goldberg, V. S. Mirrokni, H. Roeglin, and B. Voecking, Uncoordinated Twosided Markets, to be submitted to ACM EC. q M. Ghodsi, M. Mahini, V. S. Mirrokni, and M. ZadiMoghaddam Singleton Betting for Permutation Betting Markets. q R. Andersen, C. Borgs, J. Chayes, K. Jain, J. Hopcroft, V. S. Mirrokni and S. Teng, Locally Computable Link Spam Features. q V.S. Mirrokni, A. Skopalik, On the Complexity of Nash Dynamics and Sink Equilibria. q R. Andersen, V. S. Mirrokni, Overlapping Clustering for Distributed Computation.",Erratum,pro46
pap1811,9883b126a94221c7a1e607438e629b5c465604d9,con90,Computer Vision and Pattern Recognition,Extremal Graph Theory,,Erratum,pro90
pap1812,2e3409a1c57aa7d9cc69afd50ea2fa32a3f353ba,con14,Hawaii International Conference on System Sciences,Eigenvalue techniques in design and graph theory,"• A submitted manuscript is the author's version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. • The final author version and the galley proof are versions of the publication after peer review. • The final published version features the final layout of the paper including the volume, issue and page numbers.",Erratum,pro14
pap1813,627429c1f7c5f0b35bfe4e7b8f529000e4746304,con4,Conference on Innovative Data Systems Research,Graph Theory and Feynman Integrals,,Erratum,pro4
pap1814,8dff88f4f78ebaebe62d6a8adc6aad586fb5e9de,con111,International Conference on Image Analysis and Processing,Introductory Graph Theory,,Erratum,pro111
pap1815,8d15baabbc36f2bcaae9d023b8b3cc974c64428a,con48,ACM Symposium on Applied Computing,Applications of Graph Theory to Group Structure.,,Erratum,pro48
pap1816,851eb4b78f6deb8aba9d39529e462c5319940f51,con88,European Conference on Computer Vision,"Spectral Graph Theory, Regional Conference Series in Math.",,Erratum,pro88
pap1817,80aae60da893d9fa40ac4bda63f2ae1aa3a29b55,con57,International Workshop on Agent-Oriented Software Engineering,Graph theory in network analysis,,Erratum,pro57
pap1818,ab695b94256553dc06276fc44455acb559611282,jou311,Adaptive Behavior,"Affordances. Motivations, and the World Graph Theory","O'Keefe and Nadel (1978) distinguish two paradigms for navigation, the ""locale system"" for map-based navigation and the ""taxon (behavioral orientation) system"" for route navigation. This article models the taxon system, the map-based system, and their interaction, and argues that the map-based system involves the interaction of hippocampus and other systems. We relate taxes to the notion of an affordance. Just as a rat may have basic taxes for approaching food or avoiding a bright light, so does it have a wider repertoire of affordances for possible actions associated with immediate sensing of its environment. We propose that affordances are extracted by the rat posterior parietal cortex, which guides action selection by the premotor cortex and is influenced also by hypothalamic drive information. The taxon-affordances model (TAM) for taxon-based determination of movement direction is based on models of frog detour behavior, with expectations of future reward implemented using reinforcement learning. The specification of the direction of movement is refined by current affordances and motivational information to yield an appropriate course of action. The world graph (WG) theory expands the idea of a map by developing the hypothesis that cognitive and motivational states interact. This article describes an implementation of this theory, the WG model. The integrated TAM-WG model then allows us to explain data on the behavior of rats with and without fornix lesions, which disconnect the hippocampus from other neural systems.",Conference paper,vol311
pap1819,b819dcbd786315de85aa649b87c90a15206a5db0,con68,Experimental Software Engineering Network,Three short proofs in graph theory,,Erratum,pro68
pap1820,d6af6a19d83e2d4164a91e2362c4603b54470390,con63,International Colloquium on Theoretical Aspects of Computing,Combinatorics and Graph Theory,,Erratum,pro63
pap1821,f73bd6b6fa6267b08fad47ccde5700ff1def71f0,con24,International Conference on Data Technologies and Applications,The Regularity Lemma and Its Applications in Graph Theory,,Erratum,pro24
pap1822,bf69eddc4efacdaf33c135009d468df52ef77f1c,con20,ACM Conference on Economics and Computation,Graph Decompositions: A Study in Infinite Graph Theory,Note to the reader Introduction Fundamental facts and concepts Separating simplices and the existence of prime decompositions Simplicial minors and the existence of prime decompositions The uniqueness of prime decompositions Decompositions into small factors Applications of simplicial decompositions Appendix: Some notes on set theory References Subject index Index of symbols.,Erratum,pro20
pap1823,d5151713a569f203e8cc93d3aee5f3a2168ebe8a,con20,ACM Conference on Economics and Computation,The Many Facets of Graph Theory,,Erratum,pro20
pap1824,78d86a61dcba9d0ccaabf5de1f7ae19a033e0121,con92,Human Language Technology - The Baltic Perspectiv,Graph Theory As A Mathematical Model In Social Science,,Erratum,pro92
pap1825,f7f505e4984a8a60f4d7c3aafd4ad1875913fe2d,con95,IEEE International Conference on Computer Vision,Maximizing the total number of spanning trees in a graph: Two related problems in graph theory and optimum design theory,,Erratum,pro95
pap1826,025c03c7fd1c410ca3a38d94f5696ddee5c28811,con19,International Conference on Conceptual Structures,Some problems in graph theory,,Erratum,pro19
pap1827,aa31f664fb8c6fa41291832376abe94804410d14,jou313,Journal of combinatorial theory. Series A,An Extremal Problem for Sets with Applications to Graph Theory,,Conference paper,vol313
pap1828,fe01d24bd55ca63ceca04539c88ea477c3a50b07,con60,European Conference on Software Process Improvement,Algebraic Graph Theory: The multiplicative expansion,,Erratum,pro60
pap1829,7b815d6c9c0b072278979ba97eda839d141d5d41,jou314,"IEEE Transactions on Systems, Man and Cybernetics","Review of ""Graph Theory"" by Wataru Mayeda",,Letter,vol314
pap1830,ec5e9df34a4671c07d54d73e0e6b67be47cb71af,jou266,Mathematical Gazette,On a Problem in Graph Theory,Suppose there are n towns every pair of which are connected by a single one-way road (roads meet only at towns). Is it possible to choose the direction of the traffic on all the roads so that if any two towns are named there is always a third from which the two named can be reached directly by road?,Letter,vol266
pap1831,2f6d6254a33015aec2bcc0a79561262eb9f473ed,con18,International Conference on Exploring Services Science,Extremal graph theory with emphasis on probabilistic methods,Subdivisions Contractions Small graphs of large girth Large graphs of small diameter Cycles in dense graphs The evolution of random graphs The size Ramsey number of a path Weakly saturated graphs List colourings.,Erratum,pro18
pap1832,8df919ca5405ca5d1d8b192c14584b4354fa3dde,con75,Intelligent Systems in Molecular Biology,SOME ODD GRAPH THEORY,,Erratum,pro75
pap1833,dacf87fd2f31762facc1dc7d3190c3455aa78d80,jou182,Proceedings of the IEEE,Applied graph theory: Graphs and electrical networks,,Article,vol182
pap1834,21569e44f1daa0a10f635391e1fdfed43decc394,con41,Asia-Pacific Software Engineering Conference,Graph theory and Gaussian elimination.,,Erratum,pro41
pap1835,8e91e3c18bff916938fb9d5a742611b6b423bc49,con56,International Conference on Software Engineering and Knowledge Engineering,Applications of graph theory algorithms,,Erratum,pro56
pap1836,a658730fe7dd269186dca9b09cde32fed232c169,con28,International Conference Geographic Information Science,Computational chemical graph theory,,Erratum,pro28
pap1837,b134ce6220b0c39bc3d258c7388afb589016a6c4,con101,International Conference on Biometrics,Graph theory and related topics,,Erratum,pro101
pap1838,7b44986e28bddf8bc163ac34b439b67169c29b55,con45,International Conference on Global Software Engineering,Graph theory and computing,,Erratum,pro45
pap1839,4e618c4e02ec9cfe1e2e07dfeb9155071dabe9fa,con14,Hawaii International Conference on System Sciences,Extremal problems in graph theory,,Erratum,pro14
pap1840,56c979574a36c9f5bc7915964adc3d22972415c5,con10,Americas Conference on Information Systems,Graph Theory Coding Theory and Block Designs: Strongly regular graphs,"Introduction 1. A brief introduction to design theory 2. Strongly regular graphs 3, Quasi-symmetric designs 4. Strongly regular graphs with no triangles 5. Polarities of designs 6. Extension of graphs 7. Codes 8. Cyclic codes 9. Threshold decoding 10. Reed-Muller codes 11. Self-orthogonal codes and designs 12. Quadratic residue codes 13. Symmetry codes over GF(3) 14. Nearly perfect binary codes and uniformly packed codes 15. Association schemes References Index.",Erratum,pro10
pap1841,3e3d6d84f66c6f26db23711931c87db01baf9d81,con71,Annual Conference on Innovation and Technology in Computer Science Education,Graph theory and molecular orbitals,,Erratum,pro71
pap1842,bcb089785e52cfaebdf9de029ec3a6a7586dbe8c,con4,Conference on Innovative Data Systems Research,Proof Techniques in Graph Theory,,Erratum,pro4
pap1843,2b694677da99739df582a93bb7e6ca9a92aaf0fb,con96,Interspeech,Applications of graph theory,,Erratum,pro96
pap1844,024d1cffa9e112d9e81c76cc00fb44e3bd761011,con84,Workshop on Interdisciplinary Software Engineering Research,Progress in Graph Theory,,Erratum,pro84
pap1845,6a52ba688f0c813dcd1eab78ee159ef9c2f5a31f,con56,International Conference on Software Engineering and Knowledge Engineering,Algebraic methods in graph theory,,Erratum,pro56
pap1846,6fc1057ccc4d430051b69d456224ce12bca17dcc,con25,IEEE International Parallel and Distributed Processing Symposium,Graph theory in modern engineering,,Erratum,pro25
pap1847,4213b7478d12581b144e67d6355313297c80f926,jou158,Lecture Notes in Computer Science,Graph Theory and Algorithms,,Conference paper,vol158
pap1848,07dc56d86321ff89a96fd32521fbaefe24ea7376,con107,Chinese Conference on Biometric Recognition,Topics in graph theory,,Erratum,pro107
pap1849,4dd69578aaa585b5ad3f755fd38fff1402a22dec,con106,International Conference on Mobile Data Management,LANDSCAPE CONNECTIVITY: A GRAPH‐THEORETIC PERSPECTIVE,"Ecologists are familiar with two data structures commonly used to represent landscapes. Vector-based maps delineate land cover types as polygons, while raster lattices represent the landscape as a grid. Here we adopt a third lattice data structure, the graph. A graph represents a landscape as a set of nodes (e.g., habitat patches) connected to some degree by edges that join pairs of nodes functionally (e.g., via dispersal). Graph theory is well developed in other fields, including geography (transportation networks, routing ap- plications, siting problems) and computer science (circuitry and network optimization). We present an overview of basic elements of graph theory as it might be applied to issues of connectivity in heterogeneous landscapes, focusing especially on applications of metapo- pulation theory in conservation biology. We develop a general set of analyses using a hypothetical landscape mosaic of habitat patches in a nonhabitat matrix. Our results suggest that a simple graph construct, the minimum spanning tree, can serve as a powerful guide to decisions about the relative importance of individual patches to overall landscape con- nectivity. We then apply this approach to an actual conservation scenario involving the",Erratum,pro106
pap1850,137a07b135e1bf80cd4aaa1207f8081dba5764bd,jou315,Applied Mathematical Sciences,Foundations of Chemical Reaction Network Theory,,Conference paper,vol315
pap1851,133baa61ce0d210e67f12cbab6fd773a9d79a165,con34,International Conference on Agile Software Development,Graph theory,,Erratum,pro34
pap1852,31a8063d5032278580969db08e7ef5589fc58cc6,jou316,IEEE Signal Processing Magazine,Sampling Signals on Graphs: From Theory to Applications,"The study of sampling signals on graphs, with the goal of building an analog of sampling for standard signals in the time and spatial domains, has attracted considerable attention recently. Beyond adding to the growing theory on graph signal processing (GSP), sampling on graphs has various promising applications. In this article, we review the current progress on sampling over graphs, focusing on theory and potential applications.",Article,vol316
pap1853,eae7b0ee87fd63ed13783804da27c931f4e451d8,con80,International Conference on Advanced Computer Science Applications and Technologies,An Introduction to the Theory of Graph Spectra: Frontmatter,Preface 1. Introduction 2. Graph operations and modifications 3. Spectrum and structure 4. Characterizations by spectra 5. Structure and one eigenvalue 6. Spectral techniques 7. Laplacians 8. Additional topics 9. Applications Appendix Bibliography Index of symbols Index.,Erratum,pro80
pap1854,c7e6d7d5c6a9438e3ea15c2c4bd440e63847ffbf,jou317,The VLDB journal,"The core decomposition of networks: theory, algorithms and applications",,Article,vol317
pap1855,448dc7527c4031086bcdf3d117e73bc163b64c5e,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",The History of Degenerate (Bipartite) Extremal Graph Problems,,Erratum,pro73
pap1856,804b3bbc7e5b9a14d446ff9f92236652cf1b1c72,con51,Brazilian Symposium on Software Engineering,Spectra of Graphs: Theory and Applications,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,Erratum,pro51
pap1857,39cd2f4696bcc1019e218fb7724d1fa263fee9f7,con60,European Conference on Software Process Improvement,Supereulerian graphs and the Petersen graph,"A graph G is supereulerian if G has a spanning eulerian subgraph. Boesch et al. [J. Graph Theory, 1, 79–84 (1977)] proposed the problem of characterizing supereulerian graphs. In this paper, we prove that any 3-edge-connected graph with at most 11 edge-cuts of size 3 is supereulerian if and only if it cannot be contractible to the Petersen graph. This extends a former result of Catlin and Lai [J. Combin. Theory, Ser. B, 66, 123–139 (1996)].",Erratum,pro60
pap1858,92170231069fd144805ba9c356ee002db123381b,con11,European Conference on Modelling and Simulation,Riemann–Roch and Abel–Jacobi theory on a finite graph,,Erratum,pro11
pap1859,49768fb7280aec248809ec47594a4458888018a8,jou318,Nonlinear Biomedical Physics,Graph theoretical analysis of complex networks in the brain,,Article,vol318
pap1860,5874afa99a66458efe791bfa5e7196ec870348db,con23,International Conference on Open and Big Data,TOPICS IN GEOMETRIC GROUP THEORY,"We present a brief overview of methods and results in geometric group theory, with the goal of introducing the reader to both topological and metric perspectives. Prerequisites are kept to a minimum: we require only basic algebra, graph theory, and metric space topology.",Erratum,pro23
pap1861,5b64f9fe601db2cc862023efbaf56d9b74f1eef4,con80,International Conference on Advanced Computer Science Applications and Technologies,Large Networks and Graph Limits,"Recently, it became apparent that a large number of the most interesting structures and phenomena of the world can be described by networks. To develop a mathematical theory of very large networks is an important challenge. This book describes one recent approach to this theory, the limit theory of graphs, which has emerged over the last decade. The theory has rich connections with other approaches to the study of large networks, such as ""property testing"" in computer science and regularity partition in graph theory. It has several applications in extremal graph theory, including the exact formulations and partial answers to very general questions, such as which problems in extremal graph theory are decidable. It also has less obvious connections with other parts of mathematics (classical and non-classical, like probability theory, measure theory, tensor algebras, and semidefinite optimization). This book explains many of these connections, first at an informal level to emphasize the need to apply more advanced mathematical methods, and then gives an exact development of the theory of the algebraic theory of graph homomorphisms and of the analytic theory of graph limits. This is an amazing book: readable, deep, and lively. It sets out this emerging area, makes connections between old classical graph theory and graph limits, and charts the course of the future. --Persi Diaconis, Stanford University This book is a comprehensive study of the active topic of graph limits and an updated account of its present status. It is a beautiful volume written by an outstanding mathematician who is also a great expositor. --Noga Alon, Tel Aviv University, Israel Modern combinatorics is by no means an isolated subject in mathematics, but has many rich and interesting connections to almost every area of mathematics and computer science. The research presented in Lovasz's book exemplifies this phenomenon. This book presents a wonderful opportunity for a student in combinatorics to explore other fields of mathematics, or conversely for experts in other areas of mathematics to become acquainted with some aspects of graph theory. --Terence Tao, University of California, Los Angeles, CA Laszlo Lovasz has written an admirable treatise on the exciting new theory of graph limits and graph homomorphisms, an area of great importance in the study of large networks. It is an authoritative, masterful text that reflects Lovasz's position as the main architect of this rapidly developing theory. The book is a must for combinatorialists, network theorists, and theoretical computer scientists alike. --Bela Bollobas, Cambridge University, UK",Erratum,pro80
pap1862,cc0b5f0ea6a81c2f9ee95e8a246733b630c11e63,jou319,Ecosphere,Guidelines for a graph-theoretic implementation of structural equation modeling,"Structural equation modeling (SEM) is increasingly being chosen by researchers as a framework for gaining scientific insights from the quantitative analyses of data. New ideas and methods emerging from the study of causality, influences from the field of graphical modeling, and advances in statistics are expanding the rigor, capability, and even purpose of SEM. Guidelines for implementing the expanded capabilities of SEM are currently lacking. In this paper we describe new developments in SEM that we believe constitute a third-generation of the methodology. Most characteristic of this new approach is the generalization of the structural equation model as a causal graph. In this generalization, analyses are based on graph theoretic principles rather than analyses of matrices. Also, new devices such as metamodels and causal diagrams, as well as an increased emphasis on queries and probabilistic reasoning, are now included. Estimation under a graph theory framework permits the use of Bayesian or likelihood methods. The guidelines presented start from a declaration of the goals of the analysis. We then discuss how theory frames the modeling process, requirements for causal interpretation, model specification choices, selection of estimation method, model evaluation options, and use of queries, both to summarize retrospective results and for prospective analyses. 
 
The illustrative example presented involves monitoring data from wetlands on Mount Desert Island, home of Acadia National Park. Our presentation walks through the decision process involved in developing and evaluating models, as well as drawing inferences from the resulting prediction equations. In addition to evaluating hypotheses about the connections between human activities and biotic responses, we illustrate how the structural equation (SE) model can be queried to understand how interventions might take advantage of an environmental threshold to limit Typha invasions. 
 
The guidelines presented provide for an updated definition of the SEM process that subsumes the historical matrix approach under a graph-theory implementation. The implementation is also designed to permit complex specifications and to be compatible with various estimation methods. Finally, they are meant to foster the use of probabilistic reasoning in both retrospective and prospective considerations of the quantitative implications of the results.",Letter,vol319
pap1863,9e0b0decf155c3f802017238d0096255f75a263c,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Topological Graph Polynomials in Colored Group Field Theory,,Erratum,pro21
pap1864,89cc8969c288b3cf6b5c691baaafb7c8e5c82d1b,con103,IEEE International Conference on Multimedia and Expo,Open problems in the spectral theory of signed graphs,"Signed graphs are graphs whose edges get a sign $+1$ or $-1$ (the signature). Signed graphs can be studied by means of graph matrices extended to signed graphs in a natural way. Recently, the spectra of signed graphs have attracted much attention from graph spectra specialists. One motivation is that the spectral theory of signed graphs elegantly generalizes the spectral theories of unsigned graphs. On the other hand, unsigned graphs do not disappear completely, since their role can be taken by the special case of balanced signed graphs. 
Therefore, spectral problems defined and studied for unsigned graphs can be considered in terms of signed graphs, and sometimes such generalization shows nice properties which cannot be appreciated in terms of (unsigned) graphs. Here, we survey some general results on the adjacency spectra of signed graphs, and we consider some spectral problems which are inspired from the spectral theory of (unsigned) graphs.",Erratum,pro103
pap1865,050c86809f8b48c5f1254ebe67fdab4e01202cfb,jou320,Journal of Applied and Computational Topology,Toward a spectral theory of cellular sheaves,,Letter,vol320
pap1866,d224c80ac2034d832a35ba646a0062bd773ee3c4,jou100,Physical Review Letters,New Integrable 4D Quantum Field Theories from Strongly Deformed Planar N=4 Supersymmetric Yang-Mills Theory.,"We introduce a family of new integrable quantum field theories in four dimensions by considering the γ-deformed N=4 supersymmetric Yang-Mills (SYM) theory in the double scaling limit of large imaginary twists and small coupling. This limit discards the gauge fields and retains only certain Yukawa and scalar interactions with three arbitrary effective couplings. In the 't Hooft limit, these 4D theories are integrable, and contain a wealth of conformal correlators such that the whole arsenal of AdS/CFT integrability remains applicable. As a special case of these models, we obtain a quantum field theory of two complex scalars with a chiral, quartic interaction. The Berenstein-Maldacena-Nastase vacuum anomalous dimension is dominated in each loop order by a single ""wheel"" graph, whose bulk represents an integrable ""fishnet"" graph. This explicitly demonstrates the all-loop integrability of gamma-deformed planar N=4 SYM theory, at least in our limit. Using this feature and integrability results we provide an explicit conjecture for the periods of double-wheel graphs with an arbitrary number of spokes in terms of multiple zeta values of limited depth.",Article,vol100
pap1867,1d0bf1d65b61383b0f89aef7b907716f7e411128,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Image Processing and Analysis With Graphs: theory and Practice,"Graph Theory Concepts and Definitions Used in Image Processing and Analysis, O. Lezoray and L. Grady Introduction Basic Graph Theory Graph Representation Paths, Trees, and Connectivity Graph Models in Image Processing and Analysis Graph Cuts-Combinatorial Optimization in Vision, H. Ishikawa Introduction Markov Random Field Basic Graph Cuts: Binary Labels Multi-Label Minimization Examples Higher-Order Models in Computer Vision, P. Kohli and C. Rother Introduction Higher-Order Random Fields Patch and Region-Based Potentials Relating Appearance Models and Region-Based Potentials Global Potentials Maximum a Posteriori Inference A Parametric Maximum Flow Approach for Discrete Total Variation Regularization, A. Chambolle and J. Darbon Introduction Idea of the approach Numerical Computations Applications Targeted Image Segmentation Using Graph Methods, L. Grady The Regularization of Targeted Image Segmentation Target Specification Conclusion A Short Tour of Mathematical Morphology on Edge and Vertex Weighted Graphs, L. Najman and F. Meyer Introduction Graphs and lattices Neighborhood Operations on Graphs Filters Connected Operators and Filtering with the Component Tree Watershed Cuts MSF Cut Hierarchy and Saliency Maps Optimization and the Power Watershed Partial Difference Equations on Graphs for Local and Nonlocal Image Processing, A. Elmoataz, O. Lezoray, V.-T. Ta, and S. Bougleux Introduction Difference Operators on Weighted Graphs Construction of Weighted Graphs p-Laplacian Regularization on Graphs Examples Image Denoising with Nonlocal Spectral Graph Wavelets, D.K. Hammond, L. Jacques, and P. Vandergheynst Introduction Spectral Graph Wavelet Transform Nonlocal Image Graph Hybrid Local/Nonlocal Image Graph Scaled Laplacian Model Applications to Image Denoising Conclusions Acknowledgments Image and Video Matting, J. Wang Introduction Graph Construction for Image Matting Solving Image Matting Graphs Data Set Video Matting Optimal Simultaneous Multisurface and Multiobject Image Segmentation, X. Wu, M.K. Garvin, and M. Sonka Introduction Motivation and Problem Description Methods for Graph-Based Image Segmentation Case Studies Conclusion Acknowledgments Hierarchical Graph Encodings, L. Brun and W. Kropatsch Introduction Regular Pyramids Irregular Pyramids Parallel construction schemes Irregular Pyramids and Image properties Graph-Based Dimensionality Reduction, J.A. Lee and M. Verleysen Summary Introduction Classical methods Nonlinearity through Graphs Graph-Based Distances Graph-Based Similarities Graph embedding Examples and comparisons Graph Edit Distance-Theory, Algorithms, and Applications, M. Ferrer and H. Bunke Introduction Definitions and Graph Matching Theoretical Aspects of GED GED Computation Applications of GED The Role of Graphs in Matching Shapes and in Categorization, B. Kimia Introduction Using Shock Graphs for Shape Matching Using Proximity Graphs for Categorization Conclusion Acknowledgment 3D Shape Registration Using Spectral Graph Embedding and Probabilistic Matching, A. Sharma, R. Horaud, and D. Mateus Introduction Graph Matrices Spectral Graph Isomorphism Graph Embedding and Dimensionality Reduction Spectral Shape Matching Experiments and Results Discussion Appendix: Permutation and Doubly- stochastic Matrices Appendix: The Frobenius Norm Appendix: Spectral Properties of the Normalized Laplacian Modeling Images with Undirected Graphical Models, M.F. Tappen Introduction Background Graphical Models for Modeling Image Patches Pixel-Based Graphical Models Inference in Graphical Models Learning in Undirected Graphical Models Tree-Walk Kernels for Computer Vision, Z. Harchaoui and F. Bach Introduction Tree-Walk Kernels as Graph Kernels The Region Adjacency Graph Kernel as a Tree-Walk Kernel The Point Cloud Kernel as a Tree-Walk Kernel Experimental Results Conclusion Acknowledgments",Erratum,pro52
pap1868,a89cc66d5c615eb350b46eda017ff002aa3123d9,jou321,Biology,Neural Field Continuum Limits and the Structure–Function Partitioning of Cognitive–Emotional Brain Networks,"Simple Summary Pessoa postulates that bran anatomy associated with the processing and expression of emotion-laden content, such as the amygdala and limbic cortices, is resource capacity-limited. Thus, brains require multichannel or parallel structure-function connectivity to effectively perceive, motivate, integrate, represent, recall, and execute cognitive-emotional relationships. Pessoa employs 2D graph network theory to support his views on distributed brain organization and operation, concluding that brains evolve through dual-process competition and cooperation to form highly embedded computational architectures with little structure–function compartmentalization. Low-dimensional graph theory has become a popular mathematical tool to model, simulate, and visualize evolving complex, sometimes intractable, brain networks. Graph theory offers advantages to study and understand various biological and technological network behaviors and, for Pessoa, it permits a framework that accounts for structure–function features thus far poorly explained by perhaps “traditional” perspectives, which advocate for the mapping of structure–function relationships onto well-localized brain areas. Pessoa nonetheless fails to fully appreciate the significance of weak-to-strong structure-function correlations for brain dynamics and why those correlations, caused by differential control parameters such as Hebbian and antiHebbian neuronal plasticity, are best assessed using neural field theories. Neural fields demonstrate that embedded brain networks optimally evolve between exotic computational phases and continuum limits with the accompaniment of some network partitioning, rather than unconstrained embeddedness, when rendering healthy cognitive-emotional functionality. Abstract In The cognitive-emotional brain, Pessoa overlooks continuum effects on nonlinear brain network connectivity by eschewing neural field theories and physiologically derived constructs representative of neuronal plasticity. The absence of this content, which is so very important for understanding the dynamic structure-function embedding and partitioning of brains, diminishes the rich competitive and cooperative nature of neural networks and trivializes Pessoa’s arguments, and similar arguments by other authors, on the phylogenetic and operational significance of an optimally integrated brain filled with variable-strength neural connections. Riemannian neuromanifolds, containing limit-imposing metaplastic Hebbian- and antiHebbian-type control variables, simulate scalable network behavior that is difficult to capture from the simpler graph-theoretic analysis preferred by Pessoa and other neuroscientists. Field theories suggest the partitioning and performance benefits of embedded cognitive-emotional networks that optimally evolve between exotic classical and quantum computational phases, where matrix singularities and condensations produce degenerate structure-function homogeneities unrealistic of healthy brains. Some network partitioning, as opposed to unconstrained embeddedness, is thus required for effective execution of cognitive-emotional network functions and, in our new era of neuroscience, should be considered a critical aspect of proper brain organization and operation.",Article,vol321
pap1869,2b9e05c6e282934f69c366f1856b637a921fed5f,con57,International Workshop on Agent-Oriented Software Engineering,Graph Structure and Monadic Second-Order Logic - A Language-Theoretic Approach,"The study of graph structure has advanced in recent years with great strides: finite graphs can be described algebraically, enabling them to be constructed out of more basic elements. Separately the properties of graphs can be studied in a logical language called monadic second-order logic. In this book, these two features of graph structure are brought together for the first time in a presentation that unifies and synthesizes research over the last 25 years. The author not only provides a thorough description of the theory, but also details its applications, on the one hand to the construction of graph algorithms, and, on the other to the extension of formal language theory to finite graphs. Consequently the book will be of interest to graduate students and researchers in graph theory, finite model theory, formal language theory, and complexity theory.",Erratum,pro57
pap1870,c99ac1ab758d967c4c07e03ad650ecc73814feb6,con33,International Conference on Automated Software Engineering,Nonstable K-theory for Graph Algebras,,Erratum,pro33
pap1871,cd3929c6e994a712739b907f06622eb814e18881,con109,International Society for Music Information Retrieval Conference,A theory of graph comprehension.,"Venn diagrams, flow charts, tree structures, node networks, to name just a few-or to the great lengths that computer companies go to advertise the graphic capabilities of their products, to see that charts and graphs have enormous appeal to people. All of this is true despite the fact that in virtually every case, the same information can be communicated by nonpictorial means: tables of numbers, lists of propositions cross-referI ! enced by global variables, labeled bracketings, and so on. Perhaps pictorial displays are simply pleasing to the eye, but both introspection and experimental evidence (Carter, 1947; Culbertson & Powers, 1959; Schutz, l%la, I",Erratum,pro109
pap1872,b6c8e5cb261b412656c79b34268f11643ff358a2,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Introduction to the Algebraic Theory of Graph Grammars (A Survey),,Erratum,pro42
pap1873,1eae65773032fae65e7c003da3f67c47996c7f71,con79,IEEE Annual Symposium on Foundations of Computer Science,"Algorithmic graph minor theory: Decomposition, approximation, and coloring","At the core of the seminal graph minor theory of Robertson and Seymour is a powerful structural theorem capturing the structure of graphs excluding a fixed minor. This result is used throughout graph theory and graph algorithms, but is existential. We develop a polynomial-time algorithm using topological graph theory to decompose a graph into the structure guaranteed by the theorem: a clique-sum of pieces almost-embeddable into bounded-genus surfaces. This result has many applications. In particular we show applications to developing many approximation algorithms, including a 2-approximation to graph coloring, constant-factor approximations to treewidth and the largest grid minor, combinatorial polylogarithmic approximation to half-integral multicommodity flow, subexponential fixed-parameter algorithms, and PTASs for many minimization and maximization problems, on graphs excluding a fixed minor.",Letter,pro79
pap1874,db48859397cfe3d28e32892d1f8df6709bce2704,con23,International Conference on Open and Big Data,Graph Spectra for Complex Networks,"This concise and self-contained introduction builds up the spectral theory of graphs from scratch, with linear algebra and the theory of polynomials developed in the later parts. The book focuses on properties and bounds for the eigenvalues of the adjacency, Laplacian and effective resistance matrices of a graph. The goal of the book is to collect spectral properties that may help to understand the behavior or main characteristics of real-world networks. The chapter on spectra of complex networks illustrates how the theory may be applied to deduce insights into real-world networks. The second edition contains new chapters on topics in linear algebra and on the effective resistance matrix, and treats the pseudoinverse of the Laplacian. The latter two matrices and the Laplacian describe linear processes, such as the flow of current, on a graph. The concepts of spectral sparsification and graph neural networks are included.",Erratum,pro23
pap1875,e9ca95e54ec262719fa05607a694d7b85a139f0b,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine","Theory and Applications of the Analytic Network Process: Decision Making With Benefits, Opportunities, Costs, and Risks",what are the real world applications of analytic number. theory and applications of the analytic network process. the analytic network process springerlink. forthing talks the analytic network. theory and applications of the analytic network process. theory and applications of the analytic network process. theory and applications of the analytic network process. marketing applications of the analytic hierarchy process. thomas saaty google scholar citations. international journal of the analytic hierarchy process. what are some important applications of game theory quora. an introduction to graph theory and network analysis with. learning from networks algorithms theory amp applications. international journal of industrial engineering theory. 9781888603064 theory and applications of the analytic. fundamentals of decision making and priority theory with. theory and applications of the analytic network process. pdf the analytic network process researchgate. theory and applications of the analytic network process. theory and applications of the analytic network process. theory and applications of the analytic network process. analytic network process. pdf network theory social network analysis. conference on harmonic analysis function theory operator. plex analysis applications toward number theory. theory and applications of the analytic network process. network meta analysis applying graph theory. theory and applications of the analytic element method. 1888603062 theory and applications of the analytic. learning from networks algorithms theory and applications. analytic network process an overview of applications. theory and applications of the analytic element method. social network analysis theory and applications blogger. pdf generated using the open source mwlib toolkit see. using the analytic hierarchy process ahp to select and. the analytic network process springerlink. a first course in network theory ernesto estrada philip. thomas l saaty. theory and applications of the analytic network process. evaluating the performance of taiwan homestay using. graph theory in network analysis. theory and applications of the analytic network process. application of the analytic network process to facility. theory and applications of the analytic network process. decision making with the analytic network process. decision making with the analytic network process. social network analysis theory and applications in. buy theory and applications of the analytic network. decision making with the analytic network process,Erratum,pro73
pap1876,87c3e5caec704425c53b76c7f4497b9c45479554,con70,International Conference on Graph Transformation,Fundamental Theory for Typed Attributed Graph Transformation,,Conference paper,pro70
pap1877,62fa835ba18dfac1e5ee801da1e27634cc5370e7,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Graph-theoretic methods in database theory,"As in many areas of computer science and other disciplines, graph theoretic tools play an important role also in databases. Many concepts are best captured in terms of graphs or hypergraphs, and problems can then be formulated and solved using graph theoretic algorithms. There is a great number of such examples from schema design, dependency theory, transaction processing, query optimization, data distribution, and a host of other areas. We will not attempt to touch on the wide range of all these applications. Rather, we will concentrate on a particular, basic type of problems that has attracted a great deal of attention in the database literature over the last few years and has come to play a central role: techniques for searching graphs and computing transitive closure, and some of the applications and related problems in query processing. There is an extensive literature on these types of problems, which we cannot reasonably hope to cover in this space, but we shall give a flavour of the issues that arise in solving these problems in various frameworks.",Article,pro85
pap1878,0d399759edd123e62f2e81933d88411b96038a86,con46,Software Product Lines Conference,Graph minor theory,Lecture notes for the topics course on Graph Minor theory.,Erratum,pro46
pap1879,ec2701b6b5c58e1ba851e28452a303f76cb4c7c2,con97,ACM SIGMOD Conference,Term graph rewriting: theory and practice,"Partial table of contents: How to Get Confluence for Explicit Substitutions (T. Hardin) Graph Rewriting Systems for Efficient Compilation (Z. Ariola & Arvind) Abstract Reduction: Towards a Theory via Abstract Interpretation (M. van Eekelen, et al.) The Adequacy of Term Graph Rewriting for Simulating Term Rewriting (J. Kennaway, et al.) Hypergraph Rewriting: Critical Pairs and Undecidability of Confluence (D. Plump) MONSTR: Term Graph Rewriting for Parallel Machines (R. Banach) Parallel Execution of Concurrent Clean on ZAPP (R. Goldsmith, et al.) Implementing Logical Variables and Disjunctions in Graph Rewrite Systems (P. McBrien) Index.",Erratum,pro97
pap1880,fa459de6552f5cd0cbe28539c0c7c65bc112a164,con96,Interspeech,"Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods",Abstract The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception—the visual decoding of information encoded on graphs—and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction...,Erratum,pro96
pap1881,0fb7af235cac61c78c4d184f718b043f3a0e37a4,con38,International Symposium on Empirical Software Engineering and Measurement,Graph Algorithms in the Language of Linear Algebra,"The thesis presents usefulness of duality between graph and his adjacency matrix. The teoretical part provides the basis of graph theory and matrix algebra mainly focusing on sparse matrices and options of their presentation witch takes into account the number of nonzero elements in the matrix. The thesis includes presentation of possible operations on sparse matrices and algorithms that basically work on graphs, but with help of duality between graph and his adjacency matrix can be presented with sequence of operations on matrices. 
Practical part presents implementation of some algorithms that can work both with graphs or their adjacency matrices in programming language Java and testing algorithms that work with matrices. 
It focuses on comparison in efficiency of algorithm working with matrix written in standard mode and with matrix written in format for sparse matrices. It also studies witch presentation of matrices works beter for witch algorithm.",Erratum,pro38
pap1882,c41eb895616e453dcba1a70c9b942c5063cc656c,con78,Neural Information Processing Systems,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,"In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.",Letter,pro78
pap1883,60797ccfb655cc1678793cf16ec787f73816cbf4,con111,International Conference on Image Analysis and Processing,An algebraic theory of graph reduction,"We show how membership in classes of graphs definable in monadic second order logic and of bounded treewidth can be decided by finite sets of terminating reduction rules. The method is constructive in the sense that we describe an algorithm which will produce, from a formula in monadic second order logic and an integer k such that the class defined by the formula is of treewidth ≤ k, a set of rewrite rules that reduces any member of the class to one of finitely many graphs, in a number of steps bounded by the size of the graph. This reduction system corresponds to an algorithm that runs in time linear in the size of the graph.",Erratum,pro111
pap1884,19cd8122dc531dcdf70f1430331f1df9458ccda2,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,The Gewirtz Graph: An Exercise in the Theory of Graph Spectra,"We prove that there is a unique graph (on 56 vertices) with spectrum 101235(-4)20 and examine its structure. It turns out that, e.g., the Coxeter graph (on 28 vertices) and the Sylvester graph (on 36 vertices) are induced subgraphs. We give descriptions of this graph.",Erratum,pro13
pap1885,0cac090d379836a82b7aae6abc222ab7822b8763,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Graph- Theoretical Approaches to the Theory of Voting*,"In this article, language, concepts, and theorems from the theory of directed graphs are used to characterize and analyze the structure of majority preference. A number of results are then derived concerning ""sincere,"" ""sophisticated,"" and ""cooperative"" voting decisions under two common majority voting procedures. These results supplement the work of Black and Farquharson. Perhaps contrary to ""common-sense"" thinking, general strategic manipulation of voting processes has beneficial consequences. It is widely recognized-and not only by political scientists-that the decisions of a voting body may be affected not only by such obviously relevant matters as the preferences of its members and their participation in or absence from particular votes, but also by such ""technical"" matters as the nature of the voting procedure and the order in which proposals are voted on. It is also recognized that voting may have ""gamelike"" characteristics offering strategic opportunities both to voters as individuals and to voters in coalitions. Finally, most political scientists-though probably few politicians or citizens-are by now aware of the ""paradox of voting"" and may have some sense of its connection with these questions of decision, procedure, and strategy. Over the past decade or so a somewhat technical literature on the theory of voting has developed in the ""public choice"" area. The present article adds to this literature by presenting a number of new propositions concerning majority voting under two common voting procedures. These propositions pertain to the questions alluded to in the first paragraph. These new results, together with some more familiar ones, are obtained by employing language, concepts, and theorems from the mathematical theory of directed graphs. In these respects, the article will be of interest primarily to specialists in the area *This article is in part a combination and revision of two unpublished papers:",Erratum,pro73
pap1886,725aa166223bf01ab21fb6b002b1e7f13b626d82,con18,International Conference on Exploring Services Science,Spectra of graphs : theory and application,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,Erratum,pro18
pap1887,ffb03ba337560ccf1204e57e1d1bb831c55e21bf,jou182,Proceedings of the IEEE,Graph-theoretic connectivity control of mobile robot networks,"In this paper, we provide a theoretical framework for controlling graph connectivity in mobile robot networks. We discuss proximity-based communication models composed of disk-based or uniformly-fading-signal-strength communication links. A graph-theoretic definition of connectivity is provided, as well as an equivalent definition based on algebraic graph theory, which employs the adjacency and Laplacian matrices of the graph and their spectral properties. Based on these results, we discuss centralized and distributed algorithms to maintain, increase, and control connectivity in mobile robot networks. The various approaches discussed in this paper range from convex optimization and subgradient-descent algorithms, for the maximization of the algebraic connectivity of the network, to potential fields and hybrid systems that maintain communication links or control the network topology in a least restrictive manner. Common to these approaches is the use of mobility to control the topology of the underlying communication network. We discuss applications of connectivity control to multirobot rendezvous, flocking and formation control, where so far, network connectivity has been considered an assumption.",Article,vol182
pap1888,21877990650a820ac946f5fd2a2cf8085114ae73,con70,International Conference on Graph Transformation,Exploratory Factor Analysis,"Selection of factors to be extracted: Theory is the first criteria to determine the number of factors to be extracted. From theory, we know that the number of factors extracted does make sense. Most researchers use the Eigenvalue criteria for the number of factors to be extracted. Value of the percentage and variance explained method is also used for exploratory factor analysis. We can use the scree test criteria for the selection of factors. In this method, Eigenvalue is plotted on a graph and factors are selected.",Erratum,pro70
pap1889,b8af507417e61be3ab2ba21a9d8c6a8174bb1be6,con86,The Web Conference,Subgraph frequencies: mapping the empirical and extremal geography of large graph collections,"A growing set of on-line applications are generating data that can be viewed as very large collections of small, dense social graphs --- these range from sets of social groups, events, or collaboration projects to the vast collection of graph neighborhoods in large social networks. A natural question is how to usefully define a domain-independent 'coordinate system' for such a collection of graphs, so that the set of possible structures can be compactly represented and understood within a common space. In this work, we draw on the theory of graph homomorphisms to formulate and analyze such a representation, based on computing the frequencies of small induced subgraphs within each graph. We find that the space of subgraph frequencies is governed both by its combinatorial properties --- based on extremal results that constrain all graphs --- as well as by its empirical properties --- manifested in the way that real social graphs appear to lie near a simple one-dimensional curve through this space. We develop flexible frameworks for studying each of these aspects. For capturing empirical properties, we characterize a simple stochastic generative model, a single-parameter extension of Erdos-Renyi random graphs, whose stationary distribution over subgraphs closely tracks the one-dimensional concentration of the real social graph families. For the extremal properties, we develop a tractable linear program for bounding the feasible space of subgraph frequencies by harnessing a toolkit of known extremal graph theory. Together, these two complementary frameworks shed light on a fundamental question pertaining to social graphs: what properties of social graphs are 'social' properties and what properties are 'graph' properties? We conclude with a brief demonstration of how the coordinate system we examine can also be used to perform classification tasks, distinguishing between structures arising from different types of social graphs.",Letter,pro86
pap1890,31fa50edbabd71611da501f6246d92c2562c4b46,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Graph Ramsey theory and the polynomial hierarchy,"Summary form only given, as follows. In the Ramsey theory of graphs F/spl rarr/(G, H) means that for every way of coloring the edges of F red and blue F will contain either a red G or a blue H as a subgraph. The problem ARROWING of deciding whether F/spl rarr/(G, H) lies in /spl Pi//sub 2//sup P/=coNP/sup NP/ and it was shown to be coNP-hard by S.A. Burr (1990). We prove that ARROWING is actually /spl Pi//sub 2//sup P/-complete, simultaneously settling a conjecture of Burr and providing a natural example of a problem complete for a higher level of the polynomial hierarchy. We also consider several specific variants of ARROWING, where G and H are restricted to particular families of graphs. We have a general completeness result for this case under the assumption that certain graphs are constructible in polynomial time. Furthermore we show that STRONG ARROWING, the version of ARROWING for induced subgraphs, is /spl Pi//sub 2//sup P/-complete.",Erratum,pro54
pap1891,a968620dcf399d7d7f89f98dc1d3da8c114dac8d,jou0,Nature Biotechnology,"MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification",,Conference paper,vol0
pap1892,764afc61d329400886d3d027c9d50d97c431f7c8,con37,International Symposium on Search Based Software Engineering,Multibond graph elements in physical systems theory,,Erratum,pro37
pap1893,dce8146987557735a19771aefa1f027211a2c275,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,Statistical mechanics of complex networks,"The emergence of order in natural systems is a constant source of inspiration for both physical and biological sciences. While the spatial order characterizing for example the crystals has been the basis of many advances in contemporary physics, most complex systems in nature do not offer such high degree of order. Many of these systems form complex networks whose nodes are the elements of the system and edges represent the interactions between them. 
Traditionally complex networks have been described by the random graph theory founded in 1959 by Paul Erdohs and Alfred Renyi. One of the defining features of random graphs is that they are statistically homogeneous, and their degree distribution (characterizing the spread in the number of edges starting from a node) is a Poisson distribution. In contrast, recent empirical studies, including the work of our group, indicate that the topology of real networks is much richer than that of random graphs. In particular, the degree distribution of real networks is a power-law, indicating a heterogeneous topology in which the majority of the nodes have a small degree, but there is a significant fraction of highly connected nodes that play an important role in the connectivity of the network. 
The scale-free topology of real networks has very important consequences on their functioning. For example, we have discovered that scale-free networks are extremely resilient to the random disruption of their nodes. On the other hand, the selective removal of the nodes with highest degree induces a rapid breakdown of the network to isolated subparts that cannot communicate with each other. 
The non-trivial scaling of the degree distribution of real networks is also an indication of their assembly and evolution. Indeed, our modeling studies have shown us that there are general principles governing the evolution of networks. Most networks start from a small seed and grow by the addition of new nodes which attach to the nodes already in the system. This process obeys preferential attachment: the new nodes are more likely to connect to nodes with already high degree. We have proposed a simple model based on these two principles wich was able to reproduce the power-law degree distribution of real networks. Perhaps even more importantly, this model paved the way to a new paradigm of network modeling, trying to capture the evolution of networks, not just their static topology.",Erratum,pro39
pap1894,09350843d21273d36caf22675cb588f88c518462,con95,IEEE International Conference on Computer Vision,Estimating and understanding exponential random graph models,"We introduce a method for the theoretical analysis of exponential random graph models. The method is based on a large-deviations approximation to the normalizing constant shown to be consistent using theory developed by Chatterjee and Varadhan [European J. Combin. 32 (2011) 1000-1017]. The theory explains a host of difficulties encountered by applied workers: many distinct models have essentially the same MLE, rendering the problems ``practically'' ill-posed. We give the first rigorous proofs of ``degeneracy'' observed in these models. Here, almost all graphs have essentially no edges or are essentially complete. We supplement recent work of Bhamidi, Bresler and Sly [2008 IEEE 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS) (2008) 803-812 IEEE] showing that for many models, the extra sufficient statistics are useless: most realizations look like the results of a simple Erd\H{o}s-R\'{e}nyi model. We also find classes of models where the limiting graphs differ from Erd\H{o}s-R\'{e}nyi graphs. A limitation of our approach, inherited from the limitation of graph limit theory, is that it works only for dense graphs.",Erratum,pro95
pap1895,aa6be519b394b44ab24c6ad964f8a2c6a9b23571,jou182,Proceedings of the IEEE,Consensus and Cooperation in Networked Multi-Agent Systems,"This paper provides a theoretical framework for analysis of consensus algorithms for multi-agent networked systems with an emphasis on the role of directed information flow, robustness to changes in network topology due to link/node failures, time-delays, and performance guarantees. An overview of basic concepts of information consensus in networks and methods of convergence and performance analysis for the algorithms are provided. Our analysis framework is based on tools from matrix theory, algebraic graph theory, and control theory. We discuss the connections between consensus problems in networked dynamic systems and diverse applications including synchronization of coupled oscillators, flocking, formation control, fast consensus in small-world networks, Markov processes and gossip-based algorithms, load balancing in networks, rendezvous in space, distributed sensor fusion in sensor networks, and belief propagation. We establish direct connections between spectral and structural properties of complex networks and the speed of information diffusion of consensus algorithms. A brief introduction is provided on networked systems with nonlocal information flow that are considerably faster than distributed systems with lattice-type nearest neighbor interactions. Simulation results are presented that demonstrate the role of small-world effects on the speed of consensus algorithms and cooperative control of multivehicle formations",Article,vol182
pap1896,0e21202ae9085bff250fd60ce0fcf54438c95ab6,con75,Intelligent Systems in Molecular Biology,On the algebraic theory of graph colorings,,Erratum,pro75
pap1897,7b00a2ae0c927a3d3dd875dc35f9a777d74a8f3e,con10,Americas Conference on Information Systems,A homology theory for spanning tress of a graph,,Erratum,pro10
pap1898,a55df99552d6093b6150fe2ca79d017644b73935,jou133,PLoS ONE,BrainNet Viewer: A Network Visualization Tool for Human Brain Connectomics,"The human brain is a complex system whose topological organization can be represented using connectomics. Recent studies have shown that human connectomes can be constructed using various neuroimaging technologies and further characterized using sophisticated analytic strategies, such as graph theory. These methods reveal the intriguing topological architectures of human brain networks in healthy populations and explore the changes throughout normal development and aging and under various pathological conditions. However, given the huge complexity of this methodology, toolboxes for graph-based network visualization are still lacking. Here, using MATLAB with a graphical user interface (GUI), we developed a graph-theoretical network visualization toolbox, called BrainNet Viewer, to illustrate human connectomes as ball-and-stick models. Within this toolbox, several combinations of defined files with connectome information can be loaded to display different combinations of brain surface, nodes and edges. In addition, display properties, such as the color and size of network elements or the layout of the figure, can be adjusted within a comprehensive but easy-to-use settings panel. Moreover, BrainNet Viewer draws the brain surface, nodes and edges in sequence and displays brain networks in multiple views, as required by the user. The figure can be manipulated with certain interaction functions to display more detailed information. Furthermore, the figures can be exported as commonly used image file formats or demonstration video for further use. BrainNet Viewer helps researchers to visualize brain networks in an easy, flexible and quick manner, and this software is freely available on the NITRC website (www.nitrc.org/projects/bnv/).",Conference paper,vol133
pap1899,c52f657964a8014e72d925768e2dcab23819abc5,con45,International Conference on Global Software Engineering,Social Network Analysis,"This paper reports on the development of social network analysis, tracing its origins in classical sociology and its more recent formulation in social scientific and mathematical work. It is argued that the concept of social network provides a powerful model for social structure, and that a number of important formal methods of social network analysis can be discerned. Social network analysis has been used in studies of kinship structure, social mobility, science citations, contacts among members of deviant groups, corporate power, international trade exploitation, class structure, and many other areas. A review of the formal models proposed in graph theory, multidimensional scaling, and algebraic topology is followed by extended illustrations of social network analysis in the study of community structure and interlocking directorships.",Erratum,pro45
pap1900,01fb1a96b1e023b7f1f5e7af700765b5a2b1998c,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,Variable Neighborhood Search,,Erratum,pro82
pap1901,fc45cd4563ce8ab02a5fe5facb22ea69864d9ef0,con25,IEEE International Parallel and Distributed Processing Symposium,Information Theory and Network Coding,"This book contains a thorough discussion of the classical topics in information theory together with the first comprehensive treatment of network coding, a subject first emerged under information theory in the mid 1990's that has now diffused into coding theory, computer networks, wireless communications, complexity theory, cryptography, graph theory, etc. With a large number of examples, illustrations, and original problems, this book is excellent as a textbook or reference book for a senior or graduate level course on the subject, as well as a reference for researchers in related fields.",Erratum,pro25
pap1902,4060c1e0491f3cc34f77bd623592799d04f5c78a,jou94,Ecology Letters,Graph models of habitat mosaics.,"Graph theory is a body of mathematics dealing with problems of connectivity, flow, and routing in networks ranging from social groups to computer networks. Recently, network applications have erupted in many fields, and graph models are now being applied in landscape ecology and conservation biology, particularly for applications couched in metapopulation theory. In these applications, graph nodes represent habitat patches or local populations and links indicate functional connections among populations (i.e. via dispersal). Graphs are models of more complicated real systems, and so it is appropriate to review these applications from the perspective of modelling in general. Here we review recent applications of network theory to habitat patches in landscape mosaics. We consider (1) the conceptual model underlying these applications; (2) formalization and implementation of the graph model; (3) model parameterization; (4) model testing, insights, and predictions available through graph analyses; and (5) potential implications for conservation biology and related applications. In general, and for a variety of ecological systems, we find the graph model a remarkably robust framework for applications concerned with habitat connectivity. We close with suggestions for further work on the parameterization and validation of graph models, and point to some promising analytic insights.",Letter,vol94
pap1903,88496bd36dd61ca42dbd5020d23e76ebeaa994a4,jou322,IEEE Transactions on Automatic Control,Information flow and cooperative control of vehicle formations,"We consider the problem of cooperation among a collection of vehicles performing a shared task using intervehicle communication to coordinate their actions. Tools from algebraic graph theory prove useful in modeling the communication network and relating its topology to formation stability. We prove a Nyquist criterion that uses the eigenvalues of the graph Laplacian matrix to determine the effect of the communication topology on formation stability. We also propose a method for decentralized information exchange between vehicles. This approach realizes a dynamical system that supplies each vehicle with a common reference to be used for cooperative motion. We prove a separation principle that decomposes formation stability into two components: Stability of this is achieved information flow for the given graph and stability of an individual vehicle for the given controller. The information flow can thus be rendered highly robust to changes in the graph, enabling tight formation control despite limitations in intervehicle communication capability.",Conference paper,vol322
pap1904,cc2e6e4fde1560bd4839a8cb47b00899cf146a31,con22,Grid Computing Environments,Large Networks and Graph Limits,"The book Large Networks and Graph Limits, xiv + 475 pp., published in late 2012, comprises five parts, the first an illuminating introduction and the last a tantalizing taste of how the scope of the theory developed in its pages might be extended to other combinatorial structures than graphs. The three central parts treat in depth the topics of graph algebras, limits for sequences of dense graphs (this constitutes the most substantial part, occupying nearly half the book) and limits for sequences of bounded degree graphs. Primarily the book is aimed at graduate students and research mathematicians interested in graph theory and its application to networks (for example, the internet and networks in social science, biology, statistical physics and engineering). There are 23 chapters and an appendix, the latter conveniently giving necessary background from areas of mathematics outside mainstream graph theory. A bibliography collects together the extensive research in this area up to 2012, and a subject, author and notation index facilitate navigation of the book. The author maintains a webpage for corrections and supplementary material. Indeed, via the author’s homepage the reader can freely access the many papers he has written with collaborators on the topic of graph homomorphisms and graph limits. The book synthesizes much of the material in these papers, with some revision in",Erratum,pro22
pap1905,5d2f80db3312b9d203f2ed9877f0e6971f4bf7dc,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",Discrete signal processing on graphs: Graph fourier transform,"We propose a novel discrete signal processing framework for the representation and analysis of datasets with complex structure. Such datasets arise in many social, economic, biological, and physical networks. Our framework extends traditional discrete signal processing theory to structured datasets by viewing them as signals represented by graphs, so that signal coefficients are indexed by graph nodes and relations between them are represented by weighted graph edges. We discuss the notions of signals and filters on graphs, and define the concepts of the spectrum and Fourier transform for graph signals. We demonstrate their relation to the generalized eigenvector basis of the graph adjacency matrix and study their properties. As a potential application of the graph Fourier transform, we consider the efficient representation of structured data that utilizes the sparseness of graph signals in the frequency domain.",Article,pro87
pap1906,e1a50831ee71998ca4c577bb996e6353c5eb2d4a,con61,International Conference on Predictive Models in Software Engineering,Connectedness Index of uncertain Graph,"In practical applications of graph theory, non-deterministic factors are frequently encountered. This paper employs uncertainty theory to deal with non-deterministic factors in problems of graph connectivity. The concepts of uncertain graph and connectedness index of uncertain graph are proposed in this paper. It presents two algorithms to calculate connectedness index of an uncertain graph.",Erratum,pro61
pap1907,2e64c17ae0799e3995e927ec46b31e03ce26aa76,con6,Annual Conference on Genetic and Evolutionary Computation,Evolutionary Dynamics: Exploring the Equations of Life,Preface 1. Introduction 2. What Evolution Is 3. Fitness Landscapes and Sequence Spaces 4. Evolutionary Games 5. Prisoners of the Dilemma 6. Finite Populations 7. Games in Finite Populations 8. Evolutionary Graph Theory 9. Spatial Games 10. HIV Infection 11. The Evolution of Virulence 12. The Evolutionary Dynamics of Cancer 13. Language Evolution 14. Conclusion Further Reading References Index,Erratum,pro6
pap1908,acfd9ea27a4183cc6ae1d74998e2e1e0c9e98093,con56,International Conference on Software Engineering and Knowledge Engineering,Computing topological parameters of biological networks,"UNLABELLED
Rapidly increasing amounts of molecular interaction data are being produced by various experimental techniques and computational prediction methods. In order to gain insight into the organization and structure of the resultant large complex networks formed by the interacting molecules, we have developed the versatile Cytoscape plugin NetworkAnalyzer. It computes and displays a comprehensive set of topological parameters, which includes the number of nodes, edges, and connected components, the network diameter, radius, density, centralization, heterogeneity, and clustering coefficient, the characteristic path length, and the distributions of node degrees, neighborhood connectivities, average clustering coefficients, and shortest path lengths. NetworkAnalyzer can be applied to both directed and undirected networks and also contains extra functionality to construct the intersection or union of two networks. It is an interactive and highly customizable application that requires no expert knowledge in graph theory from the user.


AVAILABILITY
NetworkAnalyzer can be downloaded via the Cytoscape web site: http://www.cytoscape.org",Erratum,pro56
pap1909,98f84b38956ece37082c7bc2a8282f47454f4427,con65,IEEE International Conference on Software Engineering and Formal Methods,"Functional Analysis, Sobolev Spaces and Partial Differential Equations",,Erratum,pro65
pap1910,ef32407a7947a1051c7ecdcdeb857ed835bbed99,jou323,IEEE Control Systems,Rigid graph control architectures for autonomous formations,"This article sets out the rudiments of a theory for analyzing and creating architectures appropriate to the control of formations of autonomous vehicles. The theory rests on ideas of rigid graph theory, some but not all of which are old. The theory, however, has some gaps in it, and their elimination would help in applications. Some of the gaps in the relevant graph theory are as follows. First, there is as yet no analogue for three-dimensional graphs of Laman's theorem, which provides a combinatorial criterion for rigidity in two-dimensional graphs. Second, for three-dimensional graphs there is no analogue of the two-dimensional Henneberg construction for growing or deconstructing minimally rigid graphs although there are conjectures. Third, global rigidity can easily be characterized for two-dimensional graphs, but not for three-dimensional graphs.",Letter,vol323
pap1911,d2b93dfbe50f3c642c64b8ea581cb6e449c71f82,jou324,Multiscale Modeling & simulation,Nonlocal Operators with Applications to Image Processing,"We propose the use of nonlocal operators to define new types of flows and functionals for image processing and elsewhere. A main advantage over classical PDE-based algorithms is the ability to handle better textures and repetitive structures. This topic can be viewed as an extension of spectral graph theory and the diffusion geometry framework to functional analysis and PDE-like evolutions. Some possible applications and numerical examples are given, as is a general framework for approximating Hamilton–Jacobi equations on arbitrary grids in high demensions, e.g., for control theory.",Conference paper,vol324
pap1912,636b0754486f29f0bbc68cc2b410f564a3dfefe6,con74,IEEE International Conference on Information Reuse and Integration,Spectra of Graphs,"This book gives an elementary treatment of the basic material about graph spectra, both for ordinary, and Laplace and Seidel spectra. The text progresses systematically, by covering standard topics before presenting some new material on trees, strongly regular graphs, two-graphs, association schemes, p-ranks of configurations and similar topics. Exercises at the end of each chapter provide practice and vary from easy yet interesting applications of the treated theory, to little excursions into related topics. Tables, references at the end of the book, an author and subject index enrich the text. Spectra of Graphs is written for researchers, teachers and graduate students interested in graph spectra. The reader is assumed to be familiar with basic linear algebra and eigenvalues, although some more advanced topics in linear algebra, like the Perron-Frobenius theorem and eigenvalue interlacing are included.",Erratum,pro74
pap1913,9d061f8cc3cb4eb77579adcdbe99169b3e839b27,con88,European Conference on Computer Vision,Graph Degree Linkage: Agglomerative Clustering on a Directed Graph,,Conference paper,pro88
pap1914,86a8e3b54eb7b0dd8076d73494f5c82f853ab860,con93,International Conference on Computational Logic,A Theory of Graphs,,Erratum,pro93
pap1915,b9dd445a4e7ad794012db01339f8fe9967b923a8,jou322,IEEE Transactions on Automatic Control,Consensus Problems on Networks With Antagonistic Interactions,"In a consensus protocol an agreement among agents is achieved thanks to the collaborative efforts of all agents, expresses by a communication graph with nonnegative weights. The question we ask in this paper is the following: is it possible to achieve a form of agreement also in presence of antagonistic interactions, modeled as negative weights on the communication graph? The answer to this question is affirmative: on signed networks all agents can converge to a consensus value which is the same for all agents except for the sign. Necessary and sufficient conditions are obtained to describe cases in which this is possible. These conditions have strong analogies with the theory of monotone systems. Linear and nonlinear Laplacian feedback designs are proposed.",Conference paper,vol322
pap1916,7a498d6cef22d72dc6bf8da90f145ea8f5d9fece,con5,Technical Symposium on Computer Science Education,Wiener Index of Trees: Theory and Applications,,Erratum,pro5
pap1917,221aa3be55a4ead8fc2aa83b12aac370bfba72f5,jou325,IEEE Transactions on Systems Science and Cybernetics,A Formal Basis for the Heuristic Determination of Minimum Cost Paths,"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.",Article,vol325
pap1918,e8b8c5f4a81e11576ee2c74ab65c66a42bbad270,con65,IEEE International Conference on Software Engineering and Formal Methods,Random graphs with arbitrary degree distributions and their applications.,"Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.",Erratum,pro65
pap1919,0b4497542279def00135e7a3140b3418606679e9,con67,IEEE International Software Metrics Symposium,An Introduction to Hydrogen Bonding,"1. Brief History 2. Nature and Properties 3. Strong Hydrogen Bonds 4. Moderate Hydrogen Bonds 5. Weak Hydrogen Bonds 6. Cooperativity, Patterns, Graph Set Theory, Liquid Crystals 7. Disorder, Proton Transfer, Isotope Effect, Ferroelectrics, Transitions 8. Water, Water Dimers, Ices, Hydrates 9. Inclusion Compounds 10. Hydrogen Bonding in Biological Molecules 11. Methods",Erratum,pro67
pap1920,9ccf7b6cb32cf89752a35bd910555adac54773e0,con19,International Conference on Conceptual Structures,The Knowledge Complexity of Interactive Proof Systems,"Usually, a proof of a theorem contains more knowledge than the mere fact that the theorem is true. For instance, to prove that a graph is Hamiltonian it suffices to exhibit a Hamiltonian tour in it; however, this seems to contain more knowledge than the single bit Hamiltonian/non-Hamiltonian.In this paper a computational complexity theory of the “knowledge” contained in a proof is developed. Zero-knowledge proofs are defined as those proofs that convey no additional knowledge other than the correctness of the proposition in question. Examples of zero-knowledge proof systems are given for the languages of quadratic residuosity and 'quadratic nonresiduosity. These are the first examples of zero-knowledge proofs for languages not known to be efficiently recognizable.",Erratum,pro19
pap1921,9a84002359c777dc941e1465f148abe43cb3331f,jou326,Journal of the Royal Society Interface,Networks and epidemic models,"Networks and the epidemiology of directly transmitted infectious diseases are fundamentally linked. The foundations of epidemiology and early epidemiological models were based on population wide random-mixing, but in practice each individual has a finite set of contacts to whom they can pass infection; the ensemble of all such contacts forms a ‘mixing network’. Knowledge of the structure of the network allows models to compute the epidemic dynamics at the population scale from the individual-level behaviour of infections. Therefore, characteristics of mixing networks—and how these deviate from the random-mixing norm—have become important applied concerns that may enhance the understanding and prediction of epidemic patterns and intervention measures. Here, we review the basis of epidemiological theory (based on random-mixing models) and network theory (based on work from the social sciences and graph theory). We then describe a variety of methods that allow the mixing network, or an approximation to the network, to be ascertained. It is often the case that time and resources limit our ability to accurately find all connections within a network, and hence a generic understanding of the relationship between network structure and disease dynamics is needed. Therefore, we review some of the variety of idealized network types and approximation techniques that have been utilized to elucidate this link. Finally, we look to the future to suggest how the two fields of network theory and epidemiological modelling can deliver an improved understanding of disease dynamics and better public health through effective disease control.",Letter,vol326
pap1922,c3d47d49e7c46d2543e92a26b964f42b25371ba4,con105,British Machine Vision Conference,Geometric Algorithms and Combinatorial Optimization,,Erratum,pro105
pap1923,088ab372dff19fb8837d0f48d395d1a987251f3f,con104,Biometrics and Identity Management,Geometry of cuts and metrics,,Erratum,pro104
pap1924,59d86a93c4ef54b5489bc375cd02e64205823f42,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,Random Walks for Image Segmentation,"A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs",Letter,vol290
pap1925,7fe0ef2ddacd193101dc5ba3df97b0241a5e8fc6,jou322,IEEE Transactions on Automatic Control,Stability of multiagent systems with time-dependent communication links,"We study a simple but compelling model of network of agents interacting via time-dependent communication links. The model finds application in a variety of fields including synchronization, swarming and distributed decision making. In the model, each agent updates his current state based upon the current information received from neighboring agents. Necessary and/or sufficient conditions for the convergence of the individual agents' states to a common value are presented, thereby extending recent results reported in the literature. The stability analysis is based upon a blend of graph-theoretic and system-theoretic tools with the notion of convexity playing a central role. The analysis is integrated within a formal framework of set-valued Lyapunov theory, which may be of independent interest. Among others, it is observed that more communication does not necessarily lead to faster convergence and may eventually even lead to a loss of convergence, even for the simple models discussed in the present paper.",Conference paper,vol322
pap1926,c6b745c7ecc3fc89d0df71727e1a0f456be7187a,jou327,Annual Review of Clinical Psychology,Brain graphs: graphical models of the human brain connectome.,"Brain graphs provide a relatively simple and increasingly popular way of modeling the human brain connectome, using graph theory to abstractly define a nervous system as a set of nodes (denoting anatomical regions or recording electrodes) and interconnecting edges (denoting structural or functional connections). Topological and geometrical properties of these graphs can be measured and compared to random graphs and to graphs derived from other neuroscience data or other (nonneural) complex systems. Both structural and functional human brain graphs have consistently demonstrated key topological properties such as small-worldness, modularity, and heterogeneous degree distributions. Brain graphs are also physically embedded so as to nearly minimize wiring cost, a key geometric property. Here we offer a conceptual review and methodological guide to graphical analysis of human neuroimaging data, with an emphasis on some of the key assumptions, issues, and trade-offs facing the investigator.",Letter,vol327
pap1927,1c8003c27d0022f241b42a1d5ca12b85e44726e6,jou328,IEEE Transactions on Information Theory,Constructing free-energy approximations and generalized belief propagation algorithms,"Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a ""valid"" or ""maxent-normal"" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the ""Bethe method"", the ""junction graph method"", the ""cluster variation method"", and the ""region graph method"". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.",Article,vol328
pap1928,eb524f7c1e29bdd7d27c33a90921c5ea7f347234,jou182,Proceedings of the IEEE,Frequency assignment: Theory and applications,"In this paper we introduce the minimum-order approach to frequency assignment and present a theory which relates this approach to the traditional one. This new approach is potentially more desirable than the traditional one. We model assignment problems as both frequency-distance constrained and frequency constrained optimization problems. The frequency constrained approach should be avoided if distance separation is employed to mitigate interference. A restricted class of graphs, called disk graphs, plays a central role in frequency-distance constrained problems. We introduce two generalizations of chromatic number and show that many frequency assignment problems are equivalent to generalized graph coloring problems. Using these equivalences and recent results concerning the complexity of graph coloring, we classify many frequency assignment problems according to the ""execution time efficiency"" of algorithms that may be devised for their solution. We discuss applications to important real world problems and identify areas for further work.",Letter,vol182
pap1929,a8a18497987e8b4715cba7cd6d2f8e6a1d58b2fa,jou329,Neuroinformatics,The small world of the cerebral cortex,,Article,vol329
pap1930,e507a66243223b83c50ec8609c8e2db5a99277a7,jou328,IEEE Transactions on Information Theory,A Spectral Graph Uncertainty Principle,"The spectral theory of graphs provides a bridge between classical signal processing and the nascent field of graph signal processing. In this paper, a spectral graph analogy to Heisenberg's celebrated uncertainty principle is developed. Just as the classical result provides a tradeoff between signal localization in time and frequency, this result provides a fundamental tradeoff between a signal's localization on a graph and in its spectral domain. Using the eigenvectors of the graph Laplacian as a surrogate Fourier basis, quantitative definitions of graph and spectral “spreads” are given, and a complete characterization of the feasibility region of these two quantities is developed. In particular, the lower boundary of the region, referred to as the uncertainty curve, is shown to be achieved by eigenvectors associated with the smallest eigenvalues of an affine family of matrices. The convexity of the uncertainty curve allows it to be found to within ε by a fast approximation algorithm requiring O(ε-1/2) typically sparse eigenvalue evaluations. Closed-form expressions for the uncertainty curves for some special classes of graphs are derived, and an accurate analytical approximation for the expected uncertainty curve of Erd-s-Rényi random graphs is developed. These theoretical results are validated by numerical experiments, which also reveal an intriguing connection between diffusion processes on graphs and the uncertainty bounds.",Conference paper,vol328
pap1931,ada56e1f7575d7f542215c48625c161ab060bed0,con91,Symposium on the Theory of Computing,Static scheduling algorithms for allocating directed task graphs to multiprocessors,"Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP-complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph-theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.",Erratum,pro91
pap1932,a79d1c0f6e8bee2ca0cab3522e67b27a533e19e3,jou322,IEEE Transactions on Automatic Control,On maximizing the second smallest eigenvalue of a state-dependent graph Laplacian,"We consider the set G consisting of graphs of fixed order and weighted edges. The vertex set of graphs in G will correspond to point masses and the weight for an edge between two vertices is a functional of the distance between them. We pose the problem of finding the best vertex positional configuration in the presence of an additional proximity constraint, in the sense that, the second smallest eigenvalue of the corresponding graph Laplacian is maximized. In many recent applications of algebraic graph theory in systems and control, the second smallest eigenvalue of Laplacian has emerged as a critical parameter that influences the stability and robustness properties of dynamic systems that operate over an information network. Our motivation in the present work is to ""assign"" this Laplacian eigenvalue when relative positions of various elements dictate the interconnection of the underlying weighted graph. In this venue, one would then be able to ""synthesize"" information graphs that have desirable system theoretic properties.",Letter,vol322
pap1933,1db1447bc61a68500ec31e94daf27cf057831f83,con84,Workshop on Interdisciplinary Software Engineering Research,Shock Waves on the Highway,A simple theory of traffic flow is developed by replacing individual vehicles with a continuous “fluid” density and applying an empirical relation between speed and density. Characteristic features of the resulting theory are a simple “graph-shearing” process for following the development of traffic waves in time and the frequent appearance of shock waves. The effect of a traffic signal on traffic streams is studied and found to exhibit a threshold effect wherein the disturbances are minor for light traffic but suddenly build to large values when a critical density is exceeded.,Erratum,pro84
pap1934,2e15e356a1368e65b42417676276160110daf272,con91,Symposium on the Theory of Computing,Second-Order Consensus for Multiagent Systems With Directed Topologies and Nonlinear Dynamics,"This paper considers a second-order consensus problem for multiagent systems with nonlinear dynamics and directed topologies where each agent is governed by both position and velocity consensus terms with a time-varying asymptotic velocity. To describe the system's ability for reaching consensus, a new concept about the generalized algebraic connectivity is defined for strongly connected networks and then extended to the strongly connected components of the directed network containing a spanning tree. Some sufficient conditions are derived for reaching second-order consensus in multiagent systems with nonlinear dynamics based on algebraic graph theory, matrix theory, and Lyapunov control approach. Finally, simulation examples are given to verify the theoretical analysis.",Erratum,pro91
pap1935,439f26a0938b1c4617655d0f951eda77033aedbe,con105,British Machine Vision Conference,Introduction to Quantum Graphs,"A ""quantum graph"" is a graph considered as a one-dimensional complex and equipped with a differential operator (""Hamiltonian""). Quantum graphs arise naturally as simplified models in mathematics, physics, chemistry, and engineering when one considers propagation of waves of various nature through a quasi-one-dimensional (e.g., ""meso-"" or ""nano-scale"") system that looks like a thin neighborhood of a graph. Works that currently would be classified as discussing quantum graphs have been appearing since at least the 1930s, and since then, quantum graphs techniques have been applied successfully in various areas of mathematical physics, mathematics in general and its applications. One can mention, for instance, dynamical systems theory, control theory, quantum chaos, Anderson localization, microelectronics, photonic crystals, physical chemistry, nano-sciences, superconductivity theory, etc. Quantum graphs present many non-trivial mathematical challenges, which makes them dear to a mathematician's heart. Work on quantum graphs has brought together tools and intuition coming from graph theory, combinatorics, mathematical physics, PDEs, and spectral theory. This book provides a comprehensive introduction to the topic, collecting the main notions and techniques. It also contains a survey of the current state of the quantum graph research and applications.",Erratum,pro105
pap1936,b79267cfe6881e05275cfa14aa3f7af0e6ae9e10,con45,International Conference on Global Software Engineering,Graph removal lemmas,"The graph removal lemma states that any graph on n vertices with o(n^{v(H)}) copies of a fixed graph H may be made H-free by removing o(n^2) edges. Despite its innocent appearance, this lemma and its extensions have several important consequences in number theory, discrete geometry, graph theory and computer science. In this survey we discuss these lemmas, focusing in particular on recent improvements to their quantitative aspects.",Erratum,pro45
pap1937,e5a0fff54abb5eca5e61f2b8d73a5f2acaad6c3a,jou59,Nature,Topological quantum chemistry,,Conference paper,vol59
pap1938,65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,jou316,IEEE Signal Processing Magazine,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",Article,vol316
pap1939,b8523a1ae8f7ebc53b2af47b6541f6348383c0f8,con37,International Symposium on Search Based Software Engineering,Challenges with graph interpretation: a review of the literature,"With the growing emphasis on the development of scientific inquiry skills, the display and interpretation of data are becoming increasingly important. Graph interpretation competence is, in fact, essential to understanding today’s world and to be scientifically literate. However, graph interpretation is a complex and challenging activity. Graph interpretation competence is affected by many factors, including aspects of graph characteristics, the content of the graph and viewers’ prior knowledge. For instance, the prior theory and expectations that students have may lead to biases and misinterpretation of graphs. One basic controversy that remains unanswered, for example, is what should we teach first in order to make students scientific literate, how to graph or how to interpret a graph? If it is the case that the ability to interpret a graph be developed prior to the ability to create, then it is important to understand what graph interpretation entails. This paper reviews current literature on graph interpretation competence and argues that it should be explicitly taught given its importance and its complexity.",Erratum,pro37
pap1940,3e502fb40768c140ef24ea742a212c263f380f71,con100,International Conference on Automatic Face and Gesture Recognition,Modeling and control of formations of nonholonomic mobile robots,"This paper addresses the control of a team of nonholonomic mobile robots navigating in a terrain with obstacles while maintaining a desired formation and changing formations when required, using graph theory. We model the team as a triple, (g, r, H), consisting of a group element g that describes the gross position of the lead robot, a set of shape variables r that describe the relative positions of robots, and a control graph H that describes the behaviors of the robots in the formation. Our framework enables the representation and enumeration of possible control graphs and the coordination of transitions between any two formations.",Erratum,pro100
pap1941,56a44211b353f02b4f9dd31369edde8e3804b4d6,jou330,Journal of Statistical Software,"ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.","We describe some of the capabilities of the ergm package and the statistical theory underlying it. This package contains tools for accomplishing three important, and interrelated, tasks involving exponential-family random graph models (ERGMs): estimation, simulation, and goodness of fit. More precisely, ergm has the capability of approximating a maximum likelihood estimator for an ERGM given a network data set; simulating new network data sets from a fitted ERGM using Markov chain Monte Carlo; and assessing how well a fitted ERGM does at capturing characteristics of a particular network data set.",Conference paper,vol330
pap1942,afc34303fcd5a4175f33d5161eb056826f64b880,jou331,IEEE Transactions on Circuits and Systems Part 1: Regular Papers,Kron Reduction of Graphs With Applications to Electrical Networks,"Consider a weighted undirected graph and its corresponding Laplacian matrix, possibly augmented with additional diagonal elements corresponding to self-loops. The Kron reduction of this graph is again a graph whose Laplacian matrix is obtained by the Schur complement of the original Laplacian matrix with respect to a specified subset of nodes. The Kron reduction process is ubiquitous in classic circuit theory and in related disciplines such as electrical impedance tomography, smart grid monitoring, transient stability assessment, and analysis of power electronics. Kron reduction is also relevant in other physical domains, in computational applications, and in the reduction of Markov chains. Related concepts have also been studied as purely theoretic problems in the literature on linear algebra. In this paper we analyze the Kron reduction process from the viewpoint of algebraic graph theory. Specifically, we provide a comprehensive and detailed graph-theoretic analysis of Kron reduction encompassing topological, algebraic, spectral, resistive, and sensitivity analyses. Throughout our theoretic elaborations we especially emphasize the practical applicability of our results to various problem setups arising in engineering, computation, and linear algebra. Our analysis of Kron reduction leads to novel insights both on the mathematical and the physical side.",Conference paper,vol331
pap1943,e1b10e80013766521e82bc56babaab63c2265847,con3,Knowledge Discovery and Data Mining,Renormalization in Quantum Field Theory and the Riemann–Hilbert Problem I: The Hopf Algebra Structure of Graphs and the Main Theorem,,Erratum,pro3
pap1944,1e890895a38fe79be13636e563ea669ea63133e1,con50,International Workshop on Green and Sustainable Software,Applications of Hyperstructure Theory,,Erratum,pro50
pap1945,78bb1ded151a2674d634d04d717d458339b7cb2c,jou22,Proceedings of the National Academy of Sciences of the United States of America,Spectral redemption in clustering sparse networks,"Significance Spectral algorithms are widely applied to data clustering problems, including finding communities or partitions in graphs and networks. We propose a way of encoding sparse data using a “nonbacktracking” matrix, and show that the corresponding spectral algorithm performs optimally for some popular generative models, including the stochastic block model. This is in contrast with classical spectral algorithms, based on the adjacency matrix, random walk matrix, and graph Laplacian, which perform poorly in the sparse case, failing significantly above a recently discovered phase transition for the detectability of communities. Further support for the method is provided by experiments on real networks as well as by theoretical arguments and analogies from probability theory, statistical physics, and the theory of random matrices. Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.",Article,vol22
pap1946,ca47be74efccb005d88f8455aff73e0622949e96,con60,European Conference on Software Process Improvement,Mathematical Concepts in Organic Chemistry,,Erratum,pro60
pap1947,2c03d0e5113cc34ff607c652c68a5e542e607735,con22,Grid Computing Environments,Property testing and its connection to learning and approximation,"The authors study the question of determining whether an unknown function has a particular property or is /spl epsiv/-far from any function with that property. A property testing algorithm is given a sample of the value of the function on instances drawn according to some distribution, and possibly may query the function on instances of its choice. First, they establish some connections between property testing and problems in learning theory. Next, they focus on testing graph properties, and devise algorithms to test whether a graph has properties such as being k-colorable or having a /spl rho/-clique (clique of density /spl rho/ w.r.t. the vertex set). The graph property testing algorithms are probabilistic and make assertions which are correct with high probability utilizing only poly(1//spl epsiv/) edge-queries into the graph, where /spl epsiv/ is the distance parameter. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph which correspond to the property being tested, if it holds for the input graph.",Erratum,pro22
pap1948,b623b893faa4c6ef54ba87af04a970b8250c5274,con81,International Conference on Learning Representations,"Sharp thresholds of graph properties, and the -sat problem","Consider G(n, p) to be the probability space of random graphs on n vertices with edge probability p. We will be considering subsets of this space defined by monotone graph properties. A monotone graph property P is a property of graphs such that a) P is invariant under graph automorphisims. b) If graph H has property P , then so does any graph G having H as a subgraph. A monotone symmetric family of graphs is a family defined by such a property. One of the first observations made about random graphs by Erdos and Renyi in their seminal work on random graph theory [12] was the existence of threshold phenomena, the fact that for many interesting properties P , the probability of P appearing in G(n, p) exhibits a sharp increase at a certain critical value of the parameter p. Bollobas and Thomason proved the existence of threshold functions for all monotone set properties ([6]), and in [14] it is shown that this behavior is quite general, and that all monotone graph properties exhibit threshold behavior, i.e. the probability of their appearance increases from values very close to 0 to values close to 1 in a very small interval. More precise analysis of the size of the threshold interval is done in [7]. This threshold behavior which occurs in various settings which arise in combinatorics and computer science is an instance of the phenomenon of phase transitions which is the subject of much interest in statistical physics. One of the main questions that arises in studying phase transitions is: how “sharp” is the transition? For example, one of the motivations for this paper arose from the question of the sharpness of the phase transition for the property of satisfiability of a random kCNF Boolean formula. Nati Linial, who introduced me to this problem, suggested that although much concrete analysis was being performed on this problem the best approach would be to find general conditions for sharpness of the phase transition, answering the question posed in [14] as to the relation between the length of the threshold interval and the value of the critical probability. In this paper we indeed introduce a simple condition and prove it is sufficient. Stated roughly, in the setting of random graphs, the main theorem states that if a property has a coarse threshold, then it can be approximated by the property of having certain given graphs as a subgraph. This condition can be applied in a more",Erratum,pro81
pap1949,5ce9e830146f9e4b095511f693a939e749686430,jou332,IEEE Transactions on Mobile Computing,A Theory of Network Localization,"In this paper, we provide a theoretical foundation for the problem of network localization in which some nodes know their locations and other nodes determine their locations by measuring the distances to their neighbors. We construct grounded graphs to model network localization and apply graph rigidity theory to test the conditions for unique localizability and to construct uniquely localizable networks. We further study the computational complexity of network localization and investigate a subclass of grounded graphs where localization can be computed efficiently. We conclude with a discussion of localization in sensor networks where the sensors are placed randomly",Conference paper,vol332
pap1950,796cd1df17ac1eedbc504dd9eaf2f1ca30b8a6be,con51,Brazilian Symposium on Software Engineering,KEGGgraph: a graph approach to KEGG PATHWAY in R and bioconductor,"Motivation: KEGG PATHWAY is a service of Kyoto Encyclopedia of Genes and Genomes (KEGG), constructing manually curated pathway maps that represent current knowledge on biological networks in graph models. While valuable graph tools have been implemented in R/Bioconductor, to our knowledge there is currently no software package to parse and analyze KEGG pathways with graph theory. Results: We introduce the software package KEGGgraph in R and Bioconductor, an interface between KEGG pathways and graph models as well as a collection of tools for these graphs. Superior to existing approaches, KEGGgraph captures the pathway topology and allows further analysis or dissection of pathway graphs. We demonstrate the use of the package by the case study of analyzing human pancreatic cancer pathway. Availability:KEGGgraph is freely available at the Bioconductor web site (http://www.bioconductor.org). KGML files can be downloaded from KEGG FTP site (ftp://ftp.genome.jp/pub/kegg/xml). Contact: j.zhang@dkfz-heidelberg.de Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro51
pap1951,6350d7697aa098fadc46296218223325076826a3,con35,IEEE Working Conference on Mining Software Repositories,Topological properties of hypercubes,"The n-dimensional hypercube is a highly concurrent loosely coupled multiprocessor based on the binary n-cube topology. Machines based on the hypercube topology have been advocated as ideal parallel architectures for their powerful interconnection features. The authors examine the hypercube from the graph-theory point of view and consider those features that make its connectivity so appealing. Among other things, they propose a theoretical characterization of the n-cube as a graph and and show how to map various other topologies into a hypercube. >",Erratum,pro35
pap1952,015f6fcfe7fb621dd84fd24ea2beb35321896005,con46,Software Product Lines Conference,Simplifying multiloop integrands and ultraviolet divergences of gauge theory and gravity amplitudes,"We use the duality between color and kinematics to simplify the construction of the complete four-loop four-point amplitude of N = 4 super-Yang-Mills theory, including the nonplanar contributions. The duality completely determines the amplitude's integrand in terms of just two planar graphs. The existence of a manifestly dual gauge-theory amplitude trivializes the construction of the corresponding N = 8 supergravity integrand, whose graph numerators are double copies (squares) of the N = 4 super-Yang-Mills numerators. The success of this procedure provides further nontrivial evidence that the duality and double-copy properties hold at loop level. The new form of the four-loop four-point supergravity amplitude makes manifest the same ultraviolet power counting as the corresponding N = 4 super-Yang-Mills amplitude. We determine the amplitude's ultraviolet pole in the critical dimension of D = 11/2, the same dimension as for N = 4 super-Yang-Mills theory. Strikingly, exactly the same combination of vacuum integrals (after simplification) describes the ultraviolet divergence of N = 8 supergravity as the subleading-in-1/N{sub c}{sup 2} single-trace divergence in N = 4 super-Yang-Mills theory.",Erratum,pro46
pap1953,eff3e2a802a63b15ce57498611165eccca0ddbe3,jou22,Proceedings of the National Academy of Sciences of the United States of America,The average distances in random graphs with given expected degrees,"Random graph theory is used to examine the “small-world phenomenon”; any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees the average distance is almost surely of order log n/log d̃, where d̃ is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/kβ for some fixed exponent β. For the case of β > 3, we prove that the average distance of the power law graphs is almost surely of order log n/log d̃. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 < β < 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, which we call the core, having nc/log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.",Conference paper,vol22
pap1954,2d03b2d4cbd2c483a46c0c39b5fecdf407319eec,con78,Neural Information Processing Systems,Stochastic blockmodel approximation of a graphon: Theory and consistent estimation,"Non-parametric approaches for analyzing network data based on exchangeable graph models (ExGM) have recently gained interest. The key object that defines an ExGM is often referred to as a graphon. This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. In this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.",Conference paper,pro78
pap1955,4dc403313b0fa80139fe6ac21802f0e1d16b772e,con16,International Conference on Data Science and Advanced Analytics,Spectral sparsification of graphs: theory and algorithms,"Graph sparsification is the approximation of an arbitrary graph by a sparse graph.
 We explain what it means for one graph to be a spectral approximation of another and review the development of algorithms for spectral sparsification. In addition to being an interesting concept, spectral sparsification has been an important tool in the design of nearly linear-time algorithms for solving systems of linear equations in symmetric, diagonally dominant matrices. The fast solution of these linear systems has already led to breakthrough results in combinatorial optimization, including a faster algorithm for finding approximate maximum flows and minimum cuts in an undirected network.",Erratum,pro16
pap1956,3d0e01922b2bf5ef53b8e7fc0885ab1f80a1d945,con37,International Symposium on Search Based Software Engineering,Topological Indices and Related Descriptors in QSAR and QSPR,"There are several different broad approaches to making correlations between chemical structure and some desired property or bioactivity (either of which is here spoken of simply as the “activity”). The relevance of such approaches to quantitative-structure -activity-relationships (QSAR) has now achieved widespread use, perhaps most especially in evaluating bioactivities (e.g., for drug development). One general approach to QSAR seeks to correlate a desired activity with another reference property (e.g., the octanol -water partition coefficient) which is more easily measured; a second general approach seeks to correlate a desired activity to quantum-chemically computed descriptors; and a third general approach seeks to correlate a desired activity with various “topological indices” (which in the mathematical graph-theory literature are usually referred to as “graph invariants”). Especially the first two approaches seem often to have been imagined to associate to a surmised mechanism giving rise to the desired activity, though for bioactivities mechanistic details are in practice often wanting. The present book is dedicated to the third sometimes somewhat controversial but now increasingly successful topological-index approach sindeed the book is focused not only on QSAR but also on quantitative structureproperty relations (QSPR) for other properties. The book consists of 17 chapters by several different leading practitioners in the field. There is a rational plan, with evident coordination between chapters, sometimes engendered by the participation of one of the editors. These chapters might be somewhat approximately divided into four categories: the first consisting of two or three introductory chapters; the second category consisting of six or so chapters presenting a diversity of topological indices; the third consisting of five or so chapters using particular chemical philosophies for the selection of the topological indices to be used; and the fourth consisting of three chapters attending to a few further computational problems and related strategies. The first introductory category includes chapters by Devillers, by Balaban and O. Ivanciuc, and by O. Ivanciuc and Balaban (though the last of these chapters could be included in the next category). Devillers gives some general history of QSAR relating to the other broad approaches not detailed in the present book and makes some related philosophical remarks. Balaban and Ivanciuc present a history focusing on the introductions of various topological indices. The third of these chapters presents a general review of chemical graph-theoretic rudiments, with nice illustrative examples. Multilinear regression analysis is a presumed prerequisite throughout the present book (much as it is also similarly utilized for other broad QSAR approaches outside the scope of the present book). The second set of chapters presenting a great variety of graph invariants includes chapters by O. Ivanciuc, T. Ivanciuc, and Balaban, by O. and T. Ivanciuc, by S. Nikolic ́, N. Trinajstić, and Z. Mihalić, by L. H. Hall and L. B. Kier, and by D. Bonchev (though the last chapter could be placed in the next category instead). Here the graph invariants may often be viewed as obtained in some manner from various graphtheoretic matrices, such as the adjacency, Laplacian, shortest-path distance, Szeged, path, or detour matrices as well as matrices derived therefrom by inverting the elements or “complementing” them or raising them to a power. Particularly in the first chapter of the present set of chapters, attention is directed to the incorporation of heteroatom and bond-weighting aspects of molecular graphs into the design of the topological indices, though such considerations appear in passing in other chapters of this set too, and in the subsequent categories of chapters such considerations are generally explicitly developed or already presumed. The third set of more tightly application-focused chapters are by E. Estrada, by Kier and Hall, by Hall and Kier, by S. C. Basak, by Devillers, and by J. E. Dubois, J. P. Doucet, A. Panaye, and B. T. Fan (though here the first two of these chapters could also be placed in the previous category). Often the topological indices in a chapter here become much restricted in comparison with the possibilities enunciated in the previous set of chapters, but the oft-major point then is that the topological indices are rationally selected within the chemical philosophical approach of the authors. For instance, Kier and Hall focus on their set of “kappa” indices for encoding global shape and flexibility information and their “electrotopological” indices for encoding mean local structural information (including electronegativity characteristics for the various atoms). And Dubois et al . focus on their ordering of substructural features to describe activities which are viewable to be determined by a local region as perturbed by the surrounding environment within the molecule. Basak also describes a method of average neighborhood analysis, using information-theory-related graph invariants. The fourth and final category of chapters concerned with miscellaneous additional problems and related computational strategies are by Basak, B. D. Gute, and G. D. Grunwald, by O. Ivanciuc, and by O. Ivanciuc and Devillers. The chapter by Basak incorporates a variety of extra-graph-theoretic (quantum-chemical or property) information in fitting for an activity. (There are a couple of other earlier chapters which also mention in passing the idea that various pieces of geometric information can be incorporated in indices otherwise resembling standard purely graph-theoretic topological indices.) The chapter by Ivanciuc concerns the use of neural networks to aid in identifying (especially nonlinear) correlations between activity and topological indices. The final chapter is an overview mentioning several available software packages. Overall this∼800-page book provides a reasonably comprehensive and fair presentation of the current field of use of topological indices for QSAR and QSPR. Throughout the book (in virtually every chapter) a variety of illustrative examples are developed, and resultant fits are noted. The book is certainly of value for anyone interested in QSAR and QSPR, regardless of whether the researcher is a practitioner of the topological-index approach or of one of the other oft-used approaches to predict activities. D. J. Klein Texas A&M Uni Versity/Gal Veston",Erratum,pro37
pap1957,86ee21d690eec2a73806c2086949e944f8b46f7c,jou333,Optics Express,Automatic segmentation of seven retinal layers in SDOCT images congruent with expert manual segmentation,"Segmentation of anatomical and pathological structures in ophthalmic images is crucial for the diagnosis and study of ocular diseases. However, manual segmentation is often a time-consuming and subjective process. This paper presents an automatic approach for segmenting retinal layers in Spectral Domain Optical Coherence Tomography images using graph theory and dynamic programming. Results show that this method accurately segments eight retinal layer boundaries in normal adult eyes more closely to an expert grader as compared to a second expert grader.",Conference paper,vol333
pap1958,eac8fb1c9883da002a8a9fb1d514bde116219dc1,con7,International Symposium on Intelligent Data Analysis,Resistance distance,,Erratum,pro7
pap1959,4fcfc3a3263d3d8487e887165ae4200cdac269c5,con11,European Conference on Modelling and Simulation,Graph limits and exchangeable random graphs,"We develop a clear connection between deFinetti's theorem for exchangeable arrays (work of Aldous{Hoover{Kallenberg) and the emerging area of graph limits (work of Lov asz and many coauthors). Along the way, we translate the graph theory into more classical prob- ability.",Erratum,pro11
pap1960,cc127ca14e04bb68cefe4848c4c0b8c5219309fd,con96,Interspeech,The theory of the monetary circuit,"The present paper reviews the pre-history, the process of formation and the possible directions of future development of the theory of monetary circuit. The author reveals the main theoretical constructions of Graziany and the other leading representatives of the circuitist school. The principles of derivation of transaction and balance sheet matrices reflecting the main ideas of the theory are discussed. The dynamic variants of the theory as well as the connection between the circuitist approach and the input-output model are subject to examination. The paper studies the possibility the circuitist approach to be further broadened on the basis of the mathematical graphs theory. The author emphasizes that the theory of monetary circuit denies the neoclassical dichotomy and rejects the postulate of the neutrality of money. The opportunity is also offered to upgrade the monetary circuit theory by using the mathematical graph theory. The paper includes also a critical evaluation of the presented theory.",Erratum,pro96
pap1961,be4bebe5282233f1fa94a5d8a6fc6e452310a27b,con103,IEEE International Conference on Multimedia and Expo,Topological index based on the ratios of geometrical and arithmetical means of end-vertex degrees of edges,,Erratum,pro103
pap1962,c45b789b42e9a85d3193f43e3f0ddffb7d6aa423,con5,Technical Symposium on Computer Science Education,Handbook of Combinatorics,"Part 1 Structures: graphs - basic graph theory - paths and circuits, J.A. Bondy, connectivity and network flows, A. Frank, matchings and extensions, W.R. Pulleyblank, colouring, stable sets and perfect graphs, B. Toft, embeddings and minors, C. Thomassen, random graphs, M. Karonski finite sets and relations - hypergraphs, P. Duchet, partially ordered sets, W.T. Trotter matroids - matroids - fundamental concepts, D.J.A. Welsh, matroid minors, P.D. Seymour, matroid optimization and algorithms, R.E. Bixby and W.H. Cunningham symmetric structures - permutation groups, P.J. Cameron, finite geometries, P.J. Cameron, block designs, A.E. Brouwer, association schemes, A.E. Brouwer and W. Haemers, codes, J.H. van Lint combinatorial structures in geometry and number theory - extremal problems in combinatorial geometry, P. Erdos and G. Purdy, convex polytopes and related complexes, V. Klee and P. Kleinschmidt, point lattices, J.C. Lagarias, combinatorial number theory, C. Pomerance and A. Sarkozy. Part 2 Aspects: algebraic enumeration, I.M. Gessel and R.P. Stanley asymptotic enumeration methods, A.M. Odlyzko extremal graph theory, B. Bollobas extremal set systems, P. Frankl Ramsey theory, J. Nesetril discrepancy theory, J. Beck and V.T. Sos automorphism groups, isomorphism, reconstruction, L. Babai optimization, M. Grotschel and L. Lovasz computational complexity, D.B. Shmoys and E. Tardos. Part 3 Methods: polyhedral combinatorics, A. Schrijver tools from linear algebra, C.D. Godsil tools from higher algebra, N. Alon probabilistic methods, J. Spencer topological methods, A. Bjorner. Part 4 Applications: combinatorics in operations research, A. Kolen and J.K. Lenstra combinatorics in electrical engineering and statics, A. Recski combinatorics in statistical mechanics, C.D. Godsil et al combinatorics in chemistry, D.H. Rouvray applications of combinatorics to molecular biology, M.S. Waterman combinatorics in computer science, L. Lovasz et al combinatorics in pure mathematics, L. Lovasz et al. Part 5 Horizons: infinite combinatorics, A. Hajnal combinatorial games, R.K. Guy the history of combinatorics, N.L. Biggs et al.",Erratum,pro5
pap1963,77c4a58c801f233400e71ebd2591df62345f3616,con81,International Conference on Learning Representations,A New Theory of Deadlock-Free Adaptive Routing in Wormhole Networks,"The theoretical background for the design of deadlock-free adaptive routing algorithms for wormhole networks is developed. The author proposes some basic definitions and two theorems. These create the conditions to verify that an adaptive algorithm is deadlock-free, even when there are cycles in the channel dependency graph. Two design methodologies are also proposed. The first supplies algorithms with a high degree of freedom, without increasing the number of physical channels. The second methodology is intended for the design of fault-tolerant algorithms. Some examples are given to show the application of the methodologies. Simulations show the performance improvement that can be achieved by designing the routing algorithms with the new theory. >",Erratum,pro81
pap1964,4e892deef85c4cb62716776657d66dc417574bcc,jou334,SIAM Journal on Scientific Computing,An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations,Efficient use of a distributed memory parallel computer requires that the computational load be balanced across processors in a way that minimizes interprocessor communication. A new domain mapping algorithm is presented that extends recent work in which ideas from spectral graph theory have been applied to this problem. The generalization of spectral graph bisection involves a novel use of multiple eigenvectors to allow for division of a computation into four or eight parts at each stage of a recursive decomposition. The resulting method is suitable for scientific computations like irregular finite elements or differences performed on hypercube or mesh architecture machines. Experimental results confirm that the new method provides better decompositions arrived at more economically and robustly than with previous spectral methods. This algorithm allows for arbitrary nonnegative weights on both vertices and edges to model inhomogeneous computation and communication. A new spectral lower bound for graph bi...,Article,vol334
pap1965,8dd09cfe9d7b2d4a13e02693112b1f8afa37f222,jou335,Internet Mathematics,"Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications","There is a large, popular, and growing literature on ""scale-free"" networks with the Internet along with metabolic networks representing perhaps the canonical examples. While this has in many ways reinvigorated graph theory, there is unfortunately no consistent, precise definition of scale-free graphs and few rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and that the most celebrated claims regarding the Internet and biology are verifiably false. In this paper, we introduce a structural metric that allows us to differentiate between all simple, connected graphs having an identical degree sequence, which is of particular interest when that sequence satisfies a power law relationship. We demonstrate that the proposed structural metric yields considerable insight into the claimed properties of SF graphs and provides one possible measure of the extent to which a graph is scale-free. This structural view can be related to previously studied graph properties such as the various notions of self-similarity, likelihood, betweenness and assortativity. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the current literature, and offers a rigorous and quantitative alternative, while suggesting the potential for a rich and interesting theory. This paper is aimed at readers familiar with the basics of Internet technology and comfortable with a theorem-proof style of exposition, but who may be unfamiliar with the existing literature on scale-free networks.",Conference paper,vol335
pap1966,d89cc6a8911156f671ee60ddbf6af20ff33cd146,con78,Neural Information Processing Systems,Shock Graphs and Shape Matching,,Erratum,pro78
pap1967,8fc012941dccba5017bfd73dfd40e414001b74c4,con43,IEEE International Conference on Software Maintenance and Evolution,Complex Graphs and Networks,Graph theory in the information age Old and new concentration inequalities A generative model--the preferential attachment scheme Duplication models for biological networks Random graphs with given expected degrees The rise of the giant component Average distance and the diameter Eigenvalues of the adjacency matrix of $G(\mathbf{w})$ The semi-circle law for $G(\mathbf{w})$ Coupling on-line and off-line analyses of random graphs The configuration model for power law graphs The small world phenomenon in hybrid graphs Bibliography Index.,Erratum,pro43
pap1968,c81698f8a3014854f44744152d377421041da70f,jou304,Journal of Graph Theory,The rainbow connection of a graph is (at most) reciprocal to its minimum degree,"An edge‐colored graph Gis rainbow edge‐connected if any two vertices are connected by a path whose edges have distinct colors. The rainbow connection of a connected graph G, denoted by rc(G), is the smallest number of colors that are needed in order to make Grainbow edge‐connected. We prove that if Ghas nvertices and minimum degree δ then rc(G)<20n/δ. This solves open problems from Y. Caro, A. Lev, Y. Roditty, Z. Tuza, and R. Yuster (Electron J Combin 15 (2008), #R57) and S. Chakrborty, E. Fischer, A. Matsliah, and R. Yuster (Hardness and algorithms for rainbow connectivity, Freiburg (2009), pp. 243–254). A vertex‐colored graph Gis rainbow vertex‐connected if any two vertices are connected by a path whose internal vertices have distinct colors. The rainbow vertex‐connection of a connected graph G, denoted by rvc(G), is the smallest number of colors that are needed in order to make Grainbow vertex‐connected. One cannot upper‐bound one of these parameters in terms of the other. Nevertheless, we prove that if Ghas nvertices and minimum degree δ then rvc(G)<11n/δ. We note that the proof in this case is different from the proof for the edge‐colored case, and we cannot deduce one from the other. © 2009 Wiley Periodicals, Inc. J Graph Theory 63: 185–191, 2010",Article,vol304
pap1969,49ab911541401d4bb031870d0691379da63d6d28,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Algebraic Approaches to Graph Transformation - Part I: Basic Concepts and Double Pushout Approach,"The algebraic approaches to graph transformation are based on the concept of gluing of graphs, modelled by pushouts in suitable categories of graphs and graph morphisms. This allows one not only to give an explicit algebraic or set theoretical description of the constructions, but also to use concepts and results from category theory in order to build up a rich theory and to give elegant proofs even in complex situations. In this chapter we start with an overwiev of the basic notions common to the two algebraic approaches, the ""double-pushout (DPO) approach"" and the ""single-pushout (SPO) approach""; next we present the classical theory and some recent development of the double-pushout approach. The next chapter is devoted instead to the single-pushout approach, and it is closed by a comparison between the two approaches. -- This document will appear as a chapter of the ""The Handbook of Graph Grammars. Volume I: Foundations"", G. Rozenberg (Ed.), World Scientific.",Erratum,pro21
pap1970,f7b91f04795c4fc1449587d9c900a3ec6d39d79a,con92,Human Language Technology - The Baltic Perspectiv,A graph-theoretic approach to the method of global Lyapunov functions,"A class of global Lyapunov functions is revisited and used to resolve a long-standing open problem on the uniqueness and global stability of the endemic equilibrium of a class of multi-group models in mathematical epidemiology. We show how the group structure of the models, as manifested in the derivatives of the Lyapunov function, can be completely described using graph theory.",Erratum,pro92
pap1971,a83e9a3dce59292d7dbdab2a8dd20c6b73db3005,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,"The image foresting transform: theory, algorithms, and applications","The image foresting transform (IFT) is a graph-based approach to the design of image processing operators based on connectivity. It naturally leads to correct and efficient implementations and to a better understanding of how different operators relate to each other. We give here a precise definition of the IFT, and a procedure to compute it-a generalization of Dijkstra's algorithm-with a proof of correctness. We also discuss implementation issues and illustrate the use of the IFT in a few applications.",Article,vol290
pap1972,1e6f2b06416eb3e2df0c73c90e1bb3f81627a32a,con89,Conference on Uncertainty in Artificial Intelligence,Graphical Models for Game Theory,We introduce a compact graph-theoretic representation for multi-party game theory. Our main result is a provably correct and efficient algorithm for computing approximate Nash equilibria in one-stage games represented by trees or sparse graphs.,Conference paper,pro89
pap1973,7d41e80b97db14ea86d6c41e5dc090fd7e8da938,jou322,IEEE Transactions on Automatic Control,Consensus Conditions of Multi-Agent Systems With Time-Varying Topologies and Stochastic Communication Noises,"This paper investigates the average-consensus problem of first-order discrete-time multi-agent networks in uncertain communication environments. Each agent can only use its own and neighbors' information to design its control input. To attenuate the communication noises, a distributed stochastic approximation type protocol is used. By using probability limit theory and algebraic graph theory, consensus conditions for this kind of protocols are obtained: (A) For the case of fixed topologies, a necessary and sufficient condition for mean square average-consensus is given, which is also sufficient for almost sure consensus. (B) For the case of time-varying topologies, sufficient conditions for mean square average-consensus and almost sure consensus are given, respectively. Especially, if the network switches between jointly-containing-spanning-tree, instantaneously balanced graphs, then the designed protocol can guarantee that each individual state converges, both almost surely and in mean square, to a common random variable, whose expectation is right the average of the initial states of the whole system, and whose variance describes the static maximum mean square error between each individual state and the average of the initial states of the whole system.",Letter,vol322
pap1974,0cd7469b7fc4ad90a0552e7124f60f1d216f467f,jou336,Journal of Computational Neuroscience,"Two’s company, three (or more) is a simplex",,Conference paper,vol336
pap1975,c2d7f7428a81a18fb5269dd548755fcb1d6998d3,con81,International Conference on Learning Representations,"Mean Curvature, Threshold Dynamics, and Phase Field Theory on Finite Graphs",,Erratum,pro81
pap1976,a0095ea6b13b95073088f2afb6e33e4d7761b04e,con34,International Conference on Agile Software Development,Entanglement in Graph States and its Applications,"Graph states form a rich class of entangled states that exhibit important aspects of multi-partite entanglement. At the same time, they can be described by a number of parameters that grows only moderately with the system size. They have a variety of applications in quantum information theory, most prominently as algorithmic resources in the context of the one-way quantum computer, but also in other fields such as quantum error correction and multi-partite quantum communication, as well as in the study of foundational issues such as non-locality and decoherence. In this review, we give a tutorial introduction into the theory of graph states. We introduce various equivalent ways how to define graph states, and discuss the basic notions and properties of these states. The focus of this review is on their entanglement properties. These include aspects of non-locality, bi-partite and multi-partite entanglement and its classification in terms of the Schmidt measure, the distillability properties of mixed entangled states close to a pure graph state, as well as the robustness of their entanglement under decoherence. We review some of the known applications of graph states, as well as proposals for their experimental implementation.",Erratum,pro34
pap1977,8d03e9bccb5efb02fee168ab968c6cfa8884905c,con99,North American Chapter of the Association for Computational Linguistics,Combinatorial Algebraic Topology,,Erratum,pro99
pap1978,4b2b0821d9d6c9535f4f1aaf80b1e6be79a3ca3e,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Networks,"The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in recent years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyse network data on an unprecendented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social science. This book brings together the most important breakthroughts in each of these fields and presents them in a unified fashion, highlighting the strong interconnections between work in different areas. Topics covered include the measurement of networks; methods for analysing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms, including spectral algorithms and community detection; mathematical models of networks such as random graph models and generative models; and models of processes taking place on networks.",Erratum,pro42
pap1979,370b784d04cb952cfac2b4d179ddfd9dfb997288,con5,Technical Symposium on Computer Science Education,Horizontal visibility graphs: exact results for random time series.,"The visibility algorithm has been recently introduced as a mapping between time series and complex networks. This procedure allows us to apply methods of complex network theory for characterizing time series. In this work we present the horizontal visibility algorithm, a geometrically simpler and analytically solvable version of our former algorithm, focusing on the mapping of random series (series of independent identically distributed random variables). After presenting some properties of the algorithm, we present exact results on the topological properties of graphs associated with random series, namely, the degree distribution, the clustering coefficient, and the mean path length. We show that the horizontal visibility algorithm stands as a simple method to discriminate randomness in time series since any random series maps to a graph with an exponential degree distribution of the shape P(k)=(1/3)(2/3)(k-2), independent of the probability distribution from which the series was generated. Accordingly, visibility graphs with other P(k) are related to nonrandom series. Numerical simulations confirm the accuracy of the theorems for finite series. In a second part, we show that the method is able to distinguish chaotic series from independent and identically distributed (i.i.d.) theory, studying the following situations: (i) noise-free low-dimensional chaotic series, (ii) low-dimensional noisy chaotic series, even in the presence of large amounts of noise, and (iii) high-dimensional chaotic series (coupled map lattice), without needs for additional techniques such as surrogate data or noise reduction methods. Finally, heuristic arguments are given to explain the topological properties of chaotic series, and several sequences that are conjectured to be random are analyzed.",Erratum,pro5
pap1980,f7dccb8cb2e795b9a6d882523b316aa2930b72ad,con10,Americas Conference on Information Systems,On Communicating Finite-State Machines,"A model of commumcations protocols based on finite-state machines is investigated. The problem addressed is how to ensure certain generally desirable properties, which make protocols ""wellformed,"" that is, specify a response to those and only those events that can actually occur. It is determined to what extent the problem is solvable, and one approach to solving it ts described. Categories and SubJect Descriptors' C 2 2 [Computer-Conununication Networks]: Network Protocols-protocol verification; F 1 1 [Computation by Abstract Devices] Models of Computation--automata; G.2.2 [Discrete Mathematics] Graph Theory--graph algoruhms; trees General Terms: Reliability, Verification Additional",Erratum,pro10
pap1981,dce4848f314d1bc4eae80ea5efd1390654a4c341,con53,Workshop on Web 2.0 for Software Engineering,Mathematics of networks,"An introduction to the mathematical tools used in the study of networks. Topics discussed include: the adjacency matrix; weighted, directed, acyclic, and bipartite networks; multilayer and dynamic networks; trees; planar networks. Some basic properties of networks are then discussed, including degrees, density and sparsity, paths on networks, component structure, and connectivity and cut sets. The final part of the chapter focuses on the graph Laplacian and its applications to network visualization, graph partitioning, the theory of random walks, and other problems.",Erratum,pro53
pap1982,3742c17f24437a310b05888de2618212fe82cedf,con88,European Conference on Computer Vision,Ramanujan Graphs,"In the last two decades, the theory of Ramanujan graphs has gained prominence primarily for two reasons. First, from a practical viewpoint, these graphs resolve an extremal problem in communication network theory (see for example [2]). Second, from a more aesthetic viewpoint, they fuse diverse branches of pure mathematics, namely, number theory, representation theory and algebraic geometry. The purpose of this survey is to unify some of the recent developments and expose certain open problems in the area. This survey is by no means an exhaustive one and demonstrates a highly number-theoretic bias. For more comprehensive surveys, we refer the reader to [27], [9] or [13]. For a more up-to-date survey highlighting the connection between graph theory and automorphic representations, we refer the reader to Winnie Li’s recent survey article [11]. A is a triple consisting of avertex (X), an and a map that associates to each edge two vertices (not necessarily distinct) called itsendpoints. A is an edge whose endpoints are equal. Multiple edges are edges having the same pair of endpoints. A is one having no loops or multiple edges. If a graph has loops or multiple edges, we will call it a multigraph. When two verticesu andv are endpoints of an edge, we say they are and (sometimes) write to indicate this. To any graph, we may associate the which is ann matrix (wheren |) with rows and columns indexed by the elements of the vertex set and the y)-th entry is the number of edges connecting and y. Since our graphs are undirected, the matrix is symmetric. Consequently, all of its eigenvalues are real. The convention regarding terminology is not clear in the literature. Most use the term ‘graph’ to mean a simple graph as we have defined it above. Thus, the",Erratum,pro88
pap1983,0dbffa44f08aec11d35cb404beac6c0f748498b1,jou307,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,A Contribution to the Theory of Chromatic Polynomials,"Summary Two polynomials θ(G, n) and ϕ(G, n) connected with the colourings of a graph G or of associated maps are discussed. A result believed to be new is proved for the lesser-known polynomial ϕ(G, n). Attention is called to some unsolved problems concerning ϕ(G, n) which are natural generalizations of the Four Colour Problem from planar graphs to general graphs. A polynomial χ(G, x, y) in two variables x and y, which can be regarded as generalizing both θ(G, n) and ϕ(G, n) is studied. For a connected graph χ(G, x, y) is defined in terms of the “spanning” trees of G (which include every vertex) and in terms of a fixed enumeration of the edges.",Letter,vol307
pap1984,c8cb556b5cb996e9bab3745aa889c4b4608faba9,jou337,SIAM Journal of Control and Optimization,"Reaching a Consensus in a Dynamically Changing Environment: Convergence Rates, Measurement Delays, and Asynchronous Events","This paper uses recently established properties of compositions of directed graphs together with results from the theory of nonhomogeneous Markov chains to derive worst case convergence rates for the headings of a group of mobile autonomous agents which arise in connection with the widely studied Vicsek consensus problem. The paper also uses graph-theoretic constructions to solve modified versions of the Vicsek problem in which there are measurement delays, asynchronous events, or a group leader. In all three cases the conditions under which consensus is achieved prove to be almost the same as the conditions under which consensus is achieved in the synchronous, delay-free, leaderless case.",Article,vol337
pap1985,9f5b65929d14447a77063ceb854660b5f9f8ad05,con46,Software Product Lines Conference,Graph-based Knowledge Representation - Computational Foundations of Conceptual Graphs,,Erratum,pro46
pap1986,3bc4736f9b8512043ed47357a81f26b93a1204b6,con100,International Conference on Automatic Face and Gesture Recognition,Semi-supervised learning with graphs,"In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice. 
We present a series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances. They address the following questions: How to use unlabeled data? (label propagation); What is the probabilistic interpretation? (Gaussian fields and harmonic functions); What if we can choose labeled data? (active learning); How to construct good graphs? (hyperparameter learning); How to work with kernel machines like SVM? (graph kernels); How to handle complex data like sequences? (kernel conditional random fields); How to handle scalability and induction? (harmonic mixtures). An extensive literature review is included at the end.",Erratum,pro100
pap1987,9496a84e79463f1004e7880669e7661d5203ed4a,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Koszul duality for Operads,"(0.1) The purpose of this paper is to relate two seemingly disparate developments. One is the theory of graph cohomology of Kontsevich [Kon 2 3] which arose out of earlier works of Penner [Pe] and Kontsevich [Kon 1] on the cell decomposition and intersection theory on the moduli spaces of curves. The other is the theory of Koszul duality for quadratic associative algebras which was introduced by Priddy [Pr] and has found many applications in homological algebra, algebraic geometry and representation theory (see e.g., [Be] [BGG] [BGS] [Ka 1] [Man]). The unifying concept here is that of an operad. This paper can be divided into two parts consisting of chapters 1, 3 and 2, 4, respectively. The purpose of the first part is to establish a relationship between operads, moduli spaces of stable curves and graph complexes. To each operad we associate a collection of sheaves on moduli spaces. We introduce, in a natural way, the cobar complex of an operad and show that it is nothing but a (special case of the) graph complex, and that both constructions can be interpreted as the Verdier duality functor on sheaves. In the second part we introduce a class of operads, called quadratic, and introduce a distinguished subclass of Koszul operads. The main reason for introducing Koszul operads (and in fact for writing this paper) is that most of the operads ”arising from nature” are Koszul, cf. (0.8) below. We define a natural duality on quadratic operads (which is",Erratum,pro73
pap1988,60fca57bca813e06d2bdf7acb1b970bfce3e858d,con53,Workshop on Web 2.0 for Software Engineering,Strategic Interaction and Networks,"This paper brings a general network analysis to a wide class of economic games. A network, or interaction matrix, tells who directly interacts with whom. A major challenge is determining how network structure shapes overall outcomes. We have a striking result. Equilibrium conditions depend on a single number: the lowest eigenvalue of a network matrix. Combining tools from potential games, optimization, and spectral graph theory, we study games with linear best replies and characterize the Nash and stable equilibria for any graph and for any impact of players’ actions. When the graph is sufficiently absorptive (as measured by this eigenvalue), there is a unique equilibrium. When it is less absorptive, stable equilibria always involve extreme play where some agents take no actions at all. This paper is the first to show the importance of this measure to social and economic outcomes, and we relate it to different network link patterns.",Erratum,pro53
pap1989,ba5f513fc3be3432018a3f93b12e17a3f1580324,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"Protein-to-protein interactions: Technologies, databases, and algorithms","Studying proteins and their structures has an important role for understanding protein functionalities. Recently, due to important results obtained with proteomics, a great interest has been given to interactomics, that is, the study of protein-to-protein interactions, called PPI, or more generally, interactions among macromolecules, particularly within cells. Interactomics means studying, modeling, storing, and retrieving protein-to-protein interactions as well as algorithms for manipulating, simulating, and predicting interactions. PPI data can be obtained from biological experiments studying interactions. Modeling and storing PPIs can be realized by using graph theory and graph data management, thus graph databases can be queried for further experiments. PPI graphs can be used as input for data-mining algorithms, where raw data are binary interactions forming interaction graphs, and analysis algorithms retrieve biological interactions among proteins (i.e., PPI biological meanings). For instance, predicting the interactions between two or more proteins can be obtained by mining interaction networks stored in databases. In this article we survey modeling, storing, analyzing, and manipulating PPI data. After describing the main PPI models, mostly based on graphs, the article reviews PPI data representation and storage, as well as PPI databases. Algorithms and software tools for analyzing and managing PPI networks are discussed in depth. The article concludes by discussing the main challenges and research directions in PPI networks.",Erratum,pro98
pap1990,bc3c2d97e967e18e1eafd9f2b1b887bf79c9d545,con90,Computer Vision and Pattern Recognition,Graph embedding: a general framework for dimensionality reduction,"In the last decades, a large family of algorithms - supervised or unsupervised; stemming from statistic or geometry theory - have been proposed to provide different solutions to the problem of dimensionality reduction. In this paper, beyond the different motivations of these algorithms, we propose a general framework, graph embedding along with its linearization and kernelization, which in theory reveals the underlying objective shared by most previous algorithms. It presents a unified perspective to understand these algorithms; that is, each algorithm can be considered as the direct graph embedding or its linear/kernel extension of some specific graph characterizing certain statistic or geometry property of a data set. Furthermore, this framework is a general platform to develop new algorithm for dimensionality reduction. To this end, we propose a new supervised algorithm, Marginal Fisher Analysis (MFA), for dimensionality reduction by designing two graphs that characterize the intra-class compactness and inter-class separability, respectively. MFA measures the intra-class compactness with the distance between each data point and its neighboring points of the same class, and measures the inter-class separability with the class margins; thus it overcomes the limitations of traditional Linear Discriminant Analysis algorithm in terms of data distribution assumptions and available projection directions. The toy problem on artificial data and the real face recognition experiments both show the superiority of our proposed MFA in comparison to LDA.",Letter,pro90
pap1991,1467a3ff88a9b9fbd54e8c8afa0cb1d62bcf6a22,con72,Bioinformatics and Computational Biology,Applications of Combinatorial Matrix Theory to Laplacian Matrices of Graphs,"Matrix Theory Preliminaries Vector Norms, Matrix Norms, and the Spectral Radius of a Matrix Location of Eigenvalues Perron-Frobenius Theory M-Matrices Doubly Stochastic Matrices Generalized Inverses Graph Theory Preliminaries Introduction to Graphs Operations of Graphs and Special Classes of Graphs Trees Connectivity of Graphs Degree Sequences and Maximal Graphs Planar Graphs and Graphs of Higher Genus Introduction to Laplacian Matrices Matrix Representations of Graphs The Matrix Tree Theorem The Continuous Version of the Laplacian Graph Representations and Energy Laplacian Matrices and Networks The Spectra of Laplacian Matrices The Spectra of Laplacian Matrices Under Certain Graph Operations Upper Bounds on the Set of Laplacian Eigenvalues The Distribution of Eigenvalues Less than One and Greater than One The Grone-Merris Conjecture Maximal (Threshold) Graphs and Integer Spectra Graphs with Distinct Integer Spectra The Algebraic Connectivity Introduction to the Algebraic Connectivity of Graphs The Algebraic Connectivity as a Function of Edge Weight The Algebraic Connectivity with Regard to Distances and Diameters The Algebraic Connectivity in Terms of Edge Density and the Isoperimetric Number The Algebraic Connectivity of Planar Graphs The Algebraic Connectivity as a Function Genus k where k is greater than 1 The Fiedler Vector and Bottleneck Matrices for Trees The Characteristic Valuation of Vertices Bottleneck Matrices for Trees Excursion: Nonisomorphic Branches in Type I Trees Perturbation Results Applied to Extremizing the Algebraic Connectivity of Trees Application: Joining Two Trees by an Edge of Infinite Weight The Characteristic Elements of a Tree The Spectral Radius of Submatrices of Laplacian Matrices for Trees Bottleneck Matrices for Graphs Constructing Bottleneck Matrices for Graphs Perron Components of Graphs Minimizing the Algebraic Connectivity of Graphs with Fixed Girth Maximizing the Algebraic Connectivity of Unicyclic Graphs with Fixed Girth Application: The Algebraic Connectivity and the Number of Cut Vertices The Spectral Radius of Submatrices of Laplacian Matrices for Graphs The Group Inverse of the Laplacian Matrix Constructing the Group Inverse for a Laplacian Matrix of a Weighted Tree The Zenger Function as a Lower Bound on the Algebraic Connectivity The Case of the Zenger Equalling the Algebraic Connectivity in Trees Application: The Second Derivative of the Algebraic Connectivity as a Function of Edge Weight",Erratum,pro72
pap1992,76b76de8318457348973d8a655d1212fe51cf142,con70,International Conference on Graph Transformation,The topology of multidimensional potential energy surfaces: Theory and application to peptide structure and kinetics,"Topological characteristics of multidimensional potential energy surfaces are explored and the full conformation space is mapped on the set of local minima. This map partitions conformation space into energy-dependent or temperature-dependent “attraction basins’’ and generates a “disconnectivity’’ graph that reflects the basin connectivity and characterizes the shape of the multidimensional surface. The partitioning of the conformation space is used to express the temporal behavior of the system in terms of basin-to-basin kinetics instead of the usual state-to-state transitions. For this purpose the transition matrix of the system is expressed in terms of basin-to-basin transitions and the corresponding master equation is solved. As an example, the approach is applied to the tetrapeptide, isobutyryl-(ala)3-NH-methyl (IAN), which is the shortest peptide that can form a full helical turn. A nearly complete list of minima and barriers is available for this system from the work of Czerminiski and Elber. The m...",Erratum,pro70
pap1993,b4642da8677ca40ec888c6d450c3308473281b52,con88,European Conference on Computer Vision,Incidence matrices and interval graphs,"Abstract : According to present genetic theory, the fine structure of genes consists of linearly ordered elements. A mutant gene is obtained by alteration of some connected portion of this structure. By examining data obtained from suitable experiments, it can be determined whether or not the blemished portions of two mutant genes intersect or not, and thus intersection data for a large number of mutants can be represented as an undirected graph. If this graph is an interval graph, then the observed data is consistent with a linear model of the gene. The problem of determining when a graph is an interval graph is a special case of the following problem concerning (0, 1)-matrices: When can the rows of such a matrix be permuted so as to make the 1's in each colum appear consecutively. A complete theory is obtained for this latter problem, culminating in a decomposition theorem which leads to a rapid algorithm for deciding the question, and for constructing the desired permutation when one exists.",Erratum,pro88
pap1994,5ccd1eefd94af0a20e10a0a5e0da22aa2f481c76,con90,Computer Vision and Pattern Recognition,Every monotone graph property has a sharp threshold,"In their seminal work which initiated random graph theory Erdos and Renyi discovered that many graph properties have sharp thresholds as the number of vertices tends to infinity. We prove a conjecture of Linial that every monotone graph property has a sharp threshold. This follows from the following theorem. Let Vn(p) = {0, 1}n denote the Hamming space endowed with the probability measure μp defined by μp( 1, 2, . . . , n) = pk · (1 − p)n−k, where k = 1 + 2 + · · · + n. Let A be a monotone subset of Vn. We say that A is symmetric if there is a transitive permutation group Γ on {1, 2, . . . , n} such that A is invariant under Γ. Theorem. For every symmetric monotone A, if μp(A) > then μq(A) > 1− for q = p+ c1 log(1/2 )/ logn. (c1 is an absolute constant.)",Erratum,pro90
pap1995,6a4f4038032d5a89c39fcdceba4a9c5ba5ecae27,con91,Symposium on the Theory of Computing,Twin-width IV: ordered graphs and matrices,"We establish a list of characterizations of bounded twin-width for hereditary classes of totally ordered graphs: as classes of at most exponential growth studied in enumerative combinatorics, as monadically NIP classes studied in model theory, as classes that do not transduce the class of all graphs studied in finite model theory, and as classes for which model checking first-order logic is fixed-parameter tractable studied in algorithmic graph theory. This has several consequences. First, it allows us to show that every hereditary class of ordered graphs either has at most exponential growth, or has at least factorial growth. This settles a question first asked by Balogh, Bollobás, and Morris [Eur. J. Comb. ’06] on the growth of hereditary classes of ordered graphs, generalizing the Stanley-Wilf conjecture/Marcus-Tardos theorem. Second, it gives a fixed-parameter approximation algorithm for twin-width on ordered graphs. Third, it yields a full classification of fixed-parameter tractable first-order model checking on hereditary classes of ordered binary structures. Fourth, it provides a model-theoretic characterization of classes with bounded twin-width. Finally, it settles our small conjecture [SODA ’21] in the case of ordered graphs.",Article,pro91
pap1996,6a70edfc099614866996222893e91826b4da3677,con66,International Conference on Software Reuse,Thermodynamic perturbation theory of polymerization,"We derive several extensions of a previously given first‐order perturbation theory (TPT 1) for fluids in which chain and ring polymers can be formed, due to the presence of two singly bondable molecular attraction sites. We retain graphs which contain a single chain of attraction bonds, and evaluate second‐order perturbation theory (TPT 2) within this framework. The previous formulation with sites fixed in the molecule is generalized to allow movable sites, thus permitting calculations for flexible bead polymers. The TPT 1 result for the equation of state of an equilibrium mixture of chain lengths is in good agreement with simulations of flexible bead polymers of fixed bead number N, when the mean number ν of beads is equated to the fixed N of the simulations. TPT 2 differs from TPT 1 by a rather small term, with improved agreement. If the distribution of movable sites includes configurations such that bonding of one site blocks bonding of the other, then graph resummation must be used. Resummed TPT yield...",Erratum,pro66
pap1997,fadb7306b33dff429b0c0be4b9605c42366693ec,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing","Towards a spectral theory of graphs based on the signless Laplacian, I","A spectral graph theory is a theory in which graphs are studied by means of eigenvalues of a matrix �� which is in a prescribed way defined for any graph. This theory is called �� -theory. We outline a spectral theory of graphs based on the signless Laplacians �� and compare it with other spectral theories, in particular with those based on the adjacency matrix �� and the Laplacian �� . The �� -theory can be composed using various connections to other theories: equivalency with �� -theory and �� -theory for regular graphs, or with �� -theory for bipartite graphs, general analogies with �� -theory and analogies with �� -theory via line graphs and subdivision graphs. We present results on graph operations, inequalities for eigenvalues and reconstruction problems.",Erratum,pro87
pap1998,f276c00bac7594107c291947f560b7b48b1439d7,jou338,Mathematics of Operations Research,A Best Possible Heuristic for the k-Center Problem,"In this paper we present a 2-approximation algorithm for the k-center problem with triangle inequality. This result is “best possible” since for any δ < 2 the existence of δ-approximation algorithm would imply that P = NP. It should be noted that no δ-approximation algorithm, for any constant δ, has been reported to date. Linear programming duality theory provides interesting insight to the problem and enables us to derive, in O|E| log |E| time, a solution with value no more than twice the k-center optimal value. 
 
A by-product of the analysis is an O|E| algorithm that identifies a dominating set in G2, the square of a graph G, the size of which is no larger than the size of the minimum dominating set in the graph G. The key combinatorial object used is called a strong stable set, and we prove the NP-completeness of the corresponding decision problem.",Article,vol338
pap1999,897aa57d2811be3f2d5aca74123cf7bfa9e75344,con33,International Conference on Automated Software Engineering,Computational Invariant Theory,,Erratum,pro33
pap2000,7fc9a268aeebfa25b77a784fb47d0959523cff00,con23,International Conference on Open and Big Data,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",Erratum,pro23
pap2001,948fd800ecdd3c99488dde36b41480ca1b8acce3,con93,International Conference on Computational Logic,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",Erratum,pro93
pap2002,95cd83603a0d2b6918a8e34a5637a8f382da96f5,jou18,Scientific Data,"MIMIC-III, a freely accessible critical care database",,Conference paper,vol18
pap2003,98128fd412ebfa90201a276f2c59020ccc696a75,con68,Experimental Software Engineering Network,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",Erratum,pro68
pap2004,da692ee969d9c33986196372c3f7cb87fa6b6f8f,con17,International Conference on Statistical and Scientific Database Management,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",Erratum,pro17
pap2005,6e1e6afb314f9c5a24d744252a30aa5efc313571,con94,Vision,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",Erratum,pro94
pap2006,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,jou240,International Journal of Systematic and Evolutionary Microbiology,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",Conference paper,vol240
pap2007,0f5c63182b5d40850c741888a89e6c055a3593af,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",Erratum,pro47
pap2008,16b0744424f02e01fe2f01b3ea03e2862f1359fc,con41,Asia-Pacific Software Engineering Conference,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",Erratum,pro41
pap2009,f986968735459e789890f24b6b277b0920a9725d,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",Article,vol290
pap2010,93d5369a0be3134c6018373d5290923f3d718815,con29,ACM-SIAM Symposium on Discrete Algorithms,The Molecular Signatures Database Hallmark Gene Set Collection,,Erratum,pro29
pap2011,d2c733e34d48784a37d717fe43d9e93277a8c53e,con76,IEEE International Conference on Tools with Artificial Intelligence,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",Erratum,pro76
pap2012,02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,jou106,Nucleic Acids Research,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.",Article,vol106
pap2013,7f19972754ac0c15329666b3a6efbf569b27d8d5,con70,International Conference on Graph Transformation,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",Erratum,pro70
pap2014,3283f9c33e4b3fbd51d58a54dc236a92f5a98f80,con108,International Conference on Information Integration and Web-based Applications & Services,The PRIDE database resources in 2022: a hub for mass spectrometry-based proteomics evidences,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world's largest data repository of mass spectrometry-based proteomics data. PRIDE is one of the founding members of the global ProteomeXchange (PX) consortium and an ELIXIR core data resource. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2019. The number of submitted datasets to PRIDE Archive (the archival component of PRIDE) has reached on average around 500 datasets per month during 2021. In addition to continuous improvements in PRIDE Archive data pipelines and infrastructure, the PRIDE Spectra Archive has been developed to provide direct access to the submitted mass spectra using Universal Spectrum Identifiers. As a key point, the file format MAGE-TAB for proteomics has been developed to enable the improvement of sample metadata annotation. Additionally, the resource PRIDE Peptidome provides access to aggregated peptide/protein evidences across PRIDE Archive. Furthermore, we will describe how PRIDE has increased its efforts to reuse and disseminate high-quality proteomics data into other added-value resources such as UniProt, Ensembl and Expression Atlas.",Erratum,pro108
pap2015,9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,con35,IEEE Working Conference on Mining Software Repositories,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",Erratum,pro35
pap2016,b204970b0503a923359bff532726666f5e0e971b,con95,IEEE International Conference on Computer Vision,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",Erratum,pro95
pap2017,3aca912f21d54b3931fa1fdfac0c199c557374a4,con89,Conference on Uncertainty in Artificial Intelligence,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,Erratum,pro89
pap2018,633f318876c41fed36b3905b8af5fdc27f734615,jou339,Cell Systems,The Molecular Signatures Database (MSigDB) hallmark gene set collection.,"The Molecular Signatures Database (MSigDB) is one of the most widely used and comprehensive databases of gene sets for performing gene set enrichment analysis. Since its creation, MSigDB has grown beyond its roots in metabolic disease and cancer to include >10,000 gene sets. These better represent a wider range of biological processes and diseases, but the utility of the database is reduced by increased redundancy across, and heterogeneity within, gene sets. To address this challenge, here we use a combination of automated approaches and expert curation to develop a collection of ""hallmark"" gene sets as part of MSigDB. Each hallmark in this collection consists of a ""refined"" gene set, derived from multiple ""founder"" sets, that conveys a specific biological state or process and displays coherent expression. The hallmarks effectively summarize most of the relevant information of the original founder sets and, by reducing both variation and redundancy, provide more refined and concise inputs for gene set enrichment analysis.",Conference paper,vol339
pap2019,a98753021c6a076a5307f4dfb7fd1fcb14089910,con107,Chinese Conference on Biometric Recognition,The Pfam protein families database,"Pfam is a widely used database of protein families and domains. This article describes a set of major updates that we have implemented in the latest release (version 24.0). The most important change is that we now use HMMER3, the latest version of the popular profile hidden Markov model package. This software is ∼100 times faster than HMMER2 and is more sensitive due to the routine use of the forward algorithm. The move to HMMER3 has necessitated numerous changes to Pfam that are described in detail. Pfam release 24.0 contains 11 912 families, of which a large number have been significantly updated during the past two years. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/).",Erratum,pro107
pap2020,68c03788224000794d5491ab459be0b2a2c38677,con92,Human Language Technology - The Baltic Perspectiv,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",Letter,pro92
pap2021,d87ceda3042f781c341ac17109d1e94a717f5f60,con93,International Conference on Computational Logic,Book Reviews: WordNet: An Electronic Lexical Database,"WordNet is perhaps the most important and widely used lexical resource for natural language processing systems up to now. WordNet: An Electronic Lexical Database, edited by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive Sciences Laboratory of Princeton University, where WordNet was produced, and nine chapters contributed by scientists from elsewhere. Miller's foreword offers a fascinating account of the history of WordNet. He discusses the presuppositions of such a lexical database, how the top-level noun categories were determined, and the sources of the words in WordNet. He also writes about the evolution of WordNet from its original incarnation as a dictionary browser to a broad-coverage lexicon, and the involvement of different people during its various stages of development over a decade. It makes very interesting reading for casual and serious users of WordNet and anyone who is grateful for the existence of WordNet. The book is organized in three parts. Part I is about WordNet itself and consists of four chapters: ""Nouns in WordNet"" by George Miller, ""Modifiers in WordNet"" by Katherine Miller, ""A semantic network of English verbs"" by Christiane Fellbaum, and ""Design and implementation of the WordNet lexical database and search software"" by Randee Tengi. These chapters are essentially updated versions of four papers from Miller (1990). Compared with the earlier papers, the chapters in this book focus more on the underlying assumptions and rationales behind the design decisions. The description of the information contained in WordNet, however, is not as detailed as in Miller (1990). The main new additions in these chapters include an explanation of sense grouping in George Miller's chapter, a section about adverbs in Katherine Miller's chapter, observations about autohyponymy (one sense of a word being a hyponym of another sense of the same word) and autoantonymy (one sense of a word being an antonym of another sense of the same word) in Fellbaum's chapter, and Tengi's description of the Grinder, a program that converts the files the lexicographers work with to searchable lexical databases. The three papers in Part II are characterized as ""extensions, enhancements and",Article,pro93
pap2022,54cc1f2e86d1913521b466cef19d72ed02b6c800,jou81,BMC Bioinformatics,Argonaute—a database for gene regulation by mammalian microRNAs,,Letter,vol81
pap2023,0e5bccdedb82fbafece8ca71d64b16ff05ec9145,con61,International Conference on Predictive Models in Software Engineering,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",Erratum,pro61
pap2024,6a074a3fa856e86b2e6bc60e83d66cc488090ae9,jou340,The International Journal of Life Cycle Assessment,The ecoinvent database version 3 (part I): overview and methodology,,Letter,vol340
pap2025,57dfc18815bba1c3737dbc2e5497fd1fc595edb5,jou240,International Journal of Systematic and Evolutionary Microbiology,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.",Article,vol240
pap2026,66470cf9df2f932f80094a309abcc14bcc1b9373,con12,The Compass,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",Erratum,pro12
pap2027,bbe6e5fcc96e685db714d6aa11ffe6f49567c585,jou55,Nature Human Behaviour,A global database of COVID-19 vaccinations,,Conference paper,vol55
pap2028,e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,jou308,Acta Crystallographica Section B Structural Science,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.",Article,vol308
pap2029,c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3,con96,Interspeech,Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",Erratum,pro96
pap2030,76eb8e5688ee2951e5f04fb14956abf93a890149,con97,ACM SIGMOD Conference,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",Erratum,pro97
pap2031,7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,con81,International Conference on Learning Representations,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",Erratum,pro81
pap2032,8b3b8848a311c501e704c45c6d50430ab7068956,con94,Vision,HMDB: A large video database for human motion recognition,"With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",Conference paper,pro94
pap2033,317325439a0ce543d7629848a35adea04b6e7d12,con78,Neural Information Processing Systems,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",Erratum,pro78
pap2034,f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,con22,Grid Computing Environments,Molecular signatures database (MSigDB) 3.0,"MOTIVATION
Well-annotated gene sets representing the universe of the biological processes are critical for meaningful and insightful interpretation of large-scale genomic data. The Molecular Signatures Database (MSigDB) is one of the most widely used repositories of such sets.


RESULTS
We report the availability of a new version of the database, MSigDB 3.0, with over 6700 gene sets, a complete revision of the collection of canonical pathways and experimental signatures from publications, enhanced annotations and upgrades to the web site.


AVAILABILITY AND IMPLEMENTATION
MSigDB is freely available for non-commercial use at http://www.broadinstitute.org/msigdb.",Erratum,pro22
pap2035,716000409a3a2e2c75801b3d58b9b17b68eeaef7,con44,International Conference Knowledge Engineering and Knowledge Management,An improved method of constructing a database of monthly climate observations and associated high‐resolution grids,"A database of monthly climate observations from meteorological stations is constructed. The database includes six climate elements and extends over the global land surface. The database is checked for inhomogeneities in the station records using an automated method that refines previous methods by using incomplete and partially overlapping records and by detecting inhomogeneities with opposite signs in different seasons. The method includes the development of reference series using neighbouring stations. Information from different sources about a single station may be combined, even without an overlapping period, using a reference series. Thus, a longer station record may be obtained and fragmentation of records reduced. The reference series also enables 1961–90 normals to be calculated for a larger proportion of stations.",Erratum,pro44
pap2036,80e394ee3e1834091596e8b55c9ad9bf11456e09,jou5,Genome Biology,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",,Conference paper,vol5
pap2037,1976c9eeccc7115d18a04f1e7fb5145db6b96002,con76,IEEE International Conference on Tools with Artificial Intelligence,Freebase: a collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",Erratum,pro76
pap2038,c07eca9862e0144aa4c0c29c7978caa6eff60e2f,con92,Human Language Technology - The Baltic Perspectiv,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",Erratum,pro92
pap2039,8c1a1e761b715b23668b4f850e2bcc958fa21ad2,jou216,Analytical Chemistry,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.",Article,vol216
pap2040,298d799da82395a64a3bda38ef9d2a4646828ccb,con91,Symposium on the Theory of Computing,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",Letter,pro91
pap2041,dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2,con89,Conference on Uncertainty in Artificial Intelligence,The mnist database of handwritten digits,Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough.,Erratum,pro89
pap2042,80777d42513103bede188b2eebbdce7fb6f91390,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",Erratum,pro73
pap2043,5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,jou184,Journal of Molecular Biology,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,,Conference paper,vol184
pap2044,ceee6447b291f8052a28c9eb00ca360d6f39f9b1,con36,Central and Eastern European Software Engineering Conference in Russia,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",Erratum,pro36
pap2045,cdad2f8ca559f425ab7fa402535354a86b0a370a,con62,Australian Software Engineering Conference,CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",Erratum,pro62
pap2046,46f74231b9afeb0c290d6d550043c55045284e5f,jou316,IEEE Signal Processing Magazine,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",Conference paper,vol316
pap2047,761020759f7e9f84c3ac77f59a42862cc6a6004e,con55,Workshop on Learning from Authoritative Security Experiment Results,Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems,"cont.): Active database semantics can be supported on an existing SQL Server (we use Oracle SQL Server as the test SQL Server) by the ECA Agent between the SQL Server and multiple clients. ECA rules are completely supported through the ECA Agent without changing applications in the SQL Server. Both primitive and composite events can be detected in the ECA Agent and actions are invoked in SQL Server. All events are persistent in RDBMS. The Java Local Event Detector (Java LED) is used to notify and detect both primitive events and composite events. The ECA Agent uses Java Database Connectivity (JDBC) to connect to the SQL server. The architecture of the ECA Agent and implementation details are shown in this thesis. Alternative approaches are discussed in details, and the features and limitations are identified. Database and Expert Systems Applications This book contains the refereed proceedings of the 8th International Conference on Database and Expert Systems Applications, DEXA '97, held in Toulouse, France, September 1997. The 62 revised full papers presented in the book, together with three invited contributions, were selected from a total of 159 submissions. The papers are organized in sections on modeling, object-oriented databases, active and temporal aspects, images, integrity constraints, multimedia databases, deductive databases and knowledge-based systems, allocation concepts, data interchange, digital libraries, transaction concepts, learning issues, optimization and performance, query languages, maintenance, Page 3/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems fed rated databases, uncertainty handling and qualitative reasoning, and software engineering and reusable software. Advanced Database Systems ""Focused on the latest research on text and document management, this guide addresses the information management needs of organizations by providing the most recent findings. How the need for effective databases to house information is impacting organizations worldwide and how some organizations that possess a vast amount of data are not able to use the data in an economic and efficient manner is demonstrated. A taxonomy for object-oriented databases, metrics for controlling database complexity, and a guide to accommodating hierarchies in relational databases are provided. Also covered is how to apply Java-triggers for X-Link management and how to build signatures."" Flexible and Efficient Information Handling This book is the proceedings of a workshop held at Heriot-Watt University in Edinburgh in August 1993. The central theme of the workshop was rules in database systems, and the papers presented covered a range of different aspects of database rule systems. These aspects are reflected in the sessions of the workshop, which are the same as the sections in this proceedings: Active Databases Architectures Incorporating Temporal Rules Rules and Transactions Analysis and Debugging of Active Rules Integrating Graphs/Objects with Deduction Integrating Deductive and Active Rules Integrity Constraints Deductive Databases The incorporation of rules into database systems is an important area of research, as it is a major component in the integration of behavioural information with the structural data with which commercial databases have traditionally been associated. This integration of the behavioural aspects of an application with the data to which it applies in database systems leads to more straightforward application development and more efficient processing of data. Many novel applications seem to need database systems in which structural and behavioural information are fully integrated. Rules are only one means of expressing behavioural information, but it is clear that different types of rule can be used to capture directly different properties of an application which are cumbersome to support using conventional database architectures. In recent years there has been a surge of research activity focusing upon active database systems, and this volume opens with a collection of papers devoted specifically to this topic. Web Information Systems -WISE 2004 Active database systems enhance traditional database functionality with powerful rule-processing capabilities, providing a uniform and efficient mechanism for many database system applications. Among these applications are integrity constraints, views, authorization, statistics gathering, monitoring and alerting, knowledge-based systems, expert systems, and workflow management. This significant collection focuses on the most prominent research projects in active database systems. The project leaders for each Page 4/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems prototype system provide detailed discussion of their projects and the relevance of their results to the future of active database systems. Features: A broad overview of current active database systems and how they can be extended and improved A comprehensive introduction to the core topics of the field, including its motivation and history Coverage of active database (trigger) capabilities in commercial products Discussion of forthcoming standards Knowledge-Based Intelligent Information and Engineering Systems Knowledge Base Systems are an integration of conventional database systems with Artificial Intelligence techniques. They provide inference capabilities to the database system by encapsulating the knowledge of the application domain within the database. Knowledge is the most valuable of all corporate resources that must be captured, stored, re-used and continuously improved, in much the same way as database systems were important in the previous decade. Flexible, extensible, and yet efficient Knowledge Base Systems are needed to capture the increasing demand for knowledge-based applications which will become a significant market in the next decade. Knowledge can be expressed in many static and dynamic forms; the most prominent being domain objects, their relationships, and their rules of evolution and transformation. It is important to express and seamlessly use all types of knowledge in a single Knowledge Base System. Parallel, Object-Oriented, and Active Knowledge Base Systems presents in detail features that a Knowledge Base System should have in order to fulfill the above requirements. Parallel, Object-Oriented, and Active Knowledge Base Systems covers in detail the following topics: Integration of deductive, production, and active rules in sequential database systems. Integration and inter-operation of multiple rule types into the same Knowledge Base System. Parallel rule matching and execution, for deductive, production, and active rules, in parallel Export, Knowledge Base, and Database Systems. In-depth description of a Parallel, Object-Oriented, and Active Knowledge Base System that integrates all rule paradigms into a single database system without hindering performance. Parallel, Object-Oriented, and Active Knowledge Base Systems is intended as a graduate-level text for a course on Knowledge Base Systems and as a reference for researchers and practitioners in the areas of database systems, knowledge base systems and Artificial Intelligence. Active, Real-Time, and Temporal Database Systems The World Wide Web has become a ubiquitous global tool, used for finding infor mation, communicating ideas, carrying out distributed computation and conducting business, learning and science. The Web is highly dynamic in both the content and quantity of the information that it encompasses. In order to fully exploit its enormous potential as a global repository of information, we need to understand how its size, topology and content are evolv ing. This then allows the development of new techniques for locating and retrieving information that are better able to adapt and scale to its change and growth. The Web's users are highly diverse and can access the Web from a variety of devices and interfaces, at different places and times, and for varying purposes. We thus also need techniques for personalising the presentation and content of Page 5/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems Web based information dependi g on how it i being accessed and on the specific user's requirements. As well as being accessed by human users, the Web is also accessed by appli cations. New applications in areas such as ebusiness, sensor networks, and mobile and ubiquitous computing need to be able to detect and react quickly to events and changes in Web-based information. Traditional approaches using query-based 'pull' of information to find out if events or changes of interest have occurred may not be able to scale to the quantity and frequency of events and changes being generated, and new 'push' -based techniques are needed. Advances in Databases and Information Systems This book constitutes the strictly refereed post-workshop proceedings of the International Workshop on Logic in Databases, LID'96, held in San Miniato, Italy, in July 1996, as the final meeting of an EC-US cooperative activity. The volume presents 21 revised full papers selected from 49 submissions as well as 3 invited contributions and a summary of a panel discussion on deductive databases: challenges, opportunities and future directions. The retrospective survey on logic and databases by Jack Minker deserves a special mention: it is a 56-page overview and lists 357 references. The papers are organized in sections on uncertainty, temporal and spatial reasoning, updates, active databases, semantics, advanced applications, query evaluation, language extensions, and logic constructs and expressive power. Data Management Systems This book constitutes the ",Erratum,pro55
pap2048,6dd9508b8311852afec88bce55283551da5aa7b7,con35,IEEE Working Conference on Mining Software Repositories,The IPD-IMGT/HLA Database,"Abstract It is 24 years since the IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, was first released, providing the HLA community with a searchable repository of highly curated HLA sequences. The database now contains over 35 000 alleles of the human Major Histocompatibility Complex (MHC) named by the WHO Nomenclature Committee for Factors of the HLA System. This complex contains the most polymorphic genes in the human genome and is now considered hyperpolymorphic. The IPD-IMGT/HLA Database provides a stable and user-friendly repository for this information. Uptake of Next Generation Sequencing technology in recent years has driven an increase in the number of alleles and the length of sequences submitted. As the size of the database has grown the traditional methods of accessing and presenting this data have been challenged, in response, we have developed a suite of tools providing an enhanced user experience to our traditional web-based users while creating new programmatic access for our bioinformatics user base. This suite of tools is powered by the IPD-API, an Application Programming Interface (API), providing scalable and flexible access to the database. The IPD-API provides a stable platform for our future development allowing us to meet the future challenges of the HLA field and needs of the community.",Erratum,pro35
pap2049,11f647b95a7c9a94c346cd8dc53987105cb0f7c1,con2,International Conference on Software Engineering,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.",Erratum,pro2
pap2050,d7c78b7071ea150346320e5b43a03824263e0fa9,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",Erratum,pro49
pap2051,62f5ffb09a4c9543509c38f005b9c6eb308c6974,con36,Central and Eastern European Software Engineering Conference in Russia,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.",Erratum,pro36
pap2052,82524ddee00fa0895dfca43995a7ec8bdb16f0d5,con103,IEEE International Conference on Multimedia and Expo,Fundamentals of Database Systems,"From the Publisher: 
Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition 
Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries",Erratum,pro103
pap2053,d364903a626ad70e6ce057209d9b7e004dafd4be,con107,Chinese Conference on Biometric Recognition,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.",Erratum,pro107
pap2054,b7599c8ba88e7c93edbce57df513152e8f5693e7,jou81,BMC Bioinformatics,The COG database: an updated version includes eukaryotes,,Article,vol81
pap2055,1f53996347086be3bd3a32da0976ba2db7687988,con86,The Web Conference,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",Erratum,pro86
pap2056,0e466ea033b982519f351022304dccb64a46b93c,con108,International Conference on Information Integration and Web-based Applications & Services,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",Erratum,pro108
pap2057,bb967168ead7a14adcb0121dcf24a930d1a383b3,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The HITRAN 2008 molecular spectroscopic database,,Erratum,pro85
pap2058,4bd970a37c59c97804ff93cbb2c108e081de3a37,con105,British Machine Vision Conference,Introduction to WordNet: An On-line Lexical Database,"Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.",Erratum,pro105
pap2059,6ad9053940676fca029dabdc7937e5d854df61e0,con84,Workshop on Interdisciplinary Software Engineering Research,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",Erratum,pro84
pap2060,288b317e427c6bf4c94d455049bd1368ff2071eb,con51,Brazilian Symposium on Software Engineering,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",Erratum,pro51
pap2061,95162f20fa22a8cfe84b74aa118f18a6f04eb1ab,jou341,Mobile DNA,"Repbase Update, a database of repetitive elements in eukaryotic genomes",,Letter,vol341
pap2062,41abf43dc718e271299457bce65bccfe3feeb9d6,jou342,Applied and Environmental Microbiology,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",Letter,vol342
pap2063,072a0db716fb6f8332323f076b71554716a7271c,jou343,IEEE Engineering in Medicine and Biology Magazine,The impact of the MIT-BIH Arrhythmia Database,"The MIT-BIH Arrhythmia Database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and it has been used for that purpose as well as for basic research into cardiac dynamics at about 500 sites worldwide since 1980. It has lived a far longer life than any of its creators ever expected. Together with the American Heart Association Database, it played an interesting role in stimulating manufacturers of arrhythmia analyzers to compete on the basis of objectively measurable performance, and much of the current appreciation of the value of common databases, both for basic research and for medical device development and evaluation, can be attributed to this experience. In this article, we briefly review the history of the database, describe its contents, discuss what we have learned about database design and construction, and take a look at some of the later projects that have been stimulated by both the successes and the limitations of the MIT-BIH Arrhythmia Database.",Letter,vol343
pap2064,9667f8264745b626c6173b1310e2ff0298b09cfc,con78,Neural Information Processing Systems,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",Article,pro78
pap2065,908091b4a8757c3b2f7d9cfa2c4f616ee12c5157,con100,International Conference on Automatic Face and Gesture Recognition,SUN database: Large-scale scene recognition from abbey to zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",Erratum,pro100
pap2066,c369c9a40a36b013be3ef9c19a068f2d6578e4c3,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible in ARB,"Title: Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible with ARB Authors: DeSantis, T.Z. 1 Hugenholtz, P. 2 Larsen, N. 3 Rojas, M. 4 Brodie, E.L. 1 Keller, K. 5 Huber, T. 6 Dalevi, D. 7 Hu, P. 1 Andersen, G.L. 1 Center for Environmental Biotechnology Lawrence Berkeley National Laboratory 1 Cyclotron Road, Mail Stop 70A-3317 Berkeley, CA 94720 USA Microbial Ecology Program DOE Joint Genome Institute 2800 Mitchell Drive Bldg 400-404 Walnut Creek, CA 94598 USA Danish Genome Institute Gustav Wieds vej 10 C DK-8000 Aarhus C Denmark Department of Bioinformatics Baylor University P.O. Box 97356, 1311 S. 5th St. Waco, TX 76798-7356 USA Department of Bioengineering University of California Berkeley, CA 94720 USA Departments of Biochemistry and Mathematics The University of Queensland Brisbane Qld 4072 Australia Department of Computer Science Chalmers University of Technology",Erratum,pro73
pap2067,3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,jou344,Journal of Cheminformatics,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,,Conference paper,vol344
pap2068,5a2892f91addeea2f4600d28b23e684be32f5b2c,jou345,IEEE Transactions on Affective Computing,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",Letter,vol345
pap2069,fc1e3ed87c15d62148f03ff99677e6be0fc6f5b1,con94,Vision,The carbohydrate-active enzyme database: functions and literature,"Abstract Thirty years have elapsed since the emergence of the classification of carbohydrate-active enzymes in sequence-based families that became the CAZy database over 20 years ago, freely available for browsing and download at www.cazy.org. In the era of large scale sequencing and high-throughput Biology, it is important to examine the position of this specialist database that is deeply rooted in human curation. The three primary tasks of the CAZy curators are (i) to maintain and update the family classification of this class of enzymes, (ii) to classify sequences newly released by GenBank and the Protein Data Bank and (iii) to capture and present functional information for each family. The CAZy website is updated once a month. Here we briefly summarize the increase in novel families and the annotations conducted during the last 8 years. We present several important changes that facilitate taxonomic navigation, and allow to download the entirety of the annotations. Most importantly we highlight the considerable amount of work that accompanies the analysis and report of biochemical data from the literature.",Erratum,pro94
pap2070,092c275005ae49dc1303214f6d02d134457c7053,jou346,International Journal of Computer Vision,LabelMe: A Database and Web-Based Tool for Image Annotation,,Letter,vol346
pap2071,90bc0ca3feebe0215079cf575b90017170a0089f,con111,International Conference on Image Analysis and Processing,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",Erratum,pro111
pap2072,e8104b335a4e499f7b79b913a92d23263c82f6b6,con43,IEEE International Conference on Software Maintenance and Evolution,The HITRAN2012 molecular spectroscopic database,,Erratum,pro43
pap2073,d71af418eeb9f5a68062929bae12af74773ffcb2,con100,International Conference on Automatic Face and Gesture Recognition,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",Erratum,pro100
pap2074,cb56121bc38e0f4b44bcb5296a12038626152e96,con107,Chinese Conference on Biometric Recognition,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",Erratum,pro107
pap2075,960e7494ef4ec5964407488080f249104cd218f0,con74,IEEE International Conference on Information Reuse and Integration,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed® database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 34 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface and NCBI datasets. Additional resources that were updated in the past year include PMC, Bookshelf, Genome Data Viewer, SRA, ClinVar, dbSNP, dbVar, Pathogen Detection, BLAST, Primer-BLAST, IgBLAST, iCn3D and PubChem. All of these resources can be accessed through the NCBI home page at https://www.ncbi.nlm.nih.gov.",Erratum,pro74
pap2076,6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,con84,Workshop on Interdisciplinary Software Engineering Research,The AR face database,,Erratum,pro84
pap2077,3a2b869533620d2dfa076522321983c537b3c175,con95,IEEE International Conference on Computer Vision,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",Erratum,pro95
pap2078,a8db50edfe26a6ae33a6787e2049de5bacd18666,con9,Big Data,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",Erratum,pro9
pap2079,bc744742f1644c9cab6b9535ab0bd6f2eed320bb,con74,IEEE International Conference on Information Reuse and Integration,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",Erratum,pro74
pap2080,b307d55ba07058d6183991d2d2a81b340d558186,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies","NCBI Reference Sequence (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",Erratum,pro49
pap2081,50819fdfb666fdabb12f00e93e0d7e43ba4ba4bd,jou347,Journal of Chemical Information and Modeling,ZINC - A Free Database of Commercially Available Compounds for Virtual Screening,"A critical barrier to entry into structure-based virtual screening is the lack of a suitable, easy to access database of purchasable compounds. We have therefore prepared a library of 727,842 molecules, each with 3D structure, using catalogs of compounds from vendors (the size of this library continues to grow). The molecules have been assigned biologically relevant protonation states and are annotated with properties such as molecular weight, calculated LogP, and number of rotatable bonds. Each molecule in the library contains vendor and purchasing information and is ready for docking using a number of popular docking programs. Within certain limits, the molecules are prepared in multiple protonation states and multiple tautomeric forms. In one format, multiple conformations are available for the molecules. This database is available for free download (http://zinc.docking.org) in several common file formats including SMILES, mol2, 3D SDF, and DOCK flexibase format. A Web-based query tool incorporating a molecular drawing interface enables the database to be searched and browsed and subsets to be created. Users can process their own molecules by uploading them to a server. Our hope is that this database will bring virtual screening libraries to a wide community of structural biologists and medicinal chemists.",Letter,vol347
pap2082,61533dd9e41f20e2f5deaf22afb04c94b4071eac,jou348,Cytogenetic and Genome Research,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",Letter,vol348
pap2083,f8928221d290a9cdd84d1de52e121373bc836caa,con43,IEEE International Conference on Software Maintenance and Evolution,New tools in comparative political economy : the database of political institutions,"This article introduces a large new cross-country database, the database of political institutions. It covers 177 countries over 21 years, 1975-95. The article presents the intuition, construction, and definitions of the different variables. Among the novel variables introduced are several measures of checks and balances, tenure and stability, identification of party affiliation with government or opposition, and fragmentation of opposition and government parties in the legislature.",Erratum,pro43
pap2084,5cf0d213f3253cd46673d955209f8463db73cc51,jou349,Language Resources and Evaluation,IEMOCAP: interactive emotional dyadic motion capture database,,Article,vol349
pap2085,3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.",Erratum,pro21
pap2086,804836b8ad86ef8042e3dcbd45442a52f031ee03,con95,IEEE International Conference on Computer Vision,A Database and Evaluation Methodology for Optical Flow,,Article,pro95
pap2087,e274c1b6e17825feab52de205fd0bc4917d5be6c,jou350,Journal of Biomolecular NMR,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,,Letter,vol350
pap2088,ba90ae48b30594b57a5ca7bfd37cae150458ecfa,con91,Symposium on the Theory of Computing,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",Erratum,pro91
pap2089,58a63e11a45dfdb50d994454ead70243626d8ed1,con6,Annual Conference on Genetic and Evolutionary Computation,Encyclopedia of Database Systems,,Erratum,pro6
pap2090,e3f2391513693647e0ea87bfa86cd89e468f51d0,con24,International Conference on Data Technologies and Applications,Comprehensive database for facial expression analysis,"Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.",Erratum,pro24
pap2091,8c8b7c1adb6f077bb3045928767b8bc6763e0c06,con86,The Web Conference,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",Erratum,pro86
pap2092,73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,con79,IEEE Annual Symposium on Foundations of Computer Science,The Gene Expression Omnibus Database,,Erratum,pro79
pap2093,8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,con26,Decision Support Systems,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",Erratum,pro26
pap2094,ea35cd7fd2c46f86232f21ef73239f34f2d180a6,jou106,Nucleic Acids Research,Database resources of the National Center for Biotechnology Information.,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 35 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface, a sequence database search and a gene orthologs page. Additional resources that were updated in the past year include PMC, Bookshelf, My Bibliography, Assembly, RefSeq, viral genomes, the prokaryotic genome annotation pipeline, Genome Workbench, dbSNP, BLAST, Primer-BLAST, IgBLAST and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",Conference paper,vol106
pap2095,4097c531036fcda230e0174a93565e54684e0e54,con41,Asia-Pacific Software Engineering Conference,The MetaCyc database of metabolic pathways and enzymes - a 2019 update,"Abstract MetaCyc (MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains 2749 pathways derived from more than 60 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc are evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in BioCyc.org and other genomic portals. This article provides an update on the developments in MetaCyc during September 2017 to August 2019, up to version 23.1. Some of the topics that received intensive curation during this period include cobamides biosynthesis, sterol metabolism, fatty acid biosynthesis, lipid metabolism, carotenoid metabolism, protein glycosylation, antibiotics and cytotoxins biosynthesis, siderophore biosynthesis, bioluminescence, vitamin K metabolism, brominated compound metabolism, plant secondary metabolism and human metabolism. Other additions include modifications to the GlycanBuilder software that enable displaying glycans using symbolic representation, improved graphics and fonts for web displays, improvements in the PathoLogic component of Pathway Tools, and the optional addition of regulatory information to pathway diagrams.",Erratum,pro41
pap2096,eb960b5d56ed1368991eaa4f40cb7afee66edb1f,con94,Vision,ONCOMINE: a cancer microarray database and integrated data-mining platform.,,Erratum,pro94
pap2097,26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,con72,Bioinformatics and Computational Biology,"The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",Erratum,pro72
pap2098,2ecbb9d6c6e698dd51134e081fa836319801ae27,jou18,Scientific Data,"The eICU Collaborative Research Database, a freely available multi-center database for critical care research",,Letter,vol18
pap2099,dc8b25e35a3acb812beb499844734081722319b4,jou351,Image and Vision Computing,The FERET database and evaluation procedure for face-recognition algorithms,,Conference paper,vol351
pap2100,dc8b25e35a3acb812beb499844734081722319b4,jou351,Image and Vision Computing,The FERET database and evaluation procedure for face-recognition algorithms,,Conference paper,vol351
pap2101,f89df7381ea8febb419fae473725e44931f6b22c,jou352,BMC Genomics,"Generation and analysis of a 29,745 unique Expressed Sequence Tags from the Pacific oyster (Crassostrea gigas) assembled into a publicly accessible database: the GigasDatabase",,Conference paper,vol352
pap2102,12d8a9991ee7aecc65bc0991959c5b58a367b2ae,con41,Asia-Pacific Software Engineering Conference,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",Erratum,pro41
pap2103,dd31f1439a0b80cb9447a112347836b3325e953e,con111,International Conference on Image Analysis and Processing,The Standardized World Income Inequality Database,"Cross-national research on the causes and consequences of income inequality has been hindered by the limitations of existing inequality datasets: greater coverage across countries and over time is available from these sources only at the cost of significantly reduced comparability across observations. The goal of the Standardized World Income Inequality Database (SWIID) is to overcome these limitations. A custom missing-data algorithm was used to standardize the United Nations University's World Income Inequality Database and data from other sources; data collected by the Luxembourg Income Study served as the standard. The SWIID provides comparable Gini indices of gross and net income inequality for 173 countries for as many years as possible from 1960 to the present along with estimates of uncertainty in these statistics. By maximizing comparability for the largest possible sample of countries and years, the SWIID is better suited to broadly cross-national research on income inequality than previously available sources. 
 
In any papers or publications that use the SWIID, authors are asked to cite the article of record for the data set and give the version number as follows: 
 
Solt, Frederick. 2009. ""Standardizing the World Income Inequality Database."" Social Science Quarterly 90(2):231-242. SWIID Version 3.1, December 2011.",Erratum,pro111
pap2104,3f376a9b2d659e52c98d911a1fb3aa3a834f66e5,con50,International Workshop on Green and Sustainable Software,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Additional NCBI resources focus on literature (Bookshelf, PubMed Central (PMC) and PubReader); medical genetics (ClinVar, dbMHC, the Genetic Testing Registry, HIV-1/Human Protein Interaction Database and MedGen); genes and genomics (BioProject, BioSample, dbSNP, dbVar, Epigenomics, Gene, Gene Expression Omnibus (GEO), Genome, HomoloGene, the Map Viewer, Nucleotide, PopSet, Probe, RefSeq, Sequence Read Archive, the Taxonomy Browser, Trace Archive and UniGene); and proteins and chemicals (Biosystems, COBALT, the Conserved Domain Database (CDD), the Conserved Domain Architecture Retrieval Tool (CDART), the Molecular Modeling Database (MMDB), Protein Clusters, Protein and the PubChem suite of small molecule databases). The Entrez system provides search and retrieval operations for many of these databases. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of these resources can be accessed through the NCBI home page at http://www.ncbi.nlm.nih.gov.",Erratum,pro50
pap2105,439a453090e28f0858ad5ba0765cc2eeffb23626,con46,Software Product Lines Conference,A New Database on Financial Development and Structure,"The authors introduce a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of non bank financial institutions, and by presenting measures of the size of bond and primary equity markets. The compiled data permit the construction of financial structure indicators to measure whether, for example, a country's banks are larger, more active, and more efficient than its stock markets. These indicators can then be used to investigate the empirical link between the legal, regulatory, and policy environment and indicators of financial structure. They can also be used to analyze the implications of financial structure for economic growth. The authors describe the sources and construction of, and the intuition behind, different indicators and present descriptive statistics.",Erratum,pro46
pap2106,2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,jou230,Plant Physiology,GENEVESTIGATOR. Arabidopsis Microarray Database and Analysis Toolbox1[w],"High-throughput gene expression analysis has become a frequent and powerful research tool in biology. At present, however, few software applications have been developed for biologists to query large microarray gene expression databases using a Web-browser interface. We present GENEVESTIGATOR, a database and Web-browser data mining interface for Affymetrix GeneChip data. Users can query the database to retrieve the expression patterns of individual genes throughout chosen environmental conditions, growth stages, or organs. Reversely, mining tools allow users to identify genes specifically expressed during selected stresses, growth stages, or in particular organs. Using GENEVESTIGATOR, the gene expression profiles of more than 22,000 Arabidopsis genes can be obtained, including those of 10,600 currently uncharacterized genes. The objective of this software application is to direct gene functional discovery and design of new experiments by providing plant biologists with contextual information on the expression of genes. The database and analysis toolbox is available as a community resource at https://www.genevestigator.ethz.ch.",Conference paper,vol230
pap2107,01297b19ec00f5487a522a573ff6e0a9aeac4f05,con56,International Conference on Software Engineering and Knowledge Engineering,Database,,Erratum,pro56
pap2108,41e2692d9ac1434d1841d5a29e7ccb927b82b677,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,The Comparative Toxicogenomics Database: update 2019,"Abstract The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a premier public resource for literature-based, manually curated associations between chemicals, gene products, phenotypes, diseases, and environmental exposures. In this biennial update, we present our new chemical–phenotype module that codes chemical-induced effects on phenotypes, curated using controlled vocabularies for chemicals, phenotypes, taxa, and anatomical descriptors; this module provides unique opportunities to explore cellular and system-level phenotypes of the pre-disease state and allows users to construct predictive adverse outcome pathways (linking chemical–gene molecular initiating events with phenotypic key events, diseases, and population-level health outcomes). We also report a 46% increase in CTD manually curated content, which when integrated with other datasets yields more than 38 million toxicogenomic relationships. We describe new querying and display features for our enhanced chemical–exposure science module, providing greater scope of content and utility. As well, we discuss an updated MEDIC disease vocabulary with over 1700 new terms and accession identifiers. To accommodate these increases in data content and functionality, CTD has upgraded its computational infrastructure. These updates continue to improve CTD and help inform new testable hypotheses about the etiology and mechanisms underlying environmentally influenced diseases.",Erratum,pro98
pap2109,026668472fd8f0fa2ca710ce276be35d362637c2,con76,IEEE International Conference on Tools with Artificial Intelligence,"Federated database systems for managing distributed, heterogeneous, and autonomous databases","A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.",Erratum,pro76
pap2110,685db71ce0715ddf1127d72ee991c9ba9c8f89ba,con14,Hawaii International Conference on System Sciences,Human Mortality Database,,Erratum,pro14
pap2111,58a93e9cd60ce331606d31ebed62599a2b7db805,con9,Big Data,The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000,"SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotation (such as the description of the function of a protein, its domains structure, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include format and content enhancements, cross-references to additional databases, new documentation files and improvements to TrEMBL, a computer-annotated supplement to SWISS-PROT. TrEMBL consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDSs) in the EMBL Nucleotide Sequence Database, except the CDSs already included in SWISS-PROT. We also describe the Human Proteomics Initiative (HPI), a major project to annotate all known human sequences according to the quality standards of SWISS-PROT. SWISS-PROT is available at: http://www.expasy.ch/sprot/ and http://www.ebi.ac.uk/swissprot/",Erratum,pro9
pap2112,137832dd10d669a300b7751e0ed3e3b91172372a,con101,International Conference on Biometrics,Human protein reference database—2006 update,"Human Protein Reference Database (HPRD) () was developed to serve as a comprehensive collection of protein features, post-translational modifications (PTMs) and protein–protein interactions. Since the original report, this database has increased to >20 000 proteins entries and has become the largest database for literature-derived protein–protein interactions (>30 000) and PTMs (>8000) for human proteins. We have also introduced several new features in HPRD including: (i) protein isoforms, (ii) enhanced search options, (iii) linking of pathway annotations and (iv) integration of a novel browser, GenProt Viewer (), developed by us that allows integration of genomic and proteomic information. With the continued support and active participation by the biomedical community, we expect HPRD to become a unique source of curated information for the human proteome and spur biomedical discoveries based on integration of genomic, transcriptomic and proteomic data.",Erratum,pro101
pap2113,aca92bbbe7ebc8b6b0f272aca209ac5a027222bd,con15,Pacific Symposium on Biocomputing,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",Erratum,pro15
pap2114,1cf4a6954b419b5478c96119fc1e79aa90f87dea,con97,ACM SIGMOD Conference,The Cambridge Structural Database in retrospect and prospect.,"The Cambridge Crystallographic Data Centre (CCDC) was established in 1965 to record numerical, chemical and bibliographic data relating to published organic and metal-organic crystal structures. The Cambridge Structural Database (CSD) now stores data for nearly 700,000 structures and is a comprehensive and fully retrospective historical archive of small-molecule crystallography. Nearly 40,000 new structures are added each year. As X-ray crystallography celebrates its centenary as a subject, and the CCDC approaches its own 50th year, this article traces the origins of the CCDC as a publicly funded organization and its onward development into a self-financing charitable institution. Principally, however, we describe the growth of the CSD and its extensive associated software system, and summarize its impact and value as a basis for research in structural chemistry, materials science and the life sciences, including drug discovery and drug development. Finally, the article considers the CCDC's funding model in relation to open access and open data paradigms.",Erratum,pro97
pap2115,d2272dd9ff850edc448f7fde86eef6bcd57af2cc,con74,IEEE International Conference on Information Reuse and Integration,Development of a global land cover characteristics database and IGBP DISCover from 1 km AVHRR data,"Researchers from the U.S. Geological Survey, University of Nebraska-Lincoln and the European Commission's Joint Research Centre, Ispra, Italy produced a 1 km resolution global land cover characteristics database for use in a wide range of continental-to global-scale environmental studies. This database provides a unique view of the broad patterns of the biogeographical and ecoclimatic diversity of the global land surface, and presents a detailed interpretation of the extent of human development. The project was carried out as an International Geosphere-Biosphere Programme, Data and Information Systems (IGBP-DIS) initiative. The IGBP DISCover global land cover product is an integral component of the global land cover database. DISCover includes 17 general land cover classes defined to meet the needs of IGBP core science projects. A formal accuracy assessment of the DISCover data layer will be completed in 1998. The 1 km global land cover database was developed through a continent-by-continent unsupervised classification of 1 km monthly Advanced Very High Resolution Radiometer (AVHRR) Normalized Difference Vegetation Index (NDVI) composites covering 1992-1993. Extensive post-classification stratification was necessary to resolve spectral/temporal confusion between disparate land cover types. The complete global database consists of 961 seasonal land cover regions that capture patterns of land cover, seasonality and relative primary productivity. The seasonal land cover regions were aggregated to produce seven separate land cover data sets used for global environmental modelling and assessment. The data sets include IGBP DISCover, U.S. Geological Survey Anderson System, Simple Biosphere Model, Simple Biosphere Model 2, Biosphere-Atmosphere Transfer Scheme, Olson Ecosystems and Running Global Remote Sensing Land Cover. The database also includes all digital sources that were used in the classification. The complete database can be sourced from the website: http://edcwww.cr.usgs.gov/landdaac/glcc/glcc.html.",Erratum,pro74
pap2116,107cbf209b1bd25c7bfc75882347a2655da05118,con46,Software Product Lines Conference,An international database for pesticide risk assessments and management,"ABSTRACT Despite a changing world in terms of data sharing, availability, and transparency, there are still major resource issues associated with collating datasets that will satisfy the requirements of comprehensive pesticide risk assessments, especially those undertaken at a regional or national scale. In 1996, a long-term project was initiated to begin collating and formatting pesticide data to eventually create a free-to-all repository of data that would provide a comprehensive transparent, harmonized, and managed extensive dataset for all types of pesticide risk assessments. Over the last 20 years, this database has been keeping pace with improving risk assessments, their associated data requirements, and the needs and expectations of database end users. In 2007, the Pesticide Properties DataBase (PPDB) was launched as a free-to-access website. Currently, the PPDB holds data for almost 2300 pesticide active substances and over 700 metabolites. For each substance around 300 parameters are stored, covering human health, environmental quality, and biodiversity risk assessments. With the approach of the twentieth anniversary of the database, this article seeks to elucidate the current data model, data sources, its validation, and quality control processes and describes a number of existing risk assessment applications that depend upon it.",Erratum,pro46
pap2117,2157f202c8c89d924dd4da4d1bcf92d16fcd8893,jou353,Signal processing. Image communication,"Image database TID2013: Peculiarities, results and perspectives",,Letter,vol353
pap2118,4c987ffb492e44acc010cbeb2347b92e257d7b59,con105,British Machine Vision Conference,HMDB: the Human Metabolome Database,"The Human Metabolome Database (HMDB) is currently the most complete and comprehensive curated collection of human metabolite and human metabolism data in the world. It contains records for more than 2180 endogenous metabolites with information gathered from thousands of books, journal articles and electronic databases. In addition to its comprehensive literature-derived data, the HMDB also contains an extensive collection of experimental metabolite concentration data compiled from hundreds of mass spectra (MS) and Nuclear Magnetic resonance (NMR) metabolomic analyses performed on urine, blood and cerebrospinal fluid samples. This is further supplemented with thousands of NMR and MS spectra collected on purified, reference metabolites. Each metabolite entry in the HMDB contains an average of 90 separate data fields including a comprehensive compound description, names and synonyms, structural information, physico-chemical data, reference NMR and MS spectra, biofluid concentrations, disease associations, pathway information, enzyme data, gene sequence data, SNP and mutation data as well as extensive links to images, references and other public databases. Extensive searching, relational querying and data browsing tools are also provided. The HMDB is designed to address the broad needs of biochemists, clinical chemists, physicians, medical geneticists, nutritionists and members of the metabolomics community. The HMDB is available at:",Erratum,pro105
pap2119,3ef0c7784bf446de5ce5977a35f86c8b30fd668f,jou354,Systematic Reviews,Optimal database combinations for literature searches in systematic reviews: a prospective exploratory study,,Conference paper,vol354
pap2120,60258897d250a41f11cfee27de828a0130110b5e,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,The CELEX Lexical Database (CD-ROM),,Erratum,pro82
pap2121,e4beeff8cf47dcc0faf6efc8f4c1b3fefc052afb,con96,Interspeech,A database of German emotional speech,"The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).",Letter,pro96
pap2122,e57683f3eea6176441230c2b30bec2fda4984697,con105,British Machine Vision Conference,The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies,,Erratum,pro105
pap2123,3765df816dc5a061bc261e190acc8bdd9d47bec0,con24,International Conference on Data Technologies and Applications,Presentation and validation of the Radboud Faces Database,"Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.",Erratum,pro24
pap2124,3ff0d2c7621c40e6245c7ca0964b4b856255e0c2,con83,Networks,"Development and validation of a global database of lakes, reservoirs and wetlands",,Erratum,pro83
pap2125,b92ed5d8103faf9ccdfd5d3c4f60b722866038d0,con70,International Conference on Graph Transformation,PhenoScanner: a database of human genotype–phenotype associations,"Abstract Summary: PhenoScanner is a curated database of publicly available results from large-scale genetic association studies. This tool aims to facilitate ‘phenome scans’, the cross-referencing of genetic variants with many phenotypes, to help aid understanding of disease pathways and biology. The database currently contains over 350 million association results and over 10 million unique genetic variants, mostly single nucleotide polymorphisms. It is accompanied by a web-based tool that queries the database for associations with user-specified variants, providing results according to the same effect and non-effect alleles for each input variant. The tool provides the option of searching for trait associations with proxies of the input variants, calculated using the European samples from 1000 Genomes and Hapmap. Availability and Implementation: PhenoScanner is available at www.phenoscanner.medschl.cam.ac.uk. Contact: jrs95@medschl.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro70
pap2126,0414f6ffc086bf6c2015176d4b46a051d436cd2b,con109,International Society for Music Information Retrieval Conference,Mouse Genome Database (MGD) 2019,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism genetic and genome resource for the laboratory mouse. MGD is the authoritative source for biological reference data sets related to mouse genes, gene functions, phenotypes, and mouse models of human disease. MGD is the primary outlet for official gene, allele and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. In this report we describe significant enhancements to MGD, including two new graphical user interfaces: (i) the Multi Genome Viewer for exploring the genomes of multiple mouse strains and (ii) the Phenotype-Gene Expression matrix which was developed in collaboration with the Gene Expression Database (GXD) and allows researchers to compare gene expression and phenotype annotations for mouse genes. Other recent improvements include enhanced efficiency of our literature curation processes and the incorporation of Transcriptional Start Site (TSS) annotations from RIKEN’s FANTOM 5 initiative.",Erratum,pro109
pap2127,118cdcf302328a5896de4adbb1e2554641212091,con99,North American Chapter of the Association for Computational Linguistics,The BioGRID interaction database: 2017 update,"The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the annotation and archival of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2016 (build 3.4.140), the BioGRID contains 1 072 173 genetic and protein interactions, and 38 559 post-translational modifications, as manually annotated from 48 114 publications. This dataset represents interaction records for 66 model organisms and represents a 30% increase compared to the previous 2015 BioGRID update. BioGRID curates the biomedical literature for major model organism species, including humans, with a recent emphasis on central biological processes and specific human diseases. To facilitate network-based approaches to drug discovery, BioGRID now incorporates 27 501 chemical–protein interactions for human drug targets, as drawn from the DrugBank database. A new dynamic interaction network viewer allows the easy navigation and filtering of all genetic and protein interaction data, as well as for bioactive compounds and their established targets. BioGRID data are directly downloadable without restriction in a variety of standardized formats and are freely distributed through partner model organism databases and meta-databases.",Erratum,pro99
pap2128,8257167186837ff6840d6c2f552b4d23ff26ec81,jou355,Medical Physics (Lancaster),The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans.,"PURPOSE
The development of computer-aided diagnostic (CAD) methods for lung nodule detection, classification, and quantitative assessment can be facilitated through a well-characterized repository of computed tomography (CT) scans. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) completed such a database, establishing a publicly available reference for the medical imaging research community. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.


METHODS
Seven academic centers and eight medical imaging companies collaborated to identify, address, and resolve challenging organizational, technical, and clinical issues to provide a solid foundation for a robust database. The LIDC/IDRI Database contains 1018 cases, each of which includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (""nodule > or =3 mm,"" ""nodule <3 mm,"" and ""non-nodule > or =3 mm""). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.


RESULTS
The Database contains 7371 lesions marked ""nodule"" by at least one radiologist. 2669 of these lesions were marked ""nodule > or =3 mm"" by at least one radiologist, of which 928 (34.7%) received such marks from all four radiologists. These 2669 lesions include nodule outlines and subjective nodule characteristic ratings.


CONCLUSIONS
The LIDC/IDRI Database is expected to provide an essential medical imaging research resource to spur CAD development, validation, and dissemination in clinical practice.",Letter,vol355
pap2129,07b58c8bb6b9084fba84d464f5324b7933720665,con76,IEEE International Conference on Tools with Artificial Intelligence,Principles of Distributed Database Systems,,Erratum,pro76
pap2130,c1e62b537f3d30018e7979a89b0e0f15e2b6eecc,con64,British Computer Society Conference on Human-Computer Interaction,The SIDER database of drugs and side effects,"Unwanted side effects of drugs are a burden on patients and a severe impediment in the development of new drugs. At the same time, adverse drug reactions (ADRs) recorded during clinical trials are an important source of human phenotypic data. It is therefore essential to combine data on drugs, targets and side effects into a more complete picture of the therapeutic mechanism of actions of drugs and the ways in which they cause adverse reactions. To this end, we have created the SIDER (‘Side Effect Resource’, http://sideeffects.embl.de) database of drugs and ADRs. The current release, SIDER 4, contains data on 1430 drugs, 5880 ADRs and 140 064 drug–ADR pairs, which is an increase of 40% compared to the previous version. For more fine-grained analyses, we extracted the frequency with which side effects occur from the package inserts. This information is available for 39% of drug–ADR pairs, 19% of which can be compared to the frequency under placebo treatment. SIDER furthermore contains a data set of drug indications, extracted from the package inserts using Natural Language Processing. These drug indications are used to reduce the rate of false positives by identifying medical terms that do not correspond to ADRs.",Erratum,pro64
pap2131,839c069fc576c816b89d84aa7c18849874de486a,con64,British Computer Society Conference on Human-Computer Interaction,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible resource for metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are experimentally determined, small-molecule metabolic pathways and are curated from the primary scientific literature. With more than 1400 pathways, MetaCyc is the largest collection of metabolic pathways currently available. Pathways reactions are linked to one or more well-characterized enzymes, and both pathways and enzymes are annotated with reviews, evidence codes, and literature citations. BioCyc (BioCyc.org) is a collection of more than 500 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the full genome and predicted metabolic network of one organism. The network, which is predicted by the Pathway Tools software using MetaCyc as a reference, consists of metabolites, enzymes, reactions and metabolic pathways. BioCyc PGDBs also contain additional features, such as predicted operons, transport systems, and pathway hole-fillers. The BioCyc Web site offers several tools for the analysis of the PGDBs, including Omics Viewers that enable visualization of omics datasets on two different genome-scale diagrams and tools for comparative analysis. The BioCyc PGDBs generated by SRI are offered for adoption by any party interested in curation of metabolic, regulatory, and genome-related information about an organism.",Erratum,pro64
pap2132,7071b85c83035ff86c8ed3c1f3319a304bfb7fb5,con68,Experimental Software Engineering Network,The MetaCyc database of metabolic pathways and enzymes,"Abstract MetaCyc (https://MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains more than 2570 pathways derived from >54 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc is strictly evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in the BioCyc (https://BioCyc.org) and other PGDB collections. This article provides an update on the developments in MetaCyc during the past two years, including the expansion of data and addition of new features.",Erratum,pro68
pap2133,e4b52a1a00e9db941326fc857b95245cbfb60bce,con2,International Conference on Software Engineering,Reactome graph database: Efficient access to complex pathway data,"Reactome is a free, open-source, open-data, curated and peer-reviewed knowledgebase of biomolecular pathways. One of its main priorities is to provide easy and efficient access to its high quality curated data. At present, biological pathway databases typically store their contents in relational databases. This limits access efficiency because there are performance issues associated with queries traversing highly interconnected data. The same data in a graph database can be queried more efficiently. Here we present the rationale behind the adoption of a graph database (Neo4j) as well as the new ContentService (REST API) that provides access to these data. The Neo4j graph database and its query language, Cypher, provide efficient access to the complex Reactome data model, facilitating easy traversal and knowledge discovery. The adoption of this technology greatly improved query efficiency, reducing the average query time by 93%. The web service built on top of the graph database provides programmatic access to Reactome data by object oriented queries, but also supports more complex queries that take advantage of the new underlying graph-based data storage. By adopting graph database technology we are providing a high performance pathway data resource to the community. The Reactome graph database use case shows the power of NoSQL database engines for complex biological data types.",Erratum,pro2
pap2134,f4b3598f49fbb81c0ea21b8130bbdd5133403efc,jou356,JAMA Oncology,Using the National Cancer Database for Outcomes Research: A Review,"Importance The National Cancer Database (NCDB), a joint quality improvement initiative of the American College of Surgeons Commission on Cancer and the American Cancer Society, has created a shared research file that has changed the study of cancer care in the United States. A thorough understanding of the nuances, strengths, and limitations of the database by both readers and investigators is of critical importance. This review describes the use of the NCDB to study cancer care, with a focus on the advantages of using the database and important considerations that affect the interpretation of NCDB studies. Observations The NCDB is one of the largest cancer registries in the world and has rapidly become one of the most commonly used data resources to study the care of cancer in the United States. The NCDB paints a comprehensive picture of cancer care, including a number of less commonly available details that enable subtle nuances of treatment to be studied. On the other hand, several potentially important patient and treatment attributes are not collected in the NCDB, which may affect the extent to which comparisons can be adjusted. Finally, the NCDB has undergone several significant changes during the past decade that may affect its completeness and the types of available data. Conclusions and Relevance The NCDB offers a critically important perspective on cancer care in the United States. To capitalize on its strengths and adjust for its limitations, investigators and their audiences should familiarize themselves with the advantages and shortcomings of the NCDB, as well as its evolution over time.",Article,vol356
pap2135,2b20c0d15c4ec8a48bfd916f73b163a2decd0852,con22,Grid Computing Environments,The InterPro protein families database: the classification resource after 15 years,"The InterPro database (http://www.ebi.ac.uk/interpro/) is a freely available resource that can be used to classify sequences into protein families and to predict the presence of important domains and sites. Central to the InterPro database are predictive models, known as signatures, from a range of different protein family databases that have different biological focuses and use different methodological approaches to classify protein families and domains. InterPro integrates these signatures, capitalizing on the respective strengths of the individual databases, to produce a powerful protein classification resource. Here, we report on the status of InterPro as it enters its 15th year of operation, and give an overview of new developments with the database and its associated Web interfaces and software. In particular, the new domain architecture search tool is described and the process of mapping of Gene Ontology terms to InterPro is outlined. We also discuss the challenges faced by the resource given the explosive growth in sequence data in recent years. InterPro (version 48.0) contains 36 766 member database signatures integrated into 26 238 InterPro entries, an increase of over 3993 entries (5081 signatures), since 2012.",Erratum,pro22
pap2136,ca6ac75d2408d9fbc3ff49e8d62341821574337b,jou357,Immunogenetics,SYFPEITHI: database for MHC ligands and peptide motifs,,Letter,vol357
pap2137,c75ba6ef724c0c3a9c9510a70da4cc8729b59a35,jou245,IEEE Transactions on Visualization and Computer Graphics,FaceWarehouse: A 3D Facial Expression Database for Visual Computing,"We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image.",Conference paper,vol245
pap2138,683bc72f18d2eede7d4c46c55537cbce63a47f62,con105,British Machine Vision Conference,Tmbase-A database of membrane spanning protein segments,,Erratum,pro105
pap2139,62a134740314b4469c83c8921ae2e1beea22b8f5,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,A Database for Handwritten Text Recognition Research,"An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. >",Letter,vol290
pap2140,d90293c06067d1a3718b4fb8c167285bfd099702,jou358,RNA: A publication of the RNA Society,circBase: a database for circular RNAs,"Recently, several laboratories have reported thousands of circular RNAs (circRNAs) in animals. Numerous circRNAs are highly stable and have specific spatiotemporal expression patterns. Even though a function for circRNAs is unknown, these features make circRNAs an interesting class of RNAs as possible biomarkers and for further research. We developed a database and website, “circBase,” where merged and unified data sets of circRNAs and the evidence supporting their expression can be accessed, downloaded, and browsed within the genomic context. circBase also provides scripts to identify known and novel circRNAs in sequencing data. The database is freely accessible through the web server at http://www.circbase.org/.",Conference paper,vol358
pap2141,3bbc9400429ad3d6bda6d12e4449053afa1114a2,jou359,Antimicrobial Agents and Chemotherapy,The Comprehensive Antibiotic Resistance Database,"ABSTRACT The field of antibiotic drug discovery and the monitoring of new antibiotic resistance elements have yet to fully exploit the power of the genome revolution. Despite the fact that the first genomes sequenced of free living organisms were those of bacteria, there have been few specialized bioinformatic tools developed to mine the growing amount of genomic data associated with pathogens. In particular, there are few tools to study the genetics and genomics of antibiotic resistance and how it impacts bacterial populations, ecology, and the clinic. We have initiated development of such tools in the form of the Comprehensive Antibiotic Research Database (CARD; http://arpcard.mcmaster.ca). The CARD integrates disparate molecular and sequence data, provides a unique organizing principle in the form of the Antibiotic Resistance Ontology (ARO), and can quickly identify putative antibiotic resistance genes in new unannotated genome sequences. This unique platform provides an informatic tool that bridges antibiotic resistance concerns in health care, agriculture, and the environment.",Conference paper,vol359
pap2142,2ea5d8555fe5efb7b35bc40e55ead122cfac03b4,jou360,Global Change Biology,TRY – a global database of plant traits,"Plant traits – the morphological, anatomical, physiological, biochemical and phenological characteristics of plants and their organs – determine how primary producers respond to environmental factors, affect other trophic levels, influence ecosystem processes and services and provide a link from species richness to ecosystem functional diversity. Trait data thus represent the raw material for a wide range of research from evolutionary biology, community and functional ecology to biogeography. Here we present the global database initiative named TRY, which has united a wide range of the plant trait research community worldwide and gained an unprecedented buy‐in of trait data: so far 93 trait databases have been contributed. The data repository currently contains almost three million trait entries for 69 000 out of the world's 300 000 plant species, with a focus on 52 groups of traits characterizing the vegetative and regeneration stages of the plant life cycle, including growth, dispersal, establishment and persistence. A first data analysis shows that most plant traits are approximately log‐normally distributed, with widely differing ranges of variation across traits. Most trait variation is between species (interspecific), but significant intraspecific variation is also documented, up to 40% of the overall variation. Plant functional types (PFTs), as commonly used in vegetation models, capture a substantial fraction of the observed variation – but for several traits most variation occurs within PFTs, up to 75% of the overall variation. In the context of vegetation models these traits would better be represented by state variables rather than fixed parameter values. The improved availability of plant trait data in the unified global database is expected to support a paradigm shift from species to trait‐based ecology, offer new opportunities for synthetic plant trait research and enable a more realistic and empirically grounded representation of terrestrial vegetation in Earth system models.",Article,vol360
pap2143,2de0a40e9a5d4f1feb07d61af5a5d87a069653f0,jou3,IEEE Transactions on Knowledge and Data Engineering,Data Mining: An Overview from a Database Perspective,"Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.",Article,vol3
pap2144,bca0c491e3758ae2973d3374b62e1d417a3dc9c7,con108,International Conference on Information Integration and Web-based Applications & Services,Rfam 12.0: updates to the RNA families database,"The Rfam database (available at http://rfam.xfam.org) is a collection of non-coding RNA families represented by manually curated sequence alignments, consensus secondary structures and annotation gathered from corresponding Wikipedia, taxonomy and ontology resources. In this article, we detail updates and improvements to the Rfam data and website for the Rfam 12.0 release. We describe the upgrade of our search pipeline to use Infernal 1.1 and demonstrate its improved homology detection ability by comparison with the previous version. The new pipeline is easier for users to apply to their own data sets, and we illustrate its ability to annotate RNAs in genomic and metagenomic data sets of various sizes. Rfam has been expanded to include 260 new families, including the well-studied large subunit ribosomal RNA family, and for the first time includes information on short sequence- and structure-based RNA motifs present within families.",Erratum,pro108
pap2145,7ffa7a36e5414a0f2b16b1d8f93442ab15e2235d,jou290,IEEE Transactions on Pattern Analysis and Machine Intelligence,"The CMU Pose, Illumination, and Expression Database","In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.",Article,vol290
pap2146,448752b56fe4b2fc8fb15f22d9430c17aa306392,con23,International Conference on Open and Big Data,FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE: THE MESSIDOR DATABASE,"The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.",Erratum,pro23
pap2147,9a7508c7aa498cb738d3c16d35e06a662168106b,con66,International Conference on Software Reuse,The UCSC Genome Browser database: 2015 update,"Launched in 2001 to showcase the draft human genome assembly, the UCSC Genome Browser database (http://genome.ucsc.edu) and associated tools continue to grow, providing a comprehensive resource of genome assemblies and annotations to scientists and students worldwide. Highlights of the past year include the release of a browser for the first new human genome reference assembly in 4 years in December 2013 (GRCh38, UCSC hg38), a watershed comparative genomics annotation (100-species multiple alignment and conservation) and a novel distribution mechanism for the browser (GBiB: Genome Browser in a Box). We created browsers for new species (Chinese hamster, elephant shark, minke whale), ‘mined the web’ for DNA sequences and expanded the browser display with stacked color graphs and region highlighting. As our user community increasingly adopts the UCSC track hub and assembly hub representations for sharing large-scale genomic annotation data sets and genome sequencing projects, our menu of public data hubs has tripled.",Erratum,pro66
pap2148,e45a8d7176bd738c1e63de1f6791a88e704f8b4b,jou120,Nature Communications,Universal database search tool for proteomics,,Letter,vol120
pap2149,9c15ba340c2a44a7b2066d8ded94ce18ea68fe2f,con42,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",The BioGRID interaction database: 2015 update,"The Biological General Repository for Interaction Datasets (BioGRID: http://thebiogrid.org) is an open access database that houses genetic and protein interactions curated from the primary biomedical literature for all major model organism species and humans. As of September 2014, the BioGRID contains 749 912 interactions as drawn from 43 149 publications that represent 30 model organisms. This interaction count represents a 50% increase compared to our previous 2013 BioGRID update. BioGRID data are freely distributed through partner model organism databases and meta-databases and are directly downloadable in a variety of formats. In addition to general curation of the published literature for the major model species, BioGRID undertakes themed curation projects in areas of particular relevance for biomedical sciences, such as the ubiquitin-proteasome system and various human disease-associated interaction networks. BioGRID curation is coordinated through an Interaction Management System (IMS) that facilitates the compilation interaction records through structured evidence codes, phenotype ontologies, and gene annotation. The BioGRID architecture has been improved in order to support a broader range of interaction and post-translational modification types, to allow the representation of more complex multi-gene/protein interactions, to account for cellular phenotypes through structured ontologies, to expedite curation through semi-automated text-mining approaches, and to enhance curation quality control.",Erratum,pro42
pap2150,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,con54,Conference of the Centre for Advanced Studies on Collaborative Research,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",Erratum,pro54
pap2151,fa69e5e6d21650daa286ed936af9c53d1aca00df,con74,IEEE International Conference on Information Reuse and Integration,STRING: a database of predicted functional associations between proteins,"Functional links between proteins can often be inferred from genomic associations between the genes that encode them: groups of genes that are required for the same function tend to show similar species coverage, are often located in close proximity on the genome (in prokaryotes), and tend to be involved in gene-fusion events. The database STRING is a precomputed global resource for the exploration and analysis of these associations. Since the three types of evidence differ conceptually, and the number of predicted interactions is very large, it is essential to be able to assess and compare the significance of individual predictions. Thus, STRING contains a unique scoring-framework based on benchmarks of the different types of associations against a common reference set, integrated in a single confidence score per prediction. The graphical representation of the network of inferred, weighted protein interactions provides a high-level view of functional linkage, facilitating the analysis of modularity in biological processes. STRING is updated continuously, and currently contains 261 033 orthologs in 89 fully sequenced genomes. The database predicts functional interactions at an expected level of accuracy of at least 80% for more than half of the genes; it is online at http://www.bork.embl-heidelberg.de/STRING/.",Erratum,pro74
pap2152,537ab55f4bdfcbe83ab68b2032d83ae9c7d55d31,con93,International Conference on Computational Logic,The Transporter Classification Database (TCDB): recent advances,"The Transporter Classification Database (TCDB; http://www.tcdb.org) is a freely accessible reference database for transport protein research, which provides structural, functional, mechanistic, evolutionary and disease/medical information about transporters from organisms of all types. TCDB is the only transport protein classification database adopted by the International Union of Biochemistry and Molecular Biology (IUBMB). It consists of more than 10 000 non-redundant transport systems with more than 11 000 reference citations, classified into over 1000 transporter families. Transporters in TCDB can be single or multi-component systems, categorized in a functional/phylogenetic hierarchical system of classes, subclasses, families, subfamilies and transport systems. TCDB also includes updated software designed to analyze the distinctive features of transport proteins, extending its usefulness. Here we present a comprehensive update of the database contents and features and summarize recent discoveries recorded in TCDB.",Erratum,pro93
pap2153,429c25227c2225447fd3bf3d17582a19671cc872,con37,International Symposium on Search Based Software Engineering,The PLANTS Database,,Erratum,pro37
pap2154,2e5701b71ccf3352b30b584c2e48fdc307376385,con53,Workshop on Web 2.0 for Software Engineering,The immune epitope database (IEDB) 3.0,"The IEDB, www.iedb.org, contains information on immune epitopes—the molecular targets of adaptive immune responses—curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with >95% of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.",Erratum,pro53
pap2155,66fb37fee3250f68d598e45b3097b50045edc499,jou308,Acta Crystallographica Section B Structural Science,New software for searching the Cambridge Structural Database and visualizing crystal structures.,"Two new programs have been developed for searching the Cambridge Structural Database (CSD) and visualizing database entries: ConQuest and Mercury. The former is a new search interface to the CSD, the latter is a high-performance crystal-structure visualizer with extensive facilities for exploring networks of intermolecular contacts. Particular emphasis has been placed on making the programs as intuitive as possible. Both ConQuest and Mercury run under Windows and various types of Unix, including Linux.",Letter,vol308
pap2156,00bc156bac2b39fab1689fe047b23c9f216d7f29,jou106,Nucleic Acids Research,Database resources of the National Center for Biotechnology Information: update,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s website. NCBI resources include Entrez, PubMed, PubMed Central, LocusLink, the NCBI Taxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR, OrfFinder, Spidey, RefSeq, UniGene, HomoloGene, ProtEST, dbMHC, dbSNP, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker, Evidence Viewer, Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SARS Coronavirus Resource, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD) and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",Conference paper,vol106
pap2157,269b9e182815a5d805cfa1f5c1137763b0b2b7cf,con32,International Conference on Software Technology: Methods and Tools,Genevestigator V3: A Reference Expression Database for the Meta-Analysis of Transcriptomes,"The Web-based software tool Genevestigator provides powerful tools for biologists to explore gene expression across a wide variety of biological contexts. Its first releases, however, were limited by the scaling ability of the system architecture, multiorganism data storage and analysis capability, and availability of computationally intensive analysis methods. Genevestigator V3 is a novel meta-analysis system resulting from new algorithmic and software development using a client/server architecture, large-scale manual curation and quality control of microarray data for several organisms, and curation of pathway data for mouse and Arabidopsis. In addition to improved querying features, Genevestigator V3 provides new tools to analyze the expression of genes in many different contexts, to identify biomarker genes, to cluster genes into expression modules, and to model expression responses in the context of metabolic and regulatory networks. Being a reference expression database with user-friendly tools, Genevestigator V3 facilitates discovery research and hypothesis validation.",Erratum,pro32
pap2158,89c433d6a7160bba49ef164fa93c3036d9f7ac5e,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a freely accessible comprehensive database describing metabolic pathways and enzymes from all domains of life. The majority of MetaCyc pathways are small-molecule metabolic pathways that have been experimentally determined. MetaCyc contains more than 2400 pathways derived from >46 000 publications, and is the largest curated collection of metabolic pathways. BioCyc (BioCyc.org) is a collection of 5700 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems, and pathway-hole fillers. The BioCyc website offers a variety of tools for querying and analyzing PGDBs, including Omics Viewers and tools for comparative analysis. This article provides an update of new developments in MetaCyc and BioCyc during the last two years, including addition of Gibbs free energy values for compounds and reactions; redesign of the primary gene/protein page; addition of a tool for creating diagrams containing multiple linked pathways; several new search capabilities, including searching for genes based on sequence patterns, searching for databases based on an organism's phenotypes, and a cross-organism search; and a metabolite identifier translation service.",Erratum,pro13
pap2159,dc0e51b870dba980b79a3c34a044e491e7cfd5c4,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,CHIANTI - an atomic database for emission lines - I. Wavelengths greater than 50 Å,"CHIANTI consists of a critically evaluated set of atomic data and transition probabilities necessary to calculate the emission line spectrum of astrophysical plasmas. The data consist of atomic energy levels, atomic radiative data such as wavelengths, weighted oscillator strengths and A values, and electron collisional excitation rates. A set of programs that use these data to calculate the spectrum in a desired wavelength range as a function of temperature and density is also provided. A suite of programs has been developed to carry out plasma diagnostics of astrophysical plasmas. The state-of-the-art contents of the CHIANTI database will be described and some of the most important results obtained from the use of the CHIANTI database will be reviewed.",Erratum,pro58
pap2160,0be6f2f0329230040e2a2dbd4b9e321ef33a2c11,con105,British Machine Vision Conference,The UCSC Genome Browser database: update 2011,"The University of California, Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. The Browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. Among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for BAM and BigWig/BigBed file formats. New display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a ‘mean+whiskers’ windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. New data highlights include seven new genome assemblies, a Neandertal genome data portal, phenotype and disease association data, a human RNA editing track, and a zebrafish Conservation track. We also describe updates to existing tracks.",Erratum,pro105
pap2161,b7e54a5d6149cde03fd5144d38a0647d25a795f6,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,The ChEMBL bioactivity database: an update,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 Nucleic Acids Research Database Issue. Since then, a variety of new data sources and improvements in functionality have contributed to the growth and utility of the resource. In particular, more comprehensive tracking of compounds from research stages through clinical development to market is provided through the inclusion of data from United States Adopted Name applications; a new richer data model for representing drug targets has been developed; and a number of methods have been put in place to allow users to more easily identify reliable data. Finally, access to ChEMBL is now available via a new Resource Description Framework format, in addition to the web-based interface, data downloads and web services.",Erratum,pro21
pap2162,20740b17fd91394cfdf17ebf1c227312d6bb56cb,con46,Software Product Lines Conference,The Database of Interacting Proteins: 2004 update,"The Database of Interacting Proteins (http://dip.doe-mbi.ucla.edu) aims to integrate the diverse body of experimental evidence on protein-protein interactions into a single, easily accessible online database. Because the reliability of experimental evidence varies widely, methods of quality assessment have been developed and utilized to identify the most reliable subset of the interactions. This CORE set can be used as a reference when evaluating the reliability of high-throughput protein-protein interaction data sets, for development of prediction methods, as well as in the studies of the properties of protein interaction networks.",Erratum,pro46
pap2163,88fc814fc568fdfa4704ba2326eb623e95621bec,con12,The Compass,Notes on CEPII’s Distances Measures: The GeoDist Database,"GeoDist makes available the exhaustive set of gravity variables used in Mayer and Zignago (2005). GeoDist provides several geographical variables, in particular bilateral distances measured using citylevel data to assess the geographic distribution of population inside each nation. We have calculated different measures of bilateral distances available for most countries across the world (225 countries in the current version of the database). For most of them, different calculations of “intra-national distances” are also available. The GeoDist webpage provides two distinct files: a country-specific one (geo_cepii)and a dyadic one (dist_cepii) including a set of different distance and common dummy variables used in gravity equations to identify particular links between countries such as colonial past, common languages, contiguity. We try to improve upon the existing similar datasets in terms of geographical coverage, quality of measurement and number of variables provided.",Erratum,pro12
pap2164,6c20fecf959174309f8c9ddc1b21787513842af9,con45,International Conference on Global Software Engineering,Saccharomyces Genome Database: the genomics resource of budding yeast,"The Saccharomyces Genome Database (SGD, http://www.yeastgenome.org) is the community resource for the budding yeast Saccharomyces cerevisiae. The SGD project provides the highest-quality manually curated information from peer-reviewed literature. The experimental results reported in the literature are extracted and integrated within a well-developed database. These data are combined with quality high-throughput results and provided through Locus Summary pages, a powerful query engine and rich genome browser. The acquisition, integration and retrieval of these data allow SGD to facilitate experimental design and analysis by providing an encyclopedia of the yeast genome, its chromosomal features, their functions and interactions. Public access to these data is provided to researchers and educators via web pages designed for optimal ease of use.",Erratum,pro45
pap2165,7fa10285fd3532aea2e64d0d6de06608266b4366,con89,Conference on Uncertainty in Artificial Intelligence,An Overview of the Global Historical Climatology Network-Daily Database,"AbstractA database is described that has been designed to fulfill the need for daily climate data over global land areas. The dataset, known as Global Historical Climatology Network (GHCN)-Daily, was developed for a wide variety of potential applications, including climate analysis and monitoring studies that require data at a daily time resolution (e.g., assessments of the frequency of heavy rainfall, heat wave duration, etc.). The dataset contains records from over 80 000 stations in 180 countries and territories, and its processing system produces the official archive for U.S. daily data. Variables commonly include maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about two-thirds of the stations report precipitation only. Quality assurance checks are routinely applied to the full dataset, but the data are not homogenized to account for artifacts associated with the various eras in reporting practice at any particular station (i.e., for changes in systematic...",Erratum,pro89
pap2166,789ae8da65d2b26be12a8a2dcba51073cea41182,con46,Software Product Lines Conference,Materials Design and Discovery with High-Throughput Density Functional Theory: The Open Quantum Materials Database (OQMD),,Erratum,pro46
pap2167,fa154947515cd58c0a273bb170bc6dc8a8c99847,jou361,Bulletin of Earthquake Engineering,Active fault database of Turkey,,Letter,vol361
pap2168,b89f0e4f43570688dd983813c9a3efa2fa7e7ebc,con50,International Workshop on Green and Sustainable Software,"The CMU Pose, Illumination, and Expression (PIE) database","Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.",Erratum,pro50
pap2169,475bbf493d8246031a5152c8005a5c567231307c,jou347,Journal of Chemical Information and Modeling,Basis Set Exchange: A Community Database for Computational Sciences,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",Letter,vol347
pap2170,d5985ad8d754187dc36c4e2a221086f8530fe372,con109,International Society for Music Information Retrieval Conference,Completion of the 2006 National Land Cover Database for the conterminous United States.,,Erratum,pro109
pap2171,3a60678ad2b862fa7c27b11f04c93c010cc6c430,jou345,IEEE Transactions on Affective Computing,A Multimodal Database for Affect Recognition and Implicit Tagging,"MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.",Conference paper,vol345
pap2172,31dbfb575ef802a89a5c08e632f42265bcf30684,con33,International Conference on Automated Software Engineering,High Resolution XPS of Organic Polymers: The Scienta ESCA300 Database,Description of the spectrometer x-ray source monochromator electron lens hemispherical analyser multichannel detector sample analysis chamber charge compensation performance on conducting samples performance on insulating samples performance on testing of the spectrometer experimental protocol sample mounting data acquisition correction of binding energy scale for sample charging curve fitting lineshapes shake-up structure valence bands impurities x-ray degradation organization of the database list of polymers and acronyms the database appendix 1 - primary C 1s shifts appendix 2 - secondary C 1s shifts appendix 3.1 - 0 1s binding energies in CHO polymers appendix 3.2 - 0 1s binding energies in other polymers appendix 4 - N 1s binding energies appendix 5 - F 1s binding energies appendix 6 - binding energies and spin-orbit constants for core-line doublets apendix 7 - binding energies of peaks appearing in the valence band region.,Erratum,pro33
pap2173,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,con40,Conference on Software Engineering Education and Training,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",Erratum,pro40
pap2174,87752bbb228c597e12cc7e86d9dba4539a04769e,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,An Overview of the China Meteorological Administration Tropical Cyclone Database,"AbstractThe China Meteorological Administration (CMA)’s tropical cyclone (TC) database includes not only the best-track dataset but also TC-induced wind and precipitation data. This article summarizes the characteristics and key technical details of the CMA TC database. In addition to the best-track data, other phenomena that occurred with the TCs are also recorded in the dataset, such as the subcenters, extratropical transitions, outer-range severe winds associated with TCs over the South China Sea, and coastal severe winds associated with TCs landfalling in China. These data provide additional information for researchers. The TC-induced wind and precipitation data, which map the distribution of severe wind and rainfall, are also helpful for investigating the impacts of TCs. The study also considers the changing reliability of the various data sources used since the database was created and the potential causes of temporal and spatial inhomogeneities within the datasets. Because of the greater number of ...",Erratum,pro98
pap2175,9162a7e9434022c2ed6f249b129d6e50b90eb1a3,jou362,Biocontrol News and Information,Biological control of insect pests by insect parasitoids and predators: the BIOCAT database.,"The structure of the BIOCAT database, which contains records of the introductions of insect natural enemies for the control of insect pests worldwide, and is now available online, is explained. It is a useful summary of biological control effort and a guide to factors which may influence the success of introduction programmes, but is not detailed enough for making firm predictions.",Conference paper,vol362
pap2176,16cf42b0481042514a983e48b9994d60145553f1,con91,Symposium on the Theory of Computing,InterPro: the integrative protein signature database,"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or ‘signatures’ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total ∼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein–protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).",Erratum,pro91
pap2177,34284e452a59db6a3d0a867cb01626151a7781b4,con69,Formal Concept Analysis,The Dartmouth Stellar Evolution Database,"The ever-expanding depth and quality of photometric and spectroscopic observations of stellar populations increase the need for theoretical models in regions of age-composition parameter space that are largely unexplored at present. Stellar evolution models that employ the most advanced physics and cover a wide range of compositions are needed to extract the most information from current observations of both resolved and unresolved stellar populations. The Dartmouth Stellar Evolution Database is a collection of stellar evolution tracks and isochrones that spans a range of [Fe/H] from –2.5 to +0.5, [α/Fe] from –0.2 to +0.8 (for [Fe/H] ⩽ 0) or +0.2 (for [Fe/H] > 0), and initial He mass fractions from Y = 0.245 to 0.40. Stellar evolution tracks were computed for masses between 0.1 and 4 M☉, allowing isochrones to be generated for ages as young as 250 Myr. For the range in masses where the core He flash occurs, separate He-burning tracks were computed starting from the zero age horizontal branch. The tracks and isochrones have been transformed to the observational plane in a variety of photometric systems including standard UBV(RI)C, Stromgren uvby, SDSS ugriz, 2MASS JHKs, and HST ACS/WFC and WFPC2. The Dartmouth Stellar Evolution Database is accessible through a Web site at http://stellar.dartmouth.edu/~models/ where all tracks, isochrones, and additional files can be downloaded.",Erratum,pro69
pap2178,f49052eeb5607062941137c9468ff1122a19e016,con27,International Conference on Contemporary Computing,Cochrane Database of Systematic Reviews,"Here, we bring readers extracts from the latest issues of The Cochrane Database of Systematic Reviews relevant to the fields of psychiatry and neurology. We feature a summary of the results, reviewers' conclusions, and implications for clinical practice and research, from selected new reviews featured in issues 6 and 7, 2011. For further information, visit www.thecochranelibrary.com. Copyright © 2011 Wiley Interface Ltd",Erratum,pro27
pap2179,1c3588499908417312d172f1b5102e76a90fbb2c,con1,International Conference on Human Factors in Computing Systems,"HITEMP, the high-temperature molecular spectroscopic database",,Erratum,pro1
pap2180,0e17d9327501603b39200c5eb9db886de1581a09,con55,Workshop on Learning from Authoritative Security Experiment Results,Systemic Banking Crises Database,,Erratum,pro55
pap2181,7400d5e1d211294ee2147355e863287f3cbad0a5,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,PhagesDB: the actinobacteriophage database,"The Actinobacteriophage Database (PhagesDB) is a comprehensive, interactive, database-backed website that collects and shares information related to the discovery, characterization and genomics of viruses that infect Actinobacterial hosts. To date, more than 8000 bacteriophages-including over 1600 with sequenced genomes-have been entered into the database. PhagesDB plays a crucial role in organizing the discoveries of phage biologists around the world-including students in the SEA-PHAGES program-and has been cited in over 50 peer-reviewed articles.


Availability and Implementation
http://phagesdb.org/.


Contact
gfh@pitt.edu.",Erratum,pro82
pap2182,3a19b36d7f41ba20a507dee2585fd56fa019167a,jou262,Nature Genetics,Systematic meta-analyses of Alzheimer disease genetic association studies: the AlzGene database,,Conference paper,vol262
pap2183,d1de096375c58d18e167b0f2324f0288750b7411,con89,Conference on Uncertainty in Artificial Intelligence,NGA-West2 Database,"The NGA-West2 project database expands on its predecessor to include worldwide ground motion data recorded from shallow crustal earthquakes in active tectonic regimes post-2000 and a set of small-to-moderate-magnitude earthquakes in California between 1998 and 2011. The database includes 21,336 (mostly) three-component records from 599 events. The parameter space covered by the database is M 3.0 to M 7.9, closest distance of 0.05 to 1,533 km, and site time-averaged shear-wave velocity in the top 30 m of VS30 = 94 m/s to 2,100 m/s (although data becomes sparse for distances >400 km and VS30 > 1,200 m/s or <150 m/s). The database includes uniformly processed time series and response spectral ordinates for 111 periods ranging from 0.01 s to 20 s at 11 damping ratios. Ground motions and metadata for source, path, and site conditions were subject to quality checks by ground motion prediction equation developers and topical working groups.",Erratum,pro89
pap2184,5fec3cda0d994c217c737c10b0b7e56d4574edca,con75,Intelligent Systems in Molecular Biology,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfill the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. Recent developments include the following. A community annotation project has been instigated in which acknowledged experts are invited to contribute summaries for peptidases. Software has been written to provide an Internet-based data entry form. Contributors are acknowledged on the relevant web page. A new display showing the intron/exon structures of eukaryote peptidase genes and the phasing of the junctions has been implemented. It is now possible to filter the list of peptidases from a completely sequenced bacterial genome for a particular strain of the organism. The MEROPS filing pipeline has been altered to circumvent the restrictions imposed on non-interactive blastp searches, and a HMMER search using specially generated alignments to maximize the distribution of organisms returned in the search results has been added.",Erratum,pro75
pap2185,b43ffeb049adb791a82521eaaf83c367d651da4c,con89,Conference on Uncertainty in Artificial Intelligence,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",Erratum,pro89
pap2186,cf4dc5ff06fc8c17b05b399a98c164c60a834e09,jou43,Social Science Research Network,Systemic Banking Crises: A New Database,"This paper presents a new database on the timing of systemic banking crises and policy responses to resolve them. The database covers the universe of systemic banking crises for the period 1970-2007, with detailed data on crisis containment and resolution policies for 42 crisis episodes, and also includes data on the timing of currency crises and sovereign debt crises. The database extends and builds on the Caprio, Klingebiel, Laeven, and Noguera (2005) banking crisis database, and is the most complete and detailed database on banking crises to date.",Letter,vol43
pap2187,d5837fd1cf0d51e547553c3612928de880a08589,con22,Grid Computing Environments,JASPAR: an open-access database for eukaryotic transcription factor binding profiles,"The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome-wide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.",Erratum,pro22
pap2188,1fcf323ff79c46b401e7a3d8510f50da17073cb8,con31,International Conference on Evaluation & Assessment in Software Engineering,The UCSC Genome Browser Database: update 2006,"The University of California Santa Cruz Genome Browser Database (GBD) contains sequence and annotation data for the genomes of about a dozen vertebrate species and several major model organisms. Genome annotations typically include assembly data, sequence composition, genes and gene predictions, mRNA and expressed sequence tag evidence, comparative genomics, regulation, expression and variation data. The database is optimized to support fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. The Genome Browser displays a wide variety of annotations at all scales from single nucleotide level up to a full chromosome. The Table Browser provides direct access to the database tables and sequence data, enabling complex queries on genome-wide datasets. The Proteome Browser graphically displays protein properties. The Gene Sorter allows filtering and comparison of genes by several metrics including expression data and several gene properties. BLAT and In Silico PCR search for sequences in entire genomes in seconds. These tools are highly integrated and provide many hyperlinks to other databases and websites. The GBD, browsing tools, downloadable data files and links to documentation and other information can be found at .",Erratum,pro31
pap2189,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",Erratum,pro13
pap2190,f780775b85986a2bf1d5849c56f621cb99efef3c,con41,Asia-Pacific Software Engineering Conference,The Dfam database of repetitive DNA families,"Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Dfam is an open access database of families of repetitive DNA elements, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). The initial release of Dfam, featured in the 2013 NAR Database Issue, contained 1143 families of repetitive elements found in humans, and was used to produce more than 100 Mb of additional annotation of TE-derived regions in the human genome, with improved speed. Here, we describe recent advances, most notably expansion to 4150 total families including a comprehensive set of known repeat families from four new organisms (mouse, zebrafish, fly and nematode). We describe improvements to coverage, and to our methods for identifying and reducing false annotation. We also describe updates to the website interface. The Dfam website has moved to http://dfam.org. Seed alignments, profile HMMs, hit lists and other underlying data are available for download.",Erratum,pro41
pap2191,7def002796277facffe02aa09e3a1bb101ec0785,con97,ACM SIGMOD Conference,Access path selection in a relational database management system,"In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research Laboratory.",Article,pro97
pap2192,4bba202eee798202e214cbc6bfca6a78e3f26342,con74,IEEE International Conference on Information Reuse and Integration,Energy-Aware Database Management Systems,"Annual energy cost of data centers have reached one billion US dollars in the United States alone. In a typical data center, a significant portion of the computing resources (thus energy) is dedicated to running the back end database service. Therefore, making databases energy-aware is of high economic and sustainability significance. For that purpose, we have launched a research project that aims at the design and implementation of an energy-aware DBMS that enables significant energy conservation (as compared to traditional DBMSs) within a bounded performance penalty. This white paper encapsulates some of the critical ideas we explored and verified in designing such a DBMS.",Erratum,pro74
pap2193,40f19bdaa4e869ab9784880fec5e9e229a2a61ab,jou146,Astrophysical Journal Supplement Series,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",Conference paper,vol146
pap2194,677872c4051ee37a13161ecb8efa32fcbadc7a80,con57,International Workshop on Agent-Oriented Software Engineering,MEROPS: the peptidase database,"Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has a hierarchical classification in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The classification framework is used for attaching information at each level. An important focus of the database has become distinguishing one peptidase from another through identifying the specificity of the peptidase in terms of where it will cleave substrates and with which inhibitors it will interact. We have collected over 39 000 known cleavage sites in proteins, peptides and synthetic substrates. These allow us to display peptidase specificity and alignments of protein substrates to give an indication of how well a cleavage site is conserved, and thus its probable physiological relevance. While the number of new peptidase families and clans has only grown slowly the number of complete genomes has greatly increased. This has allowed us to add an analysis tool to the relevant species pages to show significant gains and losses of peptidase genes relative to related species.",Erratum,pro57
pap2195,a7a581f7f052570fa099dcb887b9934617760791,jou232,Proteomics,Comet: An open‐source MS/MS sequence database search tool,"Proteomics research routinely involves identifying peptides and proteins via MS/MS sequence database search. Thus the database search engine is an integral tool in many proteomics research groups. Here, we introduce the Comet search engine to the existing landscape of commercial and open‐source database search tools. Comet is open source, freely available, and based on one of the original sequence database search tools that has been widely used for many years.",Article,vol232
pap2196,fb1d51c023da037dac928d7991de1b77ade22728,jou363,Therapeutic Drug Monitoring,METLIN: A Metabolite Mass Spectral Database,"Endogenous metabolites have gained increasing interest over the past 5 years largely for their implications in diagnostic and pharmaceutical biomarker discovery. METLIN (http://metlin.scripps.edu), a freely accessible web-based data repository, has been developed to assist in a broad array of metabolite research and to facilitate metabolite identification through mass analysis. METLIN includes an annotated list of known metabolite structural information that is easily cross-correlated with its catalogue of high-resolution Fourier transform mass spectrometry (FTMS) spectra, tandem mass spectrometry (MS/MS) spectra, and LC/MS data.",Letter,vol363
pap2197,2e1b489e9f5964eee1f09c08cf36f5377c2688c6,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Introduction to Database Systems,,Erratum,pro98
pap2198,49ed15db181c74c7067ec01800fb5392411c868c,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Epidemic algorithms for replicated database maintenance,"When a database is replicated at many sites, maintaining mutual consistency among the sites in the face of updates is a significant problem. This paper describes several randomized algorithms for distributing updates and driving the replicas toward consistency. The algorithms are very simple and require few guarantees from the underlying communication system, yet they ensure that the effect of every update is eventually reflected in all replicas. The cost and performance of the algorithms are tuned by choosing appropriate distributions in the randomization step. The algorithms are closely analogous to epidemics, and the epidemiology literature aids in understanding their behavior. One of the algorithms has been implemented in the Clearinghouse servers of the Xerox Corporate Internet. solving long-standing problems of high traffic and database inconsistency.",Erratum,pro54
pap2199,ecf1848db9f0e81a8a0dbffeb8162772cb6be733,con2,International Conference on Software Engineering,The COG database: new developments in phylogenetic classification of proteins from complete genomes,"The database of Clusters of Orthologous Groups of proteins (COGs), which represents an attempt on a phylogenetic classification of the proteins encoded in complete genomes, currently consists of 2791 COGs including 45 350 proteins from 30 genomes of bacteria, archaea and the yeast Saccharomyces cerevisiae (http://www.ncbi.nlm.nih. gov/COG). In addition, a supplement to the COGs is available, in which proteins encoded in the genomes of two multicellular eukaryotes, the nematode Caenorhabditis elegans and the fruit fly Drosophila melanogaster, and shared with bacteria and/or archaea were included. The new features added to the COG database include information pages with structural and functional details on each COG and literature references, improvements of the COGNITOR program that is used to fit new proteins into the COGs, and classification of genomes and COGs constructed by using principal component analysis.",Erratum,pro2
pap2200,6dc7b1da99e30592b54f2148c85e8725563011f7,con97,ACM SIGMOD Conference,Storing and querying ordered XML using a relational database system,"XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.",Article,pro97
pap2201,6b092ee6d04b4fa76a0452cf0696086a7c04644d,con63,International Colloquium on Theoretical Aspects of Computing,"DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions","The Database of Interacting Proteins (DIP: http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. It provides the scientific community with an integrated set of tools for browsing and extracting information about protein interaction networks. As of September 2001, the DIP catalogs approximately 11 000 unique interactions among 5900 proteins from >80 organisms; the vast majority from yeast, Helicobacter pylori and human. Tools have been developed that allow users to analyze, visualize and integrate their own experimental data with the information about protein-protein interactions available in the DIP database.",Erratum,pro63
pap2202,928e4178fca984b1d49da75c5120e05cff7a3fa9,con34,International Conference on Agile Software Development,Development of a 2001 National land-cover database for the United States,"Multi-Resolution Land Characterization 2001 (MRLC 2001) is a second-generation Federal consortium designed to create an updated pool of nation-wide Landsat 5 and 7 imagery and derive a second-generation National Land Cover Database (NLCD 2001). The objectives of this multi-layer, multi-source database are two fold: first, to provide consistent land cover for all 50 States, and second, to provide a data framework which allows flexibility in developing and applying each independent data component to a wide variety of other applications. Components in the database include the following: (1) normalized imagery for three time periods per path/row, (2) ancillary data, including a 30 m Digital Elevation Model (DEM) derived into slope, aspect and slope position, (3) perpixel estimates of percent imperviousness and percent tree canopy (4) 29 classes of land cover data derived from the imagery, ancillary data, and derivatives, (5) classification rules, confidence estimates, and metadata from the land cover classification. This database is now being developed using a Mapping Zone approach, with 66 Zones in the continental United States and 23 Zones in Alaska. Results from three initial mapping Zones show single-pixel land cover accuracies ranging from 73 to 77 percent, imperviousness accuracies ranging from 83 to 91 percent, tree canopy accuracies ranging from 78 to 93 percent, and an estimated 50 percent increase in mapping efficiency over previous methods. The database has now entered the production phase and is being created using extensive partnering in the Federal government with planned completion by 2006.",Erratum,pro34
pap2203,f86808d344d7884296c222f0cd6892abe411c636,jou106,Nucleic Acids Research,The Ribosomal Database Project.,"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services, and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (rdp.life.uiuc.edu), electronic mail (server/rdp.life.uiuc.edu) and gopher (rdpgopher.life.uiuc.edu). The electronic mail server also provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for chimeric nature of newly sequenced rRNAs, and automated alignment.",Conference paper,vol106
pap2204,60add4035c2b6d4b9888cb1028c31e6bed793d60,con89,Conference on Uncertainty in Artificial Intelligence,The UCSC Genome Browser database: 2014 update,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a large collection of organisms, primarily vertebrates, with an emphasis on the human and mouse genomes. The Browser’s web-based tools provide an integrated environment for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic data sets. As of September 2013, the database contained genomic sequence and a basic set of annotation ‘tracks’ for ∼90 organisms. Significant new annotations include a 60-species multiple alignment conservation track on the mouse, updated UCSC Genes tracks for human and mouse, and several new sets of variation and ENCODE data. New software tools include a Variant Annotation Integrator that returns predicted functional effects of a set of variants uploaded as a custom track, an extension to UCSC Genes that displays haplotype alleles for protein-coding genes and an expansion of data hubs that includes the capability to display remotely hosted user-provided assembly sequence in addition to annotation data. To improve European access, we have added a Genome Browser mirror (http://genome-euro.ucsc.edu) hosted at Bielefeld University in Germany.",Erratum,pro89
pap2205,fd2d9588818d4bd6d5cc59a26642c3d3c05ab915,con55,Workshop on Learning from Authoritative Security Experiment Results,IT’IS Database for Thermal and Electromagnetic Parameters of Biological Tissues,,Erratum,pro55
pap2206,b0738246da135894f966469ed110be183ec4bbff,con68,Experimental Software Engineering Network,CircFunBase: a database for functional circular RNAs,"Abstract Increasing evidence reveals that circular RNAs (circRNAs) are widespread in eukaryotes and play important roles in diverse biological processes. However, a comprehensive functionally annotated circRNA database is still lacking. CircFunBase is a web-accessible database that aims to provide a high-quality functional circRNA resource including experimentally validated and computationally predicted functions. The current version of CircFunBase documents more than 7000 manually curated functional circRNA entries, mainly including Homo sapiens, Mus musculus etc. CircFunBase provides visualized circRNA-miRNA interaction networks. In addition, a genome browser is provided to visualize the genome context of circRNAs. As a biological information platform for circRNAs, CircFunBase will contribute for circRNA studies and bridge the gap between circRNAs and their functions.",Erratum,pro68
pap2207,b62628ac06bbac998a3ab825324a41a11bc3a988,con45,International Conference on Global Software Engineering,XM2VTSDB: The Extended M2VTS Database,"Keywords: vision Reference EPFL-CONF-82502 URL: ftp://ftp.idiap.ch/pub/papers/vision/avbpa99.pdf Record created on 2006-03-10, modified on 2017-05-10",Erratum,pro45
pap2208,10b40befe5942e997f88ab40bac1acd931147893,con67,IEEE International Software Metrics Symposium,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",Erratum,pro67
pap2209,d594c19d91c55a4e7f341fd1d45fc30c5fac0b1a,con78,Neural Information Processing Systems,Atlantic Hurricane Database Uncertainty and Presentation of a New Database Format,"Abstract“Best tracks” are National Hurricane Center (NHC) poststorm analyses of the intensity, central pressure, position, and size of Atlantic and eastern North Pacific basin tropical and subtropical cyclones. This paper estimates the uncertainty (average error) for Atlantic basin best track parameters through a survey of the NHC Hurricane Specialists who maintain and update the Atlantic hurricane database. A comparison is then made with a survey conducted over a decade ago to qualitatively assess changes in the uncertainties. Finally, the implications of the uncertainty estimates for NHC analysis and forecast products as well as for the prediction goals of the Hurricane Forecast Improvement Program are discussed.",Erratum,pro78
pap2210,99a4acc4de2097d5ed6c4ec257f284c9dc86b3b8,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",A major upgrade of the VALD database,"Vienna atomic line database (VALD) is a collection of critically evaluated laboratory parameters for individual atomic transitions, complemented by theoretical calculations. VALD is actively used by astronomers for stellar spectroscopic studies—model atmosphere calculations, atmospheric parameter determinations, abundance analysis etc. The two first VALD releases contained parameters for atomic transitions only. In a major upgrade of VALD—VALD3, publically available from spring 2014, atomic data was complemented with parameters of molecular lines. The diatomic molecules C2, CH, CN, CO, OH, MgH, SiH, TiO are now included. For each transition VALD provides species name, wavelength, energy, quantum number J and Landé-factor of the lower and upper levels, radiative, Stark and van der Waals damping factors and a full description of electronic configurarion and term information of both levels. Compared to the previous versions we have revised and verify all of the existing data and added new measurements and calculations for transitions in the range between 20 Å and 200 microns. All transitions were complemented with term designations in a consistent way and electron configurations when available. All data were checked for consistency: listed wavelength versus Ritz, selection rules etc. A new bibliographic system keeps track of literature references for each parameter in a given transition throughout the merging process so that every selected data entry can be traced to the original source. The query language and the extraction tools can now handle various units, vacuum and air wavelengths. In the upgrade process we had an intensive interaction with data producers, which was very helpful for improving the quality of the VALD content.",Erratum,pro87
pap2211,24019050c30b7e5bf1be28e48b8cb5278c4286fd,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,PH2 - A dermoscopic image database for research and benchmarking,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identification of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.",Letter,pro98
pap2212,6d703e6eb1c72241720bafdb42b46b70c3bdd16e,con71,Annual Conference on Innovation and Technology in Computer Science Education,MRC Psycholinguistic Database,,Erratum,pro71
pap2213,5d0b1c9e2e24b38489c8021261217b023e42a652,con66,International Conference on Software Reuse,OPM database and PPM web server: resources for positioning of proteins in membranes,"The Orientations of Proteins in Membranes (OPM) database is a curated web resource that provides spatial positions of membrane-bound peptides and proteins of known three-dimensional structure in the lipid bilayer, together with their structural classification, topology and intracellular localization. OPM currently contains more than 1200 transmembrane and peripheral proteins and peptides from approximately 350 organisms that represent approximately 3800 Protein Data Bank entries. Proteins are classified into classes, superfamilies and families and assigned to 21 distinct membrane types. Spatial positions of proteins with respect to the lipid bilayer are optimized by the PPM 2.0 method that accounts for the hydrophobic, hydrogen bonding and electrostatic interactions of the proteins with the anisotropic water-lipid environment described by the dielectric constant and hydrogen-bonding profiles. The OPM database is freely accessible at http://opm.phar.umich.edu. Data can be sorted, searched or retrieved using the hierarchical classification, source organism, localization in different types of membranes. The database offers downloadable coordinates of proteins and peptides with membrane boundaries. A gallery of protein images and several visualization tools are provided. The database is supplemented by the PPM server (http://opm.phar.umich.edu/server.php) which can be used for calculating spatial positions in membranes of newly determined proteins structures or theoretical models.",Erratum,pro66
pap2214,d57ca29d73272e139c04f118d5c3107dfb964596,con43,IEEE International Conference on Software Maintenance and Evolution,Survey of graph database models,"Graph database models can be defined as those in which data structures for the schema and instances are modeled as graphs or generalizations of them, and data manipulation is expressed by graph-oriented operations and type constructors. These models took off in the eighties and early nineties alongside object-oriented models. Their influence gradually died out with the emergence of other database models, in particular geographical, spatial, semistructured, and XML. Recently, the need to manage information with graph-like nature has reestablished the relevance of this area. The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints.",Erratum,pro43
pap2215,e0867d523f610b32267fa7ec35a510936b8b595f,jou345,IEEE Transactions on Affective Computing,DISFA: A Spontaneous Facial Action Intensity Database,"Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.",Article,vol345
pap2216,36698b7863c7d64cf3fb9d17d0f4df1ff6d286dc,con110,Very Large Data Bases Conference,Measuring Financial Inclusion: The Global Findex Database,"This paper provides the first analysis of the Global Financial Inclusion (Global Findex) Database, a new set of indicators that measure how adults in 148 economies save, borrow, make payments, and manage risk. The data show that 50 percent of adults worldwide have an account at a formal financial institution, though account penetration varies widely across regions, income groups and individual characteristics. In addition, 22 percent of adults report having saved at a formal financial institution in the past 12 months, and 9 percent report having taken out a new loan from a bank, credit union or microfinance institution in the past year. Although half of adults around the world remain unbanked, at least 35 percent of them report barriers to account use that might be addressed by public policy. Among the most commonly reported barriers are high cost, physical distance, and lack of proper documentation, though there are significant differences across regions and individual characteristics.",Erratum,pro110
pap2217,6787a08978444d97d6860359ba24b5a735d492e2,con70,International Conference on Graph Transformation,RegNetwork: an integrated database of transcriptional and post-transcriptional regulatory networks in human and mouse,"Transcriptional and post-transcriptional regulation of gene expression is of fundamental importance to numerous biological processes. Nowadays, an increasing amount of gene regulatory relationships have been documented in various databases and literature. However, to more efficiently exploit such knowledge for biomedical research and applications, it is necessary to construct a genome-wide regulatory network database to integrate the information on gene regulatory relationships that are widely scattered in many different places. Therefore, in this work, we build a knowledge-based database, named ‘RegNetwork’, of gene regulatory networks for human and mouse by collecting and integrating the documented regulatory interactions among transcription factors (TFs), microRNAs (miRNAs) and target genes from 25 selected databases. Moreover, we also inferred and incorporated potential regulatory relationships based on transcription factor binding site (TFBS) motifs into RegNetwork. As a result, RegNetwork contains a comprehensive set of experimentally observed or predicted transcriptional and post-transcriptional regulatory relationships, and the database framework is flexibly designed for potential extensions to include gene regulatory networks for other organisms in the future. Based on RegNetwork, we characterized the statistical and topological properties of genome-wide regulatory networks for human and mouse, we also extracted and interpreted simple yet important network motifs that involve the interplays between TF-miRNA and their targets. In summary, RegNetwork provides an integrated resource on the prior information for gene regulatory relationships, and it enables us to further investigate context-specific transcriptional and post-transcriptional regulatory interactions based on domain-specific experimental data. Database URL: http://www.regnetworkweb.org",Erratum,pro70
pap2218,5637a5e6cf3a858a8adc17efdf714ce21dbe1a2e,con89,Conference on Uncertainty in Artificial Intelligence,Mouse Genome Database (MGD)-2018: knowledgebase for the laboratory mouse,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the key community mouse database which supports basic, translational and computational research by providing integrated data on the genetics, genomics, and biology of the laboratory mouse. MGD serves as the source for biological reference data sets related to mouse genes, gene functions, phenotypes and disease models with an increasing emphasis on the association of these data to human biology and disease. We report here on recent enhancements to this resource, including improved access to mouse disease model and human phenotype data and enhanced relationships of mouse models to human disease.",Erratum,pro89
pap2219,ad9a8e6346a1259022abf526670eaf592270e59f,con100,International Conference on Automatic Face and Gesture Recognition,Mouse Genome Database (MGD)-2017: community knowledge resource for the laboratory mouse,"The Mouse Genome Database (MGD: http://www.informatics.jax.org) is the primary community data resource for the laboratory mouse. It provides a highly integrated and highly curated system offering a comprehensive view of current knowledge about mouse genes, genetic markers and genomic features as well as the associations of those features with sequence, phenotypes, functional and comparative information, and their relationships to human diseases. MGD continues to enhance access to these data, to extend the scope of data content and visualizations, and to provide infrastructure and user support that ensures effective and efficient use of MGD in the advancement of scientific knowledge. Here, we report on recent enhancements made to the resource and new features.",Erratum,pro100
pap2220,7d25222fcf62dae782ae6b18fae0b33d9f7923fe,jou221,Human Mutation,Human Gene Mutation Database (HGMD®): 2003 update,"The Human Gene Mutation Database (HGMD) constitutes a comprehensive core collection of data on germ‐line mutations in nuclear genes underlying or associated with human inherited disease (www.hgmd.org). Data catalogued includes: single base‐pair substitutions in coding, regulatory and splicing‐relevant regions; micro‐deletions and micro‐insertions; indels; triplet repeat expansions as well as gross deletions; insertions; duplications; and complex rearrangements. Each mutation is entered into HGMD only once in order to avoid confusion between recurrent and identical‐by‐descent lesions. By March 2003, the database contained in excess of 39,415 different lesions detected in 1,516 different nuclear genes, with new entries currently accumulating at a rate exceeding 5,000 per annum. Since its inception, HGMD has been expanded to include cDNA reference sequences for more than 87% of listed genes, splice junction sequences, disease‐associated and functional polymorphisms, as well as links to data present in publicly available online locus‐specific mutation databases. Although HGMD has recently entered into a licensing agreement with Celera Genomics (Rockville, MD), mutation data will continue to be made freely available via the Internet. Hum Mutat 21:577–581, 2003. © 2003 Wiley‐Liss, Inc.",Conference paper,vol221
pap2221,a094ea0a295db1b4d86d504cd885fde5d3396c75,con32,International Conference on Software Technology: Methods and Tools,The RDP-II (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [Nucleic Acids Res. (2000), 28, 173-174], continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 8.0 (June 1, 2000) consisted of 16 277 aligned prokaryotic small subunit (SSU) rRNA sequences while the number of eukaryotic and mitochondrial SSU rRNA sequences in aligned form remained at 2055 and 1503, respectively. The number of prokaryotic SSU rRNA sequences more than doubled from the previous release 14 months earlier, and approximately 75% are longer than 899 bp. An RDP-II mirror site in Japan is now available (http://wdcm.nig.ac.jp/RDP/html/index.h tml). RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/). Analysis services include rRNA probe checking, approximate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment polymorphism experiments. The RDP-II email address for questions and comments has been changed from curator@cme.msu.edu to rdpstaff@msu.edu.",Erratum,pro32
pap2222,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,con27,International Conference on Contemporary Computing,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",Erratum,pro27
pap2223,355f5e57eab748d07ec768e4c07b06dfd66a3ba5,con53,Workshop on Web 2.0 for Software Engineering,BindingDB: a web-accessible database of experimentally determined protein–ligand binding affinities,"BindingDB () is a publicly accessible database currently containing ∼20 000 experimentally determined binding affinities of protein–ligand complexes, for 110 protein targets including isoforms and mutational variants, and ∼11 000 small molecule ligands. The data are extracted from the scientific literature, data collection focusing on proteins that are drug-targets or candidate drug-targets and for which structural data are present in the Protein Data Bank. The BindingDB website supports a range of query types, including searches by chemical structure, substructure and similarity; protein sequence; ligand and protein names; affinity ranges and molecular weight. Data sets generated by BindingDB queries can be downloaded in the form of annotated SDfiles for further analysis, or used as the basis for virtual screening of a compound database uploaded by the user. The data in BindingDB are linked both to structural data in the PDB via PDB IDs and chemical and sequence searches, and to the literature in PubMed via PubMed IDs.",Erratum,pro53
pap2224,ef12383f516840ec1ec998cd5921dfc6e197c9b2,con99,North American Chapter of the Association for Computational Linguistics,PPDB: The Paraphrase Database,"We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.",Letter,pro99
pap2225,cc589c499dcf323fe4a143bbef0074c3e31f9b60,con100,International Conference on Automatic Face and Gesture Recognition,A 3D facial expression database for facial behavior research,"Traditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluation",Letter,pro100
pap2226,0afa75ad56cc8ca3cfa176f89443e9a70e09434c,jou43,Social Science Research Network,Systemic Banking Crises Database: An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",Article,vol43
pap2227,a057d7736e3ae4675054a104a1301dba2ff8dbba,con99,North American Chapter of the Association for Computational Linguistics,The Pfam protein families database,"Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allowing Pfam domain definitions to be closer to those found in structure databases. Pfam is available on the web in the UK (http://www.sanger.ac.uk/Software/Pfam/), the USA (http://pfam.wustl.edu/), France (http://pfam.jouy.inra.fr/) and Sweden (http://Pfam.cgb.ki.se/).",Erratum,pro99
pap2228,788b43b7c62b497cf69b31544c6f81c6f4856d42,con71,Annual Conference on Innovation and Technology in Computer Science Education,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",Erratum,pro71
pap2229,e3bc4caca9a5115c61281acb99ab9b978edd6387,jou364,ACM Transactions on Graphics,The sketchy database,"We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.",Letter,vol364
pap2230,dc85ca80fb3d75fe63106f631a2f7cd251d2851e,jou262,Nature Genetics,Identification of protein coding regions by database similarity search,,Article,vol262
pap2231,3962ddcab496d4a2a7196dcef370c87f58a3133d,con6,Annual Conference on Genetic and Evolutionary Computation,The NCBI Taxonomy database,"The NCBI Taxonomy database (http://www.ncbi.nlm.nih.gov/taxonomy) is the standard nomenclature and classification repository for the International Nucleotide Sequence Database Collaboration (INSDC), comprising the GenBank, ENA (EMBL) and DDBJ databases. It includes organism names and taxonomic lineages for each of the sequences represented in the INSDC’s nucleotide and protein sequence databases. The taxonomy database is manually curated by a small group of scientists at the NCBI who use the current taxonomic literature to maintain a phylogenetic taxonomy for the source organisms represented in the sequence databases. The taxonomy database is a central organizing hub for many of the resources at the NCBI, and provides a means for clustering elements within other domains of NCBI web site, for internal linking between domains of the Entrez system and for linking out to taxon-specific external resources on the web. Our primary purpose is to index the domain of sequences as conveniently as possible for our user community.",Erratum,pro6
pap2232,a0e5802cf66257d0412de878682dc9caccca0719,con9,Big Data,Rfam: an RNA family database,"Rfam is a collection of multiple sequence alignments and covariance models representing non-coding RNA families. Rfam is available on the web in the UK at http://www.sanger.ac.uk/Software/Rfam/ and in the US at http://rfam.wustl.edu/. These websites allow the user to search a query sequence against a library of covariance models, and view multiple sequence alignments and family annotation. The database can also be downloaded in flatfile form and searched locally using the INFERNAL package (http://infernal.wustl.edu/). The first release of Rfam (1.0) contains 25 families, which annotate over 50 000 non-coding RNA genes in the taxonomic divisions of the EMBL nucleotide database.",Erratum,pro9
pap2233,c2b381b24aabf237394059fed7920cd6fd0e67b8,jou3,IEEE Transactions on Knowledge and Data Engineering,Database Mining: A Performance Perspective,"The authors' perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology is presented. Three classes of database mining problems involving classification, associations, and sequences are described. It is argued that these problems can be uniformly viewed as requiring discovery of rules embedded in massive amounts of data. A model and some basic operations for the process of rule discovery are described. It is shown how the database mining problems considered map to this model, and how they can be solved by using the basic operations proposed. An example is given of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm is efficient in discovering classification rules and has accuracy comparable to ID3, one of the best current classifiers. >",Conference paper,vol3
pap2234,0c13dc68a9b91e94dc23676ec878fa9f1794b866,jou365,Accounts of Chemical Research,"The Reticular Chemistry Structure Resource (RCSR) database of, and symbols for, crystal nets.","During the past decade, interest has grown tremendously in the design and synthesis of crystalline materials constructed from molecular clusters linked by extended groups of atoms. Most notable are metal-organic frameworks (MOFs), in which polyatomic inorganic metal-containing clusters are joined by polytopic linkers. (Although these materials are sometimes referred to as coordination polymers, we prefer to differentiate them, because MOFs are based on strong linkages that yield robust frameworks.) The realization that MOFs could be designed and synthesized in a rational way from molecular building blocks led to the emergence of a discipline that we call reticular chemistry. MOFs can be represented as a special kind of graph called a periodic net. Such descriptions date back to the earliest crystallographic studies but have become much more common recently because thousands of new structures and hundreds of underlying nets have been reported. In the simplest cases (e.g., the structure of diamond), the atoms in the crystal become the vertices of the net, and bonds are the links (edges) that connect them. In the case of MOFs, polyatomic groups act as the vertices and edges of the net. Because of the explosive growth in this area, a need has arisen for a universal system of nomenclature, classification, identification, and retrieval of these topological structures. We have developed a system of symbols for the identification of three periodic nets of interest, and this system is now in wide use. In this Account, we explain the underlying methodology of assigning symbols and describe the Reticular Chemistry Structure Resource (RCSR), in which about 1600 such nets are collected and illustrated in a database that can be searched by symbol, name, keywords, and attributes. The resource also contains searchable data for polyhedra and layers. The database entries come from systematic enumerations or from known chemical compounds or both. In the latter case, references to occurrences are provided. We describe some crystallographic, topological, and other attributes of nets and explain how they are reported in the database. We also describe how the database can be used as a tool for the design and structural analysis of new materials. Associated with each net is a natural tiling, which is a natural partition of space into space-filling tiles. The database allows export of data that can be used to analyze and illustrate such tilings.",Article,vol365
pap2235,987a42cc5a8d7c8536e7e5a308b1ba6aa15d454f,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The UMIST database for astrochemistry 2012,"We present the fifth release of the UMIST Database for Astrochemistry (UDfA). The new reaction network contains 6173 gas-phase reactions, involving 467 species, 47 of which are new to this release. We have updated rate coefficients across all reaction types. We have included 1171 new anion reactions and updated and reviewed all photorates. In addition to the usual reaction network, we also now include, for download, state-specific deuterated rate coefficients, deuterium exchange reactions and a list of surface binding energies for many neutral species. Where possible, we have referenced the original source of all new and existing data. We have tested the main reaction network using a dark cloud model and a carbon-rich circumstellar envelope model. We present and briefly discuss the results of these models.",Erratum,pro85
pap2236,a175453ceff401c67ce503750418e66936331b06,con100,International Conference on Automatic Face and Gesture Recognition,NCBI GEO: mining tens of millions of expression profiles—database and tools update,"The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray and other forms of high-throughput data generated by the scientific community. The database has a minimum information about a microarray experiment (MIAME)-compliant infrastructure that captures fully annotated raw and processed data. Several data deposit options and formats are supported, including web forms, spreadsheets, XML and Simple Omnibus Format in Text (SOFT). In addition to data storage, a collection of user-friendly web-based interfaces and applications are available to help users effectively explore, visualize and download the thousands of experiments and tens of millions of gene expression patterns stored in GEO. This paper provides a summary of the GEO database structure and user facilities, and describes recent enhancements to database design, performance, submission format options, data query and retrieval utilities. GEO is accessible at",Erratum,pro100
pap2237,b02d2cf90e5b06e3278e23e7984c29b0307c5ef3,con26,Decision Support Systems,Principles of database and knowledge- base systems,,Erratum,pro26
pap2238,ffc8c09a4cb5a174166ac16d19e646bc150c2c31,con4,Conference on Innovative Data Systems Research,Phone Recognition on the TIMIT Database,",",Erratum,pro4
pap2239,187f31d400d711e25fd3cddd195f0af730729912,con16,International Conference on Data Science and Advanced Analytics,Cloud-Screening and Quality Control Algorithms for the AERONET Database,,Erratum,pro16
pap2240,d53bcbac7ea19173e95d3bd855b998fab765737d,con19,International Conference on Conceptual Structures,WordNet: An Electronic Lexical Database,,Erratum,pro19
pap2241,854b86751d0d02bf205c6fb4bf2f2d13804430ee,jou366,Journal of Human Genetics,"Human genetic variation database, a reference database of genetic variations in the Japanese population",,Article,vol366
pap2242,7bbe0235f583b27f96c0f288043876f86795d6c2,con59,Annual Workshop of the Psychology of Programming Interest Group,AVA: A large-scale database for aesthetic visual analysis,"With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.",Erratum,pro59
pap2243,a35934a0b4c63925ee27d2bd77c75b31cf6d8072,con71,Annual Conference on Innovation and Technology in Computer Science Education,VFDB: a reference database for bacterial virulence factors,"Bacterial pathogens continue to impose a major threat to public health worldwide in the 21st century. Intensified studies on bacterial pathogenesis have greatly expanded our knowledge about the mechanisms of the disease processes at the molecular level over the last decades. To facilitate future research, it becomes necessary to form a database collectively presenting the virulence factors (VFs) of various medical significant bacterial pathogens. The aim of virulence factor database (VFDB) (http://www.mgc.ac.cn/VFs/) is to provide such a source for scientists to rapidly access to current knowledge about VFs from various bacterial pathogens. VFDB is comprehensive and user-friendly. One can search VFDB by browsing each genus or by typing keywords. Furthermore, a BLAST search tool against all known VF-related genes is also available. VFDB provides a unified gateway to store, search, retrieve and update information about VFs from various bacterial pathogens.",Erratum,pro71
pap2244,6827419c860e6b9c825a1138d48633ecef00d4d5,con15,Pacific Symposium on Biocomputing,HPIDB 2.0: a curated database for host–pathogen interactions,"Identification and analysis of host–pathogen interactions (HPI) is essential to study infectious diseases. However, HPI data are sparse in existing molecular interaction databases, especially for agricultural host–pathogen systems. Therefore, resources that annotate, predict and display the HPI that underpin infectious diseases are critical for developing novel intervention strategies. HPIDB 2.0 (http://www.agbase.msstate.edu/hpi/main.html) is a resource for HPI data, and contains 45, 238 manually curated entries in the current release. Since the first description of the database in 2010, multiple enhancements to HPIDB data and interface services were made that are described here. Notably, HPIDB 2.0 now provides targeted biocuration of molecular interaction data. As a member of the International Molecular Exchange consortium, annotations provided by HPIDB 2.0 curators meet community standards to provide detailed contextual experimental information and facilitate data sharing. Moreover, HPIDB 2.0 provides access to rapidly available community annotations that capture minimum molecular interaction information to address immediate researcher needs for HPI network analysis. In addition to curation, HPIDB 2.0 integrates HPI from existing external sources and contains tools to infer additional HPI where annotated data are scarce. Compared to other interaction databases, our data collection approach ensures HPIDB 2.0 users access the most comprehensive HPI data from a wide range of pathogens and their hosts (594 pathogen and 70 host species, as of February 2016). Improvements also include enhanced search capacity, addition of Gene Ontology functional information, and implementation of network visualization. The changes made to HPIDB 2.0 content and interface ensure that users, especially agricultural researchers, are able to easily access and analyse high quality, comprehensive HPI data. All HPIDB 2.0 data are updated regularly, are publically available for direct download, and are disseminated to other molecular interaction resources. Database URL: http://www.agbase.msstate.edu/hpi/main.html",Erratum,pro15
pap2245,0a4365f6c30d40ca7610e9afac6c339f3c72224c,jou367,Pharmacoepidemiology and Drug Safety,Validation of the national health insurance research database with ischemic stroke cases in Taiwan,The National Health Insurance Research Database (NHIRD) is commonly used for pharmacoepidemiological research in Taiwan. This study evaluated the validity of the database for patients with a principal diagnosis of ischemic stroke.,Letter,vol367
pap2246,1e9e0538ae54be22a515430199dab0befb9e29cb,con11,European Conference on Modelling and Simulation,Database: The Journal of Biological Databases and Curation,"Evolution provides the unifying framework with which to understand biology. The coherent investigation of genic and genomic data often requires comparative genomics analyses based on whole-genome alignments, sets of homologous genes and other relevant datasets in order to evaluate and answer evolutionary-related questions. However, the complexity and computational requirements of producing such data are substantial: this has led to only a small number of reference resources that are used for most comparative analyses. The Ensembl comparative genomics resources are one such reference set that facilitates comprehensive and reproducible analysis of chordate genome data. Ensembl computes pairwise and multiple whole-genome alignments from which large-scale synteny, per-base conservation scores and constrained elements are obtained. Gene alignments are used to define Ensembl Protein Families, GeneTrees and homologies for both protein-coding and non-coding RNA genes. These resources are updated frequently and have a consistent informatics infrastructure and data presentation across all supported species. Specialized web-based visualizations are also available including synteny displays, collapsible gene tree plots, a gene family locator and different alignment views. The Ensembl comparative genomics infrastructure is extensively reused for the analysis of non-vertebrate species by other projects including Ensembl Genomes and Gramene and much of the information here is relevant to these projects. The consistency of the annotation across species and the focus on vertebrates makes Ensembl an ideal system to perform and support vertebrate comparative genomic analyses. We use robust software and pipelines to produce reference comparative data and make it freely available.Database URL: http://www.ensembl.org.",Erratum,pro11
pap2247,768b97bd5a8d4944be4a6f71b12ffad372a6ce5e,con74,IEEE International Conference on Information Reuse and Integration,The UCSC Genome Browser database: extensions and updates 2013,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic datasets. As of September 2012, genomic sequence and a basic set of annotation ‘tracks’ are provided for 63 organisms, including 26 mammals, 13 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms, yeast and sea hare. In the past year 19 new genome assemblies have been added, and we anticipate releasing another 28 in early 2013. Further, a large number of annotation tracks have been either added, updated by contributors or remapped to the latest human reference genome. Among these are an updated UCSC Genes track for human and mouse assemblies. We have also introduced several features to improve usability, including new navigation menus. This article provides an update to the UCSC Genome Browser database, which has been previously featured in the Database issue of this journal.",Erratum,pro74
pap2248,bcc73dd05b7b7a6616c41df428e3624375c95e56,con36,Central and Eastern European Software Engineering Conference in Russia,ATtRACT—a database of RNA-binding proteins and associated motifs,"RNA-binding proteins (RBPs) play a crucial role in key cellular processes, including RNA transport, splicing, polyadenylation and stability. Understanding the interaction between RBPs and RNA is key to improve our knowledge of RNA processing, localization and regulation in a global manner. Despite advances in recent years, a unified non-redundant resource that includes information on experimentally validated motifs, RBPs and integrated tools to exploit this information is lacking. Here, we developed a database named ATtRACT (available at http://attract.cnic.es) that compiles information on 370 RBPs and 1583 RBP consensus binding motifs, 192 of which are not present in any other database. To populate ATtRACT we (i) extracted and hand-curated experimentally validated data from CISBP-RNA, SpliceAid–F, RBPDB databases, (ii) integrated and updated the unavailable ASD database and (iii) extracted information from Protein-RNA complexes present in Protein Data Bank database through computational analyses. ATtRACT provides also efficient algorithms to search a specific motif and scan one or more RNA sequences at a time. It also allows discovering de novo motifs enriched in a set of related sequences and compare them with the motifs included in the database. Database URL: http:// attract. cnic. es",Erratum,pro36
pap2249,0b9182d502e62fb7e1ebd7e01de7523005d35677,con59,Annual Workshop of the Psychology of Programming Interest Group,The notions of consistency and predicate locks in a database system,"In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints. This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates. An implementation of predicate locks which satisfies the consistency condition is suggested.",Erratum,pro59
pap2250,0ec2a62edef364f4bef1e1b265d3d7869bb4444a,con95,IEEE International Conference on Computer Vision,Database Systems: The Complete Book,"From the Publisher: 
This introduction to database systems offers a readable comprehensive approach with engaging, real-world examplesusers will learn how to successfully plan a database application before building it. The first half of the book provides in-depth coverage of databases from the point of view of the database designer, user, and application programmer, while the second half of the book provides in-depth coverage of databases from the point of view of the DBMS implementor. The first half of the book focuses on database design, database use, and implementation of database applications and database management systemsit covers the latest database standards SQL:1999, SQL/PSM, SQL/CLI, JDBC, ODL, and XML, with broader coverage of SQL than most other books. The second half of the book focuses on storage structures, query processing, and transaction managementit covers the main techniques in these areas with broader coverage of query optimization than most other books, along with advanced topics including multidimensional and bitmap indexes, distributed transactions, and information integration techniques. A professional reference for database designers, users, and application programmers.",Erratum,pro95
pap2251,148c4770b5fc2f841179f4c4f800f41c2171841a,con104,Biometrics and Identity Management,A New Database on the Structure and Development of the Financial Sector,"This article introduces a new database of indicators of financial structure and financial development across countries and over time. The database is unique in that it combines a wide variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, introducing indicators of the size and activity of nonbank financial institutions, and constructing measures of the size of bond and primary equity markets. This article introduces a new database, the first to provide comprehensive measures of the development, structure, and performance of the financial sector. This database is the first to define and construct indicators of the size and activity of nonbank financial intermediaries, such as insurance companies, pension funds, and non-deposit money banks. It is also the first to include indicators of the size of primary equity markets and primary and secondary bond markets. In constructing the database, authors carefully deflate measures and match stock and flow variables.",Erratum,pro104
pap2252,ff2218b349f89026ffaaccdf807228fa497c04bd,con99,North American Chapter of the Association for Computational Linguistics,THE DIGITAL DATABASE FOR SCREENING MAMMOGRAPHY,,Erratum,pro99
pap2253,49b60b92201710d095684b6df1b1d93f92122dd0,con59,Annual Workshop of the Psychology of Programming Interest Group,MINT: the Molecular INTeraction database,"The Molecular INTeraction database (MINT, ) aims at storing, in a structured format, information about molecular interactions (MIs) by extracting experimental details from work published in peer-reviewed journals. At present the MINT team focuses the curation work on physical interactions between proteins. Genetic or computationally inferred interactions are not included in the database. Over the past four years MINT has undergone extensive revision. The new version of MINT is based on a completely remodeled database structure, which offers more efficient data exploration and analysis, and is characterized by entries with a richer annotation. Over the past few years the number of curated physical interactions has soared to over 95 000. The whole dataset can be freely accessed online in both interactive and batch modes through web-based interfaces and an FTP server. MINT now includes, as an integrated addition, HomoMINT, a database of interactions between human proteins inferred from experiments with ortholog proteins in model organisms ().",Erratum,pro59
pap2254,c6804d16e1f4e3bde4d535384a699e65479634b6,con22,Grid Computing Environments,MODOMICS: a database of RNA modification pathways—2013 update,"MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, RNA-modifying enzymes and location of modified residues in RNA sequences. In the current database version, accessible at http://modomics.genesilico.pl, we included new features: a census of human and yeast snoRNAs involved in RNA-guided RNA modification, a new section covering the 5′-end capping process, and a catalogue of ‘building blocks’ for chemical synthesis of a large variety of modified nucleosides. The MODOMICS collections of RNA modifications, RNA-modifying enzymes and modified RNAs have been also updated. A number of newly identified modified ribonucleosides and more than one hundred functionally and structurally characterized proteins from various organisms have been added. In the RNA sequences section, snRNAs and snoRNAs with experimentally mapped modified nucleosides have been added and the current collection of rRNA and tRNA sequences has been substantially enlarged. To facilitate literature searches, each record in MODOMICS has been cross-referenced to other databases and to selected key publications. New options for database searching and querying have been implemented, including a BLAST search of protein sequences and a PARALIGN search of the collected nucleic acid sequences.",Erratum,pro22
pap2255,ff9ef51fbc40937d38dcea754b43a3e06ca39a01,con70,International Conference on Graph Transformation,EpiFactors: a comprehensive database of human epigenetic factors and complexes,"Epigenetics refers to stable and long-term alterations of cellular traits that are not caused by changes in the DNA sequence per se. Rather, covalent modifications of DNA and histones affect gene expression and genome stability via proteins that recognize and act upon such modifications. Many enzymes that catalyse epigenetic modifications or are critical for enzymatic complexes have been discovered, and this is encouraging investigators to study the role of these proteins in diverse normal and pathological processes. Rapidly growing knowledge in the area has resulted in the need for a resource that compiles, organizes and presents curated information to the researchers in an easily accessible and user-friendly form. Here we present EpiFactors, a manually curated database providing information about epigenetic regulators, their complexes, targets and products. EpiFactors contains information on 815 proteins, including 95 histones and protamines. For 789 of these genes, we include expressions values across several samples, in particular a collection of 458 human primary cell samples (for approximately 200 cell types, in many cases from three individual donors), covering most mammalian cell steady states, 255 different cancer cell lines (representing approximately 150 cancer subtypes) and 134 human postmortem tissues. Expression values were obtained by the FANTOM5 consortium using Cap Analysis of Gene Expression technique. EpiFactors also contains information on 69 protein complexes that are involved in epigenetic regulation. The resource is practical for a wide range of users, including biologists, pharmacologists and clinicians. Database URL: http://epifactors.autosome.ru",Erratum,pro70
pap2256,529124bc3ff7fa655729c49bf10aedaddd49787e,con4,Conference on Innovative Data Systems Research,Phenol-Explorer 3.0: a major update of the Phenol-Explorer database to incorporate data on the effects of food processing on polyphenol content,"Polyphenols are a major class of bioactive phytochemicals whose consumption may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, type II diabetes and cancers. Phenol-Explorer, launched in 2009, is the only freely available web-based database on the content of polyphenols in food and their in vivo metabolism and pharmacokinetics. Here we report the third release of the database (Phenol-Explorer 3.0), which adds data on the effects of food processing on polyphenol contents in foods. Data on >100 foods, covering 161 polyphenols or groups of polyphenols before and after processing, were collected from 129 peer-reviewed publications and entered into new tables linked to the existing relational design. The effect of processing on polyphenol content is expressed in the form of retention factor coefficients, or the proportion of a given polyphenol retained after processing, adjusted for change in water content. The result is the first database on the effects of food processing on polyphenol content and, following the model initially defined for Phenol-Explorer, all data may be traced back to original sources. The new update will allow polyphenol scientists to more accurately estimate polyphenol exposure from dietary surveys. Database URL: http://www.phenol-explorer.eu",Erratum,pro4
pap2257,66c0e24d97786f0382ce9f7acde37de9a349537c,con104,Biometrics and Identity Management,The Reptile Database,,Erratum,pro104
pap2258,0d3e8b3a4bb5ffc8bc0527c443a99eb1438956a0,con101,International Conference on Biometrics,A face antispoofing database with diverse attacks,"Face antispoofing has now attracted intensive attention, aiming to assure the reliability of face biometrics. We notice that currently most of face antispoofing databases focus on data with little variations, which may limit the generalization performance of trained models since potential attacks in real world are probably more complex. In this paper we release a face antispoofing database which covers a diverse range of potential attack variations. Specifically, the database contains 50 genuine subjects, and fake faces are made from the high quality records of the genuine faces. Three imaging qualities are considered, namely the low quality, normal quality and high quality. Three fake face attacks are implemented, which include warped photo attack, cut photo attack and video attack. Therefore each subject contains 12 videos (3 genuine and 9 fake), and the final database contains 600 video clips. Test protocol is provided, which consists of 7 scenarios for a thorough evaluation from all possible aspects. A baseline algorithm is also given for comparison, which explores the high frequency information in the facial region to determine the liveness. We hope such a database can serve as an evaluation platform for future researches in the literature.",Article,pro101
pap2259,8df1903824290707e09ee754f3c40d90b088d65b,con17,International Conference on Statistical and Scientific Database Management,World Database on Protected Areas (WDPA),,Erratum,pro17
pap2260,afef030037e621538ac53e18d23a1a4660df79d8,con0,International Conference on Machine Learning,Harmonized World Soil Database (version 1.2),"METIS-ID: 167825 The Harmonized World Soil Database is a 30 arc-second raster database with over 15000 different soil mapping units that combines existing regional and national updates of soil information worldwide (SOTER, ESD, Soil Map of China, ISRIC-WISE) with the information contained within the 1:5 000 000 scale FAO-UNESCO Soil Map of the World (FAO, 1971-1981). The resulting raster database consists of ... 21600 rows and 43200 columns, which are linked to harmonized soil property data. The use of a standardized structure allows for the linkage of the attribute data with the raster map to display or query the composition in terms of soil units and the characterization of selected soil parameters (organic Carbon, pH, water storage capacity, soil depth, cation exchange capacity of the soil and the clay fraction, total exchangeable nutrients, lime and gypsum contents, sodium exchange percentage, salinity, textural class and granulometry). Reliability of the information contained in the database is variable: the parts of the database that still make use of the Soil Map of the World such as North America, Australia, West Africa and South Asia are considered less reliable, while most of the areas covered by SOTER databases are considered to have the highest reliability (Southern Africa, Latin America and the Caribbean, Central and Eastern Europe). Further expansion and update of the HWSD is foreseen for the near future, notably with the excellent databases held in the USA (Natural Resources Conservation Service US General Soil Map, STATSGO), Canada (Agriculture and AgriFood Canada: The National Soil Database NSDB), and Australia (CSIRO, ACLEP, Nnatural Heritage Trust and National Land and Water Resources Audit: ASRIS), and with the recently released SOTER database for Central Africa (FAO/ISRIC/Univ. Gent, 2007)",Erratum,pro0
pap2261,00899cac0e2e770fe6b28deac002eceb8c3c4bea,con28,International Conference Geographic Information Science,Phenol-Explorer: an online comprehensive database on polyphenol contents in foods,"A number of databases on the plant metabolome describe the chemistry and biosynthesis of plant chemicals. However, no such database is specifically focused on foods and more precisely on polyphenols, one of the major classes of phytochemicals. As antoxidants, polyphenols influence human health and may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, some cancers or type 2 diabetes. To determine polyphenol intake in populations and study their association with health, it is essential to have detailed information on their content in foods. However this information is not easily collected due to the variety of their chemical structures and the variability of their content in a given food. Phenol-Explorer is the first comprehensive web-based database on polyphenol content in foods. It contains more than 37 000 original data points collected from 638 scientific articles published in peer-reviewed journals. The quality of these data has been evaluated before they were aggregated to produce final representative mean content values for 502 polyphenols in 452 foods. The web interface allows making various queries on the aggregated data to identify foods containing a given polyphenol or polyphenols present in a given food. For each mean content value, it is possible to trace all original content values and their literature sources. Phenol-Explorer is a major step forward in the development of databases on food constituents and the food metabolome. It should help researchers to better understand the role of phytochemicals in the technical and nutritional quality of food, and food manufacturers to develop tailor-made healthy foods. Database URL: http://www.phenol-explorer.eu",Erratum,pro28
pap2262,04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59,jou368,International Journal on Document Analysis and Recognition,The IAM-database: an English sentence database for offline handwriting recognition,,Letter,vol368
pap2263,0d2072d81d03247949126e87d6201788cb646526,con30,PS,"The PROSITE database, its status in 2002","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.",Erratum,pro30
pap2264,756686211e7d253ac7abb8cb9e290141880f0889,jou229,Plant and Cell Physiology,Rice Annotation Project Database (RAP-DB): An Integrative and Interactive Database for Rice Genomics,"The Rice Annotation Project Database (RAP-DB, http://rapdb.dna.affrc.go.jp/) has been providing a comprehensive set of gene annotations for the genome sequence of rice, Oryza sativa (japonica group) cv. Nipponbare. Since the first release in 2005, RAP-DB has been updated several times along with the genome assembly updates. Here, we present our newest RAP-DB based on the latest genome assembly, Os-Nipponbare-Reference-IRGSP-1.0 (IRGSP-1.0), which was released in 2011. We detected 37,869 loci by mapping transcript and protein sequences of 150 monocot species. To provide plant researchers with highly reliable and up to date rice gene annotations, we have been incorporating literature-based manually curated data, and 1,626 loci currently incorporate literature-based annotation data, including commonly used gene names or gene symbols. Transcriptional activities are shown at the nucleotide level by mapping RNA-Seq reads derived from 27 samples. We also mapped the Illumina reads of a Japanese leading japonica cultivar, Koshihikari, and a Chinese indica cultivar, Guangluai-4, to the genome and show alignments together with the single nucleotide polymorphisms (SNPs) and gene functional annotations through a newly developed browser, Short-Read Assembly Browser (S-RAB). We have developed two satellite databases, Plant Gene Family Database (PGFD) and Integrative Database of Cereal Gene Phylogeny (IDCGP), which display gene family and homologous gene relationships among diverse plant species. RAP-DB and the satellite databases offer simple and user-friendly web interfaces, enabling plant and genome researchers to access the data easily and facilitating a broad range of plant research topics.",Article,vol229
pap2265,e8ddd8c820dba886f618f0e84ce38ecd0b967b39,con2,International Conference on Software Engineering,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",Erratum,pro2
pap2266,607ca163b2635f9992e773d1b1d07d39d5d33f4e,jou5,Genome Biology,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",,Letter,vol5
pap2267,c80b987fe2d52214772f435417cb6666f60613d2,con74,IEEE International Conference on Information Reuse and Integration,An Introduction to Database Systems,"From the Publisher: 
For over 25 years, C. J. Date's An Introduction to Database Systems has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of database systems. This revision continues to provide a solid grounding in the foundations of database technology and to provide some ideas as to how the field is likely to develop in the future.. ""Readers of this book will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems.",Erratum,pro74
pap2268,2bb7426e6ecdab0f120c89f6a324cf0c2a7266d4,con81,International Conference on Learning Representations,BrainWeb: Online Interface to a 3D MRI Simulated Brain Database,"Introduction: The increased importance of automated computer techniques for anatomical brain mapping from MR images and quantitative brain image analysis methods leads to an increased need for validation and evaluation of the effect of image acquisition parameters on performance of these procedures. Validation of analysis techniques of in-vivo acquired images is complicated due to the lack of reference data (“ground truth”). Also, optimal selection of the MR imaging parameters is difficult due to the large parameter space. BrainWeb makes available to the neuroimaging community, online on WWW, a set of realistic simulated brain MR image volumes (Simulated Brain Database, SBD) that allows the above issues to be examined in a controlled, systematic way.",Erratum,pro81
pap2269,f707571329e1aa7180a1a289b1aa250eabdc8618,jou106,Nucleic Acids Research,InterPro in 2011: new developments in the family and domain prediction database,"InterPro (http://www.ebi.ac.uk/interpro/) is a database that integrates diverse information about protein families, domains and functional sites, and makes it freely available to the public via Web-based interfaces and services. Central to the database are diagnostic models, known as signatures, against which protein sequences can be searched to determine their potential function. InterPro has utility in the large-scale analysis of whole genomes and meta-genomes, as well as in characterizing individual protein sequences. Herein we give an overview of new developments in the database and its associated software since 2009, including updates to database content, curation processes and Web and programmatic interfaces.",Article,vol106
pap2270,93c5bb23406778eb421a3dfa5978a231e022792a,jou369,Academic Radiology,INbreast: toward a full-field digital mammographic database.,,Article,vol369
pap2271,334511feb95634c91d06359fd497d01fc60767f7,con89,Conference on Uncertainty in Artificial Intelligence,DIP: The Database of Interacting Proteins: 2001 update,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla. edu) is a database that documents experimentally determined protein-protein interactions. Since January 2000 the number of protein-protein interactions in DIP has nearly tripled to 3472 and the number of proteins to 2659. New interactive tools have been developed to aid in the visualization, navigation and study of networks of protein interactions.",Erratum,pro89
pap2272,2129930aa4803610ea25860e770cbc73440acbf4,jou299,"Proteins: Structure, Function, and Bioinformatics",Database of homology‐derived protein structures and the structural meaning of sequence alignment,"The database of known protein three‐dimensional structures can be significantly increased by the use of sequence homology, based on the following observations. (1) The database of known sequences, currently at more than 12,000 proteins, is two orders of magnitude larger than the database of known structures. (2) The currently most powerful method of predicting protein structures is model building by homology. (3) Structural homology can be inferred from the level of sequence similarity. (4) The threshold of sequence similarity sufficient for structural homology depends strongly on the length of the alignment. Here, we first quantify the relation between sequence similarity, structure similarity, and alignment length by an exhaustive survey of alignments between proteins of known structure and report a homology threshold curve as a function of alignment length. We then produce a database of homology‐derived secondary structure of proteins (HSSP) by aligning to each protein of known structure all sequences deemed homologous on the basis of the threshold curve. For each known protein structure, the derived database contains the aligned sequences, secondary structure, sequence variability, and sequence profile. Tertiary structures of the aligned sequences are implied, but not modeled explicity. The database effectively increases the number of known protein structures by a factor of five to more than 1800. The results may be useful in assessing the structural significance of matches in sequence database searches, in deriving preferences and patterns for structure prediction, in elucidating the structural role of conserved residues, and in modeling three‐dimensional detail by homology.",Letter,vol299
pap2273,acd36b17c486957cebacc3ad68bd83ed417bf9cc,jou221,Human Mutation,The IARC TP53 database: New online mutation analysis and recommendations to users,"Mutations in the tumor suppressor gene TP53 are frequent in most human cancers. Comparison of the mutation patterns in different cancers may reveal clues on the natural history of the disease. Over the past 10 years, several databases of TP53 mutations have been developed. The most extensive of these databases is maintained and developed at the International Agency for Research on Cancer. The database compiles all mutations (somatic and inherited), as well as polymorphisms, that have been reported in the published literature since 1989. The IARC TP53 mutation dataset is the largest dataset available on the variations of any human gene. The database is available at www.iarc.fr/P53/. In this paper, we describe recent developments of the database. These developments include restructuring of the database, which is now patient‐centered, with more detailed annotations on the patient (carcinogen exposure, virus infection, genetic background). In addition, a new on‐line application to retrieve somatic mutation data and analyze mutation patterns is now available. We also discuss limitations on the use of the database and provide recommendations to users. Hum Mutat 19:607–614, 2002. © 2002 Wiley‐Liss, Inc.",Article,vol221
pap2274,e03a1c0f0a8d95aa84bb21f2aa95e5053e4cb655,con105,British Machine Vision Conference,miR2Disease: a manually curated database for microRNA deregulation in human disease,"‘miR2Disease’, a manually curated database, aims at providing a comprehensive resource of microRNA deregulation in various human diseases. The current version of miR2Disease documents 1939 curated relationships between 299 human microRNAs and 94 human diseases by reviewing more than 600 published papers. Around one-seventh of the microRNA–disease relationships represent the pathogenic roles of deregulated microRNA in human disease. Each entry in the miR2Disease contains detailed information on a microRNA–disease relationship, including a microRNA ID, the disease name, a brief description of the microRNA–disease relationship, an expression pattern of the microRNA, the detection method for microRNA expression, experimentally verified target gene(s) of the microRNA and a literature reference. miR2Disease provides a user-friendly interface for a convenient retrieval of each entry by microRNA ID, disease name, or target gene. In addition, miR2Disease offers a submission page that allows researchers to submit established microRNA–disease relationships that are not documented. Once approved by the submission review committee, the submitted records will be included in the database. miR2Disease is freely available at http://www.miR2Disease.org.",Erratum,pro105
pap2275,feae36b19d8f8566aefbb8b30e9fb55c1592f0a7,jou262,Nature Genetics,A gene expression database for the molecular pharmacology of cancer,,Letter,vol262
pap2276,aec58dd1363d679e9e1d7917d74c0bcde504b65f,jou370,Novartis Foundation symposium,The KEGG database.,"KEGG (http://www.genome.ad.jp/kegg/) is a suite of databases and associated software for understanding and simulating higher-order functional behaviours of the cell or the organism from its genome information. First, KEGG computerizes data and knowledge on protein interaction networks (PATHWAY database) and chemical reactions (LIGAND database) that are responsible for various cellular processes. Second, KEGG attempts to reconstruct protein interaction networks for all organisms whose genomes are completely sequenced (GENES and SSDB databases). Third, KEGG can be utilized as reference knowledge for functional genomics (EXPRESSION database) and proteomics (BRITE database) experiments. I will review the current status of KEGG and report on new developments in graph representation and graph computations.",Conference paper,vol370
pap2277,3e83d54c5e8dfba82638b4f75ace31505ea60ff0,con78,Neural Information Processing Systems,The IntAct molecular interaction database in 2010,"IntAct is an open-source, open data molecular interaction database and toolkit. Data is abstracted from the literature or from direct data depositions by expert curators following a deep annotation model providing a high level of detail. As of September 2009, IntAct contains over 200.000 curated binary interaction evidences. In response to the growing data volume and user requests, IntAct now provides a two-tiered view of the interaction data. The search interface allows the user to iteratively develop complex queries, exploiting the detailed annotation with hierarchical controlled vocabularies. Results are provided at any stage in a simplified, tabular view. Specialized views then allows ‘zooming in’ on the full annotation of interactions, interactors and their properties. IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",Erratum,pro78
pap2278,b7340682cd94e4a40df2823cee68ff166be93f86,con51,Brazilian Symposium on Software Engineering,The ConsensusPathDB interaction database: 2013 update,"Knowledge of the various interactions between molecules in the cell is crucial for understanding cellular processes in health and disease. Currently available interaction databases, being largely complementary to each other, must be integrated to obtain a comprehensive global map of the different types of interactions. We have previously reported the development of an integrative interaction database called ConsensusPathDB (http://ConsensusPathDB.org) that aims to fulfill this task. In this update article, we report its significant progress in terms of interaction content and web interface tools. ConsensusPathDB has grown mainly due to the integration of 12 further databases; it now contains 215 541 unique interactions and 4601 pathways from overall 30 databases. Binary protein interactions are scored with our confidence assessment tool, IntScore. The ConsensusPathDB web interface allows users to take advantage of these integrated interaction and pathway data in different contexts. Recent developments include pathway analysis of metabolite lists, visualization of functional gene/metabolite sets as overlap graphs, gene set analysis based on protein complexes and induced network modules analysis that connects a list of genes through various interaction types. To facilitate the interactive, visual interpretation of interaction and pathway data, we have re-implemented the graph visualization feature of ConsensusPathDB using the Cytoscape.js library.",Erratum,pro51
pap2279,a6e72ff479fb58f0b714f07b0292c612dfe4ff05,con66,International Conference on Software Reuse,Principles Of Database And Knowledge-Base Systems,"This book goes into the details of database conception and use, it tells you everything on relational databases. from theory to the actual used algorithms.",Erratum,pro66
pap2280,c378df148d9e42376e4f47888b04a1f679d2e25f,con46,Software Product Lines Conference,XCOM : Photon Cross Sections Database,,Erratum,pro46
pap2281,30ae1edd4e6f13a28f87ec150c407e6820c7da60,jou371,BMC Cancer,PROGgeneV2: enhancements on the existing database,,Conference paper,vol371
pap2282,03cea6194b4d402f83f48382cbaf52b369d3d700,con91,Symposium on the Theory of Computing,Physical Database Design for Relational Databases,,Erratum,pro91
pap2283,3c2d95624ede725cc629cfe63affb57237f009f7,con64,British Computer Society Conference on Human-Computer Interaction,A New Database of Financial Reforms,,Erratum,pro64
pap2284,cd4ef5b8da1a543871357f4bca7483e89ff9e3b5,con0,International Conference on Machine Learning,CDD: a Conserved Domain Database for protein classification,"The Conserved Domain Database (CDD) is the protein classification component of NCBI's Entrez query and retrieval system. CDD is linked to other Entrez databases such as Proteins, Taxonomy and PubMed®, and can be accessed at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. CD-Search, which is available at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi, is a fast, interactive tool to identify conserved domains in new protein sequences. CD-Search results for protein sequences in Entrez are pre-computed to provide links between proteins and domain models, and computational annotation visible upon request. Protein–protein queries submitted to NCBI's BLAST search service at http://www.ncbi.nlm.nih.gov/BLAST are scanned for the presence of conserved domains by default. While CDD started out as essentially a mirror of publicly available domain alignment collections, such as SMART, Pfam and COG, we have continued an effort to update, and in some cases replace these models with domain hierarchies curated at the NCBI. Here, we report on the progress of the curation effort and associated improvements in the functionality of the CDD information retrieval system.",Erratum,pro0
pap2285,ab99f2afd0f09ee707540022f0895a09fa1107f1,con89,Conference on Uncertainty in Artificial Intelligence,Database resources of the National Center for Biotechnology,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI's Web site. NCBI resources include Entrez, PubMed, PubMed Central (PMC), LocusLink, the NCBITaxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR (e-PCR), Open Reading Frame (ORF) Finder, References Sequence (RefSeq), UniGene, HomoloGene, ProtEST, Database of Single Nucleotide Polymorphisms (dbSNP), Human/Mouse Homology Map, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker (MM), Evidence Viewer (EV), Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD), and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",Erratum,pro89
pap2286,42a77b1006675dc954495459ba3bf9c194258c04,con8,Frontiers in Education Conference,BIND: the Biomolecular Interaction Network Database,"The Biomolecular Interaction Network Database (BIND: http://bind.ca) archives biomolecular interaction, complex and pathway information. A web-based system is available to query, view and submit records. BIND continues to grow with the addition of individual submissions as well as interaction data from the PDB and a number of large-scale interaction and complex mapping experiments using yeast two hybrid, mass spectrometry, genetic interactions and phage display. We have developed a new graphical analysis tool that provides users with a view of the domain composition of proteins in interaction and complex records to help relate functional domains to protein interactions. An interaction network clustering tool has also been developed to help focus on regions of interest. Continued input from users has helped further mature the BIND data specification, which now includes the ability to store detailed information about genetic interactions. The BIND data specification is available as ASN.1 and XML DTD.",Erratum,pro8
pap2287,e606ccf581b507149e4bdaba972ab58682eef57b,con101,International Conference on Biometrics,The SIMBAD astronomical database. The CDS reference database for astronomical objects,"Simbad is the reference database for identification and bibliography of astronomical objects. It contains identifications, “basic data”, bibliography, and selected observational measurements for several million astronomical objects.  Simbad is developed and maintained by CDS, Strasbourg. Building the database contents is achieved with the help of several contributing institutes. Scanning the bibliography is the result of the collaboration of CDS with bibliographers in Observatoire de Paris (DASGAL), Institut d'Astrophysique de Paris, and Observatoire de Bordeaux. When selecting catalogues and tables for inclusion, priority is given to optimal multi-wavelength coverage of the database, and to support of research developments linked to large projects. In parallel, the systematic scanning of the bibliography reflects the diversity and general trends of astronomical research. A WWW interface to Simbad is available at: http://simbad.u-strasbg.fr/Simbad.",Erratum,pro101
pap2288,a5edce377759894482464a133cb9ec6791709eb2,con32,International Conference on Software Technology: Methods and Tools,Database System Concepts,"From the Publisher: 
This acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results.",Erratum,pro32
pap2289,6dd37b57f3391b438fa588f98a1c2067365ae5ca,con77,International Conference on Artificial Neural Networks,TID2008 – A database for evaluation of full-reference visual quality assessment metrics,"— In this paper, a new image database, TID2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores (MOS) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained MOS can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.",Erratum,pro77
pap2290,c64e51ab702ecdfc65281eb06fae8722809a0756,jou372,"Behavoir research methods, instruments & computers",A lifespan database of adult facial stimuli,,Article,vol372
pap2291,1dd0140d51e870a713340ae30734c8438b03d1a3,con31,International Conference on Evaluation & Assessment in Software Engineering,Unit selection in a concatenative speech synthesis system using a large speech database,"One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",Erratum,pro31
pap2292,1bdc29257650e2bed7d11a9a4afb4eeea0bb1296,con106,International Conference on Mobile Data Management,"The PROSITE database, its status in 1997","The PROSITE database consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",Erratum,pro106
pap2293,81e89f25baed869a690ffc6f93cd0306c58efe14,con14,Hawaii International Conference on System Sciences,The Mammographic Image Analysis Society digital mammogram database,"A clamp or grip for heavy duty work with twisted wire cables and the like, such as in marine and industrial uses and especially where reasonably easy application of the cable grip to the cable is important and undue bending moments on the heavy cables are to be avoided. The feature of a removable jaw is coupled with dual link bar structure for the jaws without sacrificing strength and with a considerable reduction in overall weight of the clamp as compared to presently available equipment, this being accomplished in part by elimination of a frame as such and providing the principal jaw with a slotted stabilizing arm having a sliding connection with a unique hanger bar, which latter is designed to be connected to the lift hook of a crane or the like.",Erratum,pro14
pap2294,20bdfd777432f7eab7ab2cc146297df1db654090,con46,Software Product Lines Conference,"MINT, the molecular interaction database: 2012 update","The Molecular INTeraction Database (MINT, http://mint.bio.uniroma2.it/mint/) is a public repository for protein–protein interactions (PPI) reported in peer-reviewed journals. The database grows steadily over the years and at September 2011 contains approximately 235 000 binary interactions captured from over 4750 publications. The web interface allows the users to search, visualize and download interactions data. MINT is one of the members of the International Molecular Exchange consortium (IMEx) and adopts the Molecular Interaction Ontology of the Proteomics Standard Initiative (PSI-MI) standards for curation and data exchange. MINT data are freely accessible and downloadable at http://mint.bio.uniroma2.it/mint/download.do. We report here the growth of the database, the major changes in curation policy and a new algorithm to assign a confidence to each interaction.",Erratum,pro46
pap2295,5c814bc6b49f21a1b84cbc3d4dc662a24165baee,con102,Annual Haifa Experimental Systems Conference,Spanner: Google's globally-distributed database,"Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It provides strong transactional semantics, consistent replication, and high performance reads and writes for a variety of Google's applications. I'll discuss the design and implementation of Spanner, as well as some of the lessons we have learned along the way. I'll also discuss some open challenges that we still see in building scalable distributed storage systems.",Conference paper,pro102
pap2296,f1d6cbac2f1ad7443c60c035e1c819bef402c5c7,con20,ACM Conference on Economics and Computation,Systemic Banking Crises Database II,,Erratum,pro20
pap2297,2055c63fd081abf321ad0ff61987df112f8871c4,jou373,ACM Transactions on Database Systems,Extending the database relational model to capture more meaning,"During the last three or four years several investigators have been exploring “semantic models” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear. (1) the search for meaningful units that are as small as possible—atomic semantics; (2) the search for meaningful units that are larger than the usual n-ary relation—molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.",Conference paper,vol373
pap2298,107e98602c1be84b1654d6a1b241b7c97a94c71f,con84,Workshop on Interdisciplinary Software Engineering Research,"ExoCarta 2012: database of exosomal proteins, RNA and lipids","Exosomes are membraneous nanovesicles of endocytic origin released by most cell types from diverse organisms; they play a critical role in cell–cell communication. ExoCarta (http://www.exocarta.org) is a manually curated database of exosomal proteins, RNA and lipids. The database catalogs information from both published and unpublished exosomal studies. The mode of exosomal purification and characterization, the biophysical and molecular properties are listed in the database aiding biomedical scientists in assessing the quality of the exosomal preparation and the corresponding data obtained. Currently, ExoCarta (Version 3.1) contains information on 11 261 protein entries, 2375 mRNA entries and 764 miRNA entries that were obtained from 134 exosomal studies. In addition to the data update, as a new feature, lipids identified in exosomes are added to ExoCarta. We believe that this free web-based community resource will aid researchers in identifying molecular signatures (proteins/RNA/lipids) that are specific to certain tissue/cell type derived exosomes and trigger new exosomal studies.",Erratum,pro84
pap2299,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,con30,PS,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",Erratum,pro30
pap2300,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,con90,Computer Vision and Pattern Recognition,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",Erratum,pro90
pap2301,fd8ba9431c2a1780e27b6f5e1e4947db9c85801c,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Crystallography Open Database (COD): an open-access collection of crystal structures and platform for world-wide collaboration,"Using an open-access distribution model, the Crystallography Open Database (COD, http://www.crystallography.net) collects all known ‘small molecule / small to medium sized unit cell’ crystal structures and makes them available freely on the Internet. As of today, the COD has aggregated ∼150 000 structures, offering basic search capabilities and the possibility to download the whole database, or parts thereof using a variety of standard open communication protocols. A newly developed website provides capabilities for all registered users to deposit published and so far unpublished structures as personal communications or pre-publication depositions. Such a setup enables extension of the COD database by many users simultaneously. This increases the possibilities for growth of the COD database, and is the first step towards establishing a world wide Internet-based collaborative platform dedicated to the collection and curation of structural knowledge.",Erratum,pro58
pap2302,8162d4f3bfce2055c9a53c267af66103c3bfd167,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Survey on NoSQL database,"With the development of the Internet and cloud computing, there need databases to be able to store and process big data effectively, demand for high-performance when reading and writing, so the traditional relational database is facing many new challenges. Especially in large scale and high-concurrency applications, such as search engines and SNS, using the relational database to store and query dynamic user data has appeared to be inadequate. In this case, NoSQL database created. This paper describes the background, basic characteristics, data model of NoSQL. In addition, this paper classifies NoSQL databases according to the CAP theorem. Finally, the mainstream NoSQL databases are separately described in detail, and extract some properties to help enterprises to choose NoSQL.",Erratum,pro98
pap2303,5745c2aff2a61d6f4a5b1083663fe7ed54d22672,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",MODOMICS: a database of RNA modification pathways,"MODOMICS is the first comprehensive database resource for systems biology of RNA modification. It integrates information about the chemical structure of modified nucleosides, their localization in RNA sequences, pathways of their biosynthesis and enzymes that carry out the respective reactions. MODOMICS also provides literature information, and links to other databases, including the available protein sequence and structure data. The current list of modifications and pathways is comprehensive, while the dataset of enzymes is limited to Escherichia coli and Saccharomyces cerevisiae and sequence alignments are presented only for tRNAs from these organisms. RNAs and enzymes from other organisms will be included in the near future. MODOMICS can be queried by the type of nucleoside (e.g. A, G, C, U, I, m1A, nm5s2U, etc.), type of RNA, position of a particular nucleoside, type of reaction (e.g. methylation, thiolation, deamination, etc.) and name or sequence of an enzyme of interest. Options for data presentation include graphs of pathways involving the query nucleoside, multiple sequence alignments of RNA sequences and tabular forms with enzyme and literature data. The contents of MODOMICS can be accessed through the World Wide Web at .",Erratum,pro52
pap2304,ef50e6878d6addcbd5d1ca96e08eef51b9ddec9e,con2,International Conference on Software Engineering,Reference database of Raman spectra of biological molecules,"Raman spectra of biological materials are very complex, because they consist of signals from all molecules present in cells. In order to obtain chemical information from these spectra, it is necessary to know the Raman patterns of the possible components of a cell. In this paper, we present a collection of Raman spectra of biomolecules that can serve as references for the interpretation of Raman spectra of biological materials. We included the most important components present in a cell: (1) DNA and RNA bases (adenine, cytosine, guanine, thymine and uracil), (2) amino acids (glycine, L-alanine, L-valine, L-serine, L-glutamic acid, L-arginine, L-phenylalanine, L-tyrosine, L-tryptophan, L-histidine, L-proline), (3) fatty acids and fats (lauric acid, myristic acid, palmitic acid, stearic acid, 12-methyltetradecanoic acid, 13-methylmyristic acid, 14-methylpentadecanoic acid, 14-methylhexadecanoic acid, 15-methylpalmitic acid, oleic acid, vaccenic acid, glycerol, triolein, trilinolein, trilinolenin), (4) saccharides (β-D-glucose, lactose, cellulose, D-(+)-dextrose, D-(+)-trehalose, amylose, amylopectine, D-(+)-mannose, D-(+)-fucose, D-(−)-arabinose, D-(+)-xylose, D-(−)-fructose, D-(+)-galactosamine, N-acetyl-D-glucosamine, chitin), (5) primary metabolites (citric acid, succinic acid, fumarate, malic acid, pyruvate, phosphoenolpyruvate, coenzyme A, acetyl coenzyme A, acetoacetate, D-fructose-6-phosphate) and (6) others (β-carotene, ascorbic acid, riboflavin, glutathione). Examples of Raman spectra of bacteria and fungal spores are shown, together with band assignments to the reference products. Copyright © 2007 John Wiley & Sons, Ltd.",Erratum,pro2
pap2305,2a75f34663a60ab1b04a0049ed1d14335129e908,con103,IEEE International Conference on Multimedia and Expo,Web-based database for facial expression analysis,"In the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. Nonetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. This lack of easily accessible, suitable, common testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. In this paper, we discuss a number of issues that make the problem of creating a benchmark facial expression database difficult. We then present the MMI facial expression database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and multiple facial muscle activation. It has been built as a Web-based direct-manipulation application, allowing easy access and easy search of the available images. This database represents the most comprehensive reference set of images for studies on facial expression analysis to date.",Conference paper,pro103
pap2306,015525f864ccaf28efbdaed46029598441121a9e,con80,International Conference on Advanced Computer Science Applications and Technologies,UCID: an uncompressed color image database,"Standardised image databases or rather the lack of them are one of the main weaknesses in the field of content based image retrieval (CBIR). Authors often use their own images or do not specify the source of their datasets. Naturally this makes comparison of results somewhat difficult. While a first approach towards a common colour image set has been taken by the MPEG 7 committee their database does not cater for all strands of research in the CBIR community. In particular as the MPEG-7 images only exist in compressed form it does not allow for an objective evaluation of image retrieval algorithms that operate in the compressed domain or to judge the influence image compression has on the performance of CBIR algorithms. In this paper we introduce a new dataset, UCID (pronounced ""use it"") - an Uncompressed Colour Image Dataset which tries to bridge this gap. The UCID dataset currently consists of 1338 uncompressed images together with a ground truth of a series of query images with corresponding models that an ideal CBIR algorithm would retrieve. While its initial intention was to provide a dataset for the evaluation of compressed domain algorithms, the UCID database also represents a good benchmark set for the evaluation of any kind of CBIR method as well as an image set that can be used to evaluate image compression and colour quantisation algorithms.",Erratum,pro80
pap2307,fbbe83989ce11b91ab0dfb0d5e822b08cd885f48,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,The UCSC Genome Browser database: update 2010,"The University of California, Santa Cruz (UCSC) Genome Browser website (http://genome.ucsc.edu/) provides a large database of publicly available sequence and annotation data along with an integrated tool set for examining and comparing the genomes of organisms, aligning sequence to genomes, and displaying and sharing users’ own annotation data. As of September 2009, genomic sequence and a basic set of annotation ‘tracks’ are provided for 47 organisms, including 14 mammals, 10 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms and a yeast. New data highlights this year include an updated human genome browser, a 44-species multiple sequence alignment track, improved variation and phenotype tracks and 16 new genome-wide ENCODE tracks. New features include drag-and-zoom navigation, a Wiki track for user-added annotations, new custom track formats for large datasets (bigBed and bigWig), a new multiple alignment output tool, links to variation and protein structure tools, in silico PCR utility enhancements, and improved track configuration tools.",Erratum,pro13
pap2308,8445ff6a398279d3179bbedae01bab7c034c31ff,con109,International Society for Music Information Retrieval Conference,The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings,"Multi-microphone arrays allow for the use of spatial filtering techniques that can greatly improve noise reduction and source separation. However, for speech and audio data, work on noise reduction or separation has focused primarily on one- or two-channel systems. Because of this, databases of multichannel environmental noise are not widely available. DEMAND (Diverse Environments Multi-channel Acoustic Noise Database) addresses this problem by providing a set of 16-channel noise files recorded in a variety of indoor and outdoor settings. The data was recorded using a planar microphone array consisting of four staggered rows, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. DEMAND is freely available under a Creative Commons license to encourage research into algorithms beyond the stereo setup.",Erratum,pro109
pap2309,b7e3bec7efda0946a73d8cfa06550ef2b1a2b2bd,jou374,British Journal of Cancer,The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website,,Conference paper,vol374
pap2310,baf03356bf3403f0fc111e1b348a77a01ef48899,con6,Annual Conference on Genetic and Evolutionary Computation,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible database describing metabolic pathways and enzymes from all domains of life. MetaCyc pathways are experimentally determined, mostly small-molecule metabolic pathways and are curated from the primary scientific literature. MetaCyc contains >2100 pathways derived from >37 000 publications, and is the largest curated collection of metabolic pathways currently available. BioCyc (BioCyc.org) is a collection of >3000 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems and pathway-hole fillers. Additions to BioCyc over the past 2 years include YeastCyc, a PGDB for Saccharomyces cerevisiae, and 891 new genomes from the Human Microbiome Project. The BioCyc Web site offers a variety of tools for querying and analysis of PGDBs, including Omics Viewers and tools for comparative analysis. New developments include atom mappings in reactions, a new representation of glycan degradation pathways, improved compound structure display, better coverage of enzyme kinetic data, enhancements of the Web Groups functionality, improvements to the Omics viewers, a new representation of the Enzyme Commission system and, for the desktop version of the software, the ability to save display states.",Erratum,pro6
pap2311,5c86baa92ece7425ceb09232dddd9538c224ce5e,con108,International Conference on Information Integration and Web-based Applications & Services,The Human Oral Microbiome Database: a web accessible resource for investigating oral microbe taxonomic and genomic information,"The human oral microbiome is the most studied human microflora, but 53% of the species have not yet been validly named and 35% remain uncultivated. The uncultivated taxa are known primarily from 16S rRNA sequence information. Sequence information tied solely to obscure isolate or clone numbers, and usually lacking accurate phylogenetic placement, is a major impediment to working with human oral microbiome data. The goal of creating the Human Oral Microbiome Database (HOMD) is to provide the scientific community with a body site-specific comprehensive database for the more than 600 prokaryote species that are present in the human oral cavity based on a curated 16S rRNA gene-based provisional naming scheme. Currently, two primary types of information are provided in HOMD—taxonomic and genomic. Named oral species and taxa identified from 16S rRNA gene sequence analysis of oral isolates and cloning studies were placed into defined 16S rRNA phylotypes and each given unique Human Oral Taxon (HOT) number. The HOT interlinks phenotypic, phylogenetic, genomic, clinical and bibliographic information for each taxon. A BLAST search tool is provided to match user 16S rRNA gene sequences to a curated, full length, 16S rRNA gene reference data set. For genomic analysis, HOMD provides comprehensive set of analysis tools and maintains frequently updated annotations for all the human oral microbial genomes that have been sequenced and publicly released. Oral bacterial genome sequences, determined as part of the Human Microbiome Project, are being added to the HOMD as they become available. We provide HOMD as a conceptual model for the presentation of microbiome data for other human body sites. Database URL: http://www.homd.org",Erratum,pro108
pap2312,9c1411c9ebc6260edc5798c9339e189e759b2168,con82,International Conference on Medical Image Computing and Computer-Assisted Intervention,GMD@CSB.DB: the Golm Metabolome Database,"UNLABELLED
Metabolomics, in particular gas chromatography-mass spectrometry (GC-MS) based metabolite profiling of biological extracts, is rapidly becoming one of the cornerstones of functional genomics and systems biology. Metabolite profiling has profound applications in discovering the mode of action of drugs or herbicides, and in unravelling the effect of altered gene expression on metabolism and organism performance in biotechnological applications. As such the technology needs to be available to many laboratories. For this, an open exchange of information is required, like that already achieved for transcript and protein data. One of the key-steps in metabolite profiling is the unambiguous identification of metabolites in highly complex metabolite preparations from biological samples. Collections of mass spectra, which comprise frequently observed metabolites of either known or unknown exact chemical structure, represent the most effective means to pool the identification efforts currently performed in many laboratories around the world. Here we present GMD, The Golm Metabolome Database, an open access metabolome database, which should enable these processes. GMD provides public access to custom mass spectral libraries, metabolite profiling experiments as well as additional information and tools, e.g. with regard to methods, spectral information or compounds. The main goal will be the representation of an exchange platform for experimental research activities and bioinformatics to develop and improve metabolomics by multidisciplinary cooperation.


AVAILABILITY
http://csbdb.mpimp-golm.mpg.de/gmd.html


CONTACT
Steinhauser@mpimp-golm.mpg.de


SUPPLEMENTARY INFORMATION
http://csbdb.mpimp-golm.mpg.de/",Erratum,pro82
pap2313,fa7fa53b825228f29ac80e1a4b3d9c10e5870843,con107,Chinese Conference on Biometric Recognition,"Output, Input and Productivity Measures at the Industry Level: The EU KLEMS Database","This article describes the contents and the construction of the EU KLEMS Growth and Productivity Accounts. This database contains industry-level measures of output, inputs and productivity for 25 European countries, Japan and the US for the period from 1970 onwards. The article considers the methodology employed in constructing the database and shows how it can be useful in comparing productivity trends. Although growth accounts are the organising principle, it is argued that the database is useful for a wider range of applications. We give some guidance to prudent use and indicate possible extensions. Copyright © The Author(s). Journal compilation © Royal Economic Society 2009.",Erratum,pro107
pap2314,85331cdfdae93103b0c86bcb12bb1b47158ced08,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,The Transporter Classification Database,"The Transporter Classification Database (TCDB; http://www.tcdb.org) serves as a common reference point for transport protein research. The database contains more than 10 000 non-redundant proteins that represent all currently recognized families of transmembrane molecular transport systems. Proteins in TCDB are organized in a five level hierarchical system, where the first two levels are the class and subclass, the second two are the family and subfamily, and the last one is the transport system. Superfamilies that contain multiple families are included as hyperlinks to the five tier TC hierarchy. TCDB includes proteins from all types of living organisms and is the only transporter classification system that is both universal and recognized by the International Union of Biochemistry and Molecular Biology. It has been expanded by manual curation, contains extensive text descriptions providing structural, functional, mechanistic and evolutionary information, is supported by unique software and is interconnected to many other relevant databases. TCDB is of increasing usefulness to the international scientific community and can serve as a model for the expansion of database technologies. This manuscript describes an update of the database descriptions previously featured in NAR database issues.",Erratum,pro39
pap2315,71e9a23138a5d7c35b28bd98fd616c81719b1b7a,con97,ACM SIGMOD Conference,"NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison","Digital world is growing very fast and become more complex in the volume (terabyte to petabyte), variety (structured and un-structured and hybrid), velocity (high speed in growth) in nature. This refers to as ‘Big Data’ that is a global phenomenon. This is typically considered to be a data collection that has grown so large it can’t be effectively managed or exploited using conventional data management tools: e.g., classic relational database management systems (RDBMS) or conventional search engines. To handle this problem, traditional RDBMS are complemented by specifically designed a rich set of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This paper motivation is to provide - classification, characteristics and evaluation of NoSQL databases in Big Data Analytics. This report is intended to help users, especially to the organizations to obtain an independent understanding of the strengths and weaknesses of various NoSQL database approaches to supporting applications that process huge volumes of data.",Erratum,pro97
pap2316,b0c5efdf2f90322784283290a052797eb073b554,jou65,Data Science Journal,The World Ocean Database,"The World Ocean Database (WOD) is the most comprehensive global ocean profile-plankton database available internationally without restriction. All data are in one well-documented format and are available both on DVDs for a minimal charge and on-line without charge. The latest DVD version of the WOD is the World Ocean Database 2009 (WOD09). All data in the WOD are associated with as much metadata as possible, and every ocean data value has a quality control flag associated with it. The WOD is a product of the U.S. National Oceanographic Data Center and its co-located World Data Center for Oceanography. However, the WOD exists because of the international oceanographic data exchange that has occurred under the auspices of the Intergovernmental Oceanographic Commission (IOC) and the International Council of Science (ICSU) World Data Center (WDC) system. World Data Centers are part of the ICSU World Data System.",Conference paper,vol65
pap2317,20f5782a5fed99979ae406849d7d11bd59314996,jou262,Nature Genetics,dbEST — database for “expressed sequence tags”,,Conference paper,vol262
pap2318,d6cff906315e29b61afcf14bf7e6fe40f4d74ea5,con90,Computer Vision and Pattern Recognition,A large-scale hierarchical image database,,Article,pro90
pap2319,5ab169aed6e76c20621a23c411f651aac423efe3,con40,Conference on Software Engineering Education and Training,"Principles of Database and Knowledge-Base Systems, Volume II",,Erratum,pro40
pap2320,f322e882eec09709f7f7c2d7824722509b79f5e9,con67,IEEE International Software Metrics Symposium,Principles of Database Systems,"A large part is a description of relations, their algebra and calculus, and the query languages that have been designed using these concepts. There are explanations of how the theory can be used to design good systems. A description of the optimization of queries in relation-based query languages is provided, and a chapter is devoted to the recently developed protocols for guaranteeing consistency in databases that are operated on by many processes concurrently",Erratum,pro67
pap2321,d769ca2ac7b7057df74fdf9d4d0de91f1b07917d,con88,European Conference on Computer Vision,ARDB—Antibiotic Resistance Genes Database,"The treatment of infections is increasingly compromised by the ability of bacteria to develop resistance to antibiotics through mutations or through the acquisition of resistance genes. Antibiotic resistance genes also have the potential to be used for bio-terror purposes through genetically modified organisms. In order to facilitate the identification and characterization of these genes, we have created a manually curated database—the Antibiotic Resistance Genes Database (ARDB)—unifying most of the publicly available information on antibiotic resistance. Each gene and resistance type is annotated with rich information, including resistance profile, mechanism of action, ontology, COG and CDD annotations, as well as external links to sequence and protein databases. Our database also supports sequence similarity searches and implements an initial version of a tool for characterizing common mutations that confer antibiotic resistance. The information we provide can be used as compendium of antibiotic resistance factors as well as to identify the resistance genes of newly sequenced genes, genomes, or metagenomes. Currently, ARDB contains resistance information for 13 293 genes, 377 types, 257 antibiotics, 632 genomes, 933 species and 124 genera. ARDB is available at http://ardb.cbcb.umd.edu/.",Erratum,pro88
pap2322,151ec57b35f6a7431fdce934a57ae15451079d85,con87,"IEEE International Conference on Acoustics, Speech, and Signal Processing",NCBI’s Database of Genotypes and Phenotypes: dbGaP,"The Database of Genotypes and Phenotypes (dbGap, http://www.ncbi.nlm.nih.gov/gap) is a National Institutes of Health-sponsored repository charged to archive, curate and distribute information produced by studies investigating the interaction of genotype and phenotype. Information in dbGaP is organized as a hierarchical structure and includes the accessioned objects, phenotypes (as variables and datasets), various molecular assay data (SNP and Expression Array data, Sequence and Epigenomic marks), analyses and documents. Publicly accessible metadata about submitted studies, summary level data, and documents related to studies can be accessed freely on the dbGaP website. Individual-level data are accessible via Controlled Access application to scientists across the globe.",Erratum,pro87
pap2323,11fdda41735869a5962b698e9d4fc6524ee96d4c,jou375,Journal of Applied Crystallography,Crystallography Open Database – an open-access collection of crystal structures,"The Crystallography Open Database (COD) is an ongoing initiative by crystallographers to gather all published inorganic, metal–organic and small organic molecule structures in one database, providing a straightforward search and retrieval interface. The COD adopts an open-access model for its >80 000 structure files.",Letter,vol375
pap2324,2485c98aa44131d1a2f7d1355b1e372f2bb148ad,con69,Formal Concept Analysis,The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations,"In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.",Erratum,pro69
pap2325,9ebe338e49e63ff97348aca0db521ac3ff01bcef,con6,Annual Conference on Genetic and Evolutionary Computation,CDD: specific functional annotation with the Conserved Domain Database,"NCBI's Conserved Domain Database (CDD) is a collection of multiple sequence alignments and derived database search models, which represent protein domains conserved in molecular evolution. The collection can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml, and is also part of NCBI's Entrez query and retrieval system, cross-linked to numerous other resources. CDD provides annotation of domain footprints and conserved functional sites on protein sequences. Precalculated domain annotation can be retrieved for protein sequences tracked in NCBI's Entrez system, and CDD's collection of models can be queried with novel protein sequences via the CD-Search service at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. Starting with the latest version of CDD, v2.14, information from redundant and homologous domain models is summarized at a superfamily level, and domain annotation on proteins is flagged as either ‘specific’ (identifying molecular function with high confidence) or as ‘non-specific’ (identifying superfamily membership only).",Erratum,pro6
pap2326,6348c3490aa17188229decfd09b67e5169ce1cfc,con9,Big Data,Database indexing for production MegaBLAST searches,"Motivation: The BLAST software package for sequence comparison speeds up homology search by preprocessing a query sequence into a lookup table. Numerous research studies have suggested that preprocessing the database instead would give better performance. However, production usage of sequence comparison methods that preprocess the database has been limited to programs such as BLAT and SSAHA that are designed to find matches when query and database subsequences are highly similar. Results: We developed a new version of the MegaBLAST module of BLAST that does the initial phase of finding short seeds for matches by searching a database index. We also developed a program makembindex that preprocesses the database into a data structure for rapid seed searching. We show that the new ‘indexed MegaBLAST’ is faster than the ‘non-indexed’ version for most practical uses. We show that indexed MegaBLAST is faster than miBLAST, another implementation of BLAST nucleotide searching with a preprocessed database, for most of the 200 queries we tested. To deploy indexed MegaBLAST as part of NCBI'sWeb BLAST service, the storage of databases and the queueing mechanism were modified, so that some machines are now dedicated to serving queries for a specific database. The response time for such Web queries is now faster than it was when each computer handled queries for multiple databases. Availability: The code for indexed MegaBLAST is part of the blastn program in the NCBI C++ toolkit. The preprocessor program makembindex is also in the toolkit. Indexed MegaBLAST has been used in production on NCBI's Web BLAST service to search one version of the human and mouse genomes since October 2007. The Linux command-line executables for blastn and makembindex, documentation, and some query sets used to carry out the tests described below are available in the directory: ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast Contact: schaffer@helix.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.",Erratum,pro9
pap2327,69114cb58049c3b06ce012cf41d493d2e947b6c8,con64,British Computer Society Conference on Human-Computer Interaction,DBatVir: the database of bat-associated viruses,"Emerging infectious diseases remain a significant threat to public health. Most emerging infectious disease agents in humans are of zoonotic origin. Bats are important reservoir hosts of many highly lethal zoonotic viruses and have been implicated in numerous emerging infectious disease events in recent years. It is essential to enhance our knowledge and understanding of the genetic diversity of the bat-associated viruses to prevent future outbreaks. To facilitate further research, we constructed the database of bat-associated viruses (DBatVir). Known viral sequences detected in bat samples were manually collected and curated, along with the related metadata, such as the sampling time, location, bat species and specimen type. Additional information concerning the bats, including common names, diet type, geographic distribution and phylogeny were integrated into the database to bridge the gap between virologists and zoologists. The database currently covers >4100 bat-associated animal viruses of 23 viral families detected from 196 bat species in 69 countries worldwide. It provides an overview and snapshot of the current research regarding bat-associated viruses, which is essential now that the field is rapidly expanding. With a user-friendly interface and integrated online bioinformatics tools, DBatVir provides a convenient and powerful platform for virologists and zoologists to analyze the virome diversity of bats, as well as for epidemiologists and public health researchers to monitor and track current and future bat-related infectious diseases. Database URL: http://www.mgc.ac.cn/DBatVir/",Erratum,pro64
pap2328,bef5bdc4d49c9da09d125f4d86b15509ebff52cd,con43,IEEE International Conference on Software Maintenance and Evolution,Parallel database systems: the future of high performance database systems,,Erratum,pro43
pap2329,aef87d005e8e3d58f0a0577a7be4d55a10c2d5b3,jou106,Nucleic Acids Research,MODOMICS: a database of RNA modification pathways. 2008 update,"MODOMICS, a database devoted to the systems biology of RNA modification, has been subjected to substantial improvements. It provides comprehensive information on the chemical structure of modified nucleosides, pathways of their biosynthesis, sequences of RNAs containing these modifications and RNA-modifying enzymes. MODOMICS also provides cross-references to other databases and to literature. In addition to the previously available manually curated tRNA sequences from a few model organisms, we have now included additional tRNAs and rRNAs, and all RNAs with 3D structures in the Nucleic Acid Database, in which modified nucleosides are present. In total, 3460 modified bases in RNA sequences of different organisms have been annotated. New RNA-modifying enzymes have been also added. The current collection of enzymes includes mainly proteins for the model organisms Escherichia coli and Saccharomyces cerevisiae, and is currently being expanded to include proteins from other organisms, in particular Archaea and Homo sapiens. For enzymes with known structures, links are provided to the corresponding Protein Data Bank entries, while for many others homology models have been created. Many new options for database searching and querying have been included. MODOMICS can be accessed at http://genesilico.pl/modomics.",Article,vol106
pap2330,ff89306dcc77b387f01718b497df0116c87c260d,con50,International Workshop on Green and Sustainable Software,The IMGT/HLA database,"It is 10 years since the IMGT/HLA database was released, providing the HLA community with a searchable repository of highly curated HLA sequences. The HLA complex is located within the 6p21.3 region of human chromosome 6 and contains more than 220 genes of diverse function. Many of the genes encode proteins of the immune system and are highly polymorphic. The naming of these HLA genes and alleles, and their quality control is the responsibility of the WHO Nomenclature Committee for Factors of the HLA System. Through the work of the HLA Informatics Group and in collaboration with the European Bioinformatics Institute, we are able to provide public access to this data through the website http://www.ebi.ac.uk/imgt/hla/. The first release contained 964 sequences, the most recent release 3300 sequences, with around 450 new sequences been added each year. The tools provided on the website have been updated to allow more complex alignments, which include genomic sequence data, as well as the development of tools for probe and primer design and the inclusion of data from the HLA Dictionary. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the HLA community, and the wider research and clinical communities.",Erratum,pro50
pap2331,fefea2b9ed93a0c3163432c52a67cf34efa868f7,con59,Annual Workshop of the Psychology of Programming Interest Group,SAbDab: the structural antibody database,"Structural antibody database (SAbDab; http://opig.stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody–antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.",Erratum,pro59
pap2332,0bd83f3dc2bf04bc592ae05112adf30882db1196,jou260,Genome Medicine,The Human Gene Mutation Database: 2008 update,,Article,vol260
pap2333,e72c49c059ac9926cc7e25d18971300f7ec2feef,con28,International Conference Geographic Information Science,Principles of transaction-oriented database recovery,"In this paper, a terminological framework is provided for describing different transactionoriented recovery schemes for database systems in a conceptual rather than an implementation-dependent way. By introducing the terms materialized database, propagation strategy, and checkpoint, we obtain a means for classifying arbitrary implementations from a unified viewpoint. This is complemented by a classification scheme for logging techniques, which are precisely defined by using the other terms. It is shown that these criteria are related to all relevant questions such as speed and scope of recovery and amount of redundant information required. The primary purpose of this paper, however, is to establish an adequate and precise terminology for a topic in which the confusion of concepts and implementational aspects still imposes a lot of problems.",Erratum,pro28
pap2334,462098bcc81f5d8b8a067aa0b8988adba0eef91f,con36,Central and Eastern European Software Engineering Conference in Russia,BioNumbers—the database of key numbers in molecular and cell biology,"BioNumbers (http://www.bionumbers.hms.harvard.edu) is a database of key numbers in molecular and cell biology—the quantitative properties of biological systems of interest to computational, systems and molecular cell biologists. Contents of the database range from cell sizes to metabolite concentrations, from reaction rates to generation times, from genome sizes to the number of mitochondria in a cell. While always of importance to biologists, having numbers in hand is becoming increasingly critical for experimenting, modeling, and analyzing biological systems. BioNumbers was motivated by an appreciation of how long it can take to find even the simplest number in the vast biological literature. All numbers are taken directly from a literature source and that reference is provided with the number. BioNumbers is designed to be highly searchable and queries can be performed by keywords or browsed by menus. BioNumbers is a collaborative community platform where registered users can add content and make comments on existing data. All new entries and commentary are curated to maintain high quality. Here we describe the database characteristics and implementation, demonstrate its use, and discuss future directions for its development.",Erratum,pro36
pap2335,e3f9ed5a6c1c2045adfb5f66845338d9352149d3,con59,Annual Workshop of the Psychology of Programming Interest Group,OPM: Orientations of Proteins in Membranes database,"SUMMARY
The Orientations of Proteins in Membranes (OPM) database provides a collection of transmembrane, monotopic and peripheral proteins from the Protein Data Bank whose spatial arrangements in the lipid bilayer have been calculated theoretically and compared with experimental data. The database allows analysis, sorting and searching of membrane proteins based on their structural classification, species, destination membrane, numbers of transmembrane segments and subunits, numbers of secondary structures and the calculated hydrophobic thickness or tilt angle with respect to the bilayer normal. All coordinate files with the calculated membrane boundaries are available for downloading.


AVAILABILITY
http://opm.phar.umich.edu.",Erratum,pro59
pap2336,13bc458634865e23ed8ecd54473a34e705f7da10,con66,International Conference on Software Reuse,THE HITRAN MOLECULAR DATABASE: EDITIONS OF 1991 AND 1992,,Erratum,pro66
pap2337,fd05e825c5d076b17626996a78bdff5e7752549d,con98,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,ChEBI: a database and ontology for chemical entities of biological interest,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds. The molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. Genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in ChEBI. In addition to molecular entities, ChEBI contains groups (parts of molecular entities) and classes of entities. ChEBI includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. ChEBI is available online at http://www.ebi.ac.uk/chebi/",Erratum,pro98
pap2338,d03cab6781985ad3f62aec52048a7e15ee2dee61,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,LMSD: LIPID MAPS structure database,"The LIPID MAPS Structure Database (LMSD) is a relational database encompassing structures and annotations of biologically relevant lipids. Structures of lipids in the database come from four sources: (i) LIPID MAPS Consortium's core laboratories and partners; (ii) lipids identified by LIPID MAPS experiments; (iii) computationally generated structures for appropriate lipid classes; (iv) biologically relevant lipids manually curated from LIPID BANK, LIPIDAT and other public sources. All the lipid structures in LMSD are drawn in a consistent fashion. In addition to a classification-based retrieval of lipids, users can search LMSD using either text-based or structure-based search options. The text-based search implementation supports data retrieval by any combination of these data fields: LIPID MAPS ID, systematic or common name, mass, formula, category, main class, and subclass data fields. The structure-based search, in conjunction with optional data fields, provides the capability to perform a substructure search or exact match for the structure drawn by the user. Search results, in addition to structure and annotations, also include relevant links to external databases. The LMSD is publicly available at",Erratum,pro58
pap2339,1f159d534f2e150577718b92d3ccfd3e23b7e889,con11,European Conference on Modelling and Simulation,Constructing a research database of social and environmental reporting by UK companies,"Responds to the widely‐reported methodological problems which have arisen in research into corporate social and environmental reporting. Reports on an attempt to build a database of UK company social and environmental disclosure. The motivation behind the database is an attempt to provide, first, a data set which both refines and develops earlier attempts to capture and interpret such disclosures; second, a data set covering several years to permit longitudinal analysis; and third, a public database for accounting researchers who wish to pursue, in a systematic and comparable way, more focused hypotheses about social and environmental reporting behaviour. Explains the motivation for, the background to, and process of establishing such a database and attempts to expose the difficulties met and the assumptions made in establishing the structure of the data capture. The resultant database has already proved useful to other UK researchers. Aims to help researchers in other countries to develop their own metho...",Erratum,pro11
pap2340,eda424538bab229d38f03a97d3ed1731e2a2c871,con17,International Conference on Statistical and Scientific Database Management,"Semantic database modeling: survey, applications, and research issues","Most common database management systems represent information in a simple record-based format. Semantic modeling provides richer data structuring capabilities for database applications. In particular, research in this area has articulated a number of constructs that provide mechanisms for representing structurally complex interrelations among data typically arising in commercial applications. In general terms, semantic modeling complements work on knowledge representation (in artificial intelligence) and on the new generation of database models based on the object-oriented paradigm of programming languages.
This paper presents an in-depth discussion of semantic data modeling. It reviews the philosophical motivations of semantic models, including the need for high-level modeling abstractions and the reduction of semantic overloading of data type constructors. It then provides a tutorial introduction to the primary components of semantic models, which are the explicit representation of objects, attributes of and relationships among objects, type constructors for building complex types, ISA relationships, and derived schema components. Next, a survey of the prominent semantic models in the literature is presented. Further, since a broad area of research has developed around semantic modeling, a number of related topics based on these models are discussed, including data languages, graphical interfaces, theoretical investigations, and physical implementation strategies.",Erratum,pro17
pap2341,100f4767f087e858976013b7117f38d26bb32a66,jou216,Analytical Chemistry,Method to correlate tandem mass spectra of modified peptides to amino acid sequences in the protein database.,"A method to correlate uninterpreted tandem mass spectra of modified peptides, produced under low-energy (10-50 eV) collision conditions, with amino acid sequences in a protein database has been developed. The fragmentation patterns observed in the tandem mass spectra of peptides containing covalent modifications is used to directly search and fit linear amino acid sequences in the database. Specific information relevant to sites of modification is not contained in the character-based sequence information of the databases. The search method considers each putative modification site as both modified and unmodified in one pass through the database and simultaneously considers up to three different sites of modification. The search method will identify the correct sequence if the tandem mass spectrum did not represent a modified peptide. This approach is demonstrated with peptides containing modifications such as S-carboxymethylated cysteine, oxidized methionine, phosphoserine, phosphothreonine, or phosphotyrosine. In addition, a scanning approach is used in which neutral loss scans are used to initiate the acquisition of product ion MS/MS spectra of doubly charged phosphorylated peptides during a single chromatographic run for data analysis with the database-searching algorithm. The approach described in this paper provides a convenient method to match the nascent tandem mass spectra of modified peptides to sequences in a protein database and thereby identify previously unknown sites of modification.",Conference paper,vol216
pap2342,7b472238215ac399e119ef152c0ff93f2df1c8e6,con33,International Conference on Automated Software Engineering,Rfam: updates to the RNA families database,"Rfam is a collection of RNA sequence families, represented by multiple sequence alignments and covariance models (CMs). The primary aim of Rfam is to annotate new members of known RNA families on nucleotide sequences, particularly complete genomes, using sensitive BLAST filters in combination with CMs. A minority of families with a very broad taxonomic range (e.g. tRNA and rRNA) provide the majority of the sequence annotations, whilst the majority of Rfam families (e.g. snoRNAs and miRNAs) have a limited taxonomic range and provide a limited number of annotations. Recent improvements to the website, methodologies and data used by Rfam are discussed. Rfam is freely available on the Web at http://rfam.sanger.ac.uk/and http://rfam.janelia.org/.",Erratum,pro33
pap2343,240faf3bbdea0673f5bd6e3668e4de1de905ceee,con5,Technical Symposium on Computer Science Education,"PROSITE, a protein domain database for functional characterization and annotation","PROSITE consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. It is complemented by ProRule, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. PROSITE is largely used for the annotation of domain features of UniProtKB/Swiss-Prot entries. Among the 983 (DNA-binding) domains, repeats and zinc fingers present in Swiss-Prot (release 57.8 of 22 September 2009), 696 (∼70%) are annotated with PROSITE descriptors using information from ProRule. In order to allow better functional characterization of domains, PROSITE developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. Here, we describe AMSA, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of ScanProsite to Vital-IT, a cluster of 633 CPUs, and the adoption of the Distributed Annotation System (DAS) to facilitate PROSITE data integration and interchange with other sources. The latest version of PROSITE (release 20.54, of 22 September 2009) contains 1308 patterns, 863 profiles and 869 ProRules. PROSITE is accessible at: http://www.expasy.org/prosite/.",Erratum,pro5
pap2344,311c0501c68f8cbe0d2e3a161de1ab12d49cb9ce,con4,Conference on Innovative Data Systems Research,Physical Properties of Ionic Liquids: Database and Evaluation,"A comprehensive database on physical properties of ionic liquids (ILs), which was collected from 109 kinds of literature sources in the period from 1984 through 2004, has been presented. There are 1680 pieces of data on the physical properties for 588 available ILs, from which 276 kinds of cations and 55 kinds of anions were extracted. In terms of the collected database, the structure-property relationship was evaluated. The correlation of melting points of two most common systems, disubstituted imidazolium tetrafluoroborate and disubstituted imidazolium hexafluorophosphate, was carried out using a quantitative structure-property relationship method.",Erratum,pro4
pap2345,f82d09363e9a657749d14e97e9d6d1410331b436,con2,International Conference on Software Engineering,Database Management Systems,"Database Management Systems Introduction Data Models Relational Model Entity-Relationship (E/R) Model Key-Value Model Entity-Relationship (E/R) Model Relationships Subclasses Constraints Converting ER to Relational Model Functional Dependencies (FD) FD Properties Closure Algorithm Superkeys & Minimal Basis Schema Normalization Decomposition Lossless-Join & Chase Algorithm Dependency Preserving Boyce-Codd Normal Form (BCNF) Third Normal Form (3NF) Relational Algebra Basic Operators Derived Operators Extended RA Syntax Data Storage Buffer Manager File Organization Page Organization Record Format Index Structures Indexing Basics Hash Tables B+ Trees Bitmaps & Bitslices Supporting SQL Operators External Sorting Selection Operator Projection Operator Join Operator Aggregation Operation Query Optimizer Annotated RA Trees Optimization Plans Cost Estimation Transaction Management Definition of Transactions The ""ACID"" Principle Write-Ahead Logging (WAL) Transaction Concurrency",Erratum,pro2
pap2346,c9fc7bf985fde7246f6139809cc7b019fd1ae007,con89,Conference on Uncertainty in Artificial Intelligence,The Forest Inventory and Analysis Database: Database Description and Users Manual Version 4.0 for Phase 2,"This document is based on previous documentation of the nationally standardized Forest Inventory and Analysis database (Hansen and others 1992; Woudenberg and Farrenkopf 1995; Miles and others 2001). Documentation of the structure of the Forest Inventory and Analysis database (FIADB) for Phase 2 data, as well as codes and definitions, is provided. Examples for producing population level estimates are also presented. This database provides a consistent framework for storing forest inventory data across all ownerships for the entire United States. These data are available to the public.",Erratum,pro89
pap2347,269a6271fa98bbdc2d456bae7fb419a77c88dc70,con6,Annual Conference on Genetic and Evolutionary Computation,D2P2: database of disordered protein predictions,"We present the Database of Disordered Protein Prediction (D2P2), available at http://d2p2.pro (including website source code). A battery of disorder predictors and their variants, VL-XT, VSL2b, PrDOS, PV2, Espritz and IUPred, were run on all protein sequences from 1765 complete proteomes (to be updated as more genomes are completed). Integrated with these results are all of the predicted (mostly structured) SCOP domains using the SUPERFAMILY predictor. These disorder/structure annotations together enable comparison of the disorder predictors with each other and examination of the overlap between disordered predictions and SCOP domains on a large scale. D2P2 will increase our understanding of the interplay between disorder and structure, the genomic distribution of disorder, and its evolutionary history. The parsed data are made available in a unified format for download as flat files or SQL tables either by genome, by predictor, or for the complete set. An interactive website provides a graphical view of each protein annotated with the SCOP domains and disordered regions from all predictors overlaid (or shown as a consensus). There are statistics and tools for browsing and comparing genomes and their disorder within the context of their position on the tree of life.",Erratum,pro6
pap2348,a03d8f591bc3c2dbdecbd9d515e0469953a3f7ef,jou262,Nature Genetics,The NCBI dbGaP database of genotypes and phenotypes,,Letter,vol262
pap2349,28e702e1a352854cf0748b9a6a9ad6679b1d4e83,con100,International Conference on Automatic Face and Gesture Recognition,Progressive skyline computation in database systems,"The skyline of a d-dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing.",Erratum,pro100
pap2350,7dabd56ccd524f78f0eda5073dc358f28893a45d,con84,Workshop on Interdisciplinary Software Engineering Research,The CIPIC HRTF database,"This paper describes a public-domain database of high-spatial-resolution head-related transfer functions measured at the UC Davis CIPIC Interface Laboratory and the methods used to collect the data.. Release 1.0 (see http://interface.cipic.ucdavis.edu) includes head-related impulse responses for 45 subjects at 25 different azimuths and 50 different elevations (1250 directions) at approximately 5/spl deg/ angular increments. In addition, the database contains anthropometric measurements for each subject. Statistics of anthropometric parameters and correlations between anthropometry and some temporal and spectral features of the HRTFs are reported.",Erratum,pro84
pap2351,b791d488eef45ef79da812f7569fc2cc83196aa5,jou376,Springer Netherlands,EuroWordNet: A multilingual database with lexical semantic networks,,Article,vol376
pap2352,977fe5853db16e320917a43fb00f334456625a1e,con46,Software Product Lines Conference,DIP: the Database of Interacting Proteins,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions.",Erratum,pro46
pap2353,c2eb8cbfa71ead3e30d08fa5f2712a51950c6a40,con56,International Conference on Software Engineering and Knowledge Engineering,"Principles of database and knowledge-base systems, Vol. I",,Erratum,pro56
pap2354,a0883d134b5abb7928483eb0859832a66a51fbf9,con70,International Conference on Graph Transformation,The PROSITE database,"The PROSITE database consists of a large collection of biologically meaningful signatures that are described as patterns or profiles. Each signature is linked to a documentation that provides useful biological information on the protein family, domain or functional site identified by the signature. The PROSITE database is now complemented by a series of rules that can give more precise information about specific residues. During the last 2 years, the documentation and the ScanProsite web pages were redesigned to add more functionalities. The latest version of PROSITE (release 19.11 of September 27, 2005) contains 1329 patterns and 552 profile entries. Over the past 2 years more than 200 domains have been added, and now 52% of UniProtKB/Swiss-Prot entries (release 48.1 of September 27, 2005) have a cross-reference to a PROSITE entry. The database is accessible at .",Erratum,pro70
pap2355,695557ab15e44bee66d532d52b81a37decd87d70,con62,Australian Software Engineering Conference,The Object-Oriented Database System Manifesto,,Erratum,pro62
pap2356,c3b16176728c7f785802f84df5aacffbc82ad431,con78,Neural Information Processing Systems,Database abstractions: aggregation and generalization,"Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given.",Erratum,pro78
pap2357,2acf7e58f0a526b957be2099c10aab693f795973,con104,Biometrics and Identity Management,Bosphorus Database for 3D Face Analysis,,Letter,pro104
pap2358,bf9e27a62e100e46c5060c7ea79a0d97ce6c1a79,con24,International Conference on Data Technologies and Applications,An atomic and molecular database for analysis of submillimetre line observations,"Atomic and molecular data for the transitions of a number of astrophysically interesting species are summarized, in- cluding energy levels, statistical weights, Einstein A-coefficients and collisional rate coefficients. Available collisional data from quantum chemical calculations and experiments are extrapolated to higher energies (up to E/k ∼ 1000 K). These data, which are made publically available through the WWW at http://www.strw.leidenuniv.nl/∼moldata, are essential input for non-LTE line radiative transfer programs. An online version of a computer program for performing statistical equilibrium calcu- lations is also made available as part of the database. Comparisons of calculated emission lines using different sets of collisional rate coefficients are presented. This database should form an important tool in analyzing observations from current and future (sub)millimetre and infrared telescopes.",Erratum,pro24
pap2359,db45667093e4fa4f95bc402c10b460052119717f,con72,Bioinformatics and Computational Biology,mVOC: a database of microbial volatiles,"Scents are well known to be emitted from flowers and animals. In nature, these volatiles are responsible for inter- and intra-organismic communication, e.g. attraction and defence. Consequently, they influence and improve the establishment of organisms and populations in ecological niches by acting as single compounds or in mixtures. Despite the known wealth of volatile organic compounds (VOCs) from species of the plant and animal kingdom, in the past, less attention has been focused on volatiles of microorganisms. Although fast and affordable sequencing methods facilitate the detection of microbial diseases, however, the analysis of signature or fingerprint volatiles will be faster and easier. Microbial VOCs (mVOCs) are presently used as marker to detect human diseases, food spoilage or moulds in houses. Furthermore, mVOCs exhibited antagonistic potential against pathogens in vitro, but their biological roles in the ecosystems remain to be investigated. Information on volatile emission from bacteria and fungi is presently scattered in the literature, and no public and up-to-date collection on mVOCs is available. To address this need, we have developed mVOC, a database available online at http://bioinformatics.charite.de/mvoc.",Erratum,pro72
pap2360,02e72b05d309cbc7652666c72a2dbb0bc68cd9ea,con9,Big Data,Immune epitope database analysis resource,"The immune epitope database analysis resource (IEDB-AR: http://tools.iedb.org) is a collection of tools for prediction and analysis of molecular targets of T- and B-cell immune responses (i.e. epitopes). Since its last publication in the NAR webserver issue in 2008, a new generation of peptide:MHC binding and T-cell epitope predictive tools have been added. As validated by different labs and in the first international competition for predicting peptide:MHC-I binding, their predictive performances have improved considerably. In addition, a new B-cell epitope prediction tool was added, and the homology mapping tool was updated to enable mapping of discontinuous epitopes onto 3D structures. Furthermore, to serve a wider range of users, the number of ways in which IEDB-AR can be accessed has been expanded. Specifically, the predictive tools can be programmatically accessed using a web interface and can also be downloaded as software packages.",Erratum,pro9
pap2361,0758a501039f9e2dfb7607507f9734155c52c7fc,con106,International Conference on Mobile Data Management,The Comparative Toxicogenomics Database: update 2013,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between environmental chemicals and gene products and their relationships to diseases. Chemical–gene, chemical–disease and gene–disease interactions manually curated from the literature are integrated to generate expanded networks and predict many novel associations between different data types. CTD now contains over 15 million toxicogenomic relationships. To navigate this sea of data, we added several new features, including DiseaseComps (which finds comparable diseases that share toxicogenomic profiles), statistical scoring for inferred gene–disease and pathway–chemical relationships, filtering options for several tools to refine user analysis and our new Gene Set Enricher (which provides biological annotations that are enriched for gene sets). To improve data visualization, we added a Cytoscape Web view to our ChemComps feature, included color-coded interactions and created a ‘slim list’ for our MEDIC disease vocabulary (allowing diseases to be grouped for meta-analysis, visualization and better data management). CTD continues to promote interoperability with external databases by providing content and cross-links to their sites. Together, this wealth of expanded chemical–gene–disease data, combined with novel ways to analyze and view content, continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases.",Erratum,pro106
pap2362,aa7b1246e367b5a1154bdc877558a8a4a8474f96,con51,Brazilian Symposium on Software Engineering,The MRC Psycholinguistic Database,"This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",Erratum,pro51
pap2363,8a61d0e598d1a1d47fa4f744081cd39255a3f508,jou133,PLoS ONE,TCM Database@Taiwan: The World's Largest Traditional Chinese Medicine Database for Drug Screening In Silico,"Rapid advancing computational technologies have greatly speeded up the development of computer-aided drug design (CADD). Recently, pharmaceutical companies have increasingly shifted their attentions toward traditional Chinese medicine (TCM) for novel lead compounds. Despite the growing number of studies on TCM, there is no free 3D small molecular structure database of TCM available for virtual screening or molecular simulation. To address this shortcoming, we have constructed TCM Database@Taiwan (http://tcm.cmu.edu.tw/) based on information collected from Chinese medical texts and scientific publications. TCM Database@Taiwan is currently the world's largest non-commercial TCM database. This web-based database contains more than 20,000 pure compounds isolated from 453 TCM ingredients. Both cdx (2D) and Tripos mol2 (3D) formats of each pure compound in the database are available for download and virtual screening. The TCM database includes both simple and advanced web-based query options that can specify search clauses, such as molecular properties, substructures, TCM ingredients, and TCM classification, based on intended drug actions. The TCM database can be easily accessed by all researchers conducting CADD. Over the last eight years, numerous volunteers have devoted their time to analyze TCM ingredients from Chinese medical texts as well as to construct structure files for each isolated compound. We believe that TCM Database@Taiwan will be a milestone on the path towards modernizing traditional Chinese medicine.",Letter,vol133
pap2364,755ef09cc0f7593b792482edc5bf799138243acf,jou113,Behavior Research Methods,The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance,,Article,vol113
pap2365,ff9186e43abd68e55fbcb9ba992944c7497bacab,jou228,Trends in Genetics,Repbase update: a database and an electronic journal of repetitive elements.,,Conference paper,vol228
pap2366,5f47123f5d86019c79c89f75ef6b44a60039f347,jou377,Multimedia tools and applications,SCface – surveillance cameras face database,,Conference paper,vol377
pap2367,999db6b11c1fe6377118081c84f79f6ae6b4262d,con110,Very Large Data Bases Conference,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The database has been expanded to include proteolytic enzymes other than peptidases. Special identifiers for peptidases from a variety of model organisms have been established so that orthologues can be detected in other species. A table of predicted active-site residue and metal ligand positions and the residue ranges of the peptidase domains in orthologues has been added to each peptidase summary. New displays of tertiary structures, which can be rotated or have the surfaces displayed, have been added to the structure pages. New indexes for gene names and peptidase substrates have been made available. Among the enhancements to existing features are the inclusion of small-molecule inhibitors in the tables of peptidase–inhibitor interactions, a table of known cleavage sites for each protein substrate, and tables showing the substrate-binding preferences of peptidases derived from combinatorial peptide substrate libraries.",Erratum,pro110
pap2368,638f10c6cc396907b98424621f6420a4287d342f,con18,International Conference on Exploring Services Science,rrndb: the Ribosomal RNA Operon Copy Number Database,"The Ribosomal RNA Operon Copy Number Database (rrndb) is an Internet-accessible database containing annotated information on rRNA operon copy number among prokaryotes. Gene redundancy is uncommon in prokaryotic genomes, yet the rRNA genes can vary from one to as many as 15 copies. Despite the widespread use of 16S rRNA gene sequences for identification of prokaryotes, information on the number and sequence of individual rRNA genes in a genome is not readily accessible. In an attempt to understand the evolutionary implications of rRNA operon redundancy, we have created a phylogenetically arranged report on rRNA gene copy number for a diverse collection of prokaryotic microorganisms. Each entry (organism) in the rrndb contains detailed information linked directly to external websites including the Ribosomal Database Project, GenBank, PubMed and several culture collections. Data contained in the rrndb will be valuable to researchers investigating microbial ecology and evolution using 16S rRNA gene sequences. The rrndb web site is directly accessible on the WWW at http://rrndb.cme. msu.edu.",Erratum,pro18
pap2369,f3cdea5fe196a7558afd9fcb8f3dacb69fdbe3d3,con93,International Conference on Computational Logic,"The InterPro database, an integrated documentation resource for protein families, domains and functional sites","Signature databases are vital tools for identifying distant relationships in novel sequences and hence for inferring protein function. InterPro is an integrated documentation resource for protein families, domains and functional sites, which amalgamates the efforts of the PROSITE, PRINTS, Pfam and ProDom database projects. Each InterPro entry includes a functional description, annotation, literature references and links back to the relevant member database(s). Release 2.0 of InterPro (October 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification encoded by a total of 6804 different regular expressions, profiles, fingerprints and Hidden Markov Models. Each InterPro entry lists all the matches against SWISS-PROT and TrEMBL (more than 1,000,000 hits from 462,500 proteins in SWISS-PROT and TrEMBL). The database is accessible for text- and sequence-based searches at http://www.ebi.ac.uk/interpro/. Questions can be emailed to interhelp@ebi.ac.uk.",Erratum,pro93
pap2370,47e7dc1724b5a3c12154c134f898c58ca4e9c49c,con19,International Conference on Conceptual Structures,SGD: Saccharomyces Genome Database,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/",Erratum,pro19
pap2371,61076194ec631a89daa30edbcc90bc7be37804cc,con39,EUROMICRO Conference on Software Engineering and Advanced Applications,The NCBI BioSystems database,"The NCBI BioSystems database, found at http://www.ncbi.nlm.nih.gov/biosystems/, centralizes and cross-links existing biological systems databases, increasing their utility and target audience by integrating their pathways and systems into NCBI resources. This integration allows users of NCBI’s Entrez databases to quickly categorize proteins, genes and small molecules by metabolic pathway, disease state or other BioSystem type, without requiring time-consuming inference of biological relationships from the literature or multiple experimental datasets.",Erratum,pro39
pap2372,d70c182a71aea05a145391b24d6bc3cdeede32a5,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Database-friendly random projections,"A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions.",Letter,pro85
pap2373,6c26791be6a51844f2784cd402876b18f110c5e4,con52,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",IntAct: an open source molecular interaction database,"IntAct provides an open source database and toolkit for the storage, presentation and analysis of protein interactions. The web interface provides both textual and graphical representations of protein interactions, and allows exploring interaction networks in the context of the GO annotations of the interacting proteins. A web service allows direct computational access to retrieve interaction networks in XML format. IntAct currently contains approximately 2200 binary and complex interactions imported from the literature and curated in collaboration with the Swiss-Prot team, making intensive use of controlled vocabularies to ensure data consistency. All IntAct software, data and controlled vocabularies are available at http://www.ebi.ac.uk/intact.",Erratum,pro52
pap2374,633888a9e6ac257c3e1e3d480525231c1627dc8d,con75,Intelligent Systems in Molecular Biology,"The RNA modification database, RNAMDB: 2011 update","Since its inception in 1994, The RNA Modification Database (RNAMDB, http://rna-mdb.cas.albany.edu/RNAmods/) has served as a focal point for information pertaining to naturally occurring RNA modifications. In its current state, the database employs an easy-to-use, searchable interface for obtaining detailed data on the 109 currently known RNA modifications. Each entry provides the chemical structure, common name and symbol, elemental composition and mass, CA registry numbers and index name, phylogenetic source, type of RNA species in which it is found, and references to the first reported structure determination and synthesis. Though newly transferred in its entirety to The RNA Institute, the RNAMDB continues to grow with two notable additions, agmatidine and 8-methyladenosine, appended in the last year. The RNA Modification Database is staying up-to-date with significant improvements being prepared for inclusion within the next year and the following year. The expanded future role of The RNA Modification Database will be to serve as a primary information portal for researchers across the entire spectrum of RNA-related research.",Erratum,pro75
pap2375,7a350beede1b8eda39ce22bca62732bcd6677ebd,con97,ACM SIGMOD Conference,Concurrency Control in Distributed Database Systems,"In this paper we survey, consolidate, and present the state of the art in distributed database concurrency control. The heart of our analysts is a decomposition of the concurrency control problem into two major subproblems: read-write and write-write synchronization. We describe a series of synchromzation techniques for solving each subproblem and show how to combine these techniques into algorithms for solving the entire concurrency control problem. Such algorithms are called ""concurrency control methods."" We describe 48 principal methods, including all practical algorithms that have appeared m the literature plus several new ones. We concentrate on the structure and correctness of concurrency control algorithms. Issues of performance are given only secondary treatment.",Erratum,pro97
pap2376,fa3c3fb3db6d54105c7990b6fd3ef41f3aff439d,con8,Frontiers in Education Conference,Human immunodeficiency virus reverse transcriptase and protease sequence database,"The HIV reverse transcriptase and protease sequence database is an on-line relational database that catalogues evolutionary and drug-related sequence variation in the human immunodeficiency virus (HIV) reverse transcriptase (RT) and protease enzymes, the molecular targets of antiretroviral therapy (http://hivdb.stanford.edu). The database contains a compilation of nearly all published HIV RT and protease sequences, including submissions to GenBank, sequences published in journal articles and sequences of HIV isolates from persons participating in clinical trials. Sequences are linked to data about the source of the sequence, the antiretroviral drug treatment history of the person from whom the sequence was obtained and the results of in vitro drug susceptibility testing. Sequence data on two new molecular targets of HIV drug therapy--gp41 (cell fusion) and integrase--will be added to the database in 2003.",Erratum,pro8
pap2377,df0708235e6c40899f7c9c14dff25ea8b86fdd19,jou378,New England Journal of Medicine,The ClinicalTrials.gov results database--update and key issues.,"BACKGROUND
The ClinicalTrials.gov trial registry was expanded in 2008 to include a database for reporting summary results. We summarize the structure and contents of the results database, provide an update of relevant policies, and show how the data can be used to gain insight into the state of clinical research.


METHODS
We analyzed ClinicalTrials.gov data that were publicly available between September 2009 and September 2010.


RESULTS
As of September 27, 2010, ClinicalTrials.gov received approximately 330 new and 2000 revised registrations each week, along with 30 new and 80 revised results submissions. We characterized the 79,413 registry and 2178 results of trial records available as of September 2010. From a sample cohort of results records, 78 of 150 (52%) had associated publications within 2 years after posting. Of results records available publicly, 20% reported more than two primary outcome measures and 5% reported more than five. Of a sample of 100 registry record outcome measures, 61% lacked specificity in describing the metric used in the planned analysis. In a sample of 700 results records, the mean number of different analysis populations per study group was 2.5 (median, 1; range, 1 to 25). Of these trials, 24% reported results for 90% or less of their participants.


CONCLUSIONS
ClinicalTrials.gov provides access to study results not otherwise available to the public. Although the database allows examination of various aspects of ongoing and completed clinical trials, its ultimate usefulness depends on the research community to submit accurate, informative data.",Letter,vol378
pap2378,782d8e30d599e1499555268b5b97c4a86b6bc25b,con62,Australian Software Engineering Conference,The EMBL Nucleotide Sequence Database,"The EMBL Nucleotide Sequence Database (http://www.ebi.ac.uk/embl), maintained at the European Bioinformatics Institute (EBI) near Cambridge, UK, is a comprehensive collection of nucleotide sequences and annotation from available public sources. The database is part of an international collaboration with DDBJ (Japan) and GenBank (USA). Data are exchanged daily between the collaborating institutes to achieve swift synchrony. Webin is the preferred tool for individual submissions of nucleotide sequences, including Third Party Annotation (TPA) and alignments. Automated procedures are provided for submissions from large-scale sequencing projects and data from the European Patent Office. New and updated data records are distributed daily and the whole EMBL Nucleotide Sequence Database is released four times a year. Access to the sequence data is provided via ftp and several WWW interfaces. With the web-based Sequence Retrieval System (SRS) it is also possible to link nucleotide data to other specialist molecular biology databases maintained at the EBI. Other tools are available for sequence similarity searching (e.g. FASTA and BLAST). Changes over the past year include the removal of the sequence length limit, the launch of the EMBLCDSs dataset, extension of the Sequence Version Archive functionality and the revision of quality rules for TPA data.",Erratum,pro62
pap2379,cd7763d7c118bc875ea34b30b52d0d95257b1418,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,TRANSFAC: a database on transcription factors and their DNA binding sites,TRANSFAC is a database about eukaryotic transcription regulating DNA sequence elements and the transcription factors binding to and acting through them. This report summarizes the present status of this database and accompanying retrieval tools.,Erratum,pro13
pap2380,b0bf5eb499c483b94efd57135cf9572f2bb4bb8f,con83,Networks,Systemic Banking Crises Database; An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",Erratum,pro83
pap2381,9a0723e76b4fce1cdfd407ed31a2b45130b4b423,con89,Conference on Uncertainty in Artificial Intelligence,"The PROSITE database, its status in 1999","The PROSITE database (http://www.expasy.ch/sprot/prosite.htm l) consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",Erratum,pro89
pap2382,4d130f394bd16320ac41112eebd3af74a129c6be,con59,Annual Workshop of the Psychology of Programming Interest Group,A KINETIC DATABASE FOR ASTROCHEMISTRY (KIDA),"We present a novel chemical database for gas-phase astrochemistry. Named the KInetic Database for Astrochemistry (KIDA), this database consists of gas-phase reactions with rate coefficients and uncertainties that will be vetted to the greatest extent possible. Submissions of measured and calculated rate coefficients are welcome, and will be studied by experts before inclusion into the database. Besides providing kinetic information for the interstellar medium, KIDA is planned to contain such data for planetary atmospheres and for circumstellar envelopes. Each year, a subset of the reactions in the database (kida.uva) will be provided as a network for the simulation of the chemistry of dense interstellar clouds with temperatures between 10 K and 300 K. We also provide a code, named Nahoon, to study the time-dependent gas-phase chemistry of zero-dimensional and one-dimensional interstellar sources.",Erratum,pro59
pap2383,250718af678dedb2938775f9ca6fb8601749778a,con50,International Workshop on Green and Sustainable Software,Social Conflict in Africa: A New Database,"We describe the Social Conflict in Africa Database (SCAD), a new event dataset for conducting research and analysis on various forms of social and political unrest in Africa. SCAD contains information on over 7,200 instances of protests, riots, strikes, government repression, communal violence, and other forms of unrest for 47 African countries from 1990–2010. SCAD includes information on event dates, actors and targets, lethality, georeferenced location information, and other conflict attributes. This article gives an overview of the data collection process, presents descriptive statistics and trends across the continent, and compares SCAD to the widely used Banks event data. We believe that SCAD will be a useful resource for scholars across multiple disciplines as well as for the policy community.",Erratum,pro50
pap2384,7fbb81ee7df0f48b3e55649cf41bcc506d19b314,con3,Knowledge Discovery and Data Mining,The American Mineralogist crystal structure database,"A database has been constructed that contains all the crystal structures previously published in the American Mineralogist. The database is called “The American Mineralogist Crystal Structure Database” and is freely accessible from the websites of the Mineralogical Society of America at http://www.minsocam.org/MSA/Crystal_Database.html and the University of Arizona. In addition to the database, a suite of interactive software is provided that can be used to view and manipulate the crystal structures and compute different properties of a crystal such as geometry, diffraction patterns, and procrystal electron densities. The database is set up so that the data can be easily incorporated into other software packages. Included at the website is an evolving set of guides to instruct the user and help with classroom education. parameters; (5) incorporating comments from either the original authors or ourselves when changes are made to the originally published data. Each record in the database consists of a bibliographic reference, cell parameters, symmetry, atomic positions, displacement parameters, and site occupancies. An example of a data set is provided in Figure 1. The first part of each data set contains identifying information, bibliography and notes, while the second part contains the crystallographic parameters. The first line of a data file contains an identifier, such as the name of the mineral or formula of the chemical species. The next line(s) contain the names of the authors, each separated by a comma. This is followed by the journal reference, title of the paper, and additional notes. The crystallographic data begins with a listing of the cell parameters and space group. If the data is given with respect to a non-standard space group origin then an asterisk precedes the space group symbol and the next line contains the translation vector from the standard origin. The 1952 edition of the International Tables for X-ray Crystallography are used to define the standard origin. The rest of the data set is a fixed-formatted listing of the atoms, their positional and displacement parameters, and occupancies. A header is provided that defines rightjustified columns. The name of each atom identifies the occupying elements, with additional identifiers added when appropriate. For instance, “Oco” identifies a particular oxygen atom in the albite structure. Some data sets report a crystallographic site occupied by molecular species rather than elemental, such as OH, water or methane. In most of these cases the atom name is denoted by molecular formula. For example, “CH4” denotes methane, and “Wat” denotes water. The displacement factors are tabulated in one of two formats, U’s or b’s",Erratum,pro3
pap2385,351bbaa6d0b597175a17f59f822c8e0d1fdebe03,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Defining and cataloging exoplanets: the exoplanet.eu database,"We describe an online database for extrasolar planetary-mass candidates, which is updated regularly as new data are available. We first discuss criteria for inclusion of objects in the catalog: “definition” of a planet and several aspects of the confidence level of planet candidates. We are led to point out the contradiction between the sharpness of criteria for belonging to a catalog and the fuzziness of the confidence level for an object to be a planet. We then describe the different tables of extrasolar planetary systems, including unconfirmed candidates (which will ultimately be confirmed, or not, by direct imaging). It also provides online tools: histograms of planet and host star data, cross-correlations between these parameters, and some Virtual Observatory services. Future evolutions of the database are presented.",Erratum,pro21
pap2386,a6859f695e6b2bd967df7cdb8becf8c9465b472a,con48,ACM Symposium on Applied Computing,The 'Dresden Image Database' for benchmarking digital image forensics,"This paper introduces and documents a novel image database specifically built for the purpose of development and bench-marking of camera-based digital forensic techniques. More than 14,000 images of various indoor and outdoor scenes have been acquired under controlled and thus widely comparable conditions from altogether 73 digital cameras. The cameras were drawn from only 25 different models to ensure that device-specific and model-specific characteristics can be disentangled and studied separately, as validated with results in this paper. In addition, auxiliary images for the estimation of device-specific sensor noise pattern were collected for each camera. Another subset of images to study model-specific JPEG compression algorithms has been compiled for each model. The 'Dresden Image Database' will be made freely available for scientific purposes when this accompanying paper is presented. The database is intended to become a useful resource for researchers and forensic investigators. Using a standard database as a benchmark not only makes results more comparable and reproducible, but it is also more economical and avoids potential copyright and privacy issues that go along with self-sampled benchmark sets from public photo communities on the Internet.",Conference paper,pro48
pap2387,fe134631e96a8937b1cc93952e895d882c536655,con96,Interspeech,The International Nucleotide Sequence Database Collaboration,"Under the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org), globally comprehensive public domain nucleotide sequence is captured, preserved and presented. The partners of this long-standing collaboration work closely together to provide data formats and conventions that enable consistent data submission to their databases and support regular data exchange around the globe. Clearly defined policy and governance in relation to free access to data and relationships with journal publishers have positioned INSDC databases as a key provider of the scientific record and a core foundation for the global bioinformatics data infrastructure. While growth in sequence data volumes comes no longer as a surprise to INSDC partners, the uptake of next-generation sequencing technology by mainstream science that we have witnessed in recent years brings a step-change to growth, necessarily making a clear mark on INSDC strategy. In this article, we introduce the INSDC, outline data growth patterns and comment on the challenges of increased growth.",Erratum,pro96
pap2388,dd79f74b9f5537ceecd097563a20546dd60937f6,con64,British Computer Society Conference on Human-Computer Interaction,The VizieR database of astronomical catalogues,"VizieR is a database grouping in an homoge- neous way thousands of astronomical catalogues gath- ered for decades by the Centre de Donn ees de Strasbourg (CDS) and participating institutes. The history and cur- rent status of this large collection is briefly presented, and the way these catalogues are being standardized to t in the VizieR system is described. The architecture of the database is then presented, with emphasis on the man- agement of links and of accesses to very large catalogues. Several query interfaces are currently available, making use of the ASU protocol, for browsing purposes or for use by other data processing systems such as visualisa- tion tools.",Erratum,pro64
pap2389,48671641597e73e7e2ba3067a029d7e632fc5e59,con68,Experimental Software Engineering Network,An Overview of the Global Historical Climatology Network Temperature Database,"Abstract The Global Historical Climatology Network version 2 temperature database was released in May 1997. This century-scale dataset consists of monthly surface observations from ∼7000 stations from around the world. This archive breaks considerable new ground in the field of global climate databases. The enhancements include 1) data for additional stations to improve regional-scale analyses, particularly in previously data-sparse areas; 2) the addition of maximum–minimum temperature data to provide climate information not available in mean temperature data alone; 3) detailed assessments of data quality to increase the confidence in research results; 4) rigorous and objective homogeneity adjustments to decrease the effect of nonclimatic factors on the time series; 5) detailed metadata (e.g., population, vegetation, topography) that allow more detailed analyses to be conducted; and 6) an infrastructure for updating the archive at regular intervals so that current climatic conditions can constantly be put...",Erratum,pro68
pap2390,798e312dd67798024da74f9a8f92946af88c7cd4,con54,Conference of the Centre for Advanced Studies on Collaborative Research,Comparative study of retinal vessel segmentation methods on a new publicly available database,"In this work we compare the performance of a number of vessel segmentation algorithms on a newly constructed retinal vessel image database. Retinal vessel segmentation is important for the detection of numerous eye diseases and plays an important role in automatic retinal disease screening systems. A large number of methods for retinal vessel segmentation have been published, yet an evaluation of these methods on a common database of screening images has not been performed. To compare the performance of retinal vessel segmentation methods we have constructed a large database of retinal images. The database contains forty images in which the vessel trees have been manually segmented. For twenty of those forty images a second independent manual segmentation is available. This allows for a comparison between the performance of automatic methods and the performance of a human observer. The database is available to the research community. Interested researchers are encouraged to upload their segmentation results to our website (http://www.isi.uu.nl/Research/Databases). The performance of five different algorithms has been compared. Four of these methods have been implemented as described in the literature. The fifth pixel classification based method was developed specifically for the segmentation of retinal vessels and is the only supervised method in this test. We define the segmentation accuracy with respect to our gold standard as the performance measure. Results show that the pixel classification method performs best, but the second observer still performs significantly better.",Erratum,pro54
pap2391,062cea54e5d58ee41aea607cbf2ba0cf457aa4e7,con105,British Machine Vision Conference,The DIARETDB1 Diabetic Retinopathy Database and Evaluation Protocol,"Automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.",Conference paper,pro105
pap2392,a7352bf3b88df27f0d0faa9d7ee7198a8b304c1d,con93,International Conference on Computational Logic,Development and use of a database of hydraulic properties of European soils,,Erratum,pro93
pap2393,251c272eef27fed72dd4e4e07c202f20e6dbd55a,con23,International Conference on Open and Big Data,A new version of the RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [ Nucleic Acids Res. (1997), 25, 109-111], is now hosted by the Center for Microbial Ecology at Michigan State University. RDP-II is a curated database that offers ribosomal RNA (rRNA) nucleotide sequence data in aligned and unaligned forms, analysis services, and associated computer programs. During the past two years, data alignments have been updated and now include >9700 small subunit rRNA sequences. The recent development of an ObjectStore database will provide more rapid updating of data, better data accuracy and increased user access. RDP-II includes phylogenetically ordered alignments of rRNA sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software programs for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (ftp.cme.msu. edu) and WWW (http://www.cme.msu.edu/RDP). The WWW server provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree. Additional utilities also exist at RDP-II, including distance matrix, T-RFLP, and a Java-based viewer of the phylogenetic trees that can be used to create subtrees.",Erratum,pro23
pap2394,387e290dfd914cc2bfa4a8d76386cad4c0b882d0,con97,ACM SIGMOD Conference,On supporting containment queries in relational database management systems,"Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.",Letter,pro97
pap2395,73f072aac4f44b860ab8eefd428573dfad3d44fc,jou106,Nucleic Acids Research,MIPS: a database for genomes and protein sequences.,"The Munich Information Center for Protein Sequences (MIPS-GSF, Neuherberg, Germany) continues to provide genome-related information in a systematic way. MIPS supports both national and European sequencing and functional analysis projects, develops and maintains automatically generated and manually annotated genome-specific databases, develops systematic classification schemes for the functional annotation of protein sequences, and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the databases for the comprehensive set of genomes (PEDANT genomes), the database of annotated human EST clusters (HIB), the database of complete cDNAs from the DHGP (German Human Genome Project), as well as the project specific databases for the GABI (Genome Analysis in Plants) and HNB (Helmholtz-Netzwerk Bioinformatik) networks. The Arabidospsis thaliana database (MATDB), the database of mitochondrial proteins (MITOP) and our contribution to the PIR International Protein Sequence Database have been described elsewhere [Schoof et al. (2002) Nucleic Acids Res., 30, 91-93; Scharfe et al. (2000) Nucleic Acids Res., 28, 155-158; Barker et al. (2001) Nucleic Acids Res., 29, 29-32]. All databases described, the protein analysis tools provided and the detailed descriptions of our projects can be accessed through the MIPS World Wide Web server (http://mips.gsf.de).",Article,vol106
pap2396,97dcab33aa0f1b8c98eec95e52e13596f3fb890d,con63,International Colloquium on Theoretical Aspects of Computing,The ecoinvent Database: Overview and Methodological Framework (7 pp),,Erratum,pro63
pap2397,816b0957fa05347951a1b37c29e21b67d9257c91,con29,ACM-SIAM Symposium on Discrete Algorithms,The ENZYME database in 2000,The ENZYME database is a repository of information related to the nomenclature of enzymes. In recent years it has became an indispensable resource for the development of metabolic databases. The current version contains information on 3705 enzymes. It is available through the ExPASy WWW server (http://www.expasy.ch/enzyme/ ).,Erratum,pro29
pap2398,0d6b4182d465f70e359e30372e550121a0fc94b0,con84,Workshop on Interdisciplinary Software Engineering Research,SCOP: a structural classification of proteins database,"The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and distant evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database so far. The sequences of proteins in SCOP provide the basis of the ASTRAL sequence libraries that can be used as a source of data to calibrate sequence search algorithms and for the generation of statistics on, or selections of, protein structures. Links can be made from SCOP to PDB-ISL: a library containing sequences homologous to proteins of known structure. Sequences of proteins of unknown structure can be matched to distantly related proteins of known structure by using pairwise sequence comparison methods to find homologues in PDB-ISL. The database and its associated files are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb.cam.ac.uk/scop/",Erratum,pro84
pap2399,c4907ef7d044ad71cc8b292c8b1e146987422ec7,con77,International Conference on Artificial Neural Networks,IPD—the Immuno Polymorphism Database,"The Immuno Polymorphism Database (IPD), http://www.ebi.ac.uk/ipd/ is a set of specialist databases related to the study of polymorphic genes in the immune system. The IPD project works with specialist groups or nomenclature committees who provide and curate individual sections before they are submitted to IPD for online publication. The IPD project stores all the data in a set of related databases. IPD currently consists of four databases: IPD-KIR, contains the allelic sequences of killer-cell immunoglobulin-like receptors, IPD-MHC, a database of sequences of the major histocompatibility complex of different species; IPD-HPA, alloantigens expressed only on platelets; and IPD-ESTDAB, which provides access to the European Searchable Tumour Cell-Line Database, a cell bank of immunologically characterized melanoma cell lines. The data is currently available online from the website and FTP directory. This article describes the latest updates and additional tools added to the IPD project.",Erratum,pro77
pap2400,711d5205c29fd772e22521cc4f9150db2f338d8e,con32,International Conference on Software Technology: Methods and Tools,NIST Atomic Spectra Database,"Accurate atomic data have great importance in astrophysics, plasma research, and other fields of physics. For more than 10 years, the Atomic Spectra Database (ASD) at the National Institute of Standards and Technology has served as a convenient and robust source of critically evaluated data on tens of thousands of spectral lines and energy levels. The recent upgrade of the ASD represents a significant new step in the development of dynamic databases providing powerful tools for data analysis and manipulation. We present a detailed description of ASD 3.0 emphasizing numerous advanced features and options for data search and presentation.",Erratum,pro32
pap2401,ad21c3cd8871347e3bdb7cb2800049f7e8a97aca,con59,Annual Workshop of the Psychology of Programming Interest Group,The IntAct molecular interaction database in 2012,"IntAct is an open-source, open data molecular interaction database populated by data either curated from the literature or from direct data depositions. Two levels of curation are now available within the database, with both IMEx-level annotation and less detailed MIMIx-compatible entries currently supported. As from September 2011, IntAct contains approximately 275 000 curated binary interaction evidences from over 5000 publications. The IntAct website has been improved to enhance the search process and in particular the graphical display of the results. New data download formats are also available, which will facilitate the inclusion of IntAct's data in the Semantic Web. IntAct is an active contributor to the IMEx consortium (http://www.imexconsortium.org). IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",Erratum,pro59
pap2402,17891bdbfec1950f7c361db96dca043cfbf54769,con97,ACM SIGMOD Conference,The TIGRFAMs database of protein families,"TIGRFAMs is a collection of manually curated protein families consisting of hidden Markov models (HMMs), multiple sequence alignments, commentary, Gene Ontology (GO) assignments, literature references and pointers to related TIGRFAMs, Pfam and InterPro models. These models are designed to support both automated and manually curated annotation of genomes. TIGRFAMs contains models of full-length proteins and shorter regions at the levels of superfamilies, subfamilies and equivalogs, where equivalogs are sets of homologous proteins conserved with respect to function since their last common ancestor. The scope of each model is set by raising or lowering cutoff scores and choosing members of the seed alignment to group proteins sharing specific function (equivalog) or more general properties. The overall goal is to provide information with maximum utility for the annotation process. TIGRFAMs is thus complementary to Pfam, whose models typically achieve broad coverage across distant homologs but end at the boundaries of conserved structural domains. The database currently contains over 1600 protein families. TIGRFAMs is available for searching or downloading at www.tigr.org/TIGRFAMs.",Erratum,pro97
pap2403,b507931840ac06717d273b267254a92c8bb7e918,con35,IEEE Working Conference on Mining Software Repositories,Drugs and Lactation Database: LactMed,The National Library of Medicine's Drugs and Lactation Database is an essential resource for any health care professional treating or answering the questions of breastfeeding women. It is available to anyone free of charge online through the TOXNET platform.,Erratum,pro35
pap2404,5c06b60a6940df55271fe5917848abf7ab3ca706,con25,IEEE International Parallel and Distributed Processing Symposium,The RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous FTP (rdp.life.uiuc.edu), electronic mail (server@rdp.life.uiuc.edu), gopher (rdpgopher.life.uiuc.edu) and WWW (http://rdpwww.life.uiuc.edu/ ). The electronic mail and WWW servers provide ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree.",Erratum,pro25
pap2405,0d74d07ee64ae04e01a1893d798a93680a9211c8,con33,International Conference on Automated Software Engineering,Conceptual Database Design: An Entity-Relationship Approach,I. CONCEPTUAL DATABASE DESIGN. 1. An Introduction to Database Design. 2. Data Modeling Concepts. 3. Methodologies for Conceptual Design. 4. View Design. 5. View Integration. 6. Improving the Quality of a Database Schema. 7. Schema Documentation and Maintenance. II. FUNCTIONAL ANALYSIS FOR DATABASE DESIGN. 1. Functional Analysis Using the Dataflow Model. 2. Joint Data and Functional Analysis. 3. Case Study. III. LOGICAL DESIGN AND DESIGN TOOLS. 1. High-Level Logical Design Using the Entity-Relationship Model. 2. Logical Design for the Relational Model. 3. Logical Design for the Network Model. 4. Logical Design for the Hierarchical Model. 5. Database Design Tools. Index. 0805302441T04062001,Erratum,pro33
pap2406,83a500fcc7cd98db063b73461277ac885c8fe7c3,con106,International Conference on Mobile Data Management,Towards Sensor Database Systems,,Letter,pro106
pap2407,4deebd0fdcec477a780e950dc0299beb872ea350,con46,Software Product Lines Conference,"BRENDA , the enzyme database : updates and major new developments","BRENDA (BRaunschweig ENzyme DAtabase) represents a comprehensive collection of enzyme and metabolic information, based on primary literature. The database contains data from at least 83 000 different enzymes from 9800 different organisms, classi®ed in ~4200 EC numbers. BRENDA includes biochemical and molecular information on classi®cation and nomenclature, reaction and speci®city, functional parameters, occurrence, enzyme structure, application, engineering, stability, disease, isolation and preparation, links and literature references. The data are extracted and evaluated from ~46 000 references, which are linked to PubMed as long as the reference is cited in PubMed. In the past year BRENDA has undergone major changes including a large increase in updating speed with >50% of all data updated in 2002 or in the ®rst half of 2003, the development of a new EC-tree browser, a taxonomy-tree browser, a chemical substructure search engine for ligand structure, the development of controlled vocabulary, an ontology for some information ®elds and a thesaurus for ligand names. The database is accessible free of charge to the academic community at http://www.brenda. uni-koeln.de.",Erratum,pro46
pap2408,09d73eecceb080eb1f7cea71d7df1411c712baf6,con69,Formal Concept Analysis,The National Land Cover Database,,Erratum,pro69
pap2409,49578a040f3346f81759ac40cc174cd12cb40045,con93,International Conference on Computational Logic,"JASPAR, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update","JASPAR is a popular open-access database for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12% in size, and three new specialized sub-databases have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at http://jaspar.genereg.net.",Erratum,pro93
pap2410,440819897d051bdb57182fd2a61777c7a8b710b7,con29,ACM-SIAM Symposium on Discrete Algorithms,The BioGRID Interaction Database: 2008 update,"The Biological General Repository for Interaction Datasets (BioGRID) database (http://www.thebiogrid.org) was developed to house and distribute collections of protein and genetic interactions from major model organism species. BioGRID currently contains over 198 000 interactions from six different species, as derived from both high-throughput studies and conventional focused studies. Through comprehensive curation efforts, BioGRID now includes a virtually complete set of interactions reported to date in the primary literature for both the budding yeast Saccharomyces cerevisiae and the fission yeast Schizosaccharomyces pombe. A number of new features have been added to the BioGRID including an improved user interface to display interactions based on different attributes, a mirror site and a dedicated interaction management system to coordinate curation across different locations. The BioGRID provides interaction data with monthly updates to Saccharomyces Genome Database, Flybase and Entrez Gene. Source code for the BioGRID and the linked Osprey network visualization system is now freely available without restriction.",Erratum,pro29
pap2411,5db8051dd2ee99996484bf1c48795c5ca13e04c5,con110,Very Large Data Bases Conference,XCOM: Photon Cross Section Database (version 1.2),,Erratum,pro110
pap2412,40691c4ba3c2eb44849273eb92dca66da6b634ad,con47,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,NIST Atomic Spectra Database (version 2.0),,Erratum,pro47
pap2413,8b683b12f9efc1d8bdf330182a0afd7c14369ce1,con68,Experimental Software Engineering Network,The Comparative Toxicogenomics Database: update 2011,"The Comparative Toxicogenomics Database (CTD) is a public resource that promotes understanding about the interaction of environmental chemicals with gene products, and their effects on human health. Biocurators at CTD manually curate a triad of chemical–gene, chemical–disease and gene–disease relationships from the literature. These core data are then integrated to construct chemical–gene–disease networks and to predict many novel relationships using different types of associated data. Since 2009, we dramatically increased the content of CTD to 1.4 million chemical–gene–disease data points and added many features, statistical analyses and analytical tools, including GeneComps and ChemComps (to find comparable genes and chemicals that share toxicogenomic profiles), enriched Gene Ontology terms associated with chemicals, statistically ranked chemical–disease inferences, Venn diagram tools to discover overlapping and unique attributes of any set of chemicals, genes or disease, and enhanced gene pathway data content, among other features. Together, this wealth of expanded chemical–gene–disease data continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases. CTD is freely available at http://ctd.mdibl.org.",Erratum,pro68
pap2414,06d16ecff3eeaecd86380de53d26f520068cdb9b,con10,Americas Conference on Information Systems,"Database Systems: A Practical Approach to Design, Implementation and Management","This best-selling text introduces the theory behind databases in a concise yet comprehensive manner, providing database design methodology that can be used by both technical and non-technical readers. The methodology for relational Database Management Systems is presented in simple, step-by-step instructions in conjunction with a realistic worked example using three explicit phasesconceptual, logical, and physical database design. Background: Introduction to Databases; Database Environment; Database Architectures and the Web. The Relational Model and Languages: The Relational model; Relational Algebra and Relational Calculus; SQL: Data Manipulation; SQL: Data Definition; Query-By-Example (QBE). Database Analysis and Design: Database System Lifecycle; Database Analysis and the DreamHome Case Study; EntityRelationship Modeling; Enhanced EntityRelationship Modeling; Normalization; Advanced Normalization. Methodology: MethodologyConceptual Database Design; MethodologyLogical Database Design for Relational Model; MethodologyPhysical Database Design for Relational Databases; MethodologyMonitoring and Tuning the Operational System. Selected Database Issues: Security and Administration; Professional, Legal, and Ethical Issues; Transaction Management; Query Processing. Distributed DBMSs and Replication: Distributed DBMSsConcepts and Design; Distributed DBMSsAdvanced Concepts; Replication and Mobile Databases. Object DBMSs: Object-Oriented DBMSsConcepts and Design; Object-Oriented DBMSsStandards and Languages; Object-Relational DBMSs. Web and DBMSs: Web Technology and DBMSs; Semistructured Data and XML. Business Intelligence Technologies: Data Warehousing Concepts; Data Warehousing Design; OLAP; Data Mining. Appendices: Users' Requirements Specification for DreamHome Case Study; Other Case Studies; Alternative Data Modeling Notations; Summary of the Database Design Methodology for Relational Databases; Introduction to PyrrhoA Liteweight RDBMS. Web Appendices: File Organization and Storage Structures; When Is a DBMS Relational?; Commercial DBMSs: Access and Oracle; Programmatic SQL; Estimating Disk Space Requirements; Introduction to Object-Orientation; Example Web Scripts. This book is ideal for readers interested in database management or database design.",Erratum,pro10
pap2415,fb2896bb515ad483260f2b937202d0e7289ddd16,con107,Chinese Conference on Biometric Recognition,SDUMLA-HMT: A Multimodal Biometric Database,,Letter,pro107
pap2416,9d79185d82c03c30778f3635bfbdcf605330f41b,con88,European Conference on Computer Vision,The Immune Epitope Database 2.0,"The Immune Epitope Database (IEDB, www.iedb.org) provides a catalog of experimentally characterized B and T cell epitopes, as well as data on Major Histocompatibility Complex (MHC) binding and MHC ligand elution experiments. The database represents the molecular structures recognized by adaptive immune receptors and the experimental contexts in which these molecules were determined to be immune epitopes. Epitopes recognized in humans, nonhuman primates, rodents, pigs, cats and all other tested species are included. Both positive and negative experimental results are captured. Over the course of 4 years, the data from 180 978 experiments were curated manually from the literature, which covers ∼99% of all publicly available information on peptide epitopes mapped in infectious agents (excluding HIV) and 93% of those mapped in allergens. In addition, data that would otherwise be unavailable to the public from 129 186 experiments were submitted directly by investigators. The curation of epitopes related to autoimmunity is expected to be completed by the end of 2010. The database can be queried by epitope structure, source organism, MHC restriction, assay type or host organism, among other criteria. The database structure, as well as its querying, browsing and reporting interfaces, was completely redesigned for the IEDB 2.0 release, which became publicly available in early 2009.",Erratum,pro88
pap2417,7463a0b934ac40a353773840485bb56d35fbbb66,con96,Interspeech,Database on medicinal plants used in Ayurveda,,Erratum,pro96
pap2418,17e6076b6761788684434d1e14e85e8877fc0146,con16,International Conference on Data Science and Advanced Analytics,LandScan: A Global Population Database for Estimating Populations at Risk,"The LandScan Global Population Project produced a world-wide 1998 population database at a 30-by 30-second resolution for estimating ambient populations at risk. Best available census counts were distributed to cells based on probability coefficients which, in turn, were based on road proximity, slope, land cover, and nighttime lights, LandScan 1998 has been completed for the entire world. Verification and validation (V&V) studies were conducted routinely for all regions and more extensively for Israel, Germany, and the southwestern United States. Geographic information systems (GIS) were essential for conflation of diverse input variables, computation of probability coefficients, allocation of population to cells, and reconciliation of cell totals with aggregate (usually province) control totals. Remote sensing was an essential source of two input variables-land cover and nighttime lights-and one ancillary database-high-resolution panchromatic imagery-used in V&V of the population model and resulting LandScan database.",Erratum,pro16
pap2419,2033531aeaf7d0da158cdaacae9b208407bd4a1c,con28,International Conference Geographic Information Science,AAindex: Amino Acid Index Database,"AAindex is a database of numerical indices representing various physicochemical and biochemical properties of amino acids and pairs of amino acids. It consists of two sections: AAindex1 for the amino acid index of 20 numerical values and AAindex2 for the amino acid mutation matrix of 210 numerical values. Each entry of either AAindex1 or AAindex2 consists of the definition, the reference information, a list of related entries in terms of the correlation coefficient, and the actual data. The database may be accessed through the DBGET/LinkDB system at GenomeNet (http://www.genome.ad. jp/dbget/) or may be downloaded by anonymous FTP (ftp://ftp.genome. ad.jp/db/genomenet/aaindex/).",Erratum,pro28
pap2420,5bf9cebe3658cfbf7f67c0a2680c8233509aa5e4,con63,International Colloquium on Theoretical Aspects of Computing,UCI Repository of Machine Learning Database,,Erratum,pro63
pap2421,a047888622c576ccf06a7708ae18a5d9ec5f09fd,jou372,"Behavoir research methods, instruments & computers",Lexique 2 : A new French lexical database,,Letter,vol372
pap2422,5524cacaae93810945f1b21e77f565f6c8bdcdef,con4,Conference on Innovative Data Systems Research,Relational Cloud: a Database Service for the cloud,"This paper introduces a new transactional “database-as-a-service” (DBaaS) called Relational Cloud. A DBaaS promises to move much of the operational burden of provisioning, configuration, scaling, performance tuning, backup, privacy, and access control from the database users to the service operator, offering lower overall costs to users. Early DBaaS efforts include Amazon RDS and Microsoft SQL Azure, which are promising in terms of establishing the market need for such a service, but which do not address three important challenges: efficient multi-tenancy, elastic scalability, and database privacy. We argue that these three challenges must be overcome before outsourcing database software and management becomes attractive to many users, and cost-effective for service providers. The key technical features of Relational Cloud include: (1) a workload-aware approach to multi-tenancy that identifies the workloads that can be co-located on a database server, achieving higher consolidation and better performance than existing approaches; (2) the use of a graph-based data partitioning algorithm to achieve near-linear elastic scale-out even for complex transactional workloads; and (3) an adjustable security scheme that enables SQL queries to run over encrypted data, including ordering operations, aggregates, and joins. An underlying theme in the design of the components of Relational Cloud is the notion of workload awareness: by monitoring query patterns and data accesses, the system obtains information useful for various optimization and security functions, reducing the configuration effort for users and operators.",Letter,pro4
pap2423,369f62edea6e6ace6f68c7ebe9bdde046b9514ad,con79,IEEE Annual Symposium on Foundations of Computer Science,The asteroid lightcurve database,,Erratum,pro79
pap2424,cc42c8dac7c3fb8cd522136f1c7c31ae45a3121f,jou106,Nucleic Acids Research,The MDM2 gene amplification database.,"The p53 tumor suppressor gene is inactivated in human tumors by several distinct mechanisms. The best characterized inactivation mechanisms are: (i) gene mutation; (ii) p53 protein association with viral proteins; (iii) p53 protein association with the MDM2 cellular oncoprotein. The MDM2 gene has been shown to be abnormally up-regulated in human tumors and tumor cell lines by gene amplification, increased transcript levels and enhanced translation. This communication presents a brief review of the spectrum of MDM2 abnormalities in human tumors and compares the tissue distribution of MDM2 amplification and p53 mutation frequencies. In this study, 3889 samples from tumors or xenografts from 28 tumor types were examined for MDM2 amplification from previously published sources. The overall frequency of MDM2 amplification in these human tumors was 7%. Gene amplification was observed in 19 tumor types, with the highest frequency observed in soft tissue tumors (20%), osteosarcomas (16%) and esophageal carcinomas (13%). Tumors which showed a higher incidence of MDM2 amplification than p53 mutation were soft tissue tumors, testicular germ cell cancers and neuro-blastomas. Data from studies where both MDM2 amplification and p53 mutations were analyzed within the same samples showed that mutations in these two genes do not generally occur within the same tumor. In these studies, 29 out of a total of 33 MDM2 amplification-positive tumors had wild-type p53. We hypothesize that heretofore uncharacterized carcinogens favor MDM2 amplification over p53 mutations in certain tumor types. A database listing the MDM2 gene amplifications is available on the World Wide Web at http://www. infosci.coh.org/mdm2 . Charts of MDM2 amplification frequencies and comparisons with p53 genetic alterations are also available at this Web site.",Conference paper,vol106
pap2425,a7ece6b4ad1f1688ba8afe3720e7f6942ec68f58,con3,Knowledge Discovery and Data Mining,The MetaCyc Database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"MetaCyc (MetaCyc.org) is a universal database of metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are curated from the primary scientific literature, and are experimentally determined small-molecule metabolic pathways. Each reaction in a MetaCyc pathway is annotated with one or more well-characterized enzymes. Because MetaCyc contains only experimentally elucidated knowledge, it provides a uniquely high-quality resource for metabolic pathways and enzymes. BioCyc (BioCyc.org) is a collection of more than 350 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the predicted metabolic network of one organism, including metabolic pathways, enzymes, metabolites and reactions predicted by the Pathway Tools software using MetaCyc as a reference database. BioCyc PGDBs also contain predicted operons and predicted pathway hole fillers—predictions of which enzymes may catalyze pathway reactions that have not been assigned to an enzyme. The BioCyc website offers many tools for computational analysis of PGDBs, including comparative analysis and analysis of omics data in a pathway context. The BioCyc PGDBs generated by SRI are offered for adoption by any interested party for the ongoing integration of metabolic and genome-related information about an organism.",Erratum,pro3
pap2426,e4b9a5ed3f838da72a8f3169a01be3268c4d3c2c,con53,Workshop on Web 2.0 for Software Engineering,NGA Project Strong-Motion Database,"A key component of the NGA research project was the development of a strong-motion database with improved quality and content that could be used for ground-motion research as well as for engineering practice. Development of the NGA database was executed through the Lifelines program of the PEER Center with contributions from several research organizations and many individuals in the engineering and seismological communities. Currently, the data set consists of 3551 publicly available multi-component records from 173 shallow crustal earthquakes, ranging in magnitude from 4.2 to 7.9. Each acceleration time series has been corrected and filtered, and pseudo absolute spectral acceleration at multiple damping levels has been computed for each of the 3 components of the acceleration time series. The lowest limit of usable spectral frequency was determined based on the type of filter and the filter corner frequency. For NGA model development, the two horizontal acceleration components were further rotated to form the orientation-independent measure of horizontal ground motion (GMRotI50). In addition to the ground-motion parameters, a large and comprehensive list of metadata characterizing the recording conditions of each record was also developed. NGA data have been systematically checked and reviewed by experts and NGA developers.",Erratum,pro53
pap2427,9e9801ef47bff55074165440a17d5122520837a2,con27,International Conference on Contemporary Computing,Concurrency Control and Recovery in Database Systems,"This book is an introduction to the design and implementation of concurrency control and recovery mechanisms for transaction management in centralized and distributed database systems. Concurrency control and recovery have become increasingly important as businesses rely more and more heavily on their on-line data processing activities. For high performance, the system must maximize concurrency by multiprogramming transactions. But this can lead to interference between queries and updates, which concurrency control mechanisms must avoid. In addition, a satisfactory recovery system is necessary to ensure that inevitable transaction and database system failures do not corrupt the database.",Erratum,pro27
pap2428,075082cfbedfe3161d15354b31859ca59dfbeafb,con108,International Conference on Information Integration and Web-based Applications & Services,A global database of soil respiration data,"Abstract. Soil respiration – RS, the flux of CO2 from the soil to the atmosphere – is probably the least well constrained component of the terrestrial carbon cycle. Here we introduce the SRDB database, a near-universal compendium of published RS data, and make it available to the scientific community both as a traditional static archive and as a dynamic community database that may be updated over time by interested users. The database encompasses all published studies that report one of the following data measured in the field (not laboratory): annual RS, mean seasonal RS, a seasonal or annual partitioning of RS into its sources fluxes, RS temperature response (Q10), or RS at 10 °C. Its orientation is thus to seasonal and annual fluxes, not shorter-term or chamber-specific measurements. To date, data from 818 studies have been entered into the database, constituting 3379 records. The data span the measurement years 1961–2007 and are dominated by temperate, well-drained forests. We briefly examine some aspects of the SRDB data – its climate space coverage, mean annual RS fluxes and their correlation with other carbon fluxes, RS variability, temperature sensitivities, and the partitioning of RS source flux – and suggest some potential lines of research that could be explored using these data. The SRDB database is available online in a permanent archive as well as via a project-hosting repository; the latter source leverages open-source software technologies to encourage wider participation in the database's future development. Ultimately, we hope that the updating of, and corrections to, the SRDB will become a shared project, managed by the users of these data in the scientific community.",Erratum,pro108
pap2429,369e98d934881e9cfd464a73e56011cb807ab104,con103,IEEE International Conference on Multimedia and Expo,"THE COLOGNE DATABASE FOR MOLECULAR SPECTROSCOPY, CDMS",,Erratum,pro103
pap2430,353988f8d56224b9d46aa34059e499c638dcbc2e,jou219,Journal of chemical information and computer sciences,The United Kingdom Chemical Database Service,"The Chemical Database Service (CDS) is a national service, funded by the Chemistry Programme of the United Kingdom Engineering and Physical Sciences Research Council (EPSRC). It provides access for UK academics to a range of chemistry databases in the areas of crystallography, synthetic organic chemistry, spectroscopy, and physical chemistry. Three post-doctoral chemists are available to assist users with problems, run training courses, and also give advice to the community on accessing other sources of chemical data and software.",Article,vol219
pap2431,2a92c98a943ae21360d52730f1f2117c7edb2ecb,con73,"ACM International Conference on Bioinformatics, Computational Biology and Biomedicine",The Regulation and Supervision of Banks around the World: A New Database,"International consultants on bank regulation, and supervision for developing countries, often base their advice on how their home country does things, for lack of information on practice in other countries. Recommendations for reform have tended to be shaped by bias rather than facts. To better inform advice about bank regulation, and supervision, and to lower the marginal cost of empirical research, the authors present, and discuss a new, and comprehensive database on the regulation, and supervision of banks in a hundred and seven countries. The data, based on surveys sent to national bank regulatory, supervisory authorities, are now available to researchers, and policymakers around the world. The data cover such aspects of banking as entry requirements, ownership restrictions, capital requirements, activity restrictions, external auditing requirements, characteristics of deposit insurance schemes, loan classification and provisioning requirements, accounting and disclosure requirements, troubled bank resolution actions, and (uniquely) the quality of supervisory personnel, and their actions. The database permits users to learn how banks are currently regulated, and supervised, and about bank structures, and deposit insurance schemes, for a broad cross-section of countries. In addition to describing the data, the authors show how variables ay be grouped, and aggregated. They also show some simple correlations among selected variables. In a comparison paper (""Bank regulation and supervision: What works best"") studying the relationship between differences in bank regulation and supervision, and bank performance and stability, they conclude that: 1) Countries with policies that promote private monitoring of banks, have better bank performance, and more stability. Countries with more generous deposit insurance schemes tend to have poorer bank performance, and more bank fragility. 2) Diversification of income streams, and loan portfolios - by not restricting bank activities - also tends to improve performance, and stability. (This works best when an active securities market exists). Countries in which banks are encouraged to diversify their portfolios, domestically and internationally, suffer fewer crisis.",Erratum,pro73
pap2432,b4ea6e57966ffdab58ec410e085acc1232064303,con53,Workshop on Web 2.0 for Software Engineering,"The PANTHER database of protein families, subfamilies, functions and pathways","PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31 705 subfamilies, covering ∼90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.",Erratum,pro53
pap2433,794c048acd3d2d35e3248161729fcf142b4966c6,con83,Networks,Phospho.ELM: a database of phosphorylation sites—update 2008,"Phospho.ELM is a manually curated database of eukaryotic phosphorylation sites. The resource includes data collected from published literature as well as high-throughput data sets. The current release of Phospho.ELM (version 7.0, July 2007) contains 4078 phospho-protein sequences covering 12 025 phospho-serine, 2362 phospho-threonine and 2083 phospho-tyrosine sites. The entries provide information about the phosphorylated proteins and the exact position of known phosphorylated instances, the kinases responsible for the modification (where known) and links to bibliographic references. The database entries have hyperlinks to easily access further information from UniProt, PubMed, SMART, ELM, MSD as well as links to the protein interaction databases MINT and STRING. A new BLAST search tool, complementary to retrieval by keyword and UniProt accession number, allows users to submit a protein query (by sequence or UniProt accession) to search against the curated data set of phosphorylated peptides. Phospho.ELM is available on line at: http://phospho.elm.eu.org",Erratum,pro83
pap2434,54f4ef17f6b7a315260e001af85fc8b6fc1d2a7b,con21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,The UCSC Genome Browser Database: update 2009,"The UCSC Genome Browser Database (GBD, http://genome.ucsc.edu) is a publicly available collection of genome assembly sequence data and integrated annotations for a large number of organisms, including extensive comparative-genomic resources. In the past year, 13 new genome assemblies have been added, including two important primate species, orangutan and marmoset, bringing the total to 46 assemblies for 24 different vertebrates and 39 assemblies for 22 different invertebrate animals. The GBD datasets may be viewed graphically with the UCSC Genome Browser, which uses a coordinate-based display system allowing users to juxtapose a wide variety of data. These data include all mRNAs from GenBank mapped to all organisms, RefSeq alignments, gene predictions, regulatory elements, gene expression data, repeats, SNPs and other variation data, as well as pairwise and multiple-genome alignments. A variety of other bioinformatics tools are also provided, including BLAT, the Table Browser, the Gene Sorter, the Proteome Browser, VisiGene and Genome Graphs.",Erratum,pro21
pap2435,523a87607f06f7ed56a0506bdb4671f76244264a,con11,European Conference on Modelling and Simulation,Introducing the Global Terrorism Database,"Compared to most types of criminal violence, terrorism poses special data collection challenges. In response, there has been growing interest in open source terrorist event data bases. One of the major problems with these data bases in the past is that they have been limited to international events—those involving a national or group of nationals from one country attacking targets physically located in another country. Past research shows that domestic incidents greatly outnumber international incidents. In this paper we describe a previously unavailable open source data base that includes some 70,000 domestic and international incidents since 1970. We began the Global Terrorism Database (GTD) by computerizing data originally collected by the Pinkerton Global Intelligence Service (PGIS). Following computerization, our research team has been working for the past two years to validate and extend the data to real time. In this paper, we describe our data collection efforts, the strengths and weaknesses of open source data in general and the GTD in particular, and provide descriptive statistics on the contents of this new resource.",Erratum,pro11
pap2436,aa163167f51e580c3dcb1aaef00f60b08b6f64a8,con53,Workshop on Web 2.0 for Software Engineering,EcoCyc: a comprehensive database resource for Escherichia coli,"The EcoCyc database (http://EcoCyc.org/) is a comprehensive source of information on the biology of the prototypical model organism Escherichia coli K12. The mission for EcoCyc is to contain both computable descriptions of, and detailed comments describing, all genes, proteins, pathways and molecular interactions in E.coli. Through ongoing manual curation, extensive information such as summary comments, regulatory information, literature citations and evidence types has been extracted from 8862 publications and added to Version 8.5 of the EcoCyc database. The EcoCyc database can be accessed through a World Wide Web interface, while the downloadable Pathway Tools software and data files enable computational exploration of the data and provide enhanced querying capabilities that web interfaces cannot support. For example, EcoCyc contains carefully curated information that can be used as training sets for bioinformatics prediction of entities such as promoters, operons, genetic networks, transcription factor binding sites, metabolic pathways, functionally related genes, protein complexes and protein–ligand interactions.",Erratum,pro53
pap2437,a5881da1c592ea11d24f90992f5b210beaa3ea73,con97,ACM SIGMOD Conference,Gigascope: a stream database for network applications,"We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.",Article,pro97
pap2438,78597d4989a7f7f892067637dd3271e60569b087,con28,International Conference Geographic Information Science,DisProt: the Database of Disordered Proteins,"The Database of Protein Disorder (DisProt) links structure and function information for intrinsically disordered proteins (IDPs). Intrinsically disordered proteins do not form a fixed three-dimensional structure under physiological conditions, either in their entireties or in segments or regions. We define IDP as a protein that contains at least one experimentally determined disordered region. Although lacking fixed structure, IDPs and regions carry out important biological functions, being typically involved in regulation, signaling and control. Such functions can involve high-specificity low-affinity interactions, the multiple binding of one protein to many partners and the multiple binding of many proteins to one partner. These three features are all enabled and enhanced by protein intrinsic disorder. One of the major hindrances in the study of IDPs has been the lack of organized information. DisProt was developed to enable IDP research by collecting and organizing knowledge regarding the experimental characterization and the functional associations of IDPs. In addition to being a unique source of biological information, DisProt opens doors for a plethora of bioinformatics studies. DisProt is openly available at .",Erratum,pro28
pap2439,9aa8a679e3401f1bbf805b738095bf2ed52e08e7,jou379,PLoS Medicine,Tuberculosis Drug Resistance Mutation Database,Andreas Sandgren and colleagues describe a new comprehensive resource on drug resistance mutations inM. tuberculosis.,Conference paper,vol379
pap2440,ed431581f9537896d26b7c8d9935dce9ee73871d,con51,Brazilian Symposium on Software Engineering,The International Nucleotide Sequence Database Collaboration,"The members of the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org) set out to capture, preserve and present globally comprehensive public domain nucleotide sequence information. The work of the long-standing collaboration includes the provision of data formats, annotation conventions and routine global data exchange. Among the many developments to INSDC resources in 2011 are the newly launched BioProject database and improved handling of assembly information. In this article, we outline INSDC services and update the reader on developments in 2011.",Erratum,pro51
pap2441,b349449e4a3bf70882ec07d333fb4ec99a87ae62,con10,Americas Conference on Information Systems,NIST Computational Chemistry Comparison and Benchmark Database,,Erratum,pro10
pap2442,48fa4530c0eddf525b273e222753c978606243f7,con106,International Conference on Mobile Data Management,APD: the Antimicrobial Peptide Database,"An antimicrobial peptide database (APD) has been established based on an extensive literature search. It contains detailed information for 525 peptides (498 antibacterial, 155 antifungal, 28 antiviral and 18 antitumor). APD provides interactive interfaces for peptide query, prediction and design. It also provides statistical data for a select group of or all the peptides in the database. Peptide information can be searched using keywords such as peptide name, ID, length, net charge, hydrophobic percentage, key residue, unique sequence motif, structure and activity. APD is a useful tool for studying the structure-function relationship of antimicrobial peptides. The database can be accessed via a web-based browser at the URL: http://aps.unmc.edu/AP/main.html.",Erratum,pro106
pap2443,978a4276e0874a590d34aef84e6238ce21f0539e,con31,International Conference on Evaluation & Assessment in Software Engineering,Inparanoid: a comprehensive database of eukaryotic orthologs,"The Inparanoid eukaryotic ortholog database (http://inparanoid.cgb.ki.se/) is a collection of pairwise ortholog groups between 17 whole genomes; Anopheles gambiae, Caenorhabditis briggsae, Caenorhabditis elegans, Drosophila melanogaster, Danio rerio, Takifugu rubripes, Gallus gallus, Homo sapiens, Mus musculus, Pan troglodytes, Rattus norvegicus, Oryza sativa, Plasmodium falciparum, Arabidopsis thaliana, Escherichia coli, Saccharomyces cerevisiae and Schizosaccharomyces pombe. Complete proteomes for these genomes were derived from Ensembl and UniProt and compared pairwise using Blast, followed by a clustering step using the Inparanoid program. An Inparanoid cluster is seeded by a reciprocally best-matching ortholog pair, around which inparalogs (should they exist) are gathered independently, while outparalogs are excluded. The ortholog clusters can be searched on the website using Ensembl gene/protein or UniProt identifiers, annotation text or by Blast alignment against our protein datasets. The entire dataset can be downloaded, as can the Inparanoid program itself.",Erratum,pro31
pap2444,a5375b684c8e6640246df2eaec5f59b2ef94242b,con25,IEEE International Parallel and Distributed Processing Symposium,CDD: a curated Entrez database of conserved domain alignments,"The Conserved Domain Database (CDD) is now indexed as a separate database within the Entrez system and linked to other Entrez databases such as MEDLINE(R). This allows users to search for domain types by name, for example, or to view the domain architecture of any protein in Entrez's sequence database. CDD can be accessed on the WorldWideWeb at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. Users may also employ the CD-Search service to identify conserved domains in new sequences, at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. CD-Search results, and pre-computed links from Entrez's protein database, are calculated using the RPS-BLAST algorithm and Position Specific Score Matrices (PSSMs) derived from CDD alignments. CD-Searches are also run by default for protein-protein queries submitted to BLAST(R) at http://www.ncbi.nlm.nih.gov/BLAST. CDD mirrors the publicly available domain alignment collections SMART and PFAM, and now also contains alignment models curated at NCBI. Structure information is used to identify the core substructure likely to be present in all family members, and to produce sequence alignments consistent with structure conservation. This alignment model allows NCBI curators to annotate 'columns' corresponding to functional sites conserved among family members.",Erratum,pro25
pap2445,e13a71f639bed2c1c739b206a6a053e8f18651a6,con49,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies","BioModels Database: a free, centralized database of curated, published, quantitative kinetic models of biochemical and cellular systems","BioModels Database (), part of the international initiative BioModels.net, provides access to published, peer-reviewed, quantitative models of biochemical and cellular systems. Each model is carefully curated to verify that it corresponds to the reference publication and gives the proper numerical results. Curators also annotate the components of the models with terms from controlled vocabularies and links to other relevant data resources. This allows the users to search accurately for the models they need. The models can currently be retrieved in the SBML format, and import/export facilities are being developed to extend the spectrum of formats supported by the resource.",Erratum,pro49
pap2446,714e6dc60dba65431ecae63310da52462defd9c9,con59,Annual Workshop of the Psychology of Programming Interest Group,The Object Database Standard: ODMG-93,,Erratum,pro59
pap2447,e2a5593f587970018b9e826e5d876125b374b801,con29,ACM-SIAM Symposium on Discrete Algorithms,Integrating compression and execution in column-oriented database systems,"Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.",Erratum,pro29
pap2448,58efda5a28e5791adfde9ef6e330caf7b89349c6,con6,Annual Conference on Genetic and Evolutionary Computation,Providing database as a service,"We explore a novel paradigm for data management in which a third party service provider hosts ""database as a service"", providing its customers with seamless mechanisms to create, store, and access their databases at the host site. Such a model alleviates the need for organizations to purchase expensive hardware and software, deal with software upgrades, and hire professionals for administrative and maintenance tasks which are taken over by the service provider. We have developed and deployed a database service on the Internet, called NetDB2, which is in constant use. In a sense, a data management model supported by NetDB2 provides an effective mechanism for organizations to purchase data management as a service, thereby freeing them to concentrate on their core businesses. Among the primary challenges introduced by ""database as a service"" are the additional overhead of remote access to data, an infrastructure to guarantee data privacy, and user interface design for such a service. These issues are investigated. We identify data privacy as a particularly vital problem and propose alternative solutions based on data encryption. The paper is meant as a challenge for the database community to explore a rich set of research issues that arise in developing such a service.",Erratum,pro6
pap2449,03094c68e9333dbfb17426d22d4e61748d92e414,con64,British Computer Society Conference on Human-Computer Interaction,The BioGRID Interaction Database: 2011 update,"The Biological General Repository for Interaction Datasets (BioGRID) is a public database that archives and disseminates genetic and protein interaction data from model organisms and humans (http://www.thebiogrid.org). BioGRID currently holds 347 966 interactions (170 162 genetic, 177 804 protein) curated from both high-throughput data sets and individual focused studies, as derived from over 23 000 publications in the primary literature. Complete coverage of the entire literature is maintained for budding yeast (Saccharomyces cerevisiae), fission yeast (Schizosaccharomyces pombe) and thale cress (Arabidopsis thaliana), and efforts to expand curation across multiple metazoan species are underway. The BioGRID houses 48 831 human protein interactions that have been curated from 10 247 publications. Current curation drives are focused on particular areas of biology to enable insights into conserved networks and pathways that are relevant to human health. The BioGRID 3.0 web interface contains new search and display features that enable rapid queries across multiple data types and sources. An automated Interaction Management System (IMS) is used to prioritize, coordinate and track curation across international sites and projects. BioGRID provides interaction data to several model organism databases, resources such as Entrez-Gene and other interaction meta-databases. The entire BioGRID 3.0 data collection may be downloaded in multiple file formats, including PSI MI XML. Source code for BioGRID 3.0 is freely available without any restrictions.",Erratum,pro64
pap2450,a68f8e1f7b9f4144049537c766be3faec5b54786,con104,Biometrics and Identity Management,The Exoplanet Orbit Database,"We present a database of well-determined orbital parameters of exoplanets, and their host stars’ properties. This database comprises spectroscopic orbital elements measured for 427 planets orbiting 363 stars from radial velocity and transit measurements as reported in the literature. We have also compiled fundamental transit parameters, stellar parameters, and the method used for the planets discovery. This Exoplanet Orbit Database includes all planets with robust, well measured orbital parameters reported in peer-reviewed articles. The database is available in a searchable, filterable, and sortable form online through the Exoplanets Data Explorer table, and the data can be plotted and explored through the Exoplanet Data Explorer plotter. We use the Data Explorer to generate publication-ready plots, giving three examples of the signatures of exoplanet migration and dynamical evolution: We illustrate the character of the apparent correlation between mass and period in exoplanet orbits, the different selection biases between radial velocity and transit surveys, and that the multiplanet systems show a distinct semimajor-axis distribution from apparently singleton systems.",Erratum,pro104
pap2451,2515e0eb321c558629678b0c34447b40725ed989,con65,IEEE International Conference on Software Engineering and Formal Methods,The BioGRID Interaction Database,,Erratum,pro65
pap2452,f472655c370e4b3209f35a2834e01fb4e77ade9b,con108,International Conference on Information Integration and Web-based Applications & Services,NoSQL databases: a step to database scalability in web environment,"The paper is focused on so called NoSQL databases. In context of cloud computing, architectures and basic features of these databases are studied, particularly their horizontal scalability and concurrency model, that is mostly weaker than ACID transactions in relational SQL-like database systems. Some characteristics like a data model and querying capabilities are discussed in more detail. The paper also contains an overview of some representatives of NoSQL databases.",Conference paper,pro108
pap2453,51f3fbc8b948dd93ed6d1e27e320141d0507603d,con103,IEEE International Conference on Multimedia and Expo,CPPsite: a curated database of cell penetrating peptides,"Delivering drug molecules into the cell is one of the major challenges in the process of drug development. In past, cell penetrating peptides have been successfully used for delivering a wide variety of therapeutic molecules into various types of cells for the treatment of multiple diseases. These peptides have unique ability to gain access to the interior of almost any type of cell. Due to the huge therapeutic applications of CPPs, we have built a comprehensive database ‘CPPsite’, of cell penetrating peptides, where information is compiled from the literature and patents. CPPsite is a manually curated database of experimentally validated 843 CPPs. Each entry provides information of a peptide that includes ID, PubMed ID, peptide name, peptide sequence, chirality, origin, nature of peptide, sub-cellular localization, uptake efficiency, uptake mechanism, hydrophobicity, amino acid frequency and composition, etc. A wide range of user-friendly tools have been incorporated in this database like searching, browsing, analyzing, mapping tools. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition and physicochemical properties of peptides. This database will be very useful for developing models for predicting effective cell penetrating peptides. Database URL: http://crdd.osdd.net/raghava/cppsite/.",Erratum,pro103
pap2454,4a30343f3230dddd96fd6f79547fef9407262dbf,con58,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,A comparison of a graph database and a relational database: a data provenance perspective,"Relational databases have been around for many decades and are the database technology of choice for most traditional data-intensive storage and retrieval applications. Retrievals are usually accomplished using SQL, a declarative query language. Relational database systems are generally efficient unless the data contains many relationships requiring joins of large tables. Recently there has been much interest in data stores that do not use SQL exclusively, the so-called NoSQL movement. Examples are Google's BigTable and Facebook's Cassandra. This paper reports on a comparison of one such NoSQL graph database called Neo4j with a common relational database system, MySQL, for use as the underlying technology in the development of a software system to record and query data provenance information.",Erratum,pro58
pap2455,0b115b72214b501923852e6278b20401fee27f85,con22,Grid Computing Environments,Database Repairing and Consistent Query Answering,"Integrity constraints are semantic conditions that a database should satisfy in order to be an appropriate model of external reality. In practice, and for many reasons, a database may not satisfy those integrity constraints, and for that reason it is said to be inconsistent. However, and most likely, a large portion of the database is still semantically correct, in a sense that has to be made precise. After having provided a formal characterization of consistent data in an inconsistent database, the natural problem emerges of extracting that semantically correct data, as query answers. The consistent data in an inconsistent database is usually characterized as the data that persists across all the database instances that are consistent and minimally differ from the inconsistent instance. Those are the so-called repairs of the database. In particular, the consistent answers to a query posed to the inconsistent database are those answers that can be simultaneously obtained from all the database repairs. As expected, the notion of repair requires an adequate notion of distance that allows for the comparison of databases with respect to how much they differ from the inconsistent instance. On this basis, the minimality condition on repairs can be properly formulated. In this monograph we present and discuss these fundamental concepts, different repair semantics, algorithms for computing consistent answers to queries, and also complexity-theoretic results related to the computation of repairs and doing consistent query answering. Table of Contents: Introduction / The Notions of Repair and Consistent Answer / Tractable CQA and Query Rewriting / Logically Specifying Repairs / Decision Problems in CQA: Complexity and Algorithms / Repairs and Data Cleaning",Erratum,pro22
pap2456,e7ab23d011e5183db78cfea48e303210f6e57e2e,con22,Grid Computing Environments,The serializability of concurrent database updates,"A sequence of interleaved user transactions in a database system may not be ser:ahzable, t e, equivalent to some sequential execution of the individual transactions Using a simple transaction model, it ~s shown that recognizing the transaction histories that are serlahzable is an NP-complete problem. Several efficiently recognizable subclasses of the class of senahzable histories are therefore introduced; most of these subclasses correspond to senahzabdity principles existing in the hterature and used in practice Two new principles that subsume all previously known ones are also proposed Necessary and sufficient conditions are given for a class of histories to be the output of an efficient history scheduler, these conditions imply that there can be no efficient scheduler that outputs all of senahzable histories, and also that all subclasses of senalizable histories studied above have an efficient scheduler Finally, it is shown how these results can be extended to far more general transaction models, to transactions with partly interpreted functions, and to distributed database systems",Erratum,pro22
pap2457,b71ac5caa3a4335c311122cbacada6b17a199060,jou380,Methods in Enzymology,Saccharomyces Genome Database.,,Conference paper,vol380
pap2458,1337f14678d80f22df094a3a9ad09a695d5f86ee,con13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,THE EXTRAGALACTIC DISTANCE DATABASE,"A database can be accessed on the Web at http://edd.ifa.hawaii.edu that was developed to promote access to information related to galaxy distances. The database has three functional components. First, tables from many literature sources have been gathered and enhanced with links through a distinct galaxy naming convention. Second, comparisons of results both at the levels of parameters and of techniques have begun and are continuing, leading to increasing homogeneity and consistency of distance measurements. Third, new material is presented arising from ongoing observational programs at the University of Hawaii 2.2 m telescope, radio telescopes at Green Bank, Arecibo, and Parkes and with the Hubble Space Telescope. This new observational material is made available in tandem with related material drawn from archives and passed through common analysis pipelines.",Erratum,pro13
pap2459,f836da820f53f5bbc890647ecbf00e1031f200c7,jou232,Proteomics,The International Protein Index: An integrated database for proteomics experiments,"Despite the complete determination of the genome sequence of several higher eukaryotes, their proteomes remain relatively poorly defined. Information about proteins identified by different experimental and computational methods is stored in different databases, meaning that no single resource offers full coverage of known and predicted proteins. IPI (the International Protein Index) has been developed to address these issues and offers complete nonredundant data sets representing the human, mouse and rat proteomes, built from the Swiss‐Prot, TrEMBL, Ensembl and RefSeq databases.",Letter,vol232
pap2460,67f8831944ecc502ce74c761bd7ae0d929b5e2f8,con92,Human Language Technology - The Baltic Perspectiv,An Electronic Lexical Database,"""Natural language processing is essential for dealing efficiently with the large quantities of text now available online: fact extraction and summarization, automated indexing and text categorization, and machine translation. Another essential function is helping the user with query formulation through synonym relationships between words and hierarchical and other relationships between concepts. WordNet supports both of these functions and thus deserves careful study by the digital library community."" By Dagobert Soergel ds52@umail.umd.edu",Erratum,pro92
pap2461,dacc0018d0a0c45d93599751e53c91f88fcd45b8,con63,International Colloquium on Theoretical Aspects of Computing,"The InterPro Database, 2003 brings increased coverage and new features","InterPro, an integrated documentation resource of protein families, domains and functional sites, was created in 1999 as a means of amalgamating the major protein signature databases into one comprehensive resource. PROSITE, Pfam, PRINTS, ProDom, SMART and TIGRFAMs have been manually integrated and curated and are available in InterPro for text- and sequence-based searching. The results are provided in a single format that rationalises the results that would be obtained by searching the member databases individually. The latest release of InterPro contains 5629 entries describing 4280 families, 1239 domains, 95 repeats and 15 post-translational modifications. Currently, the combined signatures in InterPro cover more than 74% of all proteins in SWISS-PROT and TrEMBL, an increase of nearly 15% since the inception of InterPro. New features of the database include improved searching capabilities and enhanced graphical user interfaces for visualisation of the data. The database is available via a webserver (http://www.ebi.ac.uk/interpro) and anonymous FTP (ftp://ftp.ebi.ac.uk/pub/databases/interpro).",Erratum,pro63
pap2462,97232c7bba5bef3ac970bf82966a8ea97cb4fa14,con25,IEEE International Parallel and Distributed Processing Symposium,WHO global database on child growth and malnutrition,"ii The designations employed and the presentation of material do not imply the expression of any opinion whatsoever on the part of the World Health Organization concerning the legal status of any country, territory or area, its authorities, its current or former official name or the delimitation of its frontiers or boundaries. We are guilty of many errors and many faults, but our worst crime is abandoning the children, neglecting the foundation of life. Many of the things we need can wait. The child cannot. Right now is the time his bones are being formed, his blood is being made and his senses are being developed. To him we cannot answer "" Tomorrow "". His name is "" Today "". We dedicate this work to the world's children in the hope that it will alert decision-makers to how much remains to be done to ensure children's healthy growth and development. "" "" WHO/NUT/97.4 iv Acknowledgements The Programme of Nutrition appreciates the strong support from numerous individuals, institutions, governments, and nongovernmental and international organizations, without whose continual collaboration this compilation would not have been possible. A special note of gratitude is due to all those who provided standardized information and reanalyses of original data sets to conform to the database requirements. Thanks to such international cooperation in keeping the Global Database up-to-date, the Programme of Nutrition is able to present this vast compilation of data on worldwide patterns and trends in child growth and malnutrition. SD Standard deviation WHO World Health Organization Z-score (or SD-score) The deviation of an individual's value from the median value of a reference population, divided by the standard deviation of the reference population.",Erratum,pro25
pap2463,6ecb2f55ae787363712adf6e7ba6c2812d3f0b32,con102,Annual Haifa Experimental Systems Conference,miRBase: the microRNA sequence database.,,Erratum,pro102
pap2464,f86cefe05621180a48856c9f23a55bf587d7476b,con85,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Principles of Database Systems,,Conference paper,pro85
pap2465,daeabbe2ac3aa90aabf10527090f548fc125e9e6,jou381,Journal of Medicinal Chemistry,The PDBbind database: methodologies and updates.,"We have developed the PDBbind database to provide a comprehensive collection of binding affinities for the protein-ligand complexes in the Protein Data Bank (PDB). This paper gives a full description of the latest version, i.e., version 2003, which is an update to our recently reported work. Out of 23 790 entries in the PDB release No.107 (January 2004), 5897 entries were identified as protein-ligand complexes that meet our definition. Experimentally determined binding affinities (K(d), K(i), and IC(50)) for 1622 of these were retrieved from the references associated with these complexes. A total of 900 complexes were selected to form a ""refined set"", which is of particular value as a standard data set for docking and scoring studies. All of the final data, including binding affinity data, reference citations, and processed structural files, have been incorporated into the PDBbind database accessible on-line at http:// www.pdbbind.org/.",Letter,vol381
pap2466,86dc58305d4cb2766eb2746bbe578e9a58a6f238,con22,Grid Computing Environments,Carbohydrate-active enzymes : an integrated database approach,,Erratum,pro22
pap2467,78b20577738126cb80ed80ad9d1ef96ed813a14a,con68,Experimental Software Engineering Network,System R: relational approach to database management,"System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.
This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.",Erratum,pro68
pap2468,c7a77164a8ede4f536c2779f32aeb6bf98eff766,jou382,Biophysical Journal,The nucleic acid database. A comprehensive relational database of three-dimensional structures of nucleic acids.,,Letter,vol382
pap2469,c9e151ba8e59422320013d64307a17a94e018a98,jou383,Environmental Health,Scopus database: a review,,Article,vol383
pap2470,3bee5cc2d0b6bfcd564158cd0cf1bd311dae68b7,con103,IEEE International Conference on Multimedia and Expo,PHOSIDA 2011: the posttranslational modification database,"The primary purpose of PHOSIDA (http://www.phosida.com) is to manage posttranslational modification sites of various species ranging from bacteria to human. Since its last report, PHOSIDA has grown significantly in size and evolved in scope. It comprises more than 80 000 phosphorylated, N-glycosylated or acetylated sites from nine different species. All sites are obtained from high-resolution mass spectrometric data using the same stringent quality criteria. One of the main distinguishing features of PHOSIDA is the provision of a wide range of analysis tools. PHOSIDA is comprised of three main components: the database environment, the prediction platform and the toolkit section. The database environment integrates and combines high-resolution proteomic data with multiple annotations. High-accuracy species-specific phosphorylation and acetylation site predictors, trained on the modification sites contained in PHOSIDA, allow the in silico determination of modified sites on any protein on the basis of the primary sequence. The toolkit section contains methods that search for sequence motif matches or identify de novo consensus, sequences from large scale data sets.",Erratum,pro103
pap2471,d4fe4d4f62b8f14a0475bc8f8028a0d3565650f9,con34,International Conference on Agile Software Development,The Harmonized World Soil Database,"For more than 30 years the FAO/Unesco Soil map of the World has been the only harmonized source of global soil information. Recent updates and release of new soil information in all regions of the globe was an incentive to tackle the harmonization and integration of the new soil data. The task was undertaken by a consortium of institutes and organizations and resulted in a product with 30 arc second resolution that includes for each soil unit estimates for fifteen top- and subsoil properties. The data come with a viewer, are GIS compatible and are freely available on-line.",Erratum,pro34
pap2472,ecb74f4c908455f446ac455437f81e0e9dd9a319,con93,International Conference on Computational Logic,Introduction to Temporal Database Research,"A wide range of database applications manage time-varying data. In contrast, existing database technology provides little support for managing such data. The research area of temporal databases aims to change this state of affairs by characterizing the semantics of temporal data and providing expressive and efficient ways to model, store, and query temporal data. This chapter offers a brief introduction to temporal database research. It concisely introduces fundamental temporal database concepts, surveys state-of-the-art solutions to challenging aspects of temporal data management, and also offers a look into the future of temporal database research.",Erratum,pro93
pap2473,f1af714b92372c8e606485a3982eab2f16772ad8,con103,IEEE International Conference on Multimedia and Expo,The MUG facial expression database,This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the “emotion prototypes” as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.,Erratum,pro103
pap2474,ef47742e72bd64fb1ae5359cd6d5dd6dfad34dc8,con97,ACM SIGMOD Conference,Implementation techniques for main memory database systems,"With the availability of very large, relatively inexpensive main memories, it is becoming possible keep large databases resident in main memory In this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory We evaluate AVL vs B+-tree access methods for main memory databases, hash-based query processing strategies vs sort-merge, and study recovery issues when most or all of the database fits in main memory As expected, B+-trees are the preferred storage mechanism unless more than 80--90% of the database fits in main memory A somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations",Article,pro97
pap2475,d5ae5a965ac5ad79128082d7d1edf9d7ab1d840b,jou230,Plant Physiology,"ARAMEMNON, a Novel Database for Arabidopsis Integral Membrane Proteins1","A specialized database (DB) for Arabidopsis membrane proteins, ARAMEMNON, was designed that facilitates the interpretation of gene and protein sequence data by integrating features that are presently only available from individual sources. Using several publicly available prediction programs, putative integral membrane proteins were identified among the approximately 25,500 proteins in the Arabidopsis genome DBs. By averaging the predictions from seven programs, approximately 6,500 proteins were classified as transmembrane (TM) candidate proteins. Some 1,800 of these contain at least four TM spans and are possibly linked to transport functions. The ARAMEMNON DB enables direct comparison of the predictions of seven different TM span computation programs and the predictions of subcellular localization by eight signal peptide recognition programs. A special function displays the proteins related to the query and dynamically generates a protein family structure. As a first set of proteins from other organisms, all of the approximately 700 putative membrane proteins were extracted from the genome of the cyanobacterium Synechocystis sp. and incorporated in the ARAMEMNON DB. The ARAMEMNON DB is accessible at the URL http://aramemnon.botanik.uni-koeln.de.",Letter,vol230
pap2476,8bbb5e29bc76675ae3b73185c17e9077742742ee,jou43,Social Science Research Network,A Historical Public Debt Database,"This paper describes the compilation of the first truly comprehensive database on gross government debt-to-GDP ratios, covering nearly the entire IMF membership (174 countries) and spanning an exceptionally long time period. The database was constructed by bringing together a number of other datasets and information from original sources. For the most recent years, the data are linked to the IMF World Economic Outlook (WEO) database to facilitate regular updates. The paper discusses the evolution of debt-to-GDP ratios across country groups for several decades, episodes of debt spikes and reversals, and a pattern of negative correlation between debt and growth.",Letter,vol43
pap2477,e9a1699735aff36cdd1fa385165426dd18b0d9ec,con14,Hawaii International Conference on System Sciences,New developments in the InterPro database,"InterPro is an integrated resource for protein families, domains and functional sites, which integrates the following protein signature databases: PROSITE, PRINTS, ProDom, Pfam, SMART, TIGRFAMs, PIRSF, SUPERFAMILY, Gene3D and PANTHER. The latter two new member databases have been integrated since the last publication in this journal. There have been several new developments in InterPro, including an additional reading field, new database links, extensions to the web interface and additional match XML files. InterPro has always provided matches to UniProtKB proteins on the website and in the match XML file on the FTP site. Additional matches to proteins in UniParc (UniProt archive) are now available for download in the new match XML files only. The latest InterPro release (13.0) contains more than 13 000 entries, covering over 78% of all proteins in UniProtKB. The database is available for text- and sequence-based searches via a webserver (), and for download by anonymous FTP (). The InterProScan search tool is now also available via a web service at .",Erratum,pro14
pap2478,b069e125442995787119db3bfa71dff5d965f3aa,con23,International Conference on Open and Big Data,MCYT baseline corpus: a bimodal biometric database,"The current need for large multimodal databases to evaluate automatic biometric recognition systems has motivated the development of the MCYT bimodal database. The main purpose has been to consider a large scale population, with statistical significance, in a real multimodal procedure, and including several sources of variability that can be found in real environments. The acquisition process, contents and availability of the single-session baseline corpus are fully described. Some experiments showing consistency of data through the different acquisition sites and assessing data quality are also presented.",Erratum,pro23
pap2479,de31aa8e914c60189a425e174af223967e2722cc,con109,International Society for Music Information Retrieval Conference,"RWC Music Database: Popular, Classical and Jazz Music Databases","paper describes the design policy and specifications of the RWC Music Database , a music database (DB) that is available to researchers for common use and research purposes. Various com- monly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Mu- sic Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database(15 pieces), Classical Music Database(50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.",Article,pro109
pap2480,a5c4c2b5719eff7160334259b018809dc9c4ab4b,jou103,Science,Exhaustive matching of the entire protein sequence database.,"The entire protein sequence database has been exhaustively matched. Definitive mutation matrices and models for scoring gaps were obtained from the matching and used to organize the sequence database as sets of evolutionarily connected components. The methods developed are general and can be used to manage sequence data generated by major genome sequencing projects. The alignments made possible by the exhaustive matching are the starting point for successful de novo prediction of the folded structures of proteins, for reconstructing sequences of ancient proteins and metabolisms in ancient organisms, and for obtaining new perspectives in structural biochemistry.",Article,vol103
pap2481,4aa6aaeb14e5f881100c97cd5d06306f16ab80d0,con67,IEEE International Software Metrics Symposium,Current Status of the Digital Database for Screening Mammography,,Erratum,pro67
pap2482,5160fb34a6719bbf8d60743d0d27db0ed5df3d2a,con110,Very Large Data Bases Conference,Foundations of Preferences in Database Systems,,Article,pro110
pap2483,88b98d7b20ede342fe471b2889ace70d082e4db7,con38,International Symposium on Empirical Software Engineering and Measurement,Query by humming: musical information retrieval in an audio database,"The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difficulties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented.",Erratum,pro38
pap2484,64acb315b6129061c62bfabef2ac06d1a6fff95b,con78,Neural Information Processing Systems,Human protein reference database as a discovery resource for proteomics,"The rapid pace at which genomic and proteomic data is being generated necessitates the development of tools and resources for managing data that allow integration of information from disparate sources. The Human Protein Reference Database (http://www.hprd.org) is a web-based resource based on open source technologies for protein information about several aspects of human proteins including protein-protein interactions, post-translational modifications, enzyme-substrate relationships and disease associations. This information was derived manually by a critical reading of the published literature by expert biologists and through bioinformatics analyses of the protein sequence. This database will assist in biomedical discoveries by serving as a resource of genomic and proteomic information and providing an integrated view of sequence, structure, function and protein networks in health and disease.",Erratum,pro78
pap2485,a6089c7eca1d77dc199e462763ab13f99f85c663,con74,IEEE International Conference on Information Reuse and Integration,Global wood density database,,Erratum,pro74
pap2486,584af7d56a7247b9ad5a399d99b1eb0a2c8f6586,con45,International Conference on Global Software Engineering,Journal of Database Management,"INTRODUCTION Background Advances in wireless technology increase the number of mobile device users and give pace to the rapid development of e-commerce conducted with these devices. The new type of e-commerce transactions, conducted through mobile devices using wireless telecommunications networks and other wired e-commerce technologies, is called mobile commerce (increasingly known as mobile e-commerce or m-commerce). Due to the special characteristics and constraints of mobile devices and the wireless network, the emerging mobile commerce operates in an environment very different from e-commerce conducted over the wired Internet. In terms of business potential, mobile commerce promises many more alluring market opportunities than traditional e-commerce because of its inherent characteristics such as ubiquity, per-sonalization, flexibility, and dissemination. Mobile commerce will likely emerge as a major focus of the business world and telecommunication industry in the immediate future. For example, according to Guy Singh (2000), the global mobile commerce market is expected to be worth a staggering US$200 billion by 2004. The marriage of mobile devices and the Internet is, however, filled with challenges as well as opportunities. This paper presents an overview of mobile commerce development by looking at the enabling technologies, the impact of mobile commerce on business models, and the",Erratum,pro45
pap2487,be9124db35e5f451f508e095fdc25727c067a9ef,con111,International Conference on Image Analysis and Processing,UBIRIS: A Noisy Iris Image Database,,Article,pro111
pap2488,c17ee327e563536f8adaf214eb6d3bde33b73dd6,jou171,Computer,Chabot: Retrieval from a Relational Database of Images,"Selecting from a large, expanding collection of images requires carefully chosen search criteria. We present an approach that integrates a relational database retrieval system with a color analysis technique. The Chabot project was initiated at our university to study storage and retrieval of a vast collection of digitized images. These images are from the State of California Department of Water Resources. The goal was to integrate a relational database retrieval system with content analysis techniques that would give our querying system a better method for handling images. Our simple color analysis method, if used in conjunction with other search criteria, improves our ability to retrieve images efficiently. The best result is obtained when text-based search criteria are combined with content-based criteria and when a coarse granularity is used for content analysis. >",Conference paper,vol171
pap2489,89e3fae32bf72b61834fc2ae60b1f8508e714e38,con64,British Computer Society Conference on Human-Computer Interaction,World Ocean Database,"The U.S. National Oceanic and Atmospheric Administration's (NOAA) World Ocean Database 2009, released in November as an update to the 2005 version, provides about 9.1 million temperature profiles and 3.5 million salinity reports, with some information dating as far back as 1800. The updated database includes scientific information about the oceans that can be sorted in various ways, including geographically or by year. 
 
“There is now more data about the global oceans than ever before,” according to Sydney Levitus, director of the World Data Center for Oceanography, part of NOAA's National Oceanographic Data Center. “Previous databases have shown the world ocean has warmed during the last 53 years, and it's crucial we have reliable, accurate monitoring of our oceans into the future,” he said. The database is a part of the Integrated Ocean Observing System and the Global Earth Observation System of Systems.",Erratum,pro64
pap2490,d4a8e93f004c86267eead89edecbd332518dbf21,con14,Hawaii International Conference on System Sciences,Database description with SDM: a semantic database model,"SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it.
SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system.",Erratum,pro14
pap2491,666dfa8258914bf17970b20d2f7247c7c1468307,con99,North American Chapter of the Association for Computational Linguistics,On the Desirability of Acyclic Database Schemes,"A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.",Erratum,pro99
pap2492,599cc88971d2f5b375ff23b6342f17855e01791c,con20,ACM Conference on Economics and Computation,The CMU Motion of Body (MoBo) Database,"In March 2001 we started to collect the CMU Motion of Body (MoBo) database. To date the database contains 25 individuals walking on a treadmill in the CMU 3D room. The subjects perform four different walk patterns: slow walk, fast walk, incline walk and walking with a ball. All subjects are captured using six high resolution color cameras distributed evenly around the treadmill. In this technical report we describe the capture setup, the collection procedure and the organization of the database.",Erratum,pro20
pap2493,2fe0dea4a9a243ebeaae37fec9cbbaa28b5f72a7,jou384,Applied Optics,The HITRAN database: 1986 edition.,"A description and summary of the latest edition of the AFGL HITRAN molecular absorption parameters database are presented. This new database combines the information for the seven principal atmospheric absorbers and twenty-one additional molecular species previously contained on the AFGL atmospheric absorption line parameter compilation and on the trace gas compilation. In addition to updating the parameters on earlier editions of the compilation, new parameters have been added to this edition such as the self-broadened halfwidth, the temperature dependence of the air-broadened halfwidth, and the transition probability. The database contains 348043 entries between 0 and 17,900 cm(-1). A FORTRAN program is now furnished to allow rapid access to the molecular transitions and for the creation of customized output. A separate file of molecular cross sections of eleven heavy molecular species, applicable for qualitative simulation of transmission and emission in the atmosphere, has also been provided.",Conference paper,vol384
pap2494,74e10ff37b568e76c5166ce8b0eddf2abfdcbac9,con92,Human Language Technology - The Baltic Perspectiv,A common database approach for OLTP and OLAP using an in-memory column database,"When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.",Erratum,pro92
pap2495,5737a1f6fd8d928b88726ada916d7874afdfe0d7,con110,Very Large Data Bases Conference,Approximate String Joins in a Database (Almost) for Free,"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation.",Article,pro110
pap2496,a0d251f893e043175d0a6d0e12e6e166ff523255,con6,Annual Conference on Genetic and Evolutionary Computation,Documentation Mocap Database HDM05,"Preface In the past two decades, motion capture (mocap) systems have been developed that allow to track and record human motions at high spatial and temporal resolutions. The resulting motion capture data is used to analyze human motions in fields such as sports sciences and biometrics (person identification), and to synthesize realistic motion sequences in data-driven computer animation. Such applications require efficient methods and tools for the automatic analysis, synthesis and classification of motion capture data, which constitutes an active research area with many yet unsolved problems. Even though there is a rapidly growing corpus of motion capture data, the academic research community still lacks publicly available motion data, as supplied by [4], that can be freely used for systematic research on motion analysis, synthesis, and classification. Furthermore, a common dataset of annotated and well-documented motion capture data would be extremely valuable to the research community in view of an objective comparison and evaluation of the achieved research results. It is the objective of our motion capture database HDM05 1 to supply free motion capture data for research purposes. HDM05 contains more than tree hours of systematically recorded and well-documented motion capture data in the C3D as well as in the ASF/AMC data format. Furthermore, HDM05 contains for each of roughly 70 motion classes 10 to 50 realizations executed by various actors amounting to roughly 1, 500 motion clips. In this documentation, we give a detailed description of our mocap database HDM05. In Sect. 1, we provide some general information on motion capture data including references to various application fields. A detailed description of the database structure of HDM05 as well as of the content of each mocap file can be found in Sect. 2. We also provide several MATLAB tools comprising a parser for ASF/AMC and C3D as well as visualization, renaming and cutting tools, which are described in Sect. 3. Finally, Sect. 4 summarizes some facts on the mocap file formats ASF/AMC and C3D as used in our database. We appreciate any comments and suggestions for improvement. 1 The motion capture data has been recorded at the Hochschule der Medien (HDM) in the year 2005 under the supervision of Bernhard Eberhardt.",Erratum,pro6
pap2497,54a54371577545b529a8ab53b421d14a58b33ba6,jou187,Nature reviews genetics,Genetic association database,,Letter,vol187
pap2498,2c0ed016d2bc69854523aab6bcf86342692e31e2,con77,International Conference on Artificial Neural Networks,The RDP (Ribosomal Database Project) continues,"The Ribosomal Database Project (RDP-II), previously described by Maidak et al., continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 7.1 (September 17, 1999) included more than 10 700 small subunit rRNA sequences. More than 850 type strain sequences were identified and added to the prokaryotic alignment, bringing the total number of type sequences to 3324 representing 2460 different species. Availability of an RDP-II mirror site in Japan is also near completion. RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/ ). Analysis services include rRNA probe checking, approx-i-mate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment length polymorphism (T-RFLP) experiments.",Erratum,pro77
pap2499,0b6fb2fceea248bcf8ad8c05c7d56abda763e2db,con27,International Conference on Contemporary Computing,PMRD: plant microRNA database,"MicroRNAs (miRNA) are ∼21 nucleotide-long non-coding small RNAs, which function as post-transcriptional regulators in eukaryotes. miRNAs play essential roles in regulating plant growth and development. In recent years, research into the mechanism and consequences of miRNA action has made great progress. With whole genome sequence available in such plants as Arabidopsis thaliana, Oryza sativa, Populus trichocarpa, Glycine max, etc., it is desirable to develop a plant miRNA database through the integration of large amounts of information about publicly deposited miRNA data. The plant miRNA database (PMRD) integrates available plant miRNA data deposited in public databases, gleaned from the recent literature, and data generated in-house. This database contains sequence information, secondary structure, target genes, expression profiles and a genome browser. In total, there are 8433 miRNAs collected from 121 plant species in PMRD, including model plants and major crops such as Arabidopsis, rice, wheat, soybean, maize, sorghum, barley, etc. For Arabidopsis, rice, poplar, soybean, cotton, medicago and maize, we included the possible target genes for each miRNA with a predicted interaction site in the database. Furthermore, we provided miRNA expression profiles in the PMRD, including our local rice oxidative stress related microarray data (LC Sciences miRPlants_10.1) and the recently published microarray data for poplar, Arabidopsis, tomato, maize and rice. The PMRD database was constructed by open source technology utilizing a user-friendly web interface, and multiple search tools. The PMRD is freely available at http://bioinformatics.cau.edu.cn/PMRD. We expect PMRD to be a useful tool for scientists in the miRNA field in order to study the function of miRNAs and their target genes, especially in model plants and major crops.",Erratum,pro27
